///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2022, 2023, Oracle and/or its affiliates.
    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.

///////////////////////////////////////////////////////////////////////////////
= Using Network Policies

== Using Network Policies

This example covers running the Coherence Operator and Coherence clusters in Kubernetes with network policies.
In Kubernetes, a https://kubernetes.io/docs/concepts/services-networking/network-policies/[Network Policy]
is an application-centric construct which allow you to specify how a pod is allowed to communicate with various network
"entities" (we use the word "entity" here to avoid overloading the more common terms such as "endpoints" and "services",
which have specific Kubernetes connotations) over the network.

[NOTE]
====
Network policies in Kubernetes are easy to get wrong if you are not careful.
In this case a policy will either block traffic it should not, in which case your application will not work,
or it will let traffic through it should block, which will be an invisible security hole.

It is obviously important to test your policies, but Kubernetes offers next to zero visibility into what the policies
are actually doing, as it is typically the network CNI extensions that are providing the policy implementation
and each of these may work in a different way.
====

=== Introduction

Kubernetes network policies specify the access permissions for groups of pods, similar to security groups in the
cloud are used to control access to VM instances and similar to firewalls.
The default behaviour of a Kubernetes cluster is to allow all Pods to freely talk to each other.
Whilst this sounds insecure, originally Kubernetes was designed to orchestrate services that communicated with each other,
it was only later that network policies were added.

A network policy is applied to a Kubernetes namespace and controls ingress into and egress out of Pods in that namespace.
The ports specified in a `NetworkPolicy` are the ports exposed by the `Pods`, they are not any ports that may be exposed by
any `Service` that exposes the `Pod` ports. For example, if a `Pod` exposed port 8080 and a `Service` exposing the `Pod`
mapped port 80 in the `Service` to port `8080` in the `Pod`, the `NetworkPolicy` ingress rule would be for the `Pod` port 8080.

Network polices would typically end up being dictated by corporate security standards where different companies may
apply stricter or looser rules than others.
The examples in this document start from the premise that everything will be blocked by a "deny all" policy and then opened up as needed.
This is the most secure use of network policies, and hence the examples can easily be tweaked if looser rules are applied.

This example has the following sections:

* <<deny,Deny All Policy>> - denying all ingress and egress
* <<dns,Allow DNS>> - almost every use case will require egress to DNS
* <<operator,Coherence Operator Policies>> - the network policies required to run the Coherence Operator
** <<k8sapi,Kubernetes API Server>> - allow the Operator egress to the Kubernetes API server
** <<cluster-access,Coherence Clusters Pods>> - allow the Operator egress to the Coherence cluster Pods
** <<webhook,Web Hooks>> - allow ingress to the Operator's web hook port
* <<coherence,Coherence Cluster Policies>> - the network policies required to run Coherence clusters
** <<inter-cluster,Inter-Cluster Access>> - allow Coherence cluster Pods to communicate
** <<cluster-to-operator,Coherence Operator>> - allow Coherence cluster Pods to communicate with the Operator
** <<client,Clients>> - allows access by Extend and gRPC clients
** <<metrics,Metrics>> - allow Coherence cluster member metrics to be scraped
* <<testing,Testing Connectivity>> - using the Operator's network connectivity test utility to test policies


[#deny]
==== Deny All Policy

Kubernetes does not have a “deny all” policy, but this can be achieved with a regular network policy that specifies
a `policyTypes` of both 'Ingress` and `Egress` but omits any definitions.
A wild-card `podSelector: {}` applies the policy to all Pods in the namespace.

[source,yaml]
.manifests/deny-all.yaml
----
include::manifests/deny-all.yaml[]
----

The policy above can be installed into the `coherence` namespace with the following command:

[source,bash]
----
kubectl -n coherence apply -f manifests/deny-all.yaml
----

After installing the `deny-all` policy, any `Pod` in the `coherence` namespace will not be allowed either ingress,
nor egress. Very secure, but probably impractical for almost all use cases. After applying the `deny-all` policy more polices can be added to gradually open up the required access to run the Coherence Operator and Coherence clusters.

[#dns]
==== Allow DNS

When enforcing egress, such as with the `deny-all` policy above, it is important to remember that virtually every Pod needs
to communicate with other Pods or Services, and will therefore need to access DNS.

The policy below allows all Pods (using `podSelector: {}`) egress to both TCP and UDP on port 53 in all namespaces.

[source,yaml]
.manifests/allow-dns.yaml
----
include::manifests/allow-dns.yaml[]
----

If allowing DNS egress to all namespaces is overly permissive, DNS could be further restricted to just the `kube-system`
namespace, therefore restricting DNS lookups to only Kubernetes internal DNS.
Kubernetes applies the `kubernetes.io/metadata.name` label to namespaces, and sets its value to the namespace name,
so this can be used in label matchers.

With the policy below, Pods will be able to use internal Kubernetes DNS only.

[source,yaml]
.manifests/allow-dns-kube-system.yaml
----
include::manifests/allow-dns-kube-system.yaml[]
----

The policy above can be installed into the `coherence` namespace with the following command:

[source,bash]
----
kubectl -n coherence apply -f manifests/allow-dns-kube-system.yaml
----

[TIP]
====
Some documentation regarding allowing DNS with Kubernetes network policies only shows opening up UDP connections.
During our testing with network policies, we discovered that with only UDP allowed any lookup for a fully qualified
name would fail. For example `nslookup my-service.my-namespace.svc` would work, but the fully qualified
`nslookup my-service.my-namespace.svc.cluster.local` would not. Adding TCP to the DNS policy allowed DNS lookups with
`.cluster.local` to also work.

Neither the Coherence Operator, nor Coherence itself use a fully qualified service name for a DNS lookup.
It appears that Java's `InetAddress.findAllByName()` method still works only with UDP, albeit extremely slowly.
By default, the service name used for the Coherence WKA setting uses just the `.svc` suffix.
====

[#operator]
=== Coherence Operator Policies

Assuming the `coherence` namespace exists, and the `deny-all` and `allow-dns` policies described above have been applied,
if the Coherence Operator is installed, it wil fail to start as it has no access to endpoints it needs to operate.
The following sections will add network polices to allow the Coherence Operator to access Kubernetes services and
Pods it requires.

[#k8sapi]
==== Access the Kubernetes API Server

The Coherence Operator uses Kubernetes APIs to manage various resources in the Kubernetes cluster.
For this to work, the Operator Pod must be allowed egress to the Kubernetes API server.
Configuring access to the API server is not as straight forward as other network policies.
The reason for this is that there is no Pod available with labels that can be used in the configuration,
instead, the IP address of the API server itself must be used.

There are various methods to find the IP address of the API server.
The exact method required may vary depending on the type of Kubernetes cluster being used, for example a simple
development cluster running in KinD on a laptop may differ from a cluster running in a cloud provider's infrastructure.

The common way to find the API server's IP address is to use `kubectl cluster-info` as follows:

[source,bash]
----
$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
----

In the above case the IP address of the API server would be `192.168.99.100` and the port is `8443`.

In a simple KinD development cluster, the API server IP address can be obtained using `kubectl` as shown below:

[source,bash]
----
$ kubectl -n default get endpoints kubernetes -o json
{
    "apiVersion": "v1",
    "kind": "Endpoints",
    "metadata": {
        "creationTimestamp": "2023-02-08T10:31:26Z",
        "labels": {
            "endpointslice.kubernetes.io/skip-mirror": "true"
        },
        "name": "kubernetes",
        "namespace": "default",
        "resourceVersion": "196",
        "uid": "68b0a7de-c0db-4524-a1a2-9d29eb137f28"
    },
    "subsets": [
        {
            "addresses": [
                {
                    "ip": "192.168.49.2"
                }
            ],
            "ports": [
                {
                    "name": "https",
                    "port": 8443,
                    "protocol": "TCP"
                }
            ]
        }
    ]
}
----

In the above case the IP address of the API server would be `192.168.49.2` and the port is `8443`.

The IP address displayed for the API server can then be used in the network policy.
The policy shown below allows Pods with the `app.kubernetes.io/name: coherence-operator` label (which the Operator has)
egress access to the API server.

[source,yaml]
.manifests/allow-k8s-api-server.yaml
----
include::manifests/allow-k8s-api-server.yaml[]
----

The `allow-k8s-api-server.yaml` policy can be installed into the `coherence` namespace to allow the Operator to communicate with the API server.
[source,bash]
----
kubectl -n coherence apply -f manifests/allow-k8s-api-server.yaml
----

With the `allow-k8s-api-server.yaml` policy applied, the Coherence Operator should now start correctly and its Pods should reach the "ready" state.

[#cluster-access]
==== Ingress From and Egress Into Coherence Cluster Member Pods

When a Coherence cluster is deployed, on start-up of a Pod the cluster member will connect to the Operator's REST endpoint to query the site name and rack name, based on the Node the Coherence member is running on. To allow this to happen the Operator needs to be configured with the relevant ingress policy.

The `coherence-operator-rest-ingress` policy applies to the Operator Pod, as it has a `podSelector` label of `app.kubernetes.io/name: coherence-operator`, which is a label applied to the Operator Pod. The policy allows any Pod with the label `coherenceComponent: coherencePod` ingress
into the `operator` REST port. When the Operator creates a Coherence cluster, it applies the label `coherenceComponent: coherencePod` to all the Coherence cluster Pods.
The policy below allows access from all namespaces using `namespaceSelector: { }` but it could be tightened up to specific namespaces if required.

[source,yaml]
.manifests/allow-operator-rest-ingress.yaml
----
include::manifests/allow-operator-rest-ingress.yaml[]
----

During operations such as scaling and shutting down of a Coherence cluster, the Operator needs to connect to the health endpoint of the Coherence cluster Pods.

The `coherence-operator-cluster-member-egress` policy below applies to the Operator Pod, as it has a `podSelector` label of `app.kubernetes.io/name: coherence-operator`, which is a label applied to the Operator Pod. The policy allows egress to the `health` port in any Pod with the label `coherenceComponent: coherencePod`. When the Operator creates a Coherence cluster, it applies the label `coherenceComponent: coherencePod` to all the Coherence cluster Pods.
The policy below allows egress to Coherence Pods in all namespaces using `namespaceSelector: { }` but it could be tightened up to specific namespaces if required.

[source,yaml]
.manifests/allow-operator-cluster-member-egress.yaml
----
include::manifests/allow-operator-cluster-member-egress.yaml[]
----

The two policies can be applied to the `coherence` namespace.
[source,bash]
----
kubectl -n coherence apply -f manifests/allow-operator-rest-ingress.yaml
kubectl -n coherence apply -f manifests/allow-operator-cluster-member-egress.yaml
----

[#webhook]
==== Webhook Ingress

With all the above policies in place, the Operator is able to work correctly, but if a `Coherence` resource is now created
Kubernetes will be unable to call the Operator's webhook without the correct ingress policy.

The following example demonstrates this. Assume there is a minimal`Coherence` yaml file named `minimal.yaml`
that will create a single member Coherence cluster.

[source,yaml]
.minimal.yaml
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  replicas: 1
----

If `minimal.yaml` is applied using `kubectl` with a small timeout of 10 seconds, the creation of the resource will
fail due to Kubernetes not having access to the Coherence Operator webhook.

[source,bash]
----
$ kubectl apply --timeout=10s -f minimal.yaml
Error from server (InternalError): error when creating "minimal.yaml": Internal error occurred: failed calling webhook "coherence.oracle.com": failed to call webhook: Post "https://coherence-operator-webhook.operator-test.svc:443/mutate-coherence-oracle-com-v1-coherence?timeout=10s": context deadline exceeded
----

The simplest solution is to allow ingress from any IP address to the webhook on port, with a policy like that shown below.
This policy uses and empty `from: []` attribute, which allows access from anywhere to the `webhook-server` port in the Pod.

[source,yaml]
.manifests/allow-webhook-ingress-from-all.yaml
----
include::manifests/allow-webhook-ingress-from-all.yaml[]
----

Allowing access to the webhook from anywhere is not very secure, so a more restrictive `from` attribute could be used
to limit access to the IP address (or addresses) of the Kubernetes API server.
As with the API server policy above, the trick here is knowing the API server addresses to use.

The policy below only allows access from specific addresses:

[source,yaml]
.manifests/allow-webhook-ingress-from-all.yaml
----
include::manifests/allow-webhook-ingress-from-api-server.yaml[]
----

[#coherence]
=== Coherence Cluster Member Policies

Once the policies are in place to allow the Coherence Operator to work, the policies to allow Coherence clusters to run can be put in place.
The exact set of policies requires will vary depending on the Coherence functionality being used.
If Coherence is embedded in another application, such as a web-server, then additional policies may also be needed to allow ingress to other endpoints.
Conversely, if the Coherence application needs access to other services, for example a database, then additional egress policies may need to be created.

This example is only going to cover Coherence use cases, but it should be simple enough to apply the same techniques to policies for other applications.

[#inter-cluster]
==== Access Other Cluster Members

All Pods in a Coherence cluster must be able to talk to each other (otherwise they wouldn't be a cluster).
This means that there needs to be ingress and egress policies to allow this.

*Cluster port*: The default cluster port is 7574, and there is almost never any need to change this, especially in a containerised environment where there is little chance of port conflicts.

*Unicast ports*: Unicast uses TMB (default) and UDP. Each cluster member listens on one UDP and one TCP port and both ports need to be opened in the network policy. The default behaviour of Coherence is for the unicast ports to be automatically assigned from the operating system's available ephemeral port range. When securing Coherence with network policies, the use of ephemeral ports will not work, so a range of ports can be specified for coherence to operate within. The Coherence Operator sets values for both unicast ports so that ephemeral ports will not be used. The default values are `7575` and `7576`.

The two unicast ports can be changed in the `Coherence` spec by setting the `spec.coherence.localPort` field,
and the `spec.coherence.localPortAdjust` field for example:

[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    localPort: 9000
    localPortAdjust: 9001
----

Alternatively the values can also be configured using environment variables

[source,yaml]
----
env:
  - name: COHERENCE_LOCALPORT
    value: "9000"
  - name: COHERENCE_LOCALPORT_ADJUST
    value: "9001"
----

*Echo port `7`*: The default TCP port of the IpMonitor component that is used for detecting hardware failure of cluster members. Coherence doesn't bind to this port, it only tries to connect to it as a means of pinging remote machines, or in this case Pods.

The Coherence Operator applies the `coherenceComponent: coherencePod` label to all Coherence Pods, so this can be used in the network policy `podSelector`, to apply the policy to only the Coherence Pods.

The policy below works with the default ports configured by the Operator.

[source,yaml]
.manifests/allow-cluster-member-access.yaml
----
include::manifests/allow-cluster-member-access.yaml[]
----

If the Coherence local port and local port adjust values are changed, then the policy would need to be amended.
For example, if `COHERENCE_LOCALPORT=9000` and `COHERENCE_LOCALPORT_ADJUST=9100`

[source,yaml]
.manifests/allow-cluster-member-access-non-default.yaml
----
include::manifests/allow-cluster-member-access-non-default.yaml[]
----

Both of the policies above should be applied to the namespace where the Coherence cluster will be deployed.
With the two policies above in place, the Coherence Pods will be able to communicate.

[#cluster-to-operator]
==== Egress to and Ingress From the Coherence Operator

When a Coherence Pod starts Coherence calls back to the Operator to obtain the site name and rack name based on the Node the Pod is scheduled onto.
For this to work, there needs to be an egress policy to allow Coherence Pods to access the Operator.

During certain operations the Operator needs to call the Coherence members health endpoint to check health and status.
For this to work there needs to be an ingress policy to allow the Operator access to the health endpoint in the Coherence Pods

The policy below applies to Pods with the `coherenceComponent: coherencePod` label, which will match Coherence cluster member Pods.
The policy allows ingress from the Operator to the Coherence Pod health port from namespace `coherence`
using the namespace selector label `kubernetes.io/metadata.name: coherence`
and Pod selector label `app.kubernetes.io/name: coherence-operator`
The policy allows egress from the Coherence pods to the Operator's REST server `operator` port.

[source,yaml]
.manifests/allow-cluster-member-operator-access.yaml
----
include::manifests/allow-cluster-member-operator-access.yaml[]
----

If the Operator is not running in the `coherence` namespace then the namespace match label can be changed to the required value.
The policy above should be applied to the namespace where the Coherence cluster will be deployed.

[#client]
==== Client Access (Coherence*Extend and gRPC)

A typical Coherence cluster does not run in isolation but as part of a larger application.
If the application has other Pods that are Coherence clients, then they will need access to the Coherence cluster.
This would usually mean creating ingress and egress policies for the Coherence Extend port and gRPC port,
depending on which Coherence APIs are being used.

Instead of using actual port numbers, a `NetworkPolicy` can be made more flexible by using port names.
When ports are defined in a container spec of a Pod, they are usually named.
By using the names of the ports in the `NetworkPolicy` instead of port numbers, the real port numbers can be changed without
affecting the network policy.

===== Coherence Extend Access

If Coherence Extend is being used, then first the Extend Proxy must be configured to use a fixed port.
The default behaviour of Coherence is to bind the Extend proxy to an ephemeral port and clients use the Coherence
NameService to look up the port to use.

When using the default Coherence images, for example `ghcr.io/oracle/coherence-ce:22.06` the Extend proxy is already
configured to run on a fixed port `20000`. When using this image, or any image that uses the default Coherence cache
configuration file, this port can be changed by setting the `COHERENCE_EXTEND_PORT` environment variable.

When using the Coherence Concurrent extensions over Extend, the Concurrent Extend proxy also needs to be configured with a fixed port.
When using the default Coherence images, for example `ghcr.io/oracle/coherence-ce:22.06` the Concurrent Extend proxy is already
configured to run on a fixed port `20001`. When using this image, or any image that uses the default Coherence cache
configuration file, this port can be changed by setting the `COHERENCE_CONCURRENT_EXTEND_PORT` environment variable.

For the examples below, a `Coherence` deployment has the following configuration.
This will expose Extend on a port named `extend` with a port number of `20000`, and a port named `extend-atomics`
with a port number of `20001`. The polices described below will then use the port names,
so if required the port number could be changed and the policies would still work.

[source,yaml]
.coherence-cluster.yaml
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  ports:
    - name: extend
      port: 20000
    - name: extend-atomics
      port: 20001
----

The ingress policy below will work with the default Coherence image and allow ingress into the Coherence Pods
to both the default Extend port and Coherence Concurrent Extend port.
The policy allows ingress from Pods that have the `coherence.oracle.com/extendClient: true` label, from any namespace.
It could be tightened further by using a more specific namespace selector.

[source,yaml]
.manifests/allow-extend-ingress.yaml
----
include::manifests/allow-extend-ingress.yaml[]
----

The policy above should be applied to the namespace where the Coherence cluster is running.

Instead of using fixed port numbers in the

The egress policy below will work with the default Coherence image and allow egress from Pods with the
`coherence.oracle.com/extendClient: true` label to Coherence Pods with the label `coherenceComponent: coherencePod`.
on both the default Extend port and Coherence Concurrent Extend port.

[source,yaml]
.manifests/allow-extend-egress.yaml
----
include::manifests/allow-extend-egress.yaml[]
----

The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific
namespace that the Coherence cluster is deployed in.
For example, if the Coherence cluster is deployed in the `datastore` namespace, then the `to` section of policy could
be changed as follows:

[source,yaml]
.manifests/allow-extend-egress.yaml
----
- to:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: datastore
    podSelector:
      matchLabels:
        coherenceComponent: coherencePod
----

This policy must be applied to the namespace _where the client Pods will be deployed_.

===== Coherence gRPC Access

If Coherence gRPC is being used, then first the gRPC Proxy must be configured to use a fixed port.

When using the default Coherence images, for example `ghcr.io/oracle/coherence-ce:22.06` the gRPC proxy is already
configured to run on a fixed port `1408`. The gRPC proxy port can be changed by setting the `COHERENCE_GRPC_PORT` environment variable.

The ingress policy below will allow ingress into the Coherence Pods gRPC port.
The policy allows ingress from Pods that have the `coherence.oracle.com/grpcClient: true` label, from any namespace.
It could be tightened further by using a more specific namespace selector.

[source,yaml]
.manifests/allow-grpc-ingress.yaml
----
include::manifests/allow-grpc-ingress.yaml[]
----

The policy above should be applied to the namespace where the Coherence cluster is running.

The egress policy below will allow egress to the gRPC port from Pods with the `coherence.oracle.com/grpcClient: true` label to
Coherence Pods with the label `coherenceComponent: coherencePod`.

[source,yaml]
.manifests/allow-extend-egress.yaml
----
include::manifests/allow-extend-egress.yaml[]
----

The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific
namespace that the Coherence cluster is deployed in.
For example, if the Coherence cluster is deployed in the `datastore` namespace, then the `to` section of policy could
be changed as follows:

[source,yaml]
.manifests/allow-extend-egress.yaml
----
- to:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: datastore
    podSelector:
      matchLabels:
        coherenceComponent: coherencePod
----

This policy must be applied to the namespace _where the client Pods will be deployed_.

[#metrics]
==== Coherence Metrics

If Coherence metrics is enabled there will need to be an ingress policy to allow connections from metrics clients.
There would also need to be a similar egress policy in the metrics client's namespace to allow it to access the Coherence metrics endpoints.

A simple `Coherence` resource that will create a cluster with metrics enabled is shown below.
This yaml will create a Coherence cluster with a port names `metrics` that maps to the default metrics port of '9612`.

[source,yaml]
.manifests/coherence-cluster-with-metrics.yaml
----
include::manifests/coherence-cluster-with-metrics.yaml[]
----

The example below will assume that metrics will be scraped by Prometheus, and that Prometheus is installed into a namespace called `monitoring`.
An ingress policy must be created in the namespace where the Coherence cluster is deployed allowing ingress to the metrics port from the Prometheus Pods.
The Pods running Prometheus have a label `app.kubernetes.io/name: prometheus` so this can be used in the policy's Pod selector.
This policy should be applied to the namespace where the Coherence cluster is running.

[source,yaml]
.manifests/allow-metrics-ingress.yaml
----
include::manifests/allow-metrics-ingress.yaml[]
----

If the `monitoring` namespace also has a "deny-all" policy and needs egress opening up for Prometheus to scrape metrics then an egress policy will need to be added to the `monitoring` namespace.

The policy below will allow Pods with the label `app.kubernetes.io/name: prometheus` egress to Pods with the `coherenceComponent: coherencePod` label in any namespace. The policy could be further tightened up by adding a namespace selector to restrict egress to the specific namespace where the Coherence cluster is running.

[source,yaml]
.manifests/allow-metrics-egress.yaml
----
include::manifests/allow-metrics-egress.yaml[]
----

[#testing]
=== Testing Network Policies

At the time of writing this documentation, Kubernetes provides no way to verify the correctness of network policies.
It is easy to mess up a policy, in which case policies will either block too much traffic, in which case your application
will work, or worse they will not be blocking access and leave a security hole.

As we have had various requests for help from customers who cannot get Coherence to work with network policies enabled,
the Operator has a simple utility to test connectivity outside of Coherence. This will allow testing pf policies without
the complications of having to start a Coherence server.

This example includes some simple yaml files that will create simulator Pods that listen on all the ports used by the Operator
and by a Coherence cluster member. These simulator Pods are configured with the same labels that the real Operator and
Coherence Pods would have and the same labels used by the network policies in this example. Also included are some yaml files
that start a test client, that simulates either the Operator connecting to Coherence Pods or a Coherence Pod connecting to
the Operator and to other Coherence Pods.

To run these tests, the Operator does not have to be installed.

==== Create the Test Namespaces

In this example we will assume the Operator will eventually be running in a namespace called `coherence` and the Coherence
cluster will run in a namespace called `coh-test`. We can create the namespaces using `kubectl`

[source,bash]
----
kubectl create ns coherence
----

[source,bash]
----
kubectl create ns coh-test
----

At this point there are no network policies installed, this will allow us to confirm the connectivity tests work.

==== Start the Operator Simulator

The Operator simulator server should run in the `coherence` namespace.
It can be created using the following command:

[source,bash]
----
kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator-server.yaml
----

==== Start the Coherence Cluster Simulator

The Coherence cluster member simulator server should run in the `coh-test` namespace.
It can be created using the following command:

[source,bash]
----
kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence-server.yaml
----

==== Run the Operator Test

We can now run the Operator test Job. This wil run a Kubernetes Job that simulates the Operator connecting to the
Kubernetes API server and to the Operator Pods.

[source,bash]
----
kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml
----

The test Job should complete very quickly as it is only testing connectivity to various ports.
The results of the test can be seen by looking at the Pod log. The command below will display the log:

[source,bash]
----
kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name)
----

The output from a successful test will look like this:
[source]
----
1.6727606592497227e+09	INFO	runner	Operator Version: 3.2.11
1.6727606592497835e+09	INFO	runner	Operator Build Date: 2023-01-03T12:25:58Z
1.6727606592500978e+09	INFO	runner	Operator Built By: jonathanknight
1.6727606592501197e+09	INFO	runner	Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5
1.6727606592501485e+09	INFO	runner	Go Version: go1.19.2
1.6727606592501757e+09	INFO	runner	Go OS/Arch: linux/amd64
1.6727606592504115e+09	INFO	net-test	Starting test	{"Name": "Operator Simulator"}
1.6727606592504556e+09	INFO	net-test	Testing connectivity	{"PortName": "K8s API Server"}
1.6727606592664087e+09	INFO	net-test	Testing connectivity PASSED	{"PortName": "K8s API Server", "Version": "v1.24.7"}
1.6727606592674055e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Health", "Port": 6676}
1.6727606592770455e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Health", "Port": 6676}
----

We can see that the test has connected to the Kubernetes API server and has connected to the health port on the
Coherence cluster test server in the `coh-test` namespace.

The test Job can then be deleted:

[source,bash]
----
kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml
----

==== Run the Cluster Member Test

The cluster member test simulates a Coherence cluster member connecting to other cluster members in the same namespace
and also making calls to the Operator's REST endpoint.

[source,bash]
----
kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence.yaml
----

Again, the test should complete quickly as it is just connecting to various ports.
The results of the test can be seen by looking at the Pod log. The command below will display the log:

[source,bash]
----
kubectl -n coh-test logs $(kubectl -n coh-test get pod -l 'coherenceNetTest=coherence-client' -o name)
----

The output from a successful test will look like this:

[source]
----
1.6727631152848177e+09	INFO	runner	Operator Version: 3.2.11
1.6727631152849226e+09	INFO	runner	Operator Build Date: 2023-01-03T12:25:58Z
1.6727631152849536e+09	INFO	runner	Operator Built By: jonathanknight
1.6727631152849755e+09	INFO	runner	Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5
1.6727631152849965e+09	INFO	runner	Go Version: go1.19.2
1.6727631152850187e+09	INFO	runner	Go OS/Arch: linux/amd64
1.6727631152852216e+09	INFO	net-test	Starting test	{"Name": "Cluster Member Simulator"}
1.6727631152852666e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "UnicastPort1", "Port": 7575}
1.6727631152997334e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "UnicastPort1", "Port": 7575}
1.6727631152998908e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "UnicastPort2", "Port": 7576}
1.6727631153059115e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "UnicastPort2", "Port": 7576}
1.6727631153063197e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Management", "Port": 30000}
1.6727631153116117e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Management", "Port": 30000}
1.6727631153119817e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Metrics", "Port": 9612}
1.6727631153187876e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Metrics", "Port": 9612}
1.6727631153189638e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-operator-server.coherence.svc", "PortName": "OperatorRest", "Port": 8000}
1.6727631153265746e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-operator-server.coherence.svc", "PortName": "OperatorRest", "Port": 8000}
1.6727631153267298e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Echo", "Port": 7}
1.6727631153340726e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Echo", "Port": 7}
1.6727631153342876e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "ClusterPort", "Port": 7574}
1.6727631153406997e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "ClusterPort", "Port": 7574}
----

The test client successfully connected to the Coherence cluster port (7475), the two unicast ports (7575 and 7576),
the Coherence management port (30000), the Coherence metrics port (9612), the Operator REST port (8000), and the echo port (7).

The test Job can then be deleted:

[source,bash]
----
kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence.yaml
----

==== Testing the Operator Web Hook

The Operator has a web-hook that k8s calls to validate Coherence resource configurations and to provide default values.
Web hooks in Kubernetes use TLS by default and listen on port 443. The Operator server simulator also listens on port 443
to allow this connectivity to be tested.

The network policy in this example that allows ingress to the web-hook allows any client to connect.
This is because it is not always simple to work out the IP address that the API server will connect to the web-hook from.

We can use the network tester to simulate this by running a Job that will connect to the web hook port.
The web-hook test job in this example does not label the Pod and can be run from the default namespace to simulate a random
external connection.

[source,bash]
----
kubectl -n default apply -f examples/095_network_policies/manifests/net-test-webhook.yaml
----

We can then check the results of the Job by looking at the Pod log.

[source,bash]
----
kubectl -n default logs $(kubectl -n default get pod -l 'coherenceNetTest=webhook-client' -o name)
----

The output from a successful test will look like this:

[source]
----
1.6727639834559627e+09	INFO	runner	Operator Version: 3.2.11
1.6727639834562948e+09	INFO	runner	Operator Build Date: 2023-01-03T12:25:58Z
1.6727639834563956e+09	INFO	runner	Operator Built By: jonathanknight
1.6727639834565024e+09	INFO	runner	Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5
1.6727639834566057e+09	INFO	runner	Go Version: go1.19.2
1.6727639834567096e+09	INFO	runner	Go OS/Arch: linux/amd64
1.6727639834570327e+09	INFO	net-test	Starting test	{"Name": "Web-Hook Client"}
1.6727639834571698e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-operator-server.coherence.svc", "PortName": "WebHook", "Port": 443}
1.6727639834791095e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-operator-server.coherence.svc", "PortName": "WebHook", "Port": 443}
----

We can see that the client successfully connected to port 443.

The test Job can then be deleted:

[source,bash]
----
kubectl -n default delete -f examples/095_network_policies/manifests/net-test-webhook.yaml
----

==== Testing Ad-Hoc Ports

The test client is able to test connectivity to any host and port. For example suppose we want to simulate a Prometheus Pod
connecting to the metrics port of a Coherence cluster. The server simulator is listening on port 9612, so we need to run
the client to connect to that port.

We can create a Job yaml file to run the test client. As the test will simulate a Prometheus client we add the labels
that a standard Prometheus Pod would have and that we also use in the network policies in this example.

In the Job yaml, we need to set the `HOST`, `PORT` and optionally the `PROTOCOL` environment variables.
In this test, the host is the DNS name for the Service created for the Coherence server simulator `net-test-coherence-server.coh-test.svc`, the port is the metrics port `9612` and the protocol is `tcp`.

[source,yaml]
.manifests/net-test-client.yaml
----
include::manifests/net-test-client.yaml[]
----

We need to run the test Job in the `monitoring` namespace, which is the same namespace that Prometheus is
usually deployed into.

[source,bash]
----
kubectl -n monitoring apply -f examples/095_network_policies/manifests/net-test-client.yaml
----
We can then check the results of the Job by looking at the Pod log.

[source,bash]
----
kubectl -n monitoring logs $(kubectl -n monitoring get pod -l 'coherenceNetTest=client' -o name)
----

The output from a successful test will look like this:

[source]
----
1.6727665901488597e+09	INFO	runner	Operator Version: 3.2.11
1.6727665901497366e+09	INFO	runner	Operator Build Date: 2023-01-03T12:25:58Z
1.6727665901498337e+09	INFO	runner	Operator Built By: jonathanknight
1.6727665901498716e+09	INFO	runner	Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5
1.6727665901498966e+09	INFO	runner	Go Version: go1.19.2
1.6727665901499205e+09	INFO	runner	Go OS/Arch: linux/amd64
1.6727665901501486e+09	INFO	net-test	Starting test	{"Name": "Simple Client"}
1.6727665901501985e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "net-test-coherence-server.coh-test.svc-9612", "Port": 9612}
1.6727665901573336e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "net-test-coherence-server.coh-test.svc-9612", "Port": 9612}
----

We can see that the test client successfully connected to the Coherence cluster member simulator on port 9612.

The test Job can then be deleted:

[source,bash]
----
kubectl -n monitoring delete -f examples/095_network_policies/manifests/net-test-client.yaml
----

==== Test with Network Policies

All the above tests ran successfully without any network policies. We can now start to apply policies and re-run the
tests to see what happens.

In a secure environment we would start with a policy that blocks all access and then gradually open up required ports.
We can apply the `deny-all.yaml` policy and then re-run the tests. We should apply the policy to both of the namespaces we are using in this example:

[source,bash]
----
kubectl -n coherence apply -f examples/095_network_policies/manifests/deny-all.yaml
kubectl -n coh-test apply -f examples/095_network_policies/manifests/deny-all.yaml
----

Now, re-run the Operator test client:

[source,bash]
----
kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml
----

and check the result:

[source,bash]
----
kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name)
----

[source]
----
1.6727671834237397e+09	INFO	runner	Operator Version: 3.2.11
1.6727671834238796e+09	INFO	runner	Operator Build Date: 2023-01-03T12:25:58Z
1.6727671834239576e+09	INFO	runner	Operator Built By: jonathanknight
1.6727671834240365e+09	INFO	runner	Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5
1.6727671834240875e+09	INFO	runner	Go Version: go1.19.2
1.6727671834241736e+09	INFO	runner	Go OS/Arch: linux/amd64
1.6727671834244306e+09	INFO	net-test	Starting test	{"Name": "Operator Simulator"}
1.6727671834245417e+09	INFO	net-test	Testing connectivity	{"PortName": "K8s API Server"}
1.6727672134268515e+09	INFO	net-test	Testing connectivity FAILED	{"PortName": "K8s API Server", "Error": "Get \"https://10.96.0.1:443/version?timeout=32s\": dial tcp 10.96.0.1:443: i/o timeout"}
1.6727672134269848e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Health", "Port": 6676}
1.6727672234281697e+09	INFO	net-test	Testing connectivity FAILED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Health", "Port": 6676, "Error": "dial tcp: lookup net-test-coherence-server.coh-test.svc: i/o timeout"}
----

We can see that the test client failed to connect to the Kubernetes API server and failed to connect
to the Coherence cluster health port. This means the deny-all policy is working.

We can now apply the various polices to fix the test

[source,bash]
----
kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-dns.yaml
kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-k8s-api-server.yaml
kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-cluster-member-egress.yaml
kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-rest-ingress.yaml
kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-webhook-ingress-from-all.yaml

kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-dns.yaml
kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-access.yaml
kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-operator-access.yaml
kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-metrics-ingress.yaml
----

Now, delete and re-run the Operator test client:

[source,bash]
----
kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml
kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml
----

and check the result:

[source,bash]
----
kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name)
----

Now with the policies applied the test should have passed.

[source]
----
1.6727691273634596e+09	INFO	runner	Operator Version: 3.2.11
1.6727691273635025e+09	INFO	runner	Operator Build Date: 2023-01-03T12:25:58Z
1.6727691273635256e+09	INFO	runner	Operator Built By: jonathanknight
1.6727691273635616e+09	INFO	runner	Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5
1.6727691273637156e+09	INFO	runner	Go Version: go1.19.2
1.6727691273637407e+09	INFO	runner	Go OS/Arch: linux/amd64
1.6727691273639407e+09	INFO	net-test	Starting test	{"Name": "Operator Simulator"}
1.6727691273639877e+09	INFO	net-test	Testing connectivity	{"PortName": "K8s API Server"}
1.6727691273857167e+09	INFO	net-test	Testing connectivity PASSED	{"PortName": "K8s API Server", "Version": "v1.24.7"}
1.6727691273858056e+09	INFO	net-test	Testing connectivity	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Health", "Port": 6676}
1.6727691273933685e+09	INFO	net-test	Testing connectivity PASSED	{"Host": "net-test-coherence-server.coh-test.svc", "PortName": "Health", "Port": 6676}
----

The other tests can also be re-run and should also pass.

==== Clean-Up

Once the tests are completed, the test servers and Jobs can be deleted.

[source,bash]
----
kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator-server.yaml
kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence-server.yaml
----
