{
    "docs": [
        {
            "location": "/metrics/020_metrics",
            "text": " To deploy a Coherence resource with metrics enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true ports: - name: metrics Setting the coherence.metrics.enabled field to true will enable metrics To expose metrics via a Service it is added to the ports list. The metrics port is a special case where the port number is optional so in this case metrics will bind to the default port 9612 . (see Exposing Ports for details) To expose metrics on a different port the alternative port value can be set in the coherence.metrics section, for example: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true port: 8080 ports: - name: metrics metrics will now be exposed on port 8080 ",
            "title": "Deploy Coherence with Metrics Enabled"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " After installing the basic metrics-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f metrics-cluster.yaml coherence.coherence.oracle.com/metrics-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=metrics-cluster NAME READY STATUS RESTARTS AGE metrics-cluster-0 1/1 Running 0 36s metrics-cluster-1 1/1 Running 0 36s metrics-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward metrics-cluster-0 9612:9612 Forwarding from [::1]:9612 -&gt; 9612 Forwarding from 127.0.0.1:9612 -&gt; 9612 ",
            "title": "Port-forward the Metrics Port"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " Now that a port has been forwarded from localhost to a Pod in the cluster the metrics endpoint can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:9612/metrics ",
            "title": "Access the Metrics Endpoint"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " Since version 12.2.1.4 Coherence has had the ability to expose a http endpoint that can be used to scrape metrics. This would typically be used to expose metrics to something like Prometheus. The description below is only applicable if metrics will be served by Coherence using the coherence-metrics module. If Coherence metrics will be served from a different endpoint, for example from a Helidon web-server using coherence-mp-metrics then the documentation below does not apply. The metrics endpoint is disabled by default in Coherence clusters but can be enabled and configured by setting the relevant fields in the Coherence CRD. The example below shows how to enable and access Coherence metrics when served by the endpoint provided by the coherence-metrics module. For the example below to work the application deployed must have the coherence-metrics jar file and its dependencies on the classpath. Once the metrics port has been exposed, for example via a load balancer or port-forward command, the metrics endpoint is available at http://host:port/metrics . See the Using Coherence Metrics documentation for full details on the available metrics. Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. Deploy Coherence with Metrics Enabled To deploy a Coherence resource with metrics enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true ports: - name: metrics Setting the coherence.metrics.enabled field to true will enable metrics To expose metrics via a Service it is added to the ports list. The metrics port is a special case where the port number is optional so in this case metrics will bind to the default port 9612 . (see Exposing Ports for details) To expose metrics on a different port the alternative port value can be set in the coherence.metrics section, for example: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true port: 8080 ports: - name: metrics metrics will now be exposed on port 8080 Port-forward the Metrics Port After installing the basic metrics-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f metrics-cluster.yaml coherence.coherence.oracle.com/metrics-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=metrics-cluster NAME READY STATUS RESTARTS AGE metrics-cluster-0 1/1 Running 0 36s metrics-cluster-1 1/1 Running 0 36s metrics-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward metrics-cluster-0 9612:9612 Forwarding from [::1]:9612 -&gt; 9612 Forwarding from 127.0.0.1:9612 -&gt; 9612 Access the Metrics Endpoint Now that a port has been forwarded from localhost to a Pod in the cluster the metrics endpoint can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:9612/metrics ",
            "title": "Publish Metrics"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " The operator can create a Prometheus ServiceMonitor for the metrics port so that Prometheus will automatically scrape metrics from the Pods in a Coherence deployment. <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true ports: - name: metrics serviceMonitor: enabled: true The serviceMonitor.enabled field is set to true for the metrics port. See Exposing ports and Services - Service Monitors documentation for more details. ",
            "title": "Prometheus Service Monitor"
        },
        {
            "location": "/applications/030_deploy_application",
            "text": " To specify the image to use set the image field in the Coherence spec to the name of the image. For example if there was an application image called catalogue:1.0.0 it can be specified like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 The catalogue:1.0.0 will be used in the coherence container in the deployment&#8217;s Pods. The example above would assume that the catalogue:1.0.0 has a JVM on the PATH and all the required .jar files, or Java classes, in the default classpath locations used by the Operator. When this is not the case additional configuration can be added to specify additional application settings, these include: setting the classpath specifying the application main specifying application arguments specifying the working directory ",
            "title": "Specify the Image to Use"
        },
        {
            "location": "/applications/030_deploy_application",
            "text": " Once a custom application image has been built (as described in Build Custom Application Images ) a Coherence resource can be configured to use that image. Specify the Image to Use To specify the image to use set the image field in the Coherence spec to the name of the image. For example if there was an application image called catalogue:1.0.0 it can be specified like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 The catalogue:1.0.0 will be used in the coherence container in the deployment&#8217;s Pods. The example above would assume that the catalogue:1.0.0 has a JVM on the PATH and all the required .jar files, or Java classes, in the default classpath locations used by the Operator. When this is not the case additional configuration can be added to specify additional application settings, these include: setting the classpath specifying the application main specifying application arguments specifying the working directory ",
            "title": "Deploy Coherence Application Images"
        },
        {
            "location": "/coherence/010_overview",
            "text": " The Coherence resource has a number of fields to configure the behaviour of Coherence , these fields are in the spec.coherence section of the CRD. ",
            "title": "preambule"
        },
        {
            "location": "/coherence/010_overview",
            "text": " The Coherence CRD has specific fields to configure the most common Coherence settings. Any other settings can be configured by adding system properties to the JVM Settings . The following Coherence features can be directly specified in the Coherence spec. Cluster Name Cache Configuration File Operational Configuration File (aka, the override file) Storage Enabled or disabled deployments Log Level Well Known Addressing and cluster discovery Persistence Management over REST Metrics The Coherence settings in the Coherence CRD spec typically set system property values that will be passed through to the Coherence JVM command line, which in turn configure Coherence. This is the same behaviour that would occur when running Coherence outside of containers. Whether these system properties actually apply or not depends on the application code. For example, it is simple to override the Coherence operational configuration file in a jar file deployed as part of an application&#8217;s image in such a way that will cause all the normal Coherence system properties to be ignored. If that is done then the Coherence settings discussed in this documentation will not apply. For example, adding a tangosol-coherence-override.xml file to a jar on the application&#8217;s classpath that contains an overridden &lt;configurable-cache-factory-config&gt; section with a hard coded cache configuration file name would mean that the Coherence CRD spec.coherence.cacheConfig field, that sets the coherence.cacheconfig system property, would be ignored. It is, therefore, entirely at the application developer&#8217;s discretion whether they use the fields of the Coherence CRD to configure Coherence, or they put those settings into configuration files, either hard coded into jar files or picked up at runtime from files mapped from Kubernetes volumes, config maps, secrets, etc. ",
            "title": "Configuring Coherence"
        },
        {
            "location": "/coherence/050_storage_enabled",
            "text": " Partitioned cache services that manage Coherence caches are configured as storage enabled or storage disabled. Whilst it is possible to configure individual services to be storage enabled or disabled in the cache configuration file and have a mixture of modes in a single JVM, typically all the services in a JVM share the same mode by setting the coherence.distributed.localstorage system property to true for storage enabled members and to false for storage disabled members. The Coherence CRD allows this property to be set by specifying the spec.coherence.storageEnabled field to either true or false. The default value when nothing is specified is true . <markup lang=\"yaml\" title=\"storage enabled\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: storageEnabled: true The Coherence resource specifically sets coherence.distributed.localstorage to true <markup lang=\"yaml\" title=\"storage disabled\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: storageEnabled: false The Coherence resource specifically sets coherence.distributed.localstorage to false ",
            "title": "Storage Enabled or Disabled Deployments"
        },
        {
            "location": "/ports/040_servicemonitors",
            "text": " The Coherence CRD ServiceMonitorSpec contains many of the fields from the Prometheus ServiceMonitorSpec and Prometheus Endpoint to allow the ServiceMonitor to be configured for most use-cases. In situations where the Coherence CRD does not have the required fields, for example when a different version of Prometheus has been installed to that used to build the Coherence Operator, then the solution would be to manually create ServiceMonitors instead of letting them be created by the Coherence Operator. ",
            "title": "Configure the ServiceMonitor"
        },
        {
            "location": "/ports/040_servicemonitors",
            "text": " When a port exposed on a container is to be used to serve metrics to Prometheus this often requires the addition of a Prometheus ServiceMonitor resource. The Coherence Operator makes it simple to add a ServiceMonitor for an exposed port. The advantage of specifying the ServiceMonitor configuration in the Coherence CRD spec is that the ServiceMonitor resource will be created, updated and deleted as part of the lifecycle of the Coherence resource, and does not need to be managed separately. A ServiceMonitor is created for an exposed port by setting the serviceMonitor.enabled field to true . The Operator will create a ServiceMonitor with the same name as the Service . The ServiceMonitor created will have a single endpoint for the port being exposed. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 serviceMonitor: enabled: true With the serviceMonitor.enabled field set to true a ServiceMonitor resource will be created. The ServiceMonitor created from the spec above will look like this: For example: <markup lang=\"yaml\" >apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: test-cluster-rest labels: coherenceCluster: test-cluster coherenceComponent: coherence-service-monitor coherenceDeployment: test-cluster coherenceRole: test-cluster spec: endpoints: - port: rest relabelings: - action: labeldrop regex: (endpoint|instance|job|service) selector: matchLabels: coherenceCluster: test-cluster coherenceComponent: coherence-service coherenceDeployment: test-cluster coherencePort: rest coherenceRole: test-cluster Configure the ServiceMonitor The Coherence CRD ServiceMonitorSpec contains many of the fields from the Prometheus ServiceMonitorSpec and Prometheus Endpoint to allow the ServiceMonitor to be configured for most use-cases. In situations where the Coherence CRD does not have the required fields, for example when a different version of Prometheus has been installed to that used to build the Coherence Operator, then the solution would be to manually create ServiceMonitors instead of letting them be created by the Coherence Operator. ",
            "title": "Prometheus ServiceMonitors"
        },
        {
            "location": "/applications/040_application_main",
            "text": " The Coherence container in the deployment&#8217;s Pods will, by default, run com.tangosol.net.DefaultCacheServer as the Java main class. It is possible to change this when running a custom application that requires a different main. The name of the main is set in the application.main field in the Coherence spec. For example, if the deployment is using a custom image catalogue:1.0.0 that requires a custom main class called com.acme.Catalogue the Coherence resource would look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: main: com.acme.Catalogue The com.acme.Catalogue will be run as the main class. The example would be equivalent to the Coherence container running: <markup lang=\"bash\" >$ java com.acme.Catalogue ",
            "title": "Set the Application Main"
        },
        {
            "location": "/about/01_overview",
            "text": " fa-rocket Quick Start Quick start guide to running your first Coherence cluster using the Coherence Operator. fa-save Install Installing and running the Coherence Operator. cloud_upload Build and Deploy Applications Deploying Coherence Applications using the Coherence Operator. ",
            "title": "Get Going"
        },
        {
            "location": "/about/01_overview",
            "text": " fa-balance-scale Scaling Safe scaling of Coherence deployments the Coherence Operator. fa-cogs Configure Coherence Configuring Coherence behaviour. fa-cog Configure the JVM Configure the behaviour of the JVM. control_camera Expose Ports & Services Configure services to expose ports provided by the application. speed Metrics Enabling and working with Metrics in the Coherence Operator. fa-stethoscope Management and Diagnostics Management and Diagnostics in the Coherence Operator. find_in_page Logging Viewing and managing log files within using the Coherence Operator. widgets Coherence CRD Reference Coherence CRD reference guide. ",
            "title": "In Depth"
        },
        {
            "location": "/coherence/060_log_level",
            "text": " Logging granularity in Coherence is controlled by a log level, that is a number between one and nine, where the higher the number the more debug logging is produced. The Coherence CRD has a field spec.coherence.logLevel that allows the log level to be configured by setting the coherence.log.level system property. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: logLevel: 9 The Coherence spec sets the log level to 9, effectively passing -Dcoherence.log.level=9 to the Coherence JVM&#8217;s command line. ",
            "title": "Set the Coherence Log Level"
        },
        {
            "location": "/management/010_overview",
            "text": " There are a number of management and diagnostic features available with Oracle Coherence that can be used in a cluster deployed in Kubernetes. Management Over REST Coherence Management over REST feature VisualVM Coherence VisualVM plugin. SSL Enable SSL on the Management over REST endpoint. ",
            "title": "Overview"
        },
        {
            "location": "/applications/060_application_working_dir",
            "text": " When running a custom application there may be a requirement to run in a specific working directory. The working directory can be specified in the application.workingDir field in the Coherence spec. For example, a deployment uses a custom image catalogue:1.0.0 that requires a custom main class called com.acme.Catalogue , and that class takes additional arguments. In this example we&#8217;ll use two fictitious arguments such as a name and a language for the catalogue. the Coherence resource would look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: workingDir: \"/apps/catalogue\" main: \"com.acme.Catalogue\" The catalogue:1.0.0 image will be used. The Java command will be executed in the /apps/catalogue working directory. The Java main class executed will be com.acme.Catalogue The example would be equivalent to the Coherence container running: <markup lang=\"bash\" >$ cd /apps/catalogue $ java com.acme.Catalogue ",
            "title": "Set the Working Directory"
        },
        {
            "location": "/other/100_resources",
            "text": " When creating a Coherence resource you can optionally specify how much CPU and memory (RAM) each Coherence Container is allowed to consume. The container resources are specified in the resources section of the Coherence spec; the format is exactly the same as documented in the Kubernetes documentation Managing Compute Resources for Containers . When setting resource limits, in particular memory limits, for a container it is important to ensure that the Coherence JVM is properly configured so that it does not consume more memory than the limits. If the JVM attempts to consume more memory than the resource limits allow the Pod can be killed by Kubernetes. See Configuring the JVM Memory for details on the different memory settings. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" The coherence container in the Pods has a request of 0.25 cpu and 64MiB of memory. The coherence container has a limit of 0.5 cpu and 128MiB of memory. ",
            "title": "Container Resource Limits"
        },
        {
            "location": "/applications/010_overview",
            "text": " Build Custom Application Images Building custom Coherence application images for use with the Coherence Operator. Deploy Custom Application Images Deploying custom application images using the Coherence Operator. ",
            "title": "Building and Deploying Applications"
        },
        {
            "location": "/applications/010_overview",
            "text": " There are many settings in a Coherence resource that control the behaviour of Coherence, the JVM and the application code. Some of the application specific settings are shown below: Setting the Classpath Setting a custom classpath for the application. Setting a Main Class Setting a custom main class to run. Setting Application Arguments Setting arguments to pass to the main class. Working Directory Setting the application&#8217;s working directory. ",
            "title": "Configuring Applications"
        },
        {
            "location": "/applications/010_overview",
            "text": " A typical Coherence deployment contains custom application code that runs with Coherence. To run custom application code in a Coherence resource that code needs to be packaged into an image that the deployment will use. Building and Deploying Applications Build Custom Application Images Building custom Coherence application images for use with the Coherence Operator. Deploy Custom Application Images Deploying custom application images using the Coherence Operator. Configuring Applications There are many settings in a Coherence resource that control the behaviour of Coherence, the JVM and the application code. Some of the application specific settings are shown below: Setting the Classpath Setting a custom classpath for the application. Setting a Main Class Setting a custom main class to run. Setting Application Arguments Setting arguments to pass to the main class. Working Directory Setting the application&#8217;s working directory. ",
            "title": "Overview"
        },
        {
            "location": "/scaling/010_overview",
            "text": " The Coherence Operator provides the ability to safely scale up and down a Coherence deployment. A Coherence deployment is backed by a StatefulSet , which can easily be scaled using existing Kubernetes features. The problem with directly scaling down the StatefulSet is that Kubernetes will immediately kill the required number of Pods . This is obviously very bad for Coherence as killing multiple storage enabled members would almost certainly cause data loss. The Coherence Operator supports scaling by applying the scaling update directly to Coherence deployment rather than to the underlying StatefulSet . There are two methods to scale a Coherence deployment: Update the replicas field in the Coherence CRD spec. Use the kubectl scale command When either of these methods is used the Operator will detect that a change to the size of the deployment is required and ensure that the change will be applied safely. The logical steps the Operator will perform are: Detect desired replicas is different to current replicas Check the cluster is StatusHA - i.e. no cache services are endangered. If any service is not StatusHA requeue the scale request (go back to step one). If scaling up, add the required number of members. If scaling down, scale down by one member and requeue the request (go back to step one). What these steps ensure is that the deployment will not be resized unless the cluster is in a safe state. When scaling down only a single member will be removed at a time, ensuring that the cluster is in a safe state before removing the next member. The Operator will only apply safe scaling functionality to deployments that are storage enabled. If a deployment is storage disabled then it can be scaled up or down by the required number of members in one step as there is no fear of data loss in a storage disabled member. ",
            "title": "Scale Coherence Deployments"
        },
        {
            "location": "/scaling/010_overview",
            "text": " The Coherence CRD spec has a field scaling.policy that can be used to override the default scaling behaviour. The scaling policy has three possible values: Value Description ParallelUpSafeDown This is the default scaling policy. With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ) and when scaling down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the cluster have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when adding members, but offers safe, albeit slower, scaling down as Pods are removed one by one. Parallel With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ). With this policy no StatusHA check is performed either when scaling up or when scaling down. This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for deployments that are storage disabled. Safe With this policy when scaling up and down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the deployment have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy is slower to start, scale up and scale down. Both the ParallelUpSafeDown and Safe policies will ensure no data loss when scaling a deployment. The policy can be set as shown below: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: policy: Safe This deployment will scale both up and down with StatusHA checks. ",
            "title": "Scaling Policy"
        },
        {
            "location": "/scaling/010_overview",
            "text": " A HTTP get probe works the same way as a Kubernetes liveness http request The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: httpGet: port: 8080 path: /statusha This deployment will check the status of the services by performing a http GET on http://&lt;pod-ip&gt;:8080/statusha . If the response is 200 the check will pass, any other response the check is assumed to be false. ",
            "title": "Using a HTTP Get Probe"
        },
        {
            "location": "/scaling/010_overview",
            "text": " A TCP probe works the same way as a Kubernetes TCP liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: tcpSocket: port: 7000 This deployment will check the status of the services by connecting to the socket on port 7000 . ",
            "title": "Using a TCP Probe"
        },
        {
            "location": "/scaling/010_overview",
            "text": " A TCP probe works the same way as a Kubernetes Exec liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: exec: command: - /bin/ah - safe.sh This deployment will check the status of the services by running the sh safe.sh command in the Pod . ",
            "title": "Using an Exec Command Probe"
        },
        {
            "location": "/scaling/010_overview",
            "text": " The StatusHA check performed by the Operator uses a http endpoint that the Operator runs on a well-known port in the Coherence JVM. This endpoint does performs a simple check to verify that none of the partitioned cache services known about by Coherence have an endangered status. If an application has a different concept of what \"safe\" means it can implement a different method to check the status during scaling. The operator supports different types of safety check probes, these are exactly the same as those supported by Kubernetes for readiness and liveness probes. The scaling.probe section of the Coherence CRD allows different types of probe to be configured. Using a HTTP Get Probe A HTTP get probe works the same way as a Kubernetes liveness http request The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: httpGet: port: 8080 path: /statusha This deployment will check the status of the services by performing a http GET on http://&lt;pod-ip&gt;:8080/statusha . If the response is 200 the check will pass, any other response the check is assumed to be false. Using a TCP Probe A TCP probe works the same way as a Kubernetes TCP liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: tcpSocket: port: 7000 This deployment will check the status of the services by connecting to the socket on port 7000 . Using an Exec Command Probe A TCP probe works the same way as a Kubernetes Exec liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: exec: command: - /bin/ah - safe.sh This deployment will check the status of the services by running the sh safe.sh command in the Pod . ",
            "title": "Scaling StatusHA Probe"
        },
        {
            "location": "/scaling/010_overview",
            "text": " The Coherence CRD has a number of fields that control the behaviour of scaling. Scaling Policy The Coherence CRD spec has a field scaling.policy that can be used to override the default scaling behaviour. The scaling policy has three possible values: Value Description ParallelUpSafeDown This is the default scaling policy. With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ) and when scaling down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the cluster have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when adding members, but offers safe, albeit slower, scaling down as Pods are removed one by one. Parallel With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ). With this policy no StatusHA check is performed either when scaling up or when scaling down. This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for deployments that are storage disabled. Safe With this policy when scaling up and down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the deployment have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy is slower to start, scale up and scale down. Both the ParallelUpSafeDown and Safe policies will ensure no data loss when scaling a deployment. The policy can be set as shown below: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: policy: Safe This deployment will scale both up and down with StatusHA checks. Scaling StatusHA Probe The StatusHA check performed by the Operator uses a http endpoint that the Operator runs on a well-known port in the Coherence JVM. This endpoint does performs a simple check to verify that none of the partitioned cache services known about by Coherence have an endangered status. If an application has a different concept of what \"safe\" means it can implement a different method to check the status during scaling. The operator supports different types of safety check probes, these are exactly the same as those supported by Kubernetes for readiness and liveness probes. The scaling.probe section of the Coherence CRD allows different types of probe to be configured. Using a HTTP Get Probe A HTTP get probe works the same way as a Kubernetes liveness http request The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: httpGet: port: 8080 path: /statusha This deployment will check the status of the services by performing a http GET on http://&lt;pod-ip&gt;:8080/statusha . If the response is 200 the check will pass, any other response the check is assumed to be false. Using a TCP Probe A TCP probe works the same way as a Kubernetes TCP liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: tcpSocket: port: 7000 This deployment will check the status of the services by connecting to the socket on port 7000 . Using an Exec Command Probe A TCP probe works the same way as a Kubernetes Exec liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: exec: command: - /bin/ah - safe.sh This deployment will check the status of the services by running the sh safe.sh command in the Pod . ",
            "title": "Controlling Safe Scaling"
        },
        {
            "location": "/metrics/010_overview",
            "text": " Coherence metrics can be used in a cluster deployed in Kubernetes. Metrics Enabling the Coherence metrics feature. Importing Grafana Dashboards Importing the Coherence Grafana Dashboards. Using Grafana Dashboards Using the Coherence Grafana Dashboards. SSL Enable SSL on the metrics endpoint. ",
            "title": "Overview"
        },
        {
            "location": "/about/03_quickstart",
            "text": " This guide is a simple set of steps to install the Coherence Operator and then use that to install a simple Coherence cluster. ",
            "title": "preambule"
        },
        {
            "location": "/about/03_quickstart",
            "text": " Ensure that the Coherence Operator prerequisites are available. ",
            "title": "Prerequisites"
        },
        {
            "location": "/about/03_quickstart",
            "text": "<markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update ",
            "title": "1.1 Add the Coherence Operator Helm repository"
        },
        {
            "location": "/about/03_quickstart",
            "text": "<markup lang=\"bash\" title=\"helm v3 install command\" >helm install \\ --namespace &lt;namespace&gt; \\ &lt;release-name&gt; \\ coherence/coherence-operator e.g. if the Kubernetes namespace is coherence-test the command would be: <markup lang=\"bash\" title=\"helm v3 install command\" >helm install --namespace coherence-test operator coherence/coherence-operator or with Helm v2 <markup lang=\"bash\" >helm install --namespace coherence-test --name operator coherence/coherence-operator See the full install guide for more details. ",
            "title": "1.2. Install the Coherence Operator Helm chart"
        },
        {
            "location": "/about/03_quickstart",
            "text": " 1.1 Add the Coherence Operator Helm repository <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update 1.2. Install the Coherence Operator Helm chart <markup lang=\"bash\" title=\"helm v3 install command\" >helm install \\ --namespace &lt;namespace&gt; \\ &lt;release-name&gt; \\ coherence/coherence-operator e.g. if the Kubernetes namespace is coherence-test the command would be: <markup lang=\"bash\" title=\"helm v3 install command\" >helm install --namespace coherence-test operator coherence/coherence-operator or with Helm v2 <markup lang=\"bash\" >helm install --namespace coherence-test --name operator coherence/coherence-operator See the full install guide for more details. ",
            "title": "1. Install the Coherence Operator"
        },
        {
            "location": "/about/03_quickstart",
            "text": " The minimal required yaml to create a Coherence resource is shown below. <markup lang=\"yaml\" title=\"my-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-deployment The only required field is metadata.name which will be used as the Coherence cluster name, in this case my-deployment <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; apply -f my-deployment.yaml Use the same namespace that the operator was installed into, e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test create -f my-deployment.yaml ",
            "title": "2.1 Install a Coherence resource using the minimal required configuration."
        },
        {
            "location": "/about/03_quickstart",
            "text": " After installing the my-deployment.yaml above here should be a single Coherence resource named my-deployment in the Coherence Operator namespace. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coherence or alternatively using the Coherence CRD a short name of coh <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coh e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test get coherence <markup lang=\"bash\" >NAME AGE coherence.coherence.oracle.com/my-deployment 19s ",
            "title": "2.2 List the Coherence Resources"
        },
        {
            "location": "/about/03_quickstart",
            "text": " The Coherence Operator applies a coherenceDeployment label to all Pods so this label can be used with the kubectl command to find Pods for a CoherenceCoherence resource. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceDeployment=my-deployment e.g. if the namespace is coherence the command would be: kubectl -n coherence get pod -l coherenceDeployment=my-deployment <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE my-deployment-0 1/1 Running 0 2m58s my-deployment-1 1/1 Running 0 2m58s my-deployment-2 1/1 Running 0 2m58s ",
            "title": "2.3 List all of the Pods for the Coherence resource."
        },
        {
            "location": "/about/03_quickstart",
            "text": " The Coherence Operator applies a coherenceCluster label to all Pods , so this label can be used with the kubectl command to find all Pods for a Coherence cluster, which will be made up of multiple Coherence resources. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster e.g. If there is a cluster named my-cluster made up of two Coherence resources in the namespace coherence-test , one named storage and one named front-end then the kubectl command to list all Pods for the cluster would be: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=my-cluster The result of which might look something like this <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE storage-0 1/1 Running 0 2m58s storage-1 1/1 Running 0 2m58s storage-2 1/1 Running 0 2m58s front-end-0 1/1 Running 0 2m58s front-end-1 1/1 Running 0 2m58s front-end-2 1/1 Running 0 2m58s ",
            "title": "2.3 List all the Pods for the Coherence cluster."
        },
        {
            "location": "/about/03_quickstart",
            "text": " Ensure that the Coherence images can be pulled by the Kubernetes cluster, see Obtain Coherence Images . By default, a Coherence resource will use the OSS Coherence CE image from Docker Hub. If a different image is to be used the image name will need to be specified in the Coherence yaml, see Setting the Application Image for documentation on how to specify a different images to use. 2.1 Install a Coherence resource using the minimal required configuration. The minimal required yaml to create a Coherence resource is shown below. <markup lang=\"yaml\" title=\"my-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-deployment The only required field is metadata.name which will be used as the Coherence cluster name, in this case my-deployment <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; apply -f my-deployment.yaml Use the same namespace that the operator was installed into, e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test create -f my-deployment.yaml 2.2 List the Coherence Resources After installing the my-deployment.yaml above here should be a single Coherence resource named my-deployment in the Coherence Operator namespace. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coherence or alternatively using the Coherence CRD a short name of coh <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coh e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test get coherence <markup lang=\"bash\" >NAME AGE coherence.coherence.oracle.com/my-deployment 19s 2.3 List all of the Pods for the Coherence resource. The Coherence Operator applies a coherenceDeployment label to all Pods so this label can be used with the kubectl command to find Pods for a CoherenceCoherence resource. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceDeployment=my-deployment e.g. if the namespace is coherence the command would be: kubectl -n coherence get pod -l coherenceDeployment=my-deployment <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE my-deployment-0 1/1 Running 0 2m58s my-deployment-1 1/1 Running 0 2m58s my-deployment-2 1/1 Running 0 2m58s 2.3 List all the Pods for the Coherence cluster. The Coherence Operator applies a coherenceCluster label to all Pods , so this label can be used with the kubectl command to find all Pods for a Coherence cluster, which will be made up of multiple Coherence resources. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster e.g. If there is a cluster named my-cluster made up of two Coherence resources in the namespace coherence-test , one named storage and one named front-end then the kubectl command to list all Pods for the cluster would be: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=my-cluster The result of which might look something like this <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE storage-0 1/1 Running 0 2m58s storage-1 1/1 Running 0 2m58s storage-2 1/1 Running 0 2m58s front-end-0 1/1 Running 0 2m58s front-end-1 1/1 Running 0 2m58s front-end-2 1/1 Running 0 2m58s ",
            "title": "2. Install a Coherence Deployment"
        },
        {
            "location": "/about/03_quickstart",
            "text": " Using the kubectl scale command a specific Coherence resource can be scaled up or down. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; scale coherence/my-deployment --replicas=6 e.g. if the namespace is coherence-test the command would be: kubectl -n coherence scale coherence/my-deployment --replicas=6 ",
            "title": "3.1 Use kubectl to Scale Up"
        },
        {
            "location": "/about/03_quickstart",
            "text": " 3.1 Use kubectl to Scale Up Using the kubectl scale command a specific Coherence resource can be scaled up or down. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; scale coherence/my-deployment --replicas=6 e.g. if the namespace is coherence-test the command would be: kubectl -n coherence scale coherence/my-deployment --replicas=6 ",
            "title": "3. Scale the Coherence Cluster"
        },
        {
            "location": "/jvm/020_classpath",
            "text": " If the image to be run has the CLASSPATH environment variable set this will be used as part of the classpath. ",
            "title": "The CLASSPATH Environment Variable"
        },
        {
            "location": "/jvm/020_classpath",
            "text": " If the image to be run has the COHERENCE_HOME environment variable set this will be used to add the following elements to the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf These will be added to the end of the classpath. For example in an image that has CLASSPATH=/home/root/lib/* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf ",
            "title": "The COHERENCE_HOME Environment Variable"
        },
        {
            "location": "/jvm/020_classpath",
            "text": " If the image is not a JIB image there could be occasions when automatically adding /app/libs/*:/app/classes:/app/resources to the classpath causes issues, for example one or more of those locations exists with files in that should not be on the classpath. In this case the Coherence CRD spec has a field to specify that the JIB classpath should not be used. The spec.jvm.useJibClasspath field can be set to false to exclude the JIB directories from the classpath (the default value is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. ",
            "title": "Exclude the JIB Classpath"
        },
        {
            "location": "/jvm/020_classpath",
            "text": " A simple way to build Java images is using JIB . When JIB was with its Maven or Gradle plugin to produce an image it packages the application&#8217;s dependencies, classes and resources into a set of well-known locations: /app/libs/ - the jar files that the application depends on /app/classes - the application&#8217;s class files /app/resources - the application&#8217;s other resources By default, the Operator will add these locations to the classpath. These classpath elements will be added before any value set by the CLASSPATH or COHERENCE_HOME environment variables. For example in an image that has CLASSPATH=/home/root/lib/\\* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /app/libs/*:/app/classes:/app/resources:/home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf Exclude the JIB Classpath If the image is not a JIB image there could be occasions when automatically adding /app/libs/*:/app/classes:/app/resources to the classpath causes issues, for example one or more of those locations exists with files in that should not be on the classpath. In this case the Coherence CRD spec has a field to specify that the JIB classpath should not be used. The spec.jvm.useJibClasspath field can be set to false to exclude the JIB directories from the classpath (the default value is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. ",
            "title": "JIB Image Classpath"
        },
        {
            "location": "/jvm/020_classpath",
            "text": " If an image will be used that has artifacts in locations other than the defaults discussed above then it is possible to specify additional elements to be added to the classpath. The jvm.classpath field in the Coherence CRD spec allows a list of extra classpath values to be provided. These elements will be added after the JIB classpath described above. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/lib/*\" - \"/data/config\" The classpath field adds /data/lib/* and /data/config to the classpath. In an image without the CLASSPATH or COHERENCE_HOME environment variables the effective classpath would be: There is no validation of the elements of the classpath. The elements will not be verified to ensure that the locations exist. As long as they are valid values to be uased in a JVM classpath they will be accepted. ",
            "title": "Additional Classpath Elements"
        },
        {
            "location": "/jvm/020_classpath",
            "text": " The Coherence container in the Pods in a Coherence resource deployment runs a Java application and as such requires a classpath with at a minimum coherence.jar . There are certain defaults that the Operator will use to work out the classpath to use but additional classpath elements can be provided to the configuration. The CLASSPATH Environment Variable If the image to be run has the CLASSPATH environment variable set this will be used as part of the classpath. The COHERENCE_HOME Environment Variable If the image to be run has the COHERENCE_HOME environment variable set this will be used to add the following elements to the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf These will be added to the end of the classpath. For example in an image that has CLASSPATH=/home/root/lib/* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf JIB Image Classpath A simple way to build Java images is using JIB . When JIB was with its Maven or Gradle plugin to produce an image it packages the application&#8217;s dependencies, classes and resources into a set of well-known locations: /app/libs/ - the jar files that the application depends on /app/classes - the application&#8217;s class files /app/resources - the application&#8217;s other resources By default, the Operator will add these locations to the classpath. These classpath elements will be added before any value set by the CLASSPATH or COHERENCE_HOME environment variables. For example in an image that has CLASSPATH=/home/root/lib/\\* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /app/libs/*:/app/classes:/app/resources:/home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf Exclude the JIB Classpath If the image is not a JIB image there could be occasions when automatically adding /app/libs/*:/app/classes:/app/resources to the classpath causes issues, for example one or more of those locations exists with files in that should not be on the classpath. In this case the Coherence CRD spec has a field to specify that the JIB classpath should not be used. The spec.jvm.useJibClasspath field can be set to false to exclude the JIB directories from the classpath (the default value is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. Additional Classpath Elements If an image will be used that has artifacts in locations other than the defaults discussed above then it is possible to specify additional elements to be added to the classpath. The jvm.classpath field in the Coherence CRD spec allows a list of extra classpath values to be provided. These elements will be added after the JIB classpath described above. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/lib/*\" - \"/data/config\" The classpath field adds /data/lib/* and /data/config to the classpath. In an image without the CLASSPATH or COHERENCE_HOME environment variables the effective classpath would be: There is no validation of the elements of the classpath. The elements will not be verified to ensure that the locations exist. As long as they are valid values to be uased in a JVM classpath they will be accepted. ",
            "title": "Set the Classpath"
        },
        {
            "location": "/other/050_configmap_volumes",
            "text": " Additional Volumes and VolumeMounts from ConfigMaps can easily be added to a Coherence resource. ",
            "title": "preambule"
        },
        {
            "location": "/other/050_configmap_volumes",
            "text": " To add a ConfigMap as an additional volume to the Pods of a Coherence deployment add entries to the configMapVolumes list in the CRD spec. Each entry in the list has a mandatory name and mountPath field, all other fields are optional. The name field is the name of the ConfigMap to mount and is also used as the volume name. The mountPath field is the path in the container to mount the volume to. Additional volumes added in this way (either ConfigMaps shown here, or Secrets or plain Volumes ) will be added to all containers in the Pod . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: configMapVolumes: - name: storage-config mountPath: /home/coherence/config The ConfigMap named storage-config will be mounted to the Pod as an additional Volume named storage-config The ConfigMap will be mounted at /home/coherence/config in the containers. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config configMap: name: storage-config As already stated, if the Coherence resource has additional containers the ConfigMap will be mounted in all of them. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" configMapVolumes: - name: storage-config mountPath: /home/coherence/config In this example the storage-config ConfigMap will be mounted as a Volume and mounted to both the coherence container and the fluentd container. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3\" volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config configMap: name: storage-config ",
            "title": "Add ConfigMap Volumes"
        },
        {
            "location": "/jvm/030_jvm_args",
            "text": " The Coherence Operator will add the following JVM arguments by default: <markup >-Dcoherence.cluster=&lt;cluster-name&gt; -Dcoherence.role=&lt;role&gt; -Dcoherence.wka=&lt;deployment-name&gt;-wka -Dcoherence.cacheconfig=coherence-cache-config.xml -Dcoherence.health.port=6676 -Dcoherence.management.http.port=30000 -Dcoherence.metrics.http.port=9612 -Dcoherence.distributed.persistence-mode=on-demand -Dcoherence.override=k8s-coherence-override.xml -Dcoherence.ttl=0 -XX:+UseG1GC -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XshowSettings:all -XX:+UseContainerSupport -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -XX:HeapDumpPath=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/heap-dumps/&lt;member&gt;-&lt;pod-uid&gt;.hprof -XX:ErrorFile=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/hs-err-&lt;member&gt;-&lt;pod-uid&gt;.log -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics Some of the arguments and system properties above can be overridden or changed by setting values in the Coherence CDR spec. ",
            "title": "Default Arguments"
        },
        {
            "location": "/jvm/030_jvm_args",
            "text": " The Coherence CRD allows any arbitrary JVM arguments to be passed to the JVM in the coherence container by using the jvm.args field of the CRD spec. Any valid system property or JVM argument can be added to the jvm.args list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: args: - \"-Dcoherence.pof.config=storage-pof-config.xml\" - \"-Dcoherence.tracing.ratio=0.1\" - \"-agentpath:/yourkit/bin/linux-x86-64/libyjpagent.so\" In this example the args list adds two System properties coherence.pof.config=storage-pof-config.xml and coherence.tracing.ratio=0.1 and also adds the YourKit profiling agent. When the Operator builds the command line to use when starting Coherence Pods, any arguments added to the jvm.args field will be added after all the arguments added by the Operator from other configuration fields. This means that arguments such as system properties added to jvm.args will override any added by the Operator. For example <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: cacheConfig: storage-config.xml jvm: args: - \"-Dcoherence.cache.config=test-config.xml\" Setting the coherence.cacheConfig field will make the operator add -Dcoherence.cache.config=storage-config.xml to the command line. Adding -Dcoherence.cache.config=test-config.xml to the jvm.args field will make the Operator add -Dcoherence.cache.config=test-config.xml to the end of the JVM arguments in the command line. When duplicate system properties are present on a command line the last one wins so in the example above the cache configuration used would be test-config.xml . Default Arguments The Coherence Operator will add the following JVM arguments by default: <markup >-Dcoherence.cluster=&lt;cluster-name&gt; -Dcoherence.role=&lt;role&gt; -Dcoherence.wka=&lt;deployment-name&gt;-wka -Dcoherence.cacheconfig=coherence-cache-config.xml -Dcoherence.health.port=6676 -Dcoherence.management.http.port=30000 -Dcoherence.metrics.http.port=9612 -Dcoherence.distributed.persistence-mode=on-demand -Dcoherence.override=k8s-coherence-override.xml -Dcoherence.ttl=0 -XX:+UseG1GC -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XshowSettings:all -XX:+UseContainerSupport -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -XX:HeapDumpPath=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/heap-dumps/&lt;member&gt;-&lt;pod-uid&gt;.hprof -XX:ErrorFile=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/hs-err-&lt;member&gt;-&lt;pod-uid&gt;.log -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics Some of the arguments and system properties above can be overridden or changed by setting values in the Coherence CDR spec. ",
            "title": "Adding Arbitrary JVM Arguments"
        },
        {
            "location": "/other/010_overview",
            "text": " Add Containers Adding side-car containers and init-containers. ",
            "title": "Containers"
        },
        {
            "location": "/other/010_overview",
            "text": " Add Volumes Adding Volumes and volume mounts. Add ConfigMap Volumes Adding Volumes and volume mounts using ConfigMaps. Add Secret Volumes Adding Volumes and volume mounts using Secrets. ",
            "title": "Volumes"
        },
        {
            "location": "/other/010_overview",
            "text": " Pod Scheduling Taints, Tolerations and node selectors. Resources Configuring Coherence container resource constraints. ",
            "title": "Pod Scheduling"
        },
        {
            "location": "/other/010_overview",
            "text": " There are a number of miscellaneous configuration settings that can be added to containers and Pods controlled by the Coherence Operator. Environment Variables Adding environment variables to the Coherence container. Pod Labels Adding Pod labels. Pod Annotations Adding Pod annotations. Containers Add Containers Adding side-car containers and init-containers. Volumes Add Volumes Adding Volumes and volume mounts. Add ConfigMap Volumes Adding Volumes and volume mounts using ConfigMaps. Add Secret Volumes Adding Volumes and volume mounts using Secrets. Pod Scheduling Pod Scheduling Taints, Tolerations and node selectors. Resources Configuring Coherence container resource constraints. ",
            "title": "Overview"
        },
        {
            "location": "/other/070_add_volumes",
            "text": " Volumes and volume mappings can easily be added to a Coherence resource Pod to allow application code deployed in the Pods to access additional storage. Volumes are added by adding configuration to the volumes list in the Coherence CRD spec. The configuration of the volume can be any valid yaml that would be used when adding a Volume to a Pod spec. Volume mounts are added by adding configuration to the volumeMounts list in the Coherence CRD spec. The configuration of the volume mount can be any valid yaml that would be used when adding a volume mount to a container in a Pod spec. Additional volumes added in this way will be added to all containers in the Pod . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: volumes: - name: data-volume nfs: path: /shared/data server: nfs-server volumeMounts: - name: data-volume mountPath: /data An additional Volume named data-volume has been added (in this case the volume is an NFS volume). An additional volume mount has been added tthat will mount the data-volume at the /data mount point. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: data-volume mountPath: /data volumes: - name: data-volume nfs: path: /shared/data server: nfs-server ",
            "title": "Add Pod Volumes"
        },
        {
            "location": "/logging/010_overview",
            "text": " The use of Elasticsearch, Fluentd and Kibana is a common approach. For this reason the Coherence Operator has a set of Kibana dashboards that support the common Coherence logging format. The logging guides below show one approach to shipping Coherence logs to Elasticsearch and importing the Coherence dashboards into Kibana. If this approach does not meet your needs you are obviously free to configure an alternative. Enabling Log Capture Capturing and viewing Coherence cluster Logs in Elasticsearch using a Fluentd side-car. Kibana Dashboards Importing and using the Kibana Dashboards available. ",
            "title": "Logging Guides"
        },
        {
            "location": "/logging/010_overview",
            "text": " In a container environment like Kubernetes, or any cloud, it is often a requirement to centralize log files to allow easier analysis and debugging. There are many ways to do this, including collecting container logs, parsing and shipping log files with something like Fluentd, or using a specialized log appender specific to your logging framework. The Coherence Operator does not proscribe any particular method of log capture. The Coherence CRD is flexible enough to allow any method of log capture that an application or specific cloud environment requires. This could be as simple as adding JVM arguments to configure the Java logger, or it could be injecting a whole side-car container to run something like Fluentd. Different approaches have their own pros and cons that need to be weighed up on a case by case basis. Logging Guides The use of Elasticsearch, Fluentd and Kibana is a common approach. For this reason the Coherence Operator has a set of Kibana dashboards that support the common Coherence logging format. The logging guides below show one approach to shipping Coherence logs to Elasticsearch and importing the Coherence dashboards into Kibana. If this approach does not meet your needs you are obviously free to configure an alternative. Enabling Log Capture Capturing and viewing Coherence cluster Logs in Elasticsearch using a Fluentd side-car. Kibana Dashboards Importing and using the Kibana Dashboards available. ",
            "title": "Overview"
        },
        {
            "location": "/ports/020_container_ports",
            "text": " Exposing the Coherence metrics port or Coherence Management over REST port are treated as a special case in the configuration. Normally both the port&#8217;s name and port value are required fields. If the port name is metrics or management the Operator already knows the port values (either from the defaults or from the metrics or management configuration) so these do not need to be specified again. For example, if the Coherence resource above also exposed Coherence metrics and management it might look like this: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: metrics: enabled: true port: 9876 management: enabled: true port: 1234 ports: - name: rest port: 8080 - name: metrics - name: management The rest port is not a special case and must have a port defined, in this case 8080 . The metrics port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.metrics section to be 9876. The management port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.management section to be 1234. If the port value is not set in coherence.metrics.port or in coherence.management.port then the Operator will use the defaults for these values, 9612 for metrics and 30000 for management. ",
            "title": "Metrics &amp; Management Ports"
        },
        {
            "location": "/ports/020_container_ports",
            "text": " Except for rare cases most applications deployed into a Kubernetes cluster will need to expose ports that they provide services on to other applications. This is covered in the Kubernetes documentation, Connect Applications with Services The Coherence CRD makes it simple to expose ports and configure their services. The CRD contains a field named ports , which is an array of named ports. In the most basic configuration the only required values are the name and port to expose, for example: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 This example exposes a single port named rest on port 8080 . When the example above is deployed the Coherence Operator will add configure the ports for the Coherence container in the Pods to expose that port and will also create a Service for the port. For example, the relevant snippet of the StatefulSet configuration would be: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster spec: template: spec: containers: - name: coherence ports: - name: rest containerPort: 8080 The Operator has added the rest port to the coherence containers port list. The name field in the Coherence CRD&#8217;s port spec maps to the name field in the Container port spec. The port field in the Coherence CRD&#8217;s port spec maps to the containerPort in the Container port spec. For each additional port the Operator will create a Service of tyep ClusterIP with a default configuration. The name of the service will be the Coherence resource&#8217;s name with the port name appended to it, so in this case it will be test-cluster-rest . The Service might look like this: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-rest spec: ports: - name: rest port: 8080 targetPort: rest type: ClusterIP selector: coherenceDeployment: test-cluster coherenceCluster: test-cluster coherenceRole: storage coherenceComponent: coherencePod The Service name will be automatically generated (this can be overridden). The ports section will have just the single port being exposed by this service with the same name as the port. The port exposed by the Service will be the same as the container port value (this can be overridden). The target port will be set to the port being exposed from the container. The default Service type is ClusterIP (this can be overridden). A selector will be created to match the Pods in the Coherence resource. The Coherence CRD spec allows port and service to be further configured and allows a Prometheus ServiceMonitor to be created for the port if that port is to expose metrics. See also: Configure Services for Ports Prometheus ServiceMonitors Metrics &amp; Management Ports Exposing the Coherence metrics port or Coherence Management over REST port are treated as a special case in the configuration. Normally both the port&#8217;s name and port value are required fields. If the port name is metrics or management the Operator already knows the port values (either from the defaults or from the metrics or management configuration) so these do not need to be specified again. For example, if the Coherence resource above also exposed Coherence metrics and management it might look like this: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: metrics: enabled: true port: 9876 management: enabled: true port: 1234 ports: - name: rest port: 8080 - name: metrics - name: management The rest port is not a special case and must have a port defined, in this case 8080 . The metrics port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.metrics section to be 9876. The management port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.management section to be 1234. If the port value is not set in coherence.metrics.port or in coherence.management.port then the Operator will use the defaults for these values, 9612 for metrics and 30000 for management. ",
            "title": "Additional Container Ports"
        },
        {
            "location": "/ports/020_container_ports",
            "text": " The only mandatory fields when adding a port to a Coherence resource are the name and port number. There are a number of optional fields, which when not specified use the Kubernetes default values. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 protocol: TCP hostIP: 10.10.1.19 hostPort: 1000 nodePort: 5000 The additional fields, protocol , hostIP , hostPort have the same meaning and same defaults in the Coherence CRD port spec as they have in a Kubernetes container port (see the Kubernetes ContainerPort API reference). These fields map directly from the Coherence CRD port spec to the container port spec. The example above would create a container port shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster spec: template: spec: containers: - name: coherence ports: - name: rest containerPort: 8080 protocol: TCP hostIP: 10.10.1.19 hostPort: 1000 The nodePort field in the Coherence CRD port spec maps to the nodePort field in the Service port spec. The nodePort is described in the Kubernetes ServicePort API reference. The Coherence CRD example above with nodePort set would create a Service with the same nodePort value: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-rest spec: ports: - name: rest port: 8080 targetPort: rest nodePort: 5000 type: ClusterIP selector: coherenceDeployment: test-cluster coherenceCluster: test-cluster coherenceRole: storage coherenceComponent: coherencePod ",
            "title": "Configuring the Port"
        },
        {
            "location": "/other/080_add_containers",
            "text": " To add a container to the Pods specify the container in the sideCars list in the Coherence CRD spec. See the Logging Documentation for a bigger example of adding a side-car container. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" An additional container named fluentd has been added to the CRD spec. The containers will added to the sideCars will be added to the Pods exactly as configured. Any configuration that is valid in a Kubernetes Container Spec may be added to an entry in sideCars ",
            "title": "Add a Container"
        },
        {
            "location": "/other/080_add_containers",
            "text": " Just like normal containers above, additional init-containers can also be added to the Pods . To add an init-container to the Pods specify the container in the initContainers list in the Coherence CRD spec. As with containers, for init-containers any configuration that is valid in a Kubernetes Container Spec may be added to an entry in initContainers For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: initContainers: - name: setup image: \"app-setup:1.0.0\" An additional init-container named setup has been added to the CRD spec with an image named app-setup:1.0.0 . ",
            "title": "Add an Init-Container"
        },
        {
            "location": "/other/080_add_containers",
            "text": " Additional containers and init-containers can easily be added to a Coherence resource Pod. There are two types of container that can be added, init-containers and normal containers. An example use case for this would be to add something like a Fluentd side-car container to ship logs to Elasticsearch. A note about Volumes: The Operator created a number of volumes and volume mounts by default. These default volume mounts will be added to all containers in the Pod including containers added as described here. Any additional volumes and volume mounts added to the Coherence resource spec will also be added all containers. Add a Container To add a container to the Pods specify the container in the sideCars list in the Coherence CRD spec. See the Logging Documentation for a bigger example of adding a side-car container. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" An additional container named fluentd has been added to the CRD spec. The containers will added to the sideCars will be added to the Pods exactly as configured. Any configuration that is valid in a Kubernetes Container Spec may be added to an entry in sideCars Add an Init-Container Just like normal containers above, additional init-containers can also be added to the Pods . To add an init-container to the Pods specify the container in the initContainers list in the Coherence CRD spec. As with containers, for init-containers any configuration that is valid in a Kubernetes Container Spec may be added to an entry in initContainers For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: initContainers: - name: setup image: \"app-setup:1.0.0\" An additional init-container named setup has been added to the CRD spec with an image named app-setup:1.0.0 . ",
            "title": "Configure Additional Containers"
        },
        {
            "location": "/management/040_ssl",
            "text": " It is possible to configure Management over REST endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.metrics.ssl section of the CRD spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: management: enabled: true ssl: enabled: true keyStore: metrics-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: metrics-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: metrics-secret The enabled field when set to true enables SSL for metrics or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. The various files and keystores referred to in the configuration above can be any location accessible in the image used by the coherence container in the deployment&#8217;s Pods . Typically, for things such as SSL keys and certs, these would be provided by obtained from Secrets loaded as additional Pod Volumes . See Add Secrets Volumes for the documentation on how to specify secrets as additional volumes. ",
            "title": "SSL with Management over REST"
        },
        {
            "location": "/coherence/040_override_file",
            "text": " The name of the Coherence operations configuration file (commonly called the overrides file) that the Coherence processes in a Coherence resource will use can be set with the spec.coherence.overrideConfig field. By setting this field the coherence.override system property will be set in the Coherence JVM. When the spec.coherence.overrideConfig is blank or not specified, Coherence use its default behaviour to find the operational configuration file to use. Typically, this is to use the first occurrence of tangosol-coherence-override.xml that is found on the classpath (consult the Coherence documentation for an explanation of the default behaviour). To set a specific operational configuration file to use set the spec.coherence.overrideConfig field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: overrideConfig: test-override.xml The spec.coherence.overrideConfig field has been set to test-override.xml which will effectively pass -Dcoherence.override=test-override.xml to the JVM command line. ",
            "title": "Set the Operational Configuration File Name"
        },
        {
            "location": "/installation/06_openshift",
            "text": " Whilst the Coherence Operator will run out of the box on OpenShift some earlier versions of the Coherence Docker image will not work without configuration changes. These earlier versions of the Coherence Docker images that Oracle publishes default the container user as oracle . When running the Oracle images or layered images that retain the default user as oracle with OpenShift, the anyuid security context constraint is required to ensure proper access to the file system within the Docker image. Later versions of the Coherence images have been modified to work without needing anyuid . To work with older image versions , the administrator must: Ensure the anyuid security content is granted Ensure that Coherence containers are annotated with openshift.io/scc: anyuid For example, to update the OpenShift policy, use: <markup lang=\"bash\" >oc adm policy add-scc-to-user anyuid -z default and to annotate the Coherence containers, update the Coherence resource to include annotations For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: annotations: openshift.io/scc: anyuid The openshift.io/scc: anyuid annotation will be applied to all of the Coherence Pods. For additional information about OpenShift requirements see the OpenShift documentation ",
            "title": "Coherence Clusters on OpenShift"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " To deploy a Coherence resource with management over REST enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true ports: - name: management Setting the coherence.management.enabled field to true will enable Management over REST To expose Management over REST via a Service it is added to the ports list. The management port is a special case where the port number is optional so in this case Management over REST will bind to the default port 30000 . (see Exposing Ports for details) To expose Management over REST on a different port the alternative port value can be set in the coherence.management section, for example: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true port: 8080 ports: - name: management Management over REST will now be exposed on port 8080 ",
            "title": "Deploy Coherence with Management over REST Enabled"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " After installing the basic management-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f management-cluster.yaml coherence.coherence.oracle.com/management-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=management-cluster NAME READY STATUS RESTARTS AGE management-cluster-0 1/1 Running 0 36s management-cluster-1 1/1 Running 0 36s management-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward management-cluster-0 30000:30000 Forwarding from [::1]:30000 -&gt; 30000 Forwarding from 127.0.0.1:30000 -&gt; 30000 ",
            "title": "Port-forward the Management over REST Port"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Now that a port is being forwarded from localhost to a Pod in the cluster the Management over REST endpoints can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/ Which should result in a response similar to the following: <markup lang=\"json\" >{ \"links\": [ { \"rel\": \"parent\", \"href\": \"http://127.0.0.1:30000/management/coherence\" }, { \"rel\": \"self\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"canonical\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"services\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/services\" }, { \"rel\": \"caches\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/caches\" }, { \"rel\": \"members\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/members\" }, { \"rel\": \"management\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/management\" }, { \"rel\": \"journal\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/journal\" }, { \"rel\": \"hotcache\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/hotcache\" }, { \"rel\": \"reporters\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\": \"webApplications\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/webApplications\" } ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2019-10-15T03:55:46.461Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"14.1.1.0.0\", \"running\": true, \"clusterName\": \"management-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2019-10-15 03:46:15.848, Address=10.1.2.184:36531, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-1, Role=storage)\", \"Member(Id=2, Timestamp=2019-10-15 03:46:19.405, Address=10.1.2.183:40341, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-2, Role=storage)\", \"Member(Id=3, Timestamp=2019-10-15 03:46:19.455, Address=10.1.2.185:38719, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-0, Role=storage)\" ], \"type\": \"Cluster\" } ",
            "title": "Access the REST Endpoint"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Issue the following curl command to access the Sagger endpoint, which documents all the REST API&#8217;s available. <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog Which should result in a response like the following: <markup lang=\"json\" >{ \"swagger\": \"2.0\", \"info\": { \"title\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"description\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"version\": \"14.1.1.0.0\" }, \"schemes\": [ \"http\", \"https\" ], ... The above output has been truncated due to the large size. ",
            "title": "Access the Swagger Endpoint"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Management over REST can be used for all Coherence management functions, the same as would be available when using standard MBean access over JMX. Please see the Coherence REST API for more information on these features. Connecting JVisualVM to Management over REST Enabling SSL Produce and extract a Java Flight Recorder (JFR) file Access the Reporter ",
            "title": "Other REST Resources"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Since version 12.2.1.4 Coherence has had functionality to expose a management API over REST. The Management over REST API is disabled by default in Coherence clusters but can be enabled and configured by setting the relevant fields in the Coherence CRD. The example below shows how to enable and access Coherence MBeans using Management over REST. Once the Management port has been exposed, for example via a load balancer or port-forward command, the REST endpoint is available at http://host:port/management/coherence/cluster . The Swagger JSON document for the API is available at http://host:port/management/coherence/cluster/metadata-catalog . See the REST API for Managing Oracle Coherence documentation for full details on each of the endpoints. Note: Use of Management over REST is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. Deploy Coherence with Management over REST Enabled To deploy a Coherence resource with management over REST enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true ports: - name: management Setting the coherence.management.enabled field to true will enable Management over REST To expose Management over REST via a Service it is added to the ports list. The management port is a special case where the port number is optional so in this case Management over REST will bind to the default port 30000 . (see Exposing Ports for details) To expose Management over REST on a different port the alternative port value can be set in the coherence.management section, for example: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true port: 8080 ports: - name: management Management over REST will now be exposed on port 8080 Port-forward the Management over REST Port After installing the basic management-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f management-cluster.yaml coherence.coherence.oracle.com/management-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=management-cluster NAME READY STATUS RESTARTS AGE management-cluster-0 1/1 Running 0 36s management-cluster-1 1/1 Running 0 36s management-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward management-cluster-0 30000:30000 Forwarding from [::1]:30000 -&gt; 30000 Forwarding from 127.0.0.1:30000 -&gt; 30000 Access the REST Endpoint Now that a port is being forwarded from localhost to a Pod in the cluster the Management over REST endpoints can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/ Which should result in a response similar to the following: <markup lang=\"json\" >{ \"links\": [ { \"rel\": \"parent\", \"href\": \"http://127.0.0.1:30000/management/coherence\" }, { \"rel\": \"self\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"canonical\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"services\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/services\" }, { \"rel\": \"caches\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/caches\" }, { \"rel\": \"members\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/members\" }, { \"rel\": \"management\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/management\" }, { \"rel\": \"journal\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/journal\" }, { \"rel\": \"hotcache\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/hotcache\" }, { \"rel\": \"reporters\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\": \"webApplications\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/webApplications\" } ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2019-10-15T03:55:46.461Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"14.1.1.0.0\", \"running\": true, \"clusterName\": \"management-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2019-10-15 03:46:15.848, Address=10.1.2.184:36531, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-1, Role=storage)\", \"Member(Id=2, Timestamp=2019-10-15 03:46:19.405, Address=10.1.2.183:40341, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-2, Role=storage)\", \"Member(Id=3, Timestamp=2019-10-15 03:46:19.455, Address=10.1.2.185:38719, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-0, Role=storage)\" ], \"type\": \"Cluster\" } Access the Swagger Endpoint Issue the following curl command to access the Sagger endpoint, which documents all the REST API&#8217;s available. <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog Which should result in a response like the following: <markup lang=\"json\" >{ \"swagger\": \"2.0\", \"info\": { \"title\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"description\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"version\": \"14.1.1.0.0\" }, \"schemes\": [ \"http\", \"https\" ], ... The above output has been truncated due to the large size. Other REST Resources Management over REST can be used for all Coherence management functions, the same as would be available when using standard MBean access over JMX. Please see the Coherence REST API for more information on these features. Connecting JVisualVM to Management over REST Enabling SSL Produce and extract a Java Flight Recorder (JFR) file Access the Reporter ",
            "title": "Management over REST"
        },
        {
            "location": "/management/030_visualvm",
            "text": " VisualVM is a visual tool integrating commandline JDK tools and lightweight profiling capabilities, designed for both development and production time use. ",
            "title": "preambule"
        },
        {
            "location": "/management/030_visualvm",
            "text": " Install the Coherence Operator Ensure you have installed the Coherence Operator using the Install Guide . Download the JMXMP connector JAR The JMX endpoint does not use RMI, instead it uses JMXMP. This requires an additional JAR on the classpath of the Java JMX client (VisualVM and JConsole). You can use curl to download the required JAR. <markup lang=\"bash\" >curl http://central.maven.org/maven2/org/glassfish/external/opendmk_jmxremote_optional_jar/1.0-b01-ea/opendmk_jmxremote_optional_jar-1.0-b01-ea.jar \\ -o opendmk_jmxremote_optional_jar-1.0-b01-ea.jar This jar can also be downloaded as a Maven dependency if you are connecting through a Maven project. <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;org.glassfish.external&lt;/groupId&gt; &lt;artifactId&gt;opendmk_jmxremote_optional_jar&lt;/artifactId&gt; &lt;version&gt;1.0-b01-ea&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Prerequisites"
        },
        {
            "location": "/management/030_visualvm",
            "text": " In this example a simple minimal cluster will be created running the MBean server. <markup lang=\"yaml\" title=\"cluster-with-jmx.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: jvm: args: - -Dcoherence.management=all - -Dcoherence.management.remote=true - -Dcom.sun.management.jmxremote.ssl=false - -Dcom.sun.management.jmxremote.authenticate=false jmxmp: enabled: true port: 9099 ports: - name: jmx port: 9099 Additional system properties are added to enable Coherence management See the Coherence Management Documentation JMXMP is enabled on port 9099 so that a reliable JMX connection can be made to the MBean server from outside the Pods The JMXMP port is exposed as an additional port The example cluster-with-jmx.yaml can be installed into Kubernetes with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f cluster-with-jmx.yaml This should install the cluster into the namespace coherence-test with a default replica count of three, resulting in a StatefulSet with three Pods . ",
            "title": "Install a JMX Enabled Coherence Cluster"
        },
        {
            "location": "/management/030_visualvm",
            "text": " After installing the basic cluster-with-jmx.yaml from the example above there would be a three member Coherence cluster installed into Kubernetes. The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=test-cluster NAME READY STATUS RESTARTS AGE test-cluster-0 1/1 Running 0 36s test-cluster-1 1/1 Running 0 36s test-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward test-cluster-0 9099:9099 Forwarding from [::1]:9099 -&gt; 9099 Forwarding from 127.0.0.1:9099 -&gt; 9099 JMX can now be access using the URL service:jmx:jmxmp://127.0.0.1:9099 ",
            "title": "Port Forward the MBean Server Pod:"
        },
        {
            "location": "/management/030_visualvm",
            "text": " Run JConsole with the JMXMP connector on the classpath: <markup lang=\"bash\" >jconsole -J-Djava.class.path=\"$JAVA_HOME/lib/jconsole.jar:$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" service:jmx:jmxmp://127.0.0.1:9099 In the console UI, select the MBeans tab and then Coherence Cluster attributes. You should see the Coherence MBeans as shown below: ",
            "title": "Access MBeans Through JConsole"
        },
        {
            "location": "/management/030_visualvm",
            "text": " Ensure you run VisualVM with the JMXMP connector on the classpath: <markup lang=\"bash\" >jvisualvm -cp \"$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" If you have downloaded VisualVM separately (as VisualVM has not been part of the JDK from Java 9 onwards), then the executable is visualvm (or on MacOS it is /Applications/VisualVM.app/Contents/MacOS/visualvm ). From the VisualVM menu select File / Add JMX Connection Enter service:jmx:jmxmp://127.0.0.1:9099 for the Connection value and click OK . A JMX connection should be added under the Local section of the left hand panel. Double-click the new local connection to connect to the management Pod . You can see the Coherence MBeans under the MBeans tab. If you have installed the Coherence VisualVM plugin, you can also see a Coherence tab. Refer to the Coherence MBean Reference for detailed information about Coherence MBeans. ",
            "title": "Access MBeans Through VisualVM"
        },
        {
            "location": "/management/030_visualvm",
            "text": " Coherence management is implemented using Java Management Extensions (JMX). JMX is a Java standard for managing and monitoring Java applications and services. VisualVM and other JMX tools can be used to manage and monitor Coherence Clusters via JMX. The default transport used by JMX is RMI but RMI can be difficult to set-up reliably in containers and Kubernetes so that it can be accessed externally due to its use of multiple TCP ports that are difficult to configure and it does not work well with the NAT&#8217;ed type of networking typically found in these environments. JMXMP on the other hand is an alternative to RMI that does work well in containers and only requires a single TCP port. This example shows how to connect to a cluster via JMX over JMXMP. As an alternative to JMX see Management over REST for how to connect to a cluster via the VisualVM plugin using REST. See the Coherence Management Documentation for more information on JMX and Management. Prerequisites Install the Coherence Operator Ensure you have installed the Coherence Operator using the Install Guide . Download the JMXMP connector JAR The JMX endpoint does not use RMI, instead it uses JMXMP. This requires an additional JAR on the classpath of the Java JMX client (VisualVM and JConsole). You can use curl to download the required JAR. <markup lang=\"bash\" >curl http://central.maven.org/maven2/org/glassfish/external/opendmk_jmxremote_optional_jar/1.0-b01-ea/opendmk_jmxremote_optional_jar-1.0-b01-ea.jar \\ -o opendmk_jmxremote_optional_jar-1.0-b01-ea.jar This jar can also be downloaded as a Maven dependency if you are connecting through a Maven project. <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;org.glassfish.external&lt;/groupId&gt; &lt;artifactId&gt;opendmk_jmxremote_optional_jar&lt;/artifactId&gt; &lt;version&gt;1.0-b01-ea&lt;/version&gt; &lt;/dependency&gt; Install a JMX Enabled Coherence Cluster In this example a simple minimal cluster will be created running the MBean server. <markup lang=\"yaml\" title=\"cluster-with-jmx.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: jvm: args: - -Dcoherence.management=all - -Dcoherence.management.remote=true - -Dcom.sun.management.jmxremote.ssl=false - -Dcom.sun.management.jmxremote.authenticate=false jmxmp: enabled: true port: 9099 ports: - name: jmx port: 9099 Additional system properties are added to enable Coherence management See the Coherence Management Documentation JMXMP is enabled on port 9099 so that a reliable JMX connection can be made to the MBean server from outside the Pods The JMXMP port is exposed as an additional port The example cluster-with-jmx.yaml can be installed into Kubernetes with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f cluster-with-jmx.yaml This should install the cluster into the namespace coherence-test with a default replica count of three, resulting in a StatefulSet with three Pods . Port Forward the MBean Server Pod: After installing the basic cluster-with-jmx.yaml from the example above there would be a three member Coherence cluster installed into Kubernetes. The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=test-cluster NAME READY STATUS RESTARTS AGE test-cluster-0 1/1 Running 0 36s test-cluster-1 1/1 Running 0 36s test-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward test-cluster-0 9099:9099 Forwarding from [::1]:9099 -&gt; 9099 Forwarding from 127.0.0.1:9099 -&gt; 9099 JMX can now be access using the URL service:jmx:jmxmp://127.0.0.1:9099 Access MBeans Through JConsole Run JConsole with the JMXMP connector on the classpath: <markup lang=\"bash\" >jconsole -J-Djava.class.path=\"$JAVA_HOME/lib/jconsole.jar:$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" service:jmx:jmxmp://127.0.0.1:9099 In the console UI, select the MBeans tab and then Coherence Cluster attributes. You should see the Coherence MBeans as shown below: Access MBeans Through VisualVM Ensure you run VisualVM with the JMXMP connector on the classpath: <markup lang=\"bash\" >jvisualvm -cp \"$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" If you have downloaded VisualVM separately (as VisualVM has not been part of the JDK from Java 9 onwards), then the executable is visualvm (or on MacOS it is /Applications/VisualVM.app/Contents/MacOS/visualvm ). From the VisualVM menu select File / Add JMX Connection Enter service:jmx:jmxmp://127.0.0.1:9099 for the Connection value and click OK . A JMX connection should be added under the Local section of the left hand panel. Double-click the new local connection to connect to the management Pod . You can see the Coherence MBeans under the MBeans tab. If you have installed the Coherence VisualVM plugin, you can also see a Coherence tab. Refer to the Coherence MBean Reference for detailed information about Coherence MBeans. ",
            "title": "Access A Coherence Cluster via VisualVM"
        },
        {
            "location": "/ordering/010_overview",
            "text": " The startQuorum can specify a dependency on more than on deployment; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: data - deployment: proxy podCount: 1 The data and proxy deployments do not specify a startQuorum , so the StatefulSets for these deployments will be created immediately by the operator. The web deployment has a startQuorum the defines a dependency on both the data deployment and the proxy deployment. The proxy dependency also specifies a podCount of 1 . This means that the operator wil not create the web role&#8217;s StatefulSet until all 5 replicas of the data deployment are Ready and at least 1 of the proxy deployment&#8217;s Pods is Ready . ",
            "title": "Multiple Dependencies"
        },
        {
            "location": "/ordering/010_overview",
            "text": " It is also possible to chain dependencies, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 startQuorum: - deployment: data --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: proxy podCount: 1 The data deployment does not specify a startQuorum so this deployment&#8217;s StatefulSet will be created immediately by the operator. The proxy deployment defines a dependency on the data deployment without a podCount so all five Pods of the data role must be in a Ready state before the operator will create the proxy deployment&#8217;s StatefulSet . The web deployment depends on the proxy deployment with a podCount of one, so the operator will not create the web deployment&#8217;s StatefulSet until at least one proxy deployment Pod is in a Ready state. The operator does not validate that a startQuorum makes sense. It is possible to declare a quorum with circular dependencies, in which case the roles will never start. It would also be possible to create a quorum with a podCount greater than the replicas value of the dependent deployment, in which case the quorum would never be met, and the role would not start. ",
            "title": "Chained Dependencies"
        },
        {
            "location": "/ordering/010_overview",
            "text": " The default behaviour of the operator is to create the StatefulSet for a Coherence deployment immediately. Sometimes this behaviour is not suitable if, for example, when application code running in one deployment depends on the availability of another deployment. Typically, this might be storage disabled members having functionality that relies on the storage members being ready first. The Coherence CRD allows can be configured with a startQuorum that defines a deployment&#8217;s dependency on other deployments in the cluster. The startQuorum only applies when a cluster is initially being started by the operator, it does not apply in other functions such as upgrades, scaling, shut down etc. An individual deployment can depend on one or more other deployment. The dependency can be such that the deployment will not be created until all of the Pods of the dependent deployment are ready, or it can be configured so that just a single Pod of the dependent deployment must be ready. For example: In the yaml snippet below there are two Coherence deployments, data and proxy <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: startQuorum: - deployment: data podCount: 1 The data deployment does not specify a startQuorum so this role will be created immediately by the operator. The proxy deployment has a start quorum that means that the proxy deployment depends on the data deployment. The podCount field has been set to 1 meaning the proxy deployment&#8217;s StatefulSet will not be created until at least 1 of the data deployment&#8217;s Pods is in the Ready state. Omitting the podCount from the quorum means that the role will not start until all the configured replicas of the dependent deployment are ready; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: startQuorum: - deployment: data The proxy deployment&#8217;s startQuorum just specifies a dependency on the data deployment with no podCount so all 3 of the data deployment&#8217;s Pods must be Ready before the proxy deployment&#8217;s StatefulSet is created by the operator. Setting a podCount less than or equal to zero is the same as not specifying a count. Multiple Dependencies The startQuorum can specify a dependency on more than on deployment; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: data - deployment: proxy podCount: 1 The data and proxy deployments do not specify a startQuorum , so the StatefulSets for these deployments will be created immediately by the operator. The web deployment has a startQuorum the defines a dependency on both the data deployment and the proxy deployment. The proxy dependency also specifies a podCount of 1 . This means that the operator wil not create the web role&#8217;s StatefulSet until all 5 replicas of the data deployment are Ready and at least 1 of the proxy deployment&#8217;s Pods is Ready . Chained Dependencies It is also possible to chain dependencies, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 startQuorum: - deployment: data --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: proxy podCount: 1 The data deployment does not specify a startQuorum so this deployment&#8217;s StatefulSet will be created immediately by the operator. The proxy deployment defines a dependency on the data deployment without a podCount so all five Pods of the data role must be in a Ready state before the operator will create the proxy deployment&#8217;s StatefulSet . The web deployment depends on the proxy deployment with a podCount of one, so the operator will not create the web deployment&#8217;s StatefulSet until at least one proxy deployment Pod is in a Ready state. The operator does not validate that a startQuorum makes sense. It is possible to declare a quorum with circular dependencies, in which case the roles will never start. It would also be possible to create a quorum with a podCount greater than the replicas value of the dependent deployment, in which case the quorum would never be met, and the role would not start. ",
            "title": "Coherence Deployment Dependencies and Start Order"
        },
        {
            "location": "/other/110_readiness",
            "text": " The default endpoint used by the Operator for readiness checks that the Pod is a member of the Coherence cluster and that none of the partitioned cache services have a StatusHA value of endangered . If the Pod is the only cluster member at the time of the ready check the StatusHA check will be skipped. If a partitioned service has a backup count of zero the StatusHA check will be skipped for that service. There are scenarios where the StatusHA check can fail but should be ignored because the application does not care about data loss for caches on that particular cache service. Normally in this case the backup count for the cache service would be zero, and the service would automatically be skipped in the StatusHA test. The ready check used by the Operator can be configured to skip the StatusHA test for certain services. In the Coherence CRD the coherence.allowEndangeredForStatusHA is a list of string values that can be set to the names of partitioned cache services that should not be included in the StatusHA check. For a service to be skipped its name must exactly match one of the names in the allowEndangeredForStatusHA list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: allowEndangeredForStatusHA: - TempService The allowEndangeredForStatusHA field is a list of string values. In this case the TempService will not be checked for StatusHA in the ready check. ",
            "title": "Coherence Readiness"
        },
        {
            "location": "/other/110_readiness",
            "text": " The Coherence CRD spec.readinessProbe field is identical to configuring a readiness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: readinessProbe: httpGet: port: 8080 path: \"/ready\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for readiness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . ",
            "title": "Configure Readiness"
        },
        {
            "location": "/other/110_readiness",
            "text": " The Coherence CRD spec.livenessProbe field is identical to configuring a liveness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: livenessProbe: httpGet: port: 8080 path: \"/live\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for liveness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . ",
            "title": "Configure Liveness"
        },
        {
            "location": "/other/110_readiness",
            "text": " The Coherence Operator injects a Readiness/Liveness endpoint into the Coherence container that is used as the default readiness and liveness check for the Pods deployed by the operator. This endpoint is suitable for most use-cases, but it is possible to configure a different readiness and liveness probe, or just change the timings of the probes if required. Coherence Readiness The default endpoint used by the Operator for readiness checks that the Pod is a member of the Coherence cluster and that none of the partitioned cache services have a StatusHA value of endangered . If the Pod is the only cluster member at the time of the ready check the StatusHA check will be skipped. If a partitioned service has a backup count of zero the StatusHA check will be skipped for that service. There are scenarios where the StatusHA check can fail but should be ignored because the application does not care about data loss for caches on that particular cache service. Normally in this case the backup count for the cache service would be zero, and the service would automatically be skipped in the StatusHA test. The ready check used by the Operator can be configured to skip the StatusHA test for certain services. In the Coherence CRD the coherence.allowEndangeredForStatusHA is a list of string values that can be set to the names of partitioned cache services that should not be included in the StatusHA check. For a service to be skipped its name must exactly match one of the names in the allowEndangeredForStatusHA list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: allowEndangeredForStatusHA: - TempService The allowEndangeredForStatusHA field is a list of string values. In this case the TempService will not be checked for StatusHA in the ready check. Configure Readiness The Coherence CRD spec.readinessProbe field is identical to configuring a readiness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: readinessProbe: httpGet: port: 8080 path: \"/ready\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for readiness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . Configure Liveness The Coherence CRD spec.livenessProbe field is identical to configuring a liveness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: livenessProbe: httpGet: port: 8080 path: \"/live\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for liveness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . ",
            "title": "Readiness &amp; Liveness Probes"
        },
        {
            "location": "/logging/020_logging",
            "text": " Whilst any valid Java util logging configuration file may be used, the Coherence Operator injects a default logging configuration file into the coherence container that can be used to configure the logger to write logs to files under the /logs directory. The log files will have the name coherence-%g.log , where %g is the log file generation created as logs get rotated. This file will be injected into the container at /coherence-operator/utils/logging/logging.properties and will look something like this: <markup >com.oracle.coherence.handlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler com.oracle.coherence.level=FINEST java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.ConsoleHandler.level=FINEST java.util.logging.FileHandler.pattern=/logs/coherence-%g.log java.util.logging.FileHandler.limit=10485760 java.util.logging.FileHandler.count=50 java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format=%5$s%6$s%n To configure Cohrence and the logger some system properties need to be added to the jvm.args field of the Coherence CRD spec: For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" Coherence has been configured to use the Java util logging. The Coherence logger name has been set to com.oracle.coherence , which matches the logging configuration file. The Java util logging configuration file is set to the file injected by the Operator. ",
            "title": "Operator Provided Logging Configuration File"
        },
        {
            "location": "/logging/020_logging",
            "text": " The logging configuration above configures Coherence to write logs to the /logs directory. For this location to be accessible to both the coherence container and to the fluentd container it needs to be created as a Volume in the Pod and mounted to both containers. As this Volume can be ephemeral and is typically not required to live longer than the Pod the simplest type of Volume to use is an emptyDir volume source. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs An additional empty-dir Volume named logs has been added to the Coherence spec. The logs volume will be mounted at /logs in all containers in the Pod . ",
            "title": "Log Files Volume"
        },
        {
            "location": "/logging/020_logging",
            "text": " Coherence will log to the console by default so to be able to ship logs to Elasticsearch it needs to be configured to write to log files. One way to do this is to add a Java Util Logging configuration file and then to configure Coherence to use the JDK logger. In the jvm.args section of the Coherence CRD the system properties should be added to set the configuration file used by Java util logging and to configure Coherence logging. See the Coherence Logging Config documentation for more details. There are alternative ways to configure the Java util logger besides using a configuration file, just as there are alternative logging frameworks that Coherence can be configured to use to produce log files. This example is going to use Java util logging as that is the simplest to demonstrate without requiring any additional logging libraries. Operator Provided Logging Configuration File Whilst any valid Java util logging configuration file may be used, the Coherence Operator injects a default logging configuration file into the coherence container that can be used to configure the logger to write logs to files under the /logs directory. The log files will have the name coherence-%g.log , where %g is the log file generation created as logs get rotated. This file will be injected into the container at /coherence-operator/utils/logging/logging.properties and will look something like this: <markup >com.oracle.coherence.handlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler com.oracle.coherence.level=FINEST java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.ConsoleHandler.level=FINEST java.util.logging.FileHandler.pattern=/logs/coherence-%g.log java.util.logging.FileHandler.limit=10485760 java.util.logging.FileHandler.count=50 java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format=%5$s%6$s%n To configure Cohrence and the logger some system properties need to be added to the jvm.args field of the Coherence CRD spec: For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" Coherence has been configured to use the Java util logging. The Coherence logger name has been set to com.oracle.coherence , which matches the logging configuration file. The Java util logging configuration file is set to the file injected by the Operator. Log Files Volume The logging configuration above configures Coherence to write logs to the /logs directory. For this location to be accessible to both the coherence container and to the fluentd container it needs to be created as a Volume in the Pod and mounted to both containers. As this Volume can be ephemeral and is typically not required to live longer than the Pod the simplest type of Volume to use is an emptyDir volume source. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs An additional empty-dir Volume named logs has been added to the Coherence spec. The logs volume will be mounted at /logs in all containers in the Pod . ",
            "title": "Configure Coherence to Log to Files"
        },
        {
            "location": "/logging/020_logging",
            "text": " The ConfigMap used to provide the Fluentd configuration might look something like this: <markup lang=\"yaml\" >apiVersion: v1 kind: ConfigMap metadata: name: efk-config labels: component: coherence-efk-config data: fluentd-coherence.conf: | # Ignore fluentd messages &lt;match fluent.**&gt; @type null &lt;/match&gt; # Coherence Logs &lt;source&gt; @type tail path /logs/coherence-*.log pos_file /tmp/cohrence.log.pos read_from_head true tag coherence-cluster multiline_flush_interval 20s &lt;parse&gt; @type multiline format_firstline /^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3}/ format1 /^(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3})\\/(?&lt;uptime&gt;[0-9\\.]+) (?&lt;product&gt;.+) &lt;(?&lt;level&gt;[^\\s]+)&gt; \\(thread=(?&lt;thread&gt;.+), member=(?&lt;member&gt;.+)\\):[\\S\\s](?&lt;log&gt;.*)/ &lt;/parse&gt; &lt;/source&gt; &lt;filter coherence-cluster&gt; @type record_transformer &lt;record&gt; cluster \"#{ENV['COH_CLUSTER_NAME']}\" role \"#{ENV['COH_ROLE']}\" host \"#{ENV['HOSTNAME']}\" pod-uid \"#{ENV['COH_POD_UID']}\" &lt;/record&gt; &lt;/filter&gt; &lt;match coherence-cluster&gt; @type elasticsearch hosts \"http://elasticsearch-master:9200\" logstash_format true logstash_prefix coherence-cluster &lt;/match&gt; The name of the ConfigMap is efk-config to match the name specified in the Coherence CRD spec. The source section is configured to match log files with the name /logs/coherence-*.log , which is the name that Coherence logging has been configured to use. The pattern in the source section is a Fluentd pattern that matches the standard Coherence log message format. A filter section will add additional fields to the log message. These come from the environment variables that the Operator will inject into all containers in the Pod. In this case the Coherence cluster name, the Coherence role name, the Pod host name and Pod UID. The final section tells Fluentd how to ship the logs to Elasticsearch, in this case to the endpoint http://elasticsearch-master:9200 There are many ways to configure Fluentd, the example above is just one way and is in fact taken from one of the Operator&#8217;s functional tests. With the efk-config ConfigMap created in the same namespace as the Coherence resource the Coherence logs from the containers will now be shipped to Elasticsearch. ",
            "title": "The Fluentd Configuration File"
        },
        {
            "location": "/logging/020_logging",
            "text": " With Coherence configured to write to log files, and those log files visible to other containers in the Pod the Fluentd side-car container can be added. The example yaml below shows a Coherence resource with the additional Fluentd container added. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs sideCars: - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3\" args: - \"-c\" - \"/etc/fluent.conf\" env: - name: \"FLUENTD_CONF\" value: \"fluentd-coherence.conf\" - name: \"FLUENT_ELASTICSEARCH_SED_DISABLE\" value: \"true\" configMapVolumes: - name: \"efk-config\" mountPath: \"/fluentd/etc/fluentd-coherence.conf\" subPath: \"fluentd-coherence.conf\" The fluentd container has been added to the sideCars list. This will create another container in the Pod exactly as configured. The FLUENTD_CONF environment variable has been set to the name of the configuration file that Fluentd should use. The standard Fluentd behaviour is to locate this file in the /fluentd/etc/ directory. The FLUENT_ELASTICSEARCH_SED_DISABLE environment variable has been set to work around a known issue here . An additional volume has been added from a ConfigMap named efk-config , that contains the Fluentd configuration to use. This will be mounted to the fluentd container at /fluentd/etc/fluentd-coherence.conf , which corresponds to the name of the file set in the FLUENTD_CONF environment variable. There is no need to add a /logs volume mount to the fluentd container. The operator will mount the logs Volume to all containers in the Pod . In the example above the Fluentd configuration has been provided from a ConfigMap . It could just as easily have come from a Secret or some other external Volume mount, or it could have been baked into the Fluentd image to be used. The Fluentd Configuration File The ConfigMap used to provide the Fluentd configuration might look something like this: <markup lang=\"yaml\" >apiVersion: v1 kind: ConfigMap metadata: name: efk-config labels: component: coherence-efk-config data: fluentd-coherence.conf: | # Ignore fluentd messages &lt;match fluent.**&gt; @type null &lt;/match&gt; # Coherence Logs &lt;source&gt; @type tail path /logs/coherence-*.log pos_file /tmp/cohrence.log.pos read_from_head true tag coherence-cluster multiline_flush_interval 20s &lt;parse&gt; @type multiline format_firstline /^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3}/ format1 /^(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3})\\/(?&lt;uptime&gt;[0-9\\.]+) (?&lt;product&gt;.+) &lt;(?&lt;level&gt;[^\\s]+)&gt; \\(thread=(?&lt;thread&gt;.+), member=(?&lt;member&gt;.+)\\):[\\S\\s](?&lt;log&gt;.*)/ &lt;/parse&gt; &lt;/source&gt; &lt;filter coherence-cluster&gt; @type record_transformer &lt;record&gt; cluster \"#{ENV['COH_CLUSTER_NAME']}\" role \"#{ENV['COH_ROLE']}\" host \"#{ENV['HOSTNAME']}\" pod-uid \"#{ENV['COH_POD_UID']}\" &lt;/record&gt; &lt;/filter&gt; &lt;match coherence-cluster&gt; @type elasticsearch hosts \"http://elasticsearch-master:9200\" logstash_format true logstash_prefix coherence-cluster &lt;/match&gt; The name of the ConfigMap is efk-config to match the name specified in the Coherence CRD spec. The source section is configured to match log files with the name /logs/coherence-*.log , which is the name that Coherence logging has been configured to use. The pattern in the source section is a Fluentd pattern that matches the standard Coherence log message format. A filter section will add additional fields to the log message. These come from the environment variables that the Operator will inject into all containers in the Pod. In this case the Coherence cluster name, the Coherence role name, the Pod host name and Pod UID. The final section tells Fluentd how to ship the logs to Elasticsearch, in this case to the endpoint http://elasticsearch-master:9200 There are many ways to configure Fluentd, the example above is just one way and is in fact taken from one of the Operator&#8217;s functional tests. With the efk-config ConfigMap created in the same namespace as the Coherence resource the Coherence logs from the containers will now be shipped to Elasticsearch. ",
            "title": "Add the Fluentd Side-Car"
        },
        {
            "location": "/logging/020_logging",
            "text": " There are many ways to capture container logs in Kubernetes, one possibility that this guide will cover is using a Fluentd side-car container to ship log files to Elasticsearch. This is a common pattern and one the the Coherence CRD makes simple by allowing easy injection of additional containers. This guide is going to assume that the default logging related configurations provided by the operator will be used. For example, Coherence will be configured to use Java util logging for logs, and the default logging configuration file will be used. Whilst these things are not pre-requisites for shipping logs to Elasticsearch they are required to make the examples below work. To be able to send Coherence logs to Elasticsearch there are some steps that must be completed: Configure Coherence to log to files Add a Volume and VolumeMount to be used for log files Add the Fluentd side-car container Configure Coherence to Log to Files Coherence will log to the console by default so to be able to ship logs to Elasticsearch it needs to be configured to write to log files. One way to do this is to add a Java Util Logging configuration file and then to configure Coherence to use the JDK logger. In the jvm.args section of the Coherence CRD the system properties should be added to set the configuration file used by Java util logging and to configure Coherence logging. See the Coherence Logging Config documentation for more details. There are alternative ways to configure the Java util logger besides using a configuration file, just as there are alternative logging frameworks that Coherence can be configured to use to produce log files. This example is going to use Java util logging as that is the simplest to demonstrate without requiring any additional logging libraries. Operator Provided Logging Configuration File Whilst any valid Java util logging configuration file may be used, the Coherence Operator injects a default logging configuration file into the coherence container that can be used to configure the logger to write logs to files under the /logs directory. The log files will have the name coherence-%g.log , where %g is the log file generation created as logs get rotated. This file will be injected into the container at /coherence-operator/utils/logging/logging.properties and will look something like this: <markup >com.oracle.coherence.handlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler com.oracle.coherence.level=FINEST java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.ConsoleHandler.level=FINEST java.util.logging.FileHandler.pattern=/logs/coherence-%g.log java.util.logging.FileHandler.limit=10485760 java.util.logging.FileHandler.count=50 java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format=%5$s%6$s%n To configure Cohrence and the logger some system properties need to be added to the jvm.args field of the Coherence CRD spec: For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" Coherence has been configured to use the Java util logging. The Coherence logger name has been set to com.oracle.coherence , which matches the logging configuration file. The Java util logging configuration file is set to the file injected by the Operator. Log Files Volume The logging configuration above configures Coherence to write logs to the /logs directory. For this location to be accessible to both the coherence container and to the fluentd container it needs to be created as a Volume in the Pod and mounted to both containers. As this Volume can be ephemeral and is typically not required to live longer than the Pod the simplest type of Volume to use is an emptyDir volume source. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs An additional empty-dir Volume named logs has been added to the Coherence spec. The logs volume will be mounted at /logs in all containers in the Pod . Add the Fluentd Side-Car With Coherence configured to write to log files, and those log files visible to other containers in the Pod the Fluentd side-car container can be added. The example yaml below shows a Coherence resource with the additional Fluentd container added. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs sideCars: - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3\" args: - \"-c\" - \"/etc/fluent.conf\" env: - name: \"FLUENTD_CONF\" value: \"fluentd-coherence.conf\" - name: \"FLUENT_ELASTICSEARCH_SED_DISABLE\" value: \"true\" configMapVolumes: - name: \"efk-config\" mountPath: \"/fluentd/etc/fluentd-coherence.conf\" subPath: \"fluentd-coherence.conf\" The fluentd container has been added to the sideCars list. This will create another container in the Pod exactly as configured. The FLUENTD_CONF environment variable has been set to the name of the configuration file that Fluentd should use. The standard Fluentd behaviour is to locate this file in the /fluentd/etc/ directory. The FLUENT_ELASTICSEARCH_SED_DISABLE environment variable has been set to work around a known issue here . An additional volume has been added from a ConfigMap named efk-config , that contains the Fluentd configuration to use. This will be mounted to the fluentd container at /fluentd/etc/fluentd-coherence.conf , which corresponds to the name of the file set in the FLUENTD_CONF environment variable. There is no need to add a /logs volume mount to the fluentd container. The operator will mount the logs Volume to all containers in the Pod . In the example above the Fluentd configuration has been provided from a ConfigMap . It could just as easily have come from a Secret or some other external Volume mount, or it could have been baked into the Fluentd image to be used. The Fluentd Configuration File The ConfigMap used to provide the Fluentd configuration might look something like this: <markup lang=\"yaml\" >apiVersion: v1 kind: ConfigMap metadata: name: efk-config labels: component: coherence-efk-config data: fluentd-coherence.conf: | # Ignore fluentd messages &lt;match fluent.**&gt; @type null &lt;/match&gt; # Coherence Logs &lt;source&gt; @type tail path /logs/coherence-*.log pos_file /tmp/cohrence.log.pos read_from_head true tag coherence-cluster multiline_flush_interval 20s &lt;parse&gt; @type multiline format_firstline /^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3}/ format1 /^(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3})\\/(?&lt;uptime&gt;[0-9\\.]+) (?&lt;product&gt;.+) &lt;(?&lt;level&gt;[^\\s]+)&gt; \\(thread=(?&lt;thread&gt;.+), member=(?&lt;member&gt;.+)\\):[\\S\\s](?&lt;log&gt;.*)/ &lt;/parse&gt; &lt;/source&gt; &lt;filter coherence-cluster&gt; @type record_transformer &lt;record&gt; cluster \"#{ENV['COH_CLUSTER_NAME']}\" role \"#{ENV['COH_ROLE']}\" host \"#{ENV['HOSTNAME']}\" pod-uid \"#{ENV['COH_POD_UID']}\" &lt;/record&gt; &lt;/filter&gt; &lt;match coherence-cluster&gt; @type elasticsearch hosts \"http://elasticsearch-master:9200\" logstash_format true logstash_prefix coherence-cluster &lt;/match&gt; The name of the ConfigMap is efk-config to match the name specified in the Coherence CRD spec. The source section is configured to match log files with the name /logs/coherence-*.log , which is the name that Coherence logging has been configured to use. The pattern in the source section is a Fluentd pattern that matches the standard Coherence log message format. A filter section will add additional fields to the log message. These come from the environment variables that the Operator will inject into all containers in the Pod. In this case the Coherence cluster name, the Coherence role name, the Pod host name and Pod UID. The final section tells Fluentd how to ship the logs to Elasticsearch, in this case to the endpoint http://elasticsearch-master:9200 There are many ways to configure Fluentd, the example above is just one way and is in fact taken from one of the Operator&#8217;s functional tests. With the efk-config ConfigMap created in the same namespace as the Coherence resource the Coherence logs from the containers will now be shipped to Elasticsearch. ",
            "title": "Log Capture with Fluentd"
        },
        {
            "location": "/jvm/040_gc",
            "text": " To enable GC logging set the jvm.gc.logging field to true . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: logging: true Setting the field to true adds the following JVM arguments to the JVM in the coherence container: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime If different GC logging arguments are required then the relevant JVM arguments can be added to either the jvm.args field or the jvm.gc.args field. ",
            "title": "Enable GC Logging"
        },
        {
            "location": "/jvm/040_gc",
            "text": " The garbage collector to use can be set using the jvm.gc.collector field. This field can be set to either G1 , CMS or Parallel (the field is case-insensitive, invalid values will be silently ignored). The default collector set, if none has been specified, will be G1 . Parameter JVM Argument Set G1 -XX:+UseG1GC CMS -XX:+UseConcMarkSweepGC Parallel -XX:+UseParallelGC For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: collector: \"G1\" The example above will add -XX:+UseG1GC to the command line. ",
            "title": "Set the Garbage Collector"
        },
        {
            "location": "/jvm/040_gc",
            "text": " Any arbitrary GC argument can be added to the jvm.gc.args field. These arguments will be passed verbatim to the JVM command line. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=200\" In the example above the -XX:MaxGCPauseMillis=200 JVM argument will be added to the command line. The jvm.gc.args field will add the provided arguments to the end of the command line exactly as they are in the args list. This field provides the same functionality as JVM Args but sometimes it might be useful to be able to separate the two gorups of arguments in the CRD spec. ",
            "title": "Adding Arbitrary GC Args"
        },
        {
            "location": "/jvm/040_gc",
            "text": " The Coherence CRD has fields in the jvm.gc section to allow certain garbage collection parameters to be set. These include GC logging, setting the collector to use and arbitrary GC arguments. Enable GC Logging To enable GC logging set the jvm.gc.logging field to true . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: logging: true Setting the field to true adds the following JVM arguments to the JVM in the coherence container: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime If different GC logging arguments are required then the relevant JVM arguments can be added to either the jvm.args field or the jvm.gc.args field. Set the Garbage Collector The garbage collector to use can be set using the jvm.gc.collector field. This field can be set to either G1 , CMS or Parallel (the field is case-insensitive, invalid values will be silently ignored). The default collector set, if none has been specified, will be G1 . Parameter JVM Argument Set G1 -XX:+UseG1GC CMS -XX:+UseConcMarkSweepGC Parallel -XX:+UseParallelGC For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: collector: \"G1\" The example above will add -XX:+UseG1GC to the command line. Adding Arbitrary GC Args Any arbitrary GC argument can be added to the jvm.gc.args field. These arguments will be passed verbatim to the JVM command line. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=200\" In the example above the -XX:MaxGCPauseMillis=200 JVM argument will be added to the command line. The jvm.gc.args field will add the provided arguments to the end of the command line exactly as they are in the args list. This field provides the same functionality as JVM Args but sometimes it might be useful to be able to separate the two gorups of arguments in the CRD spec. ",
            "title": "Garbage Collector Settings"
        },
        {
            "location": "/metrics/030_importing",
            "text": " The Operator has a set of Grafana dashboards that can be imported into a Grafana instance. Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. ",
            "title": "preambule"
        },
        {
            "location": "/metrics/030_importing",
            "text": " The Coherence Operator provides a set of dashboards for Coherence that may be imported into Grafana. There are two ways to obtain the dashboards: Clone the Coherence Operator GitHub repo, checkout the branch or tag for the version you want to use and then obtain the dashboards from the dashboards/ directory. Download the .tar.gz dashboards package for the release you want to use. <markup lang=\"bash\" >VERSION=3.0.0 curl https://oracle.github.io/coherence-operator/dashboards/${VERSION}/coherence-dashboards.tar.gz \\ -o coherence-dashboards.tar.gz tar -zxvf coherence-dashboards.tar.gz The above commands will download the coherence-dashboards.tar.gz file and unpack it resulting in a directory named dashboards/ in the current working directory. This dashboards/ directory will contain the various Coherence dashboard files. ",
            "title": "Obtain the Coherence Dashboards"
        },
        {
            "location": "/metrics/030_importing",
            "text": " In your Grafana environment, ensure you either: have a Prometheus datasource named Prometheus which is also marked as the default datasource have added a new Prometheus datasource which you have set as the default Then continue below. Clone the git repository using git clone https://github.com/oracle/coherence-operator.git Note There are two sets of dashboards available Legacy - these are dashboards under the dashboards/grafana-legacy/ directory that are compatible with the metric names produced by the Coherence metrics publisher Microprofile - these are dashboards under the dashboards/grafana/ directory that are compatible with the metric names produced by the Coherence MP Metrics module. Decide which dashboards you will import, depending on how metrics are being published (see the note above). Login to Grafana and for each dashboard in the chosen dashboard directory carry out the following to upload to Grafana: Highlight the '+' (plus) icons and click on import Click `Upload Json file' button to select a dashboard Leave all the default values and click on Import ",
            "title": "Manually Importing Using the Defaults"
        },
        {
            "location": "/metrics/030_importing",
            "text": " If your Grafana environment has a default datasource that you cannot change or already has a datasource named Prometheus , follow these steps to import the dashboards: Login to Grafana Create a new datasource named Coherence-Prometheus and point to your Prometheus endpoint Create a temporary directory and copy all the dashboards from the cloned directory &lt;DIR&gt;/dashboards/grafana to this temporary directory Change to this temporary directory and run the following to update the datasource to Coherence-Prometheus or the datasource of your own choice: for file in *.json do sed -i '' -e 's/\"datasource\": \"Prometheus\"/\"datasource\": \"Coherence-Prometheus\"/g' \\ -e 's/\"datasource\": null/\"datasource\": \"Coherence-Prometheus\"/g' \\ -e 's/\"datasource\": \"-- Grafana --\"/\"datasource\": \"Coherence-Prometheus\"/g' $file; done Once you have completed the script, proceed to upload the dashboards as described above. ",
            "title": "Manually Importing With a Different Datasource"
        },
        {
            "location": "/metrics/030_importing",
            "text": " This example shows you how to import the Grafana dashboards into your own Grafana instance. By default, the Coherence dashboards require a datasource named Prometheus which should also be the default datasource. If this datasource is already used, and you cannot add another datasource as the default, then please go to Importing with a different datasource . Manually Importing Using the Defaults In your Grafana environment, ensure you either: have a Prometheus datasource named Prometheus which is also marked as the default datasource have added a new Prometheus datasource which you have set as the default Then continue below. Clone the git repository using git clone https://github.com/oracle/coherence-operator.git Note There are two sets of dashboards available Legacy - these are dashboards under the dashboards/grafana-legacy/ directory that are compatible with the metric names produced by the Coherence metrics publisher Microprofile - these are dashboards under the dashboards/grafana/ directory that are compatible with the metric names produced by the Coherence MP Metrics module. Decide which dashboards you will import, depending on how metrics are being published (see the note above). Login to Grafana and for each dashboard in the chosen dashboard directory carry out the following to upload to Grafana: Highlight the '+' (plus) icons and click on import Click `Upload Json file' button to select a dashboard Leave all the default values and click on Import Manually Importing With a Different Datasource If your Grafana environment has a default datasource that you cannot change or already has a datasource named Prometheus , follow these steps to import the dashboards: Login to Grafana Create a new datasource named Coherence-Prometheus and point to your Prometheus endpoint Create a temporary directory and copy all the dashboards from the cloned directory &lt;DIR&gt;/dashboards/grafana to this temporary directory Change to this temporary directory and run the following to update the datasource to Coherence-Prometheus or the datasource of your own choice: for file in *.json do sed -i '' -e 's/\"datasource\": \"Prometheus\"/\"datasource\": \"Coherence-Prometheus\"/g' \\ -e 's/\"datasource\": null/\"datasource\": \"Coherence-Prometheus\"/g' \\ -e 's/\"datasource\": \"-- Grafana --\"/\"datasource\": \"Coherence-Prometheus\"/g' $file; done Once you have completed the script, proceed to upload the dashboards as described above. ",
            "title": "Importing Grafana Dashboards."
        },
        {
            "location": "/metrics/030_importing",
            "text": " To create a ConfigMap with the Grafana dashboards directly from the ConfigMap yaml for a specific Operator release the following commands can be used: <markup lang=\"bash\" >VERSION=3.0.0 kubectl -n monitoring create \\ -f https://oracle.github.io/coherence-operator/dashboards/${VERSION}/coherence-grafana-dashboards.yaml The VERSION variable has been set to the version of the dashboards to be used (this corresponds to an Operator release version but dashboards can be used independently of the Operator). In this example the dashboards will be installed into the monitoring namespace. The example above installs the dashboards configured to use Coherence MP metrics, to use dashboards for metrics produced with Coherence legacy metric names change the yaml file name in the URL to coherence-grafana-legacy-dashboards.yaml , for example: <markup lang=\"bash\" >VERSION=3.0.0 kubectl -n monitoring create \\ -f https://oracle.github.io/coherence-operator/dashboards/${VERSION}/coherence-grafana-legacy-dashboards.yaml ",
            "title": "Create a ConfigMap from GitHub Yaml"
        },
        {
            "location": "/metrics/030_importing",
            "text": " To create a ConfigMap with the Grafana dashboards in directly from .tar.gz dashboard package for a specific Operator release the following commands can be used: <markup lang=\"bash\" >VERSION=3.0.0 curl https://oracle.github.io/coherence-operator/dashboards/${VERSION}/coherence-dashboards.tar.gz \\ -o coherence-dashboards.tar.gz tar -zxvf coherence-dashboards.tar.gz kubectl -n monitoring create configmap coherence-grafana-dashboards --from-file=dashboards/grafana The VERSION variable has been set to the version of the dashboards to be used (this corresponds to an Operator release version but dashboards can be used independently of the Operator). In this example the dashboards ConfigMap named coherence-grafana-dashboards will be installed into the monitoring namespace. ",
            "title": "Create a ConfigMap from the Dashboard Package File"
        },
        {
            "location": "/metrics/030_importing",
            "text": " In this example Grafana will be configured to import dashboards from ConfigMaps with the label grafana_dashboard , so the ConfigMap created above needs to be labelled: <markup lang=\"bash\" >kubectl -n monitoring label configmap coherence-grafana-dashboards grafana_dashboard=1 ",
            "title": "Label the ConfigMap"
        },
        {
            "location": "/metrics/030_importing",
            "text": " The Prometheus Operator will be installed using its Helm chart. Create a Helm values file like the following: <markup lang=\"yaml\" title=\"prometheus-values.yaml\" >prometheus: prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false alertmanager: enabled: false nodeExporter: enabled: true grafana: enabled: true sidecar: dashboards: enabled: true label: grafana_dashboard Grafana will be enabled. Grafana will automatically import dashboards from ConfigMaps that have the label grafana_dashboard (which was given to the ConfigMap created above). Prometheus can be installed into the monitoring namespace using the Helm command: <markup lang=\"bash\" >helm install --namespace monitoring \\ --values prometheus-values.yaml \\ prometheus stable/prometheus-operator To actually start Prometheus a Prometheus CRD resource needs to be added to Kubernetes. Create a Prometheus resource yaml file suitable for testing: <markup lang=\"yaml\" title=\"prometheus.yaml\" >apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: name: prometheus spec: serviceAccountName: prometheus serviceMonitorSelector: matchLabels: coherenceComponent: coherence-service-monitor resources: requests: memory: 400Mi enableAdminAPI: true The serviceMonitorSelector tells Prometheus to use any ServiceMonitor that is created with the coherence-service-monitor label, which is a label that the Coherence Operator adds to any ServiceMonitor that it creates. Install the prometheus.yaml file into Kubernetes: <markup lang=\"bash\" >kubectl -n monitoring create -f etc/prometheus.yaml In the monitoring namespace there should now be a number of Pods and Services , among them a Prometheus instance, and a Grafana instance. It should be possible to reach the Grafana UI on the ports exposed by the Pod and see the imported Coherence dashboards. <markup lang=\"bash\" >GRAFANA_POD=$(kubectl -n monitoring get pod -l app.kubernetes.io/name=grafana -o name) kubectl -n monitoring port-forward ${GRAFANA_POD} 3000:3000 The default username for Grafana installed by the Prometheus Operator is admin the default password is prom-operator If a Coherence cluster has been started with the Operator as described in the Publish Metrics page, its metrics will eventually appear in Prometheus and Grafana. It can sometimes take a minute or so for Prometheus to start scraping metrics and for them to appear in Grafana. ",
            "title": "Install the Prometheus Operator"
        },
        {
            "location": "/metrics/030_importing",
            "text": " There are ways to automatically import dashboards into Grafana, the exact method would depend on how Grafana is to be installed and run. The Coherence Operator test pipeline, for example, uses the Prometheus Operator to install Prometheus and Grafana and automatically imports the Coherence dashboards from a ConfigMap . The examples below show how to create the dashboards as a ConfigMap and then install them into a Grafana instances started with the Prometheus Operator. There are two ways to create the ConfigMap containing the dashboard files: Use the ConfigMap yaml available directly from GitHub Obtain the dashboards as described above and create a ConfigMap from those files. Create a ConfigMap from GitHub Yaml To create a ConfigMap with the Grafana dashboards directly from the ConfigMap yaml for a specific Operator release the following commands can be used: <markup lang=\"bash\" >VERSION=3.0.0 kubectl -n monitoring create \\ -f https://oracle.github.io/coherence-operator/dashboards/${VERSION}/coherence-grafana-dashboards.yaml The VERSION variable has been set to the version of the dashboards to be used (this corresponds to an Operator release version but dashboards can be used independently of the Operator). In this example the dashboards will be installed into the monitoring namespace. The example above installs the dashboards configured to use Coherence MP metrics, to use dashboards for metrics produced with Coherence legacy metric names change the yaml file name in the URL to coherence-grafana-legacy-dashboards.yaml , for example: <markup lang=\"bash\" >VERSION=3.0.0 kubectl -n monitoring create \\ -f https://oracle.github.io/coherence-operator/dashboards/${VERSION}/coherence-grafana-legacy-dashboards.yaml Create a ConfigMap from the Dashboard Package File To create a ConfigMap with the Grafana dashboards in directly from .tar.gz dashboard package for a specific Operator release the following commands can be used: <markup lang=\"bash\" >VERSION=3.0.0 curl https://oracle.github.io/coherence-operator/dashboards/${VERSION}/coherence-dashboards.tar.gz \\ -o coherence-dashboards.tar.gz tar -zxvf coherence-dashboards.tar.gz kubectl -n monitoring create configmap coherence-grafana-dashboards --from-file=dashboards/grafana The VERSION variable has been set to the version of the dashboards to be used (this corresponds to an Operator release version but dashboards can be used independently of the Operator). In this example the dashboards ConfigMap named coherence-grafana-dashboards will be installed into the monitoring namespace. Label the ConfigMap In this example Grafana will be configured to import dashboards from ConfigMaps with the label grafana_dashboard , so the ConfigMap created above needs to be labelled: <markup lang=\"bash\" >kubectl -n monitoring label configmap coherence-grafana-dashboards grafana_dashboard=1 Install the Prometheus Operator The Prometheus Operator will be installed using its Helm chart. Create a Helm values file like the following: <markup lang=\"yaml\" title=\"prometheus-values.yaml\" >prometheus: prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false alertmanager: enabled: false nodeExporter: enabled: true grafana: enabled: true sidecar: dashboards: enabled: true label: grafana_dashboard Grafana will be enabled. Grafana will automatically import dashboards from ConfigMaps that have the label grafana_dashboard (which was given to the ConfigMap created above). Prometheus can be installed into the monitoring namespace using the Helm command: <markup lang=\"bash\" >helm install --namespace monitoring \\ --values prometheus-values.yaml \\ prometheus stable/prometheus-operator To actually start Prometheus a Prometheus CRD resource needs to be added to Kubernetes. Create a Prometheus resource yaml file suitable for testing: <markup lang=\"yaml\" title=\"prometheus.yaml\" >apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: name: prometheus spec: serviceAccountName: prometheus serviceMonitorSelector: matchLabels: coherenceComponent: coherence-service-monitor resources: requests: memory: 400Mi enableAdminAPI: true The serviceMonitorSelector tells Prometheus to use any ServiceMonitor that is created with the coherence-service-monitor label, which is a label that the Coherence Operator adds to any ServiceMonitor that it creates. Install the prometheus.yaml file into Kubernetes: <markup lang=\"bash\" >kubectl -n monitoring create -f etc/prometheus.yaml In the monitoring namespace there should now be a number of Pods and Services , among them a Prometheus instance, and a Grafana instance. It should be possible to reach the Grafana UI on the ports exposed by the Pod and see the imported Coherence dashboards. <markup lang=\"bash\" >GRAFANA_POD=$(kubectl -n monitoring get pod -l app.kubernetes.io/name=grafana -o name) kubectl -n monitoring port-forward ${GRAFANA_POD} 3000:3000 The default username for Grafana installed by the Prometheus Operator is admin the default password is prom-operator If a Coherence cluster has been started with the Operator as described in the Publish Metrics page, its metrics will eventually appear in Prometheus and Grafana. It can sometimes take a minute or so for Prometheus to start scraping metrics and for them to appear in Grafana. ",
            "title": "Automatically Importing Dashboards"
        },
        {
            "location": "/ports/010_overview",
            "text": " Adding Ports Adding additional container ports to the Coherence container. Expose Ports via Services Configuring Services used to expose ports. Prometheus ServiceMonitors Adding Prometheus ServiceMonitors to expose ports to be scraped for metrics. ",
            "title": "Guides to Adding and Exposing Ports"
        },
        {
            "location": "/ports/010_overview",
            "text": " Almost every application deployed into a Kubernetes cluster needs to communicate with other processes to provide services to other processes or consume services to other processes. This is achieved by exposing ports on containers in Pods and optionally exposing those same ports using Services and ingress. The Coherence CRD spec makes it simple to add ports to the Coherence container and configure Services to expose those ports. Each additional port configured is exposed via its own Service . If the configuration of Services for ports provided by the Coherence CRD spec is not sufficient or cannot provide the required Service configuration then it is always possible to just create your own Services in Kubernetes. Guides to Adding and Exposing Ports Adding Ports Adding additional container ports to the Coherence container. Expose Ports via Services Configuring Services used to expose ports. Prometheus ServiceMonitors Adding Prometheus ServiceMonitors to expose ports to be scraped for metrics. ",
            "title": "Overview"
        },
        {
            "location": "/applications/050_application_args",
            "text": " When running a custom application there may be a requirement to pass arguments to the application&#8217;s main class. By default, there are no application arguments but any arguments required can be specified in the application.args list in the Coherence resource spec. The application.args is a list of string values, each value in the list is passed as an argument, in the order that they are specified in the list. For example, a deployment uses a custom image catalogue:1.0.0 that requires a custom main class called com.acme.Catalogue , and that class takes additional arguments. In this example we&#8217;ll use two fictitious arguments such as a name and a language for the catalogue. the Coherence resource would look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: main: com.acme.Catalogue args: - \"--name=Books\" - \"--language=en_GB\" The com.acme.Catalogue will be run as the main class. The arguments passed to the com.acme.Catalogue class will be --name=Books and --language=en_GB The example would be equivalent to the Coherence container running: <markup lang=\"bash\" >$ java com.acme.Catalogue --name=Books --language=en_GB ",
            "title": "Set Application Arguments"
        },
        {
            "location": "/examples/010_overview",
            "text": " This section includes links to a number of examples which uses the Coherence Operator. ",
            "title": "preambule"
        },
        {
            "location": "/examples/010_overview",
            "text": " This example showcases how to deploy Coherence applications using the Coherence Operator. The following scenarios are covered: Installing the Coherence Operator Installing a Coherence cluster Deploying a Proxy tier Deploying an storage-disabled application Enabling Active Persistence After the initial install of the Coherence cluster, the following examples build on the previous ones by issuing a kubectl apply to modify the install adding additional roles. Please see GitHub for full instructions. ",
            "title": "1. Deployment Example"
        },
        {
            "location": "/examples/010_overview",
            "text": " The Coherence Demonstration application is an application which demonstrates various Coherence related features such include Persistence, Federation and Lambda support. This demonstration can run stand alone but can also be installed on the Coherence Operator. When installed using the Coherence Operator, the setup includes two Coherence Clusters, in the same Kubernetes cluster, which are configured with Active/Active Federation. Please see The Coherence Demo GitHub project for full instructions. ",
            "title": "2. Coherence Demo"
        },
        {
            "location": "/examples/010_overview",
            "text": " There are a number of examples which show you how to build and deploy applications for the Coherence Operator. 1. Deployment Example This example showcases how to deploy Coherence applications using the Coherence Operator. The following scenarios are covered: Installing the Coherence Operator Installing a Coherence cluster Deploying a Proxy tier Deploying an storage-disabled application Enabling Active Persistence After the initial install of the Coherence cluster, the following examples build on the previous ones by issuing a kubectl apply to modify the install adding additional roles. Please see GitHub for full instructions. 2. Coherence Demo The Coherence Demonstration application is an application which demonstrates various Coherence related features such include Persistence, Federation and Lambda support. This demonstration can run stand alone but can also be installed on the Coherence Operator. When installed using the Coherence Operator, the setup includes two Coherence Clusters, in the same Kubernetes cluster, which are configured with Active/Active Federation. Please see The Coherence Demo GitHub project for full instructions. ",
            "title": "Examples Overview"
        },
        {
            "location": "/installation/04_obtain_coherence_images",
            "text": " The Coherence Operator uses the OSS Coherence CE image as the image to run when no image has been specified for a Coherence resource. Commercial Coherence images are not available from public image registries and must be pulled from the Oracle Container Registry. ",
            "title": "preambule"
        },
        {
            "location": "/installation/04_obtain_coherence_images",
            "text": " Get the Coherence Docker image from the Oracle Container Registry: In a web browser, navigate to Oracle Container Registry and click Sign In. Enter your Oracle credentials or create an account if you don&#8217;t have one. Search for coherence in the Search Oracle Container Registry field. Click coherence in the search result list. On the Oracle Coherence page, select the language from the drop-down list and click Continue. Click Accept on the Oracle Standard Terms and Conditions page. Once this is done the Oracle Container Registry credentials can be used to create Kubernetes secret to pull the Coherence image. ",
            "title": "Coherence Images from Oracle Container Registry"
        },
        {
            "location": "/installation/04_obtain_coherence_images",
            "text": " Kubernetes supports configuring pods to use imagePullSecrets for pulling images. If possible, this is the preferable and most portable route. See the kubernetes docs for this. Once secrets have been created in the namespace that the Coherence resource is to be installed in then the secret name can be specified in the Coherence CRD spec . It is possible to specify multiple secrets in the case where the different images being used are pulled from different registries. For example to use the commercial Coherence 14.1.1.0.0 image from OCR specify the image and image pull secrets in the Coherence resource yaml <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0 imagePullSecrets: - name: coherence-secret The coherence-secret will be used for pulling images from the registry associated to the secret Also see Using Private Image Registries ",
            "title": "Use ImagePullSecrets"
        },
        {
            "location": "/other/060_secret_volumes",
            "text": " Additional Volumes and VolumeMounts from Secrets can easily be added to a Coherence resource. ",
            "title": "preambule"
        },
        {
            "location": "/other/060_secret_volumes",
            "text": " To add a Secret as an additional volume to the Pods of a Coherence deployment add entries to the secretVolumes list in the CRD spec. Each entry in the list has a mandatory name and mountPath field, all other fields are optional. The name field is the name of the Secret to mount and is also used as the volume name. The mountPath field is the path in the container to mount the volume to. Additional volumes added in this way (either Secrets shown here, or Secrets or plain Volumes ) will be added to all containers in the Pod . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: secretVolumes: - name: storage-config mountPath: /home/coherence/config The Secret named storage-config will be mounted to the Pod as an additional Volume named storage-config The Secret will be mounted at /home/coherence/config in the containers. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config secret: secretName: storage-config As already stated, if the Coherence resource has additional containers the Secret will be mounted in all of them. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" secretVolumes: - name: storage-config mountPath: /home/coherence/config In this example the storage-config Secret will be mounted as a Volume and mounted to both the coherence container and the fluentd container. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3\" volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config secret: secretName: storage-config ",
            "title": "Add Secrets Volumes"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " Coherence persistence is a set of tools and technologies that manage the persistence and recovery of Coherence distributed caches. Cached data can be persisted so that it can be quickly recovered after a catastrophic failure or after a cluster restart due to planned maintenance. Persistence and federated caching can be used together as required. ",
            "title": "preambule"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " The Coherence CRD allows the default persistence mode, and the storage location of persistence data to be configured. Persistence can be configured in the spec.coherence.persistence section of the CRD. See the Coherence Persistence documentation for more details of how persistence works and its configuration. ",
            "title": "Configure Coherence Persistence"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " There are three default persistence modes available, active , active-async and on-demand ; the default mode is on-demand . The persistence mode will be set using the spec.coherence.persistence,mode field in the CRD. The value of this field will be used to set the coherence.distributed.persistence-mode system property in the Coherence JVM. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: mode: active The example above sets the persistence mode to active which will effectively pass -Dcoherence.distributed.persistence-mode=active to the Coherence JVM&#8217;s command line. ",
            "title": "Persistence Mode"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " The Coherence Operator creates a StatefulSet for each Coherence resource, so the logical place to store persistence data is in a PersistentVolumeClaim . The PVC used for persistence can be configured in the spec.coherence.persistence.persistentVolumeClaim section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: persistentVolumeClaim: storageClassName: \"SSD\" accessModes: - ReadWriteOnce resources: requests: storage: 50Gi The example above configures a 50GB PVC with a storage class name of \"SSD\" (assuming the Kubernetes cluster has a storage class of that name configured). The configuration under the spec.coherence.persistence.persistentVolumeClaim section is exactly the same as configuring a PVC for a normal Kubernetes Pod and all the possible options are beyond the scope of this document. For more details on configuring PVC, see the Kubernetes Persistent Volumes documentation. ",
            "title": "Using a PersistentVolumeClaim"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " An alternative to a PVC is to use a normal Kubernetes Volume to store the persistence data. An example of this use-case could be when the Kubernetes Nodes that the Coherence Pods are scheduled onto have locally attached fast SSD drives, which is ideal storage for persistence. In this case a normal Volume can be configured in the spec.coherence.persistence.volume section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence In the example above a Volume has been configured for persistence, in this case a HostPath volume pointing to the /mnt/ssd/coherence/persistence directory on the Node. The configuration under the spec.coherence.persistence.volume section is a normal Kubernetes VolumeSource so any valid VolumeSource configuration can be used. See the Kubernetes Volumes documentation for more details. ",
            "title": "Using a Normal Volume"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " The purpose of persistence in Coherence is to store data on disc so that it is available outside of the lifetime of the JVMs that make up the cluster. In a containerised environment like Kubernetes this means storing that data in storage that also lives outside of the containers. When persistence storage has been configured a VolumeMount will be added to the Coherence container mounted at /persistence , and the coherence.distributed.persistence.base.dir system property will be configured to point to the storage location. Using a PersistentVolumeClaim The Coherence Operator creates a StatefulSet for each Coherence resource, so the logical place to store persistence data is in a PersistentVolumeClaim . The PVC used for persistence can be configured in the spec.coherence.persistence.persistentVolumeClaim section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: persistentVolumeClaim: storageClassName: \"SSD\" accessModes: - ReadWriteOnce resources: requests: storage: 50Gi The example above configures a 50GB PVC with a storage class name of \"SSD\" (assuming the Kubernetes cluster has a storage class of that name configured). The configuration under the spec.coherence.persistence.persistentVolumeClaim section is exactly the same as configuring a PVC for a normal Kubernetes Pod and all the possible options are beyond the scope of this document. For more details on configuring PVC, see the Kubernetes Persistent Volumes documentation. Using a Normal Volume An alternative to a PVC is to use a normal Kubernetes Volume to store the persistence data. An example of this use-case could be when the Kubernetes Nodes that the Coherence Pods are scheduled onto have locally attached fast SSD drives, which is ideal storage for persistence. In this case a normal Volume can be configured in the spec.coherence.persistence.volume section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence In the example above a Volume has been configured for persistence, in this case a HostPath volume pointing to the /mnt/ssd/coherence/persistence directory on the Node. The configuration under the spec.coherence.persistence.volume section is a normal Kubernetes VolumeSource so any valid VolumeSource configuration can be used. See the Kubernetes Volumes documentation for more details. ",
            "title": "Persistence Storage"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " A PVC can be configured for persistence snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: persistentVolumeClaim: resources: requests: storage: 50Gi Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored in a 50GB PVC. ",
            "title": "Snapshots Using a PersistentVolumeClaim"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " A normal volume can be configured for snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: volume: hostPath: path: /mnt/ssd/coherence/snapshots Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored on a normal Volume using a different HostPath volume source. ",
            "title": "Snapshots Using a Normal Volumes"
        },
        {
            "location": "/coherence/080_persistence",
            "text": " Coherence allows on-demand snapshots to be taken of cache data. With the default configuration the snapshot files will be stored under the same persistence root location as active persistence data. The Coherence spec allows a different location to be specified for storage of snapshot files so that active data and snapshot data can be stored in different locations and/or on different storage types in Kubernetes. The same two options are available for snapshot storage that are available for persistence storage, namely PVCs and normal Volumes. The spec.coherence.persistence.snapshots section is used to configure snapshot storage. When this is used a VolumeMount will be added to the Coherence container with a mount path of /snapshots , and the coherence.distributed.persistence.snapshot.dir system property will be set to point to this location. Snapshots Using a PersistentVolumeClaim A PVC can be configured for persistence snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: persistentVolumeClaim: resources: requests: storage: 50Gi Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored in a 50GB PVC. Snapshots Using a Normal Volumes A normal volume can be configured for snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: volume: hostPath: path: /mnt/ssd/coherence/snapshots Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored on a normal Volume using a different HostPath volume source. ",
            "title": "Snapshot Storage"
        },
        {
            "location": "/logging/030_kibana",
            "text": " The Kibana dashboard files are located in the Coherence operator source in the dashboards/kibana directory. The method of importing the dashboards into Kibana will depend on how Kibana is being run. The simplest method is just to import the json file using the Kibana web UI. An alternative approach is to load the dashboard into a ConfigMap in Kubernetes that is mounted into the Kibana Pod and then trigger an import when Kibana starts. As there are many ways to do this depending on the specifics of the version of Kibana being used, exact instructions are beyond the scope fo this guide. ",
            "title": "Importing Kibana Dashboards"
        },
        {
            "location": "/logging/030_kibana",
            "text": "",
            "title": "Kibana Dashboards &amp; Searches"
        },
        {
            "location": "/logging/030_kibana",
            "text": " Dashboards Coherence Cluster - All Messages Coherence Cluster - Errors and Warnings Coherence Cluster - Persistence Coherence Cluster - Configuration Messages Coherence Cluster - Network Coherence Cluster - Partitions Coherence Cluster - Message Sources Searches ",
            "title": "Table of Contents"
        },
        {
            "location": "/logging/030_kibana",
            "text": " This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. ",
            "title": "1. Coherence Cluster - All Messages"
        },
        {
            "location": "/logging/030_kibana",
            "text": " This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. ",
            "title": "2. Coherence Cluster - Errors and Warnings"
        },
        {
            "location": "/logging/030_kibana",
            "text": " This dashboard shows Persistence related messages including failed and successful operations. ",
            "title": "3. Coherence Cluster - Persistence"
        },
        {
            "location": "/logging/030_kibana",
            "text": " This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. ",
            "title": "4. Coherence Cluster - Configuration Messages"
        },
        {
            "location": "/logging/030_kibana",
            "text": " Information from all dashboards (and queries) can be filtered using the standard Kibana date/time filtering in the top right of the UI, as well as the Add a filter button. 1. Coherence Cluster - All Messages This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. 2. Coherence Cluster - Errors and Warnings This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. 3. Coherence Cluster - Persistence This dashboard shows Persistence related messages including failed and successful operations. 4. Coherence Cluster - Configuration Messages This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. ",
            "title": "Dashboards"
        },
        {
            "location": "/logging/030_kibana",
            "text": " Shows partition transfer and partition loss messages. ",
            "title": "6. Coherence Cluster - Partitions"
        },
        {
            "location": "/logging/030_kibana",
            "text": " Shows the source (thread) for messages Users can drill-down by cluster, host and message level. ",
            "title": "7. Coherence Cluster - Message Sources"
        },
        {
            "location": "/logging/030_kibana",
            "text": " This dashboard hows network related messages, such as communication delays and TCP ring disconnects. 6. Coherence Cluster - Partitions Shows partition transfer and partition loss messages. 7. Coherence Cluster - Message Sources Shows the source (thread) for messages Users can drill-down by cluster, host and message level. ",
            "title": "5. Coherence Cluster - Network"
        },
        {
            "location": "/logging/030_kibana",
            "text": " A number of searches are automatically includes which can help assist in diagnosis and troubleshooting a Coherence cluster. They can be accessed via the Discover side-bar and selecting `Open . These are grouped into the following general categories: Cluster - Cluster join, discovery, heartbeat, member joining and stopping messages Cache - Cache restarting, exceptions and index exception messages Configuration - Configuration loading and not loading messages Persistence - Persistence success and failure messages Network - Network communications delays, disconnects, timeouts and terminations Partition - Partition loss, ownership and transfer related messages Member - Member thread dump, join and leave messages Errors - All Error messages only Federation - Federation participant, disconnection, connection, errors and other messages ",
            "title": "Searches"
        },
        {
            "location": "/logging/030_kibana",
            "text": " Kibana is often used to anyalze log files that have been collected into Elasticsearch. The Coherence Operator provides a number of Kibana dashboards and queries to allow you to view and analyze logs from your Coherence clusters. Importing Kibana Dashboards The Kibana dashboard files are located in the Coherence operator source in the dashboards/kibana directory. The method of importing the dashboards into Kibana will depend on how Kibana is being run. The simplest method is just to import the json file using the Kibana web UI. An alternative approach is to load the dashboard into a ConfigMap in Kubernetes that is mounted into the Kibana Pod and then trigger an import when Kibana starts. As there are many ways to do this depending on the specifics of the version of Kibana being used, exact instructions are beyond the scope fo this guide. Kibana Dashboards &amp; Searches Table of Contents Dashboards Coherence Cluster - All Messages Coherence Cluster - Errors and Warnings Coherence Cluster - Persistence Coherence Cluster - Configuration Messages Coherence Cluster - Network Coherence Cluster - Partitions Coherence Cluster - Message Sources Searches Dashboards Information from all dashboards (and queries) can be filtered using the standard Kibana date/time filtering in the top right of the UI, as well as the Add a filter button. 1. Coherence Cluster - All Messages This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. 2. Coherence Cluster - Errors and Warnings This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. 3. Coherence Cluster - Persistence This dashboard shows Persistence related messages including failed and successful operations. 4. Coherence Cluster - Configuration Messages This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. 5. Coherence Cluster - Network This dashboard hows network related messages, such as communication delays and TCP ring disconnects. 6. Coherence Cluster - Partitions Shows partition transfer and partition loss messages. 7. Coherence Cluster - Message Sources Shows the source (thread) for messages Users can drill-down by cluster, host and message level. Searches A number of searches are automatically includes which can help assist in diagnosis and troubleshooting a Coherence cluster. They can be accessed via the Discover side-bar and selecting `Open . These are grouped into the following general categories: Cluster - Cluster join, discovery, heartbeat, member joining and stopping messages Cache - Cache restarting, exceptions and index exception messages Configuration - Configuration loading and not loading messages Persistence - Persistence success and failure messages Network - Network communications delays, disconnects, timeouts and terminations Partition - Partition loss, ownership and transfer related messages Member - Member thread dump, join and leave messages Errors - All Error messages only Federation - Federation participant, disconnection, connection, errors and other messages ",
            "title": "Using Kibana Dashboards"
        },
        {
            "location": "/other/040_annotations",
            "text": " Additional annotations can be added to the Pods managed by the Operator. Annotations should be added to the annotations map in the Coherence CRD spec. The entries in the annotations map should confirm to the recommendations and rules in the Kubernetes Annotations documentation. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: annotations: prometheus.io/path: /metrics prometheus.io/port: \"9612\" prometheus.io/scheme: http prometheus.io/scrape: \"true\" A number of Prometheus annotations will be added to this Coherence deployment&#8217;s Pods ",
            "title": "Pod Annotations"
        },
        {
            "location": "/coherence/030_cache_config",
            "text": " The name of the Coherence cache configuration file that the Coherence processes in a Coherence resource will use can be set with the spec.coherence.cacheConfig field. By setting this field the coherence.cacheconfig system property will be set in the Coherence JVM. When the spec.coherence.cacheConfig is blank or not specified, Coherence use its default behaviour to find the cache configuration file to use. Typically, this is to use the first occurrence of coherence-cache-config.xml that is found on the classpath (consult the Coherence documentation for an explanation of the default behaviour). To set a specific cache configuration file to use set the spec.coherence.cacheConfig field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: cacheConfig: storage-cache-config.xml The spec.coherence.cacheConfig field has been set to storage-cache-config.xml which will effectively pass -Dcoherence.cacheconfig=storage-cache-config.xml to the JVM command line. ",
            "title": "Set the Cache Configuration File Name"
        },
        {
            "location": "/other/090_pod_scheduling",
            "text": " In Kubernetes Pods can be configured to control how, and onto which nodes, Kubernetes will schedule those Pods ; the Coherence Operator allows the same control for Pods owned by a Coherence resource. The following settings can be configured: Field Description nodeSelector nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of role spec, it specifies a map of key-value pairs. For the Pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). See Assigning Pods to Nodes in the Kubernetes documentation affinity The affinity/anti-affinity feature, greatly expands the types of constraints you can express over just using labels in a nodeSelector . See Assigning Pods to Nodes in the Kubernetes documentation tolerations nodeSelector and affinity are properties of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite  they allow a node to repel a set of Pods . Taints and tolerations work together to ensure that Pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any Pods that do not tolerate the taints. Tolerations are applied to Pods , and allow (but do not require) the Pods to schedule onto nodes with matching taints. See Taints and Tolerations in the Kubernetes documentation. The nodeSelector , affinity and tolerations fields are all part of the Coherence CRD spec. The format of the fields is that same as documented in the Kubernetes documentation Assigning Pods to Nodes and Taints and Tolerations For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: tolerations: - key: \"example-key\" operator: \"Exists\" effect: \"NoSchedule\" nodeSelector: - disktype: ssd affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 ",
            "title": "Configure Pod Scheduling"
        },
        {
            "location": "/installation/01_installation",
            "text": " The Coherence Operator is available as a Docker image oracle/coherence-operator:3.0.0-2006221649 that can easily be installed into a Kubernetes cluster. ",
            "title": "preambule"
        },
        {
            "location": "/installation/01_installation",
            "text": " In order for the Coherence Operator to be able to install Coherence clusters it needs to be able to pull Coherence Docker images. These images are not available in public Docker repositories and will typically Kubernetes will need authentication to be able to pull them. This is achived by creating pull secrets. Pull secrets are not global and hence secrets will be required in the namespace(s) that Coherence clusters will be installed into. see Obtain Coherence Images ",
            "title": "Image Pull Secrets"
        },
        {
            "location": "/installation/01_installation",
            "text": " Access to a Kubernetes v1.12.0+ cluster. Access to Oracle Coherence Docker images. OpenShift - the Coherence Operator works without modification on OpenShift but some versions of the Coherence images will not work out of the box. See the OpensShift section of the documentation that explains how to run Coherence clusters with the Operator on OpenShift. Image Pull Secrets In order for the Coherence Operator to be able to install Coherence clusters it needs to be able to pull Coherence Docker images. These images are not available in public Docker repositories and will typically Kubernetes will need authentication to be able to pull them. This is achived by creating pull secrets. Pull secrets are not global and hence secrets will be required in the namespace(s) that Coherence clusters will be installed into. see Obtain Coherence Images ",
            "title": "Prerequisites"
        },
        {
            "location": "/installation/01_installation",
            "text": " Add the coherence helm repository using the following commands: <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update ",
            "title": "Add the Coherence Helm Repository"
        },
        {
            "location": "/installation/01_installation",
            "text": " To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Uninstall the Coherence Operator Helm chart"
        },
        {
            "location": "/installation/01_installation",
            "text": " Once the Coherence Helm repo is configured the Coherence Operator can be installed using a normal Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ coherence/coherence-operator where &lt;namespace&gt; is the namespace that the Coherence Operator will be installed into and the namespace where it will manage Coherence resources Uninstall the Coherence Operator Helm chart To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Install the Coherence Operator Helm chart"
        },
        {
            "location": "/installation/01_installation",
            "text": " The simplest way to install the Coherence Operator is to use the Helm chart. This will ensure that all of the correct resources are created in Kubernetes. Add the Coherence Helm Repository Add the coherence helm repository using the following commands: <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update Install the Coherence Operator Helm chart Once the Coherence Helm repo is configured the Coherence Operator can be installed using a normal Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ coherence/coherence-operator where &lt;namespace&gt; is the namespace that the Coherence Operator will be installed into and the namespace where it will manage Coherence resources Uninstall the Coherence Operator Helm chart To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Installing With Helm"
        },
        {
            "location": "/ports/030_services",
            "text": " As described in the Additional Container Ports documentation, it is possible to expose additional ports on the Coherence container in the Pods of a Coherence resource. The Coherence Operator will create a Service to expose each additional port. By default, the name of the service is the combination of the Coherence resource name and the port name (this can default behaviour can be overridden as shown below in the section). The configuration of the Service can be altered using fields in the port spec&#8217;s service section. For example: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: {} This example exposes a single port named rest on port 8080 . The service section of the port spec is empty so the Operator will use its default behaviour to create a Service in the same namespace with the name test-cluster-rest . ",
            "title": "Configure Services for Ports"
        },
        {
            "location": "/ports/030_services",
            "text": " Sometimes it is useful to use a different name than the default for a Service for a port, for example, when the port is exposing functionality that other applications want to consume on a fixed well know endpoint. To override the generated service name with another name the service.name field can be set. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: name: payments By setting the service.name field the Service for this port will be named payments . The service for the above example would look like this: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: payments spec: ports: - name: rest port: 8080 targetPort: rest type: ClusterIP selector: coherenceDeployment: test-cluster coherenceCluster: test-cluster coherenceRole: storage coherenceComponent: coherencePod The Service name is payments instead of test-cluster-rest ",
            "title": "Override the Service Name"
        },
        {
            "location": "/ports/030_services",
            "text": " It is sometimes useful to be able to expose a service on a different port on the Service to that being used by the container. One use-case for this would be where the Coherence deployment is providing a http service where the container exposes the service on port 8080 whereas the Service can use port 80 . For example, using the same example payemnts service above: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: name: payments port: 80 The Service name will be payments The Service port will be 80 This then allows the payments service to be accessed on a simple url of http://payments ",
            "title": "Override the Service Port"
        },
        {
            "location": "/ports/030_services",
            "text": " Sometimes it may be desirable to expose a port on the Coherence container but not have the Operator automatically create a Service to expose the port. For example, maybe the port is to be exposed via some other load balancer service controlled by another system. To disable automatic service creation set the service.enabled field to false . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: enabled: false With the service.enabled field set to false no Service will be created. ",
            "title": "Disable Service Creation"
        },
        {
            "location": "/ports/030_services",
            "text": " The Coherence resource CRD allows many other settings to be configured on the Service . These fields are identical to the corresponding fields in the Kubernetes Service spec. See the Coherence CRD Service Spec documentation and the Kubernetes Service API reference . ",
            "title": "Other Service Configuration"
        },
        {
            "location": "/other/030_labels",
            "text": " Additional labels can be added to the Pods managed by the Operator. Additional labels should be added to the labels map in the Coherence CRD spec. The entries in the labels map should confirm to the recommendations and rules in the Kubernetes Labels documentation. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: labels: tier: backend environment: dev Two labels will be added to the Pods , tier=backend and environment=dev ",
            "title": "Pod Labels"
        },
        {
            "location": "/coherence/020_cluster_name",
            "text": " The default Coherence cluster name, used when the cluster field is empty, will be the same as the name of the Coherence resource, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test The name of this Coherence resource is test , which will also be used as the Coherence cluster name, effectively passing -Dcoherence.cluster=test to the JVM in the Coherence container. ",
            "title": "Default Cluster Name"
        },
        {
            "location": "/coherence/020_cluster_name",
            "text": " In a use case where multiple Coherence resources will be created to form a single Coherence cluster, the cluster field in all the Coherence resources needs to be set to the same value. <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: cluster: test-cluster --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: front-end spec: cluster: test-cluster The yaml above contains two Coherence resources, one named storage and one named front-end . Both of these Coherence resources have the same value for the cluster field, test-cluster , so the Pods in both deployments will form a single Coherence cluster named test . ",
            "title": "Specify a Cluster Name"
        },
        {
            "location": "/coherence/020_cluster_name",
            "text": " The name of the Coherence cluster that a Coherence resource is part of can be set with the cluster field in the Coherence.Spec . The cluster name is used to set the coherence.cluster system property in the JVM in the Coherence container. Default Cluster Name The default Coherence cluster name, used when the cluster field is empty, will be the same as the name of the Coherence resource, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test The name of this Coherence resource is test , which will also be used as the Coherence cluster name, effectively passing -Dcoherence.cluster=test to the JVM in the Coherence container. Specify a Cluster Name In a use case where multiple Coherence resources will be created to form a single Coherence cluster, the cluster field in all the Coherence resources needs to be set to the same value. <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: cluster: test-cluster --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: front-end spec: cluster: test-cluster The yaml above contains two Coherence resources, one named storage and one named front-end . Both of these Coherence resources have the same value for the cluster field, test-cluster , so the Pods in both deployments will form a single Coherence cluster named test . ",
            "title": "Set Coherence Cluster Name"
        },
        {
            "location": "/installation/05_private_repos",
            "text": " Sometimes the images used by a Coherence cluster need to be pulled from a private image registry that requires credentials. The Coherence Operator supports supplying credentials in the Coherence CRD configuration. The Kubernetes documentation on using a private registries gives a number of options for supplying credentials. ",
            "title": "Using Private Image Registries"
        },
        {
            "location": "/installation/05_private_repos",
            "text": " Kubernetes supports configuring pods to use imagePullSecrets for pulling images. If possible, this is the preferable, and most portable route. See the kubernetes docs for this. Once secrets have been created in the namespace that the Coherence resource is to be installed in then the secret name can be specified in the Coherence spec . It is possible to specify multiple secrets in the case where the different images being used will be pulled from different registries. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: imagePullSecrets: - name: coherence-secret The coherence-secret will be used for pulling images from the registry associated to the secret The imagePullSecrets field is a list of values in the same format that they would be specified in Kubernetes Pod specs, so multiple secrets can be specified for different authenticated registries in the case where the Coherence cluster will use images from different authenticated registries.. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: imagePullSecrets: - name: coherence-secret - name: ocr-secret The imagePullSecrets list specifies two secrets to use coherence-secret and ocr-secret ",
            "title": "Use ImagePullSecrets"
        },
        {
            "location": "/installation/02_pre_release_versions",
            "text": " Pre-release version of the Coherence Operator are made available from time to time. ",
            "title": "preambule"
        },
        {
            "location": "/installation/02_pre_release_versions",
            "text": " We cannot guarantee that pre-release versions of the Coherence Operator are bug free and hence they should not be used in production. We reserve the right to remove pre-release versions of the Helm chart and Docker images ant any time and without notice. We cannot guarantee that APIs and CRD specifications will remain stable or backwards compatible between pre-release versions. To access pre-release versions of the Helm chart add the unstable chart repository. <markup lang=\"bash\" >helm repo add coherence-unstable https://oracle.github.io/coherence-operator/charts-unstable helm repo update To list all the available Coherence Operator chart versions: <markup lang=\"bash\" >helm search coherence-operator -l The -l parameter shows all versions as opposed to just the latest versions if it was omitted. A specific pre-release version of the Helm chart can be installed using the --version argument, for example to use version 3.0.0-2005140315 : <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 3.0.0-2005140315 \\ --namespace &lt;namespace&gt; \\ --name coherence-operator The --version argument is used to specify the exact version of the chart The optional --namespace parameter to specify which namespace to install the operator into, if omitted then Helm will install into whichever is currently the default namespace for your Kubernetes configuration. When using pre-release versions of the Helm chart it is always advisable to install a specific version otherwise Helm will try to work out the latest version in the pre-release repo and as pre-release version numbers are not strictly sem-ver compliant this may be unreliable. ",
            "title": "Accessing Pre-Release Versions"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " A reference guide to the Coherence Operator CRD types. ",
            "title": "preambule"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " CoherenceResourceSpec ApplicationSpec CoherenceSpec CoherenceTracingSpec CoherenceWKASpec ConfigMapVolumeSpec ImageSpec JVMSpec JvmDebugSpec JvmGarbageCollectorSpec JvmJmxmpSpec JvmMemorySpec JvmOutOfMemorySpec LocalObjectReference NamedPortSpec NetworkSpec PersistenceSpec PersistentStorageSpec PodDNSConfig PortSpecWithSSL ProbeHandler ReadinessProbeSpec Resource Resources SSLSpec ScalingProbe ScalingSpec SecretVolumeSpec ServiceMonitorSpec ServiceSpec StartQuorum StartQuorumStatus Coherence CoherenceList CoherenceResourceStatus ",
            "title": "Table of Contents"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " CoherenceResourceSpec defines the specification of a Coherence resource. A Coherence resource is typically one or more Pods that perform the same functionality, for example storage members. Field Description Type Required image The name of the image. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; corev1.PullPolicy false imagePullSecrets ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec. If specified, these secrets will be passed to individual puller implementations for them to use. For example, in the case of docker, only DockerConfig type secrets are honored. More info: https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod [] LocalObjectReference false replicas The desired number of cluster members of this deployment. This is a pointer to distinguish between explicit zero and not specified. If not specified a default value of 3 will be used. This field cannot be negative. &#42;int32 false cluster The optional name of the Coherence cluster that this Coherence resource belongs to. If this value is set the Pods controlled by this Coherence resource will form a cluster with other Pods controlled by Coherence resources with the same cluster name. If not set the Coherence resource&#8217;s name will be used as the cluster name. &#42;string false role The name of the role that this deployment represents in a Coherence cluster. This value will be used to set the Coherence role property for all members of this role string false coherence The optional settings specific to Coherence functionality. &#42; CoherenceSpec false application The optional application specific settings. &#42; ApplicationSpec false jvm The JVM specific options &#42; JVMSpec false ports Ports specifies additional port mappings for the Pod and additional Services for those ports. [] NamedPortSpec false scaling The configuration to control safe scaling. &#42; ScalingSpec false startQuorum StartQuorum controls the start-up order of this Coherence resource in relation to other Coherence resources. [] StartQuorum false env Env is additional environment variable mappings that will be passed to the Coherence container in the Pod. To specify extra variables add them as name value pairs the same as they would be added to a Pod containers spec. [] corev1.EnvVar false labels The extra labels to add to the all of the Pods in this deployments. Labels here will add to or override those defined for the cluster. More info: http://kubernetes.io/docs/user-guide/labels map[string]string false annotations Annotations are free-form yaml that will be added to the store release as annotations Any annotations should be placed BELOW this annotations: key. For example if we wanted to include annotations for Prometheus it would look like this: annotations: prometheus.io/scrape: \\\"true\\\" + prometheus.io/port: \\\"2408\\\" map[string]string false initContainers List of additional initialization containers to add to the deployment&#8217;s Pod. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ [] corev1.Container false sideCars List of additional side-car containers to add to the deployment&#8217;s Pod. [] corev1.Container false configMapVolumes A list of ConfigMaps to add as volumes. Each entry in the list will be added as a ConfigMap Volume to the deployment&#8217;s Pods and as a VolumeMount to all of the containers and init-containers in the Pod. see: Add ConfigMap Volumes [] ConfigMapVolumeSpec false secretVolumes A list of Secrets to add as volumes. Each entry in the list will be added as a Secret Volume to the deployment&#8217;s Pods and as a VolumeMount to all of the containers and init-containers in the Pod. see: Add Secret Volumes [] SecretVolumeSpec false volumes Volumes defines extra volume mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumes section of a Pod definition + as described in https://kubernetes.io/docs/concepts/storage/volumes/ [] corev1.Volume false volumeClaimTemplates VolumeClaimTemplates defines extra PVC mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumeClaimTemplates section of a Pod definition + as described in https://kubernetes.io/docs/concepts/storage/persistent-volumes/ [] corev1.PersistentVolumeClaim false volumeMounts VolumeMounts defines extra volume mounts to map to the additional volumes or PVCs declared above in store.volumes and store.volumeClaimTemplates [] corev1.VolumeMount false healthPort The port that the health check endpoint will bind to. &#42;int32 false readinessProbe The readiness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false livenessProbe The liveness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false resources Resources is the optional resource requests and limits for the containers ref: http://kubernetes.io/docs/user-guide/compute-resources/ + By default the cpu requests is set to zero and the cpu limit set to 32. This is because it appears that K8s defaults cpu to one and since Java 10 the JVM now correctly picks up cgroup cpu limits then the JVM will only see one cpu. By setting resources.requests.cpu=0 and resources.limits.cpu=32 it ensures that the JVM will see the either the number of cpus on the host if this is &#8656; 32 or the JVM will see 32 cpus if the host has &gt; 32 cpus. The limit is set to zero so that there is no hard-limit applied. No default memory limits are applied. &#42; corev1.ResourceRequirements false affinity Affinity controls Pod scheduling preferences. ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity &#42; corev1.Affinity false nodeSelector NodeSelector is the Node labels for pod assignment ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector map[string]string false tolerations Tolerations is for nodes that have taints on them. Useful if you want to dedicate nodes to just run the coherence container + For example: tolerations: + - key: \\\"key\\\" + operator: \\\"Equal\\\" + value: \\\"value\\\" + effect: \\\"NoSchedule\\\" + ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] corev1.Toleration false securityContext SecurityContext is the PodSecurityContext that will be added to all of the Pods in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ &#42; corev1.PodSecurityContext false shareProcessNamespace Share a single process namespace between all of the containers in a pod. When this is set containers will be able to view and signal processes from other containers in the same pod, and the first process in each container will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be set. Optional: Default to false. &#42;bool false hostIPC Use the host&#8217;s ipc namespace. Optional: Default to false. &#42;bool false network Configure various networks and DNS settings for Pods in this rolw. &#42; NetworkSpec false coherenceUtils The configuration for the Coherence utils image &#42; ImageSpec false serviceAccountName The name to use for the service account to use when RBAC is enabled The role bindings must already have been created as this chart does not create them it just sets the serviceAccountName value in the Pod spec. string false automountServiceAccountToken Whether or not to auto-mount the Kubernetes API credentials for a service account &#42;bool false operatorRequestTimeout The timeout to apply to REST requests made back to the Operator from Coherence Pods. These requests are typically to obtain site and rack information for the Pod. &#42;int32 false Back to TOC ",
            "title": "CoherenceResourceSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " The specification of the application deployed into the Coherence. Field Description Type Required type The application type to execute. This field would be set if using the Coherence Graal image and running a none-Java application. For example if the application was a Node application this field would be set to \\\"node\\\". The default is to run a plain Java application. &#42;string false main Class is the Coherence container main class. The default value is com.tangosol.net.DefaultCacheServer. If the application type is non-Java this would be the name of the corresponding language specific runnable, for example if the application type is \\\"node\\\" the main may be a Javascript file. &#42;string false args Args is the optional arguments to pass to the main class. []string false workingDir The application folder in the custom artifacts Docker image containing application artifacts. This will effectively become the working directory of the Coherence container. If not set the application directory default value is \\\"/app\\\". &#42;string false Back to TOC ",
            "title": "ApplicationSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " This section of the CRD configures settings specific to Coherence. see: Coherence Configuration Field Description Type Required cacheConfig CacheConfig is the name of the cache configuration file to use see: Configure Cache Config File &#42;string false overrideConfig OverrideConfig is name of the Coherence operational configuration override file, the default is tangosol-coherence-override.xml see: Configure Operational Config File &#42;string false storageEnabled A boolean flag indicating whether members of this deployment are storage enabled. This value will set the corresponding coherence.distributed.localstorage System property. If not specified the default value is true. This flag is also used to configure the ScalingPolicy value if a value is not specified. If the StorageEnabled field is not specified or is true the scaling will be safe, if StorageEnabled is set to false scaling will be parallel. see: Configure Storage Enabled &#42;bool false persistence Persistence values configure the on-disc data persistence settings. The bool Enabled enables or disabled on disc persistence of data. see: Configure Persistence &#42; PersistenceSpec false logLevel The Coherence log level, default being 5 (info level). see: Configure Coherence log level &#42;int32 false management Management configures Coherence management over REST Note: Coherence management over REST will is available in Coherence version &gt;= 12.2.1.4. see: Management &amp; Diagnostics &#42; PortSpecWithSSL false metrics Metrics configures Coherence metrics publishing Note: Coherence metrics publishing will is available in Coherence version &gt;= 12.2.1.4. see: Metrics &#42; PortSpecWithSSL false tracing Tracing is used to configure Coherence distributed tracing functionality. &#42; CoherenceTracingSpec false allowEndangeredForStatusHA AllowEndangeredForStatusHA is a list of Coherence partitioned cache service names that are allowed to be in an endangered state when testing for StatusHA. Instances where a StatusHA check is performed include the readiness probe and when scaling a deployment. This field would not typically be used except in cases where a cache service is configured with a backup count greater than zero but it does not matter if caches in those services loose data due to member departure. Normally, such cache services would have a backup count of zero, which would automatically excluded them from the StatusHA check. []string false excludeFromWKA Exclude members of this deployment from being part of the cluster&#8217;s WKA list. see: Well Known Addressing &#42;bool false wka Specify an existing Coherence deployment to be used for WKA. If an existing deployment is to be used for WKA the ExcludeFromWKA is implicitly set to true. see: Well Known Addressing &#42; CoherenceWKASpec false skipVersionCheck Certain features rely on a version check prior to starting the server, e.g. metrics requires &gt;= 12.2.1.4. The version check relies on the ability of the start script to find coherence.jar but if due to how the image has been built this check is failing then setting this flag to true will skip version checking and assume that the latest coherence.jar is being used. &#42;bool false Back to TOC ",
            "title": "CoherenceSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " CoherenceTracingSpec configures Coherence tracing. Field Description Type Required ratio Ratio is the tracing sampling-ratio, which controls the likelihood of a tracing span being collected. For instance, a value of 1.0 will result in all tracing spans being collected, while a value of 0.1 will result in roughly 1 out of every 10 tracing spans being collected. A value of 0 indicates that tracing spans should only be collected if they are already in the context of another tracing span. With such a configuration, Coherence will not initiate tracing on its own, and it is up to the application to start an outer tracing span, from which Coherence will then collect inner tracing spans. A value of -1 disables tracing completely. The Coherence default is -1 if not overridden. For values other than -1, numbers between 0 and 1 are acceptable. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. &#42;resource.Quantity false Back to TOC ",
            "title": "CoherenceTracingSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Field Description Type Required deployment The name of the existing Coherence deployment to use for WKA. string true namespace The optional namespace of the existing Coherence deployment to use for WKA if different from this deployment&#8217;s namespace. string false Back to TOC ",
            "title": "CoherenceWKASpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Represents a ConfigMap that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add ConfigMap Volumes Field Description Type Required name The name of the ConfigMap to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the ConfigMap name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \\\"\\\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; corev1.MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \\\"\\\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the ConfigMap or its keys must be defined &#42;bool false Back to TOC ",
            "title": "ConfigMapVolumeSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " ImageSpec defines the settings for a Docker image Field Description Type Required image The image name. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; corev1.PullPolicy false Back to TOC ",
            "title": "ImageSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " The JVM configuration. Field Description Type Required classpath Classpath specifies additional items to add to the classpath of the JVM. []string false args Args specifies the options (System properties, -XX: args etc) to pass to the JVM. []string false debug The settings for enabling debug mode in the JVM. &#42; JvmDebugSpec false useContainerLimits If set to true Adds the -XX:+UseContainerSupport JVM option to ensure that the JVM respects any container resource limits. The default value is true &#42;bool false gc Set JVM garbage collector options. &#42; JvmGarbageCollectorSpec false diagnosticsVolume &#160; &#42; corev1.VolumeSource false memory Configure the JVM memory options. &#42; JvmMemorySpec false jmxmp Configure JMX using JMXMP. &#42; JvmJmxmpSpec false useJibClasspath A flag indicating whether to automatically add the default classpath for images created by the JIB tool https://github.com/GoogleContainerTools/jib If true then the /app/lib/* /app/classes and /app/resources entries are added to the JVM classpath. The default value fif not specified is true. &#42;bool false Back to TOC ",
            "title": "JVMSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " The JVM Debug specific configuration. Field Description Type Required enabled Enabled is a flag to enable or disable running the JVM in debug mode. Default is disabled. &#42;bool false suspend A boolean true if the target VM is to be suspended immediately before the main class is loaded; false otherwise. The default value is false. &#42;bool false attach Attach specifies the address of the debugger that the JVM should attempt to connect back to instead of listening on a port. &#42;string false port The port that the debugger will listen on; the default is 5005. &#42;int32 false Back to TOC ",
            "title": "JvmDebugSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Options for managing the JVM garbage collector. Field Description Type Required collector The name of the JVM garbage collector to use. G1 - adds the -XX:+UseG1GC option CMS - adds the -XX:+UseConcMarkSweepGC option Parallel - adds the -XX:+UseParallelGC Default - use the JVMs default collector The field value is case insensitive If not set G1 is used. If set to a value other than those above then the default collector for the JVM will be used. &#42;string false args Args specifies the GC options to pass to the JVM. []string false logging Enable the following GC logging args -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime Default is true &#42;bool false Back to TOC ",
            "title": "JvmGarbageCollectorSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Options for configuring JMX using JMXMP. Field Description Type Required enabled If set to true the JMXMP support will be enabled. Default is false &#42;bool false port The port tht the JMXMP MBeanServer should bind to. If not set the default port is 9099 &#42;int32 false Back to TOC ",
            "title": "JvmJmxmpSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Options for managing the JVM memory. Field Description Type Required heapSize HeapSize is the min/max heap value to pass to the JVM. The format should be the same as that used for Java&#8217;s -Xms and -Xmx JVM options. If not set the JVM defaults are used. &#42;string false maxRAM Sets the JVM option -XX:MaxRAM=N which sets the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). &#42;string false initialRAMPercentage Set initial heap size as a percentage of total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:InitialRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false maxRAMPercentage Set maximum heap size as a percentage of total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:MaxRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false minRAMPercentage Set the minimal JVM Heap size as a percentage of the total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:MinRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false stackSize StackSize is the stack size value to pass to the JVM. The format should be the same as that used for Java&#8217;s -Xss JVM option. If not set the JVM defaults are used. &#42;string false metaspaceSize MetaspaceSize is the min/max metaspace size to pass to the JVM. This sets the -XX:MetaspaceSize and -XX:MaxMetaspaceSize=size JVM options. If not set the JVM defaults are used. &#42;string false directMemorySize DirectMemorySize sets the maximum total size (in bytes) of the New I/O (the java.nio package) direct-buffer allocations. This value sets the -XX:MaxDirectMemorySize JVM option. If not set the JVM defaults are used. &#42;string false nativeMemoryTracking Adds the -XX:NativeMemoryTracking=mode JVM options where mode is on of \\\"off\\\", \\\"summary\\\" or \\\"detail\\\", the default is \\\"summary\\\" If not set to \\\"off\\\" also add -XX:+PrintNMTStatistics &#42;string false onOutOfMemory Configure the JVM behaviour when an OutOfMemoryError occurs. &#42; JvmOutOfMemorySpec false Back to TOC ",
            "title": "JvmMemorySpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Options for managing the JVM behaviour when an OutOfMemoryError occurs. Field Description Type Required exit If set to true the JVM will exit when an OOM error occurs. Default is true &#42;bool false heapDump If set to true adds the -XX:+HeapDumpOnOutOfMemoryError JVM option to cause a heap dump to be created when an OOM error occurs. Default is true &#42;bool false Back to TOC ",
            "title": "JvmOutOfMemorySpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Field Description Type Required name Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names string true Back to TOC ",
            "title": "LocalObjectReference"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Field Description Type Required name Name specifies the name of the port. string true port Port specifies the port used. int32 false protocol Protocol for container port. Must be UDP or TCP. Defaults to \\\"TCP\\\" &#42; corev1.Protocol false nodePort The port on each node on which this service is exposed when type=NodePort or LoadBalancer. Usually assigned by the system. If specified, it will be allocated to the service if unused or else creation of the service will fail. Default is to auto-allocate a port if the ServiceType of this Service requires one. More info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport &#42;int32 false hostPort Number of port to expose on the host. If specified, this must be a valid port number, 0 &lt; x &lt; 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. &#42;int32 false hostIP What host IP to bind the external port to. &#42;string false service Service configures the Kubernetes Service used to expose the port. &#42; ServiceSpec false serviceMonitor The specification of a Prometheus ServiceMonitor resource that will be created for the Service being exposed for this port. &#42; ServiceMonitorSpec false Back to TOC ",
            "title": "NamedPortSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " NetworkSpec configures various networking and DNS settings for Pods in a deployment. Field Description Type Required dnsConfig Specifies the DNS parameters of a pod. Parameters specified here will be merged to the generated DNS configuration based on DNSPolicy. &#42; PodDNSConfig false dnsPolicy Set DNS policy for the pod. Defaults to \\\"ClusterFirst\\\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. &#42; corev1.DNSPolicy false hostAliases HostAliases is an optional list of hosts and IPs that will be injected into the pod&#8217;s hosts file if specified. This is only valid for non-hostNetwork pods. [] corev1.HostAlias false hostNetwork Host networking requested for this pod. Use the host&#8217;s network namespace. If this option is set, the ports that will be used must be specified. Default to false. &#42;bool false hostname Specifies the hostname of the Pod If not specified, the pod&#8217;s hostname will be set to a system-defined value. &#42;string false Back to TOC ",
            "title": "NetworkSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " The spec for Coherence persistence. Field Description Type Required mode The persistence mode to use. Valid choices are \\\"on-demand\\\", \\\"active\\\", \\\"active-async\\\". This field will set the coherence.distributed.persistence-mode System property to \\\"default-\\\" + Mode. &#42;string false persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; corev1.VolumeSource false snapshots Snapshot values configure the on-disc persistence data snapshot (backup) settings. These settings enable a different location for persistence snapshot data. If not set then snapshot files will be written to the same volume configured for persistence data in the Persistence section. &#42; PersistentStorageSpec false Back to TOC ",
            "title": "PersistenceSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " PersistenceStorageSpec defines the persistence settings for the Coherence Field Description Type Required persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; corev1.VolumeSource false Back to TOC ",
            "title": "PersistentStorageSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Field Description Type Required nameservers A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. []string false searches A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed. []string false options A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. [] corev1.PodDNSConfigOption false Back to TOC ",
            "title": "PodDNSConfig"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " PortSpecWithSSL defines a port with SSL settings for a Coherence component Field Description Type Required enabled Enable or disable flag. &#42;bool false port The port to bind to. &#42;int32 false ssl SSL configures SSL settings for a Coherence component &#42; SSLSpec false Back to TOC ",
            "title": "PortSpecWithSSL"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " The definition of a probe handler. Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false Back to TOC ",
            "title": "ProbeHandler"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " ReadinessProbeSpec defines the settings for the Coherence Pod readiness probe Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false timeoutSeconds Number of seconds after which the probe times out. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false periodSeconds How often (in seconds) to perform the probe. &#42;int32 false successThreshold Minimum consecutive successes for the probe to be considered successful after having failed. &#42;int32 false failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. &#42;int32 false Back to TOC ",
            "title": "ReadinessProbeSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Field Description Type Required kind &#160; ResourceType true name &#160; string true spec &#160; runtime.Object true Back to TOC ",
            "title": "Resource"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Field Description Type Required version &#160; int32 true items &#160; [] Resource true Back to TOC ",
            "title": "Resources"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " SSLSpec defines the SSL settings for a Coherence component over REST endpoint. Field Description Type Required enabled Enabled is a boolean flag indicating whether enables or disables SSL on the Coherence management over REST endpoint, the default is false (disabled). &#42;bool false secrets Secrets is the name of the k8s secrets containing the Java key stores and password files. This value MUST be provided if SSL is enabled on the Coherence management over ReST endpoint. &#42;string false keyStore Keystore is the name of the Java key store file in the k8s secret to use as the SSL keystore when configuring component over REST to use SSL. &#42;string false keyStorePasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the keystore password when configuring component over REST to use SSL. &#42;string false keyPasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the key password when configuring component over REST to use SSL. &#42;string false keyStoreAlgorithm KeyStoreAlgorithm is the name of the keystore algorithm for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509 &#42;string false keyStoreProvider KeyStoreProvider is the name of the keystore provider for the keystore in the k8s secret used when configuring component over REST to use SSL. &#42;string false keyStoreType KeyStoreType is the name of the Java keystore type for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false trustStore TrustStore is the name of the Java trust store file in the k8s secret to use as the SSL trust store when configuring component over REST to use SSL. &#42;string false trustStorePasswordFile TrustStorePasswordFile is the name of the file in the k8s secret containing the trust store password when configuring component over REST to use SSL. &#42;string false trustStoreAlgorithm TrustStoreAlgorithm is the name of the keystore algorithm for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509. &#42;string false trustStoreProvider TrustStoreProvider is the name of the keystore provider for the trust store in the k8s secret used when configuring component over REST to use SSL. &#42;string false trustStoreType TrustStoreType is the name of the Java keystore type for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false requireClientCert RequireClientCert is a boolean flag indicating whether the client certificate will be authenticated by the server (two-way SSL) when configuring component over REST to use SSL. + If not set the default is false &#42;bool false Back to TOC ",
            "title": "SSLSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " ScalingProbe is the handler that will be used to determine how to check for StatusHA in a Coherence. StatusHA checking is primarily used during scaling of a deployment, a deployment must be in a safe Phase HA state before scaling takes place. If StatusHA handler is disabled for a deployment (by specifically setting Enabled to false then no check will take place and a deployment will be assumed to be safe). Field Description Type Required timeoutSeconds Number of seconds after which the handler times out (only applies to http and tcp handlers). Defaults to 1 second. Minimum value is 1. &#42;int false Back to TOC ",
            "title": "ScalingProbe"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " The configuration to control safe scaling. Field Description Type Required policy ScalingPolicy describes how the replicas of the deployment will be scaled. The default if not specified is based upon the value of the StorageEnabled field. If StorageEnabled field is not specified or is true the default scaling will be safe, if StorageEnabled is set to false the default scaling will be parallel. &#42;ScalingPolicy false probe The probe to use to determine whether a deployment is Phase HA. If not set the default handler will be used. In most use-cases the default handler would suffice but in advanced use-cases where the application code has a different concept of Phase HA to just checking Coherence services then a different handler may be specified. &#42; ScalingProbe false Back to TOC ",
            "title": "ScalingSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Represents a Secret that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add Secret Volumes Field Description Type Required name The name of the Secret to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the Secret name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \\\"\\\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; corev1.MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \\\"\\\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the Secret or its keys must be defined &#42;bool false Back to TOC ",
            "title": "SecretVolumeSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " The ServiceMonitor spec for a port service. Field Description Type Required enabled Enabled is a flag to enable or disable creation of a Prometheus ServiceMonitor for a port. If Prometheus ServiceMonitor CR is not installed no ServiceMonitor then even if this flag is true no ServiceMonitor will be created. &#42;bool false labels Additional labels to add to the ServiceMonitor. More info: http://kubernetes.io/docs/user-guide/labels map[string]string false jobLabel The label to use to retrieve the job name from. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec string false targetLabels TargetLabels transfers labels on the Kubernetes Service onto the target. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec []string false podTargetLabels PodTargetLabels transfers labels on the Kubernetes Pod onto the target. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec []string false sampleLimit SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec uint64 false path HTTP path to scrape for metrics. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false scheme HTTP scheme to use for scraping. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false params Optional HTTP URL parameters See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint map[string][]string false interval Interval at which metrics should be scraped See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false scrapeTimeout Timeout after which the scrape is ended See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false tlsConfig TLS configuration to use when scraping the endpoint See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;monitoringv1.TLSConfig false bearerTokenFile File to read bearer token for scraping targets. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false bearerTokenSecret Secret to mount to read bearer token for scraping targets. The secret needs to be in the same namespace as the service monitor and accessible by the Prometheus Operator. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint corev1.SecretKeySelector false honorLabels HonorLabels chooses the metric&#8217;s labels on collisions with target labels. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint bool false honorTimestamps HonorTimestamps controls whether Prometheus respects the timestamps present in scraped data. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;bool false basicAuth BasicAuth allow an endpoint to authenticate over basic authentication More info: https://prometheus.io/docs/operating/configuration/#endpoints See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;monitoringv1.BasicAuth false metricRelabelings MetricRelabelings to apply to samples before ingestion. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint []&#42;monitoringv1.RelabelConfig false relabelings Relabelings to apply to samples before scraping. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint []&#42;monitoringv1.RelabelConfig false proxyURL ProxyURL eg http://proxyserver:2195 Directs scrapes to proxy through this endpoint. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;string false Back to TOC ",
            "title": "ServiceMonitorSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Field Description Type Required enabled Enabled controls whether to create the service yaml or not &#42;bool false name An optional name to use to override the generated service name. &#42;string false port The service port value &#42;int32 false type Kind is the K8s service type (typically ClusterIP or LoadBalancer) The default is \\\"ClusterIP\\\". &#42; corev1.ServiceType false externalIPs externalIPs is a list of IP addresses for which nodes in the cluster will also accept traffic for this service. These IPs are not managed by Kubernetes. The user is responsible for ensuring that traffic arrives at a node with this IP. A common example is external load-balancers that are not part of the Kubernetes system. []string false clusterIP clusterIP is the IP address of the service and is usually assigned randomly by the master. If an address is specified manually and is not in use by others, it will be allocated to the service; otherwise, creation of the service will fail. This field can not be changed through updates. Valid values are \\\"None\\\", empty string (\\\"\\\"), or a valid IP address. \\\"None\\\" can be specified for headless services when proxying is not required. Only applies to types ClusterIP, NodePort, and LoadBalancer. Ignored if type is ExternalName. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42;string false loadBalancerIP LoadBalancerIP is the IP address of the load balancer &#42;string false labels The extra labels to add to the service. More info: http://kubernetes.io/docs/user-guide/labels map[string]string false annotations Annotations is free form yaml that will be added to the service annotations map[string]string false sessionAffinity Supports \\\"ClientIP\\\" and \\\"None\\\". Used to maintain session affinity. Enable client IP based session affinity. Must be ClientIP or None. Defaults to None. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42; corev1.ServiceAffinity false loadBalancerSourceRanges If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature.\\\" More info: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/ []string false externalName externalName is the external reference that kubedns or equivalent will return as a CNAME record for this service. No proxying will be involved. Must be a valid RFC-1123 hostname ( https://tools.ietf.org/html/rfc1123 ) and requires Kind to be ExternalName. &#42;string false externalTrafficPolicy externalTrafficPolicy denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints. \\\"Local\\\" preserves the client source IP and avoids a second hop for LoadBalancer and Nodeport type services, but risks potentially imbalanced traffic spreading. \\\"Cluster\\\" obscures the client source IP and may cause a second hop to another node, but should have good overall load-spreading. &#42; corev1.ServiceExternalTrafficPolicyType false healthCheckNodePort healthCheckNodePort specifies the healthcheck nodePort for the service. If not specified, HealthCheckNodePort is created by the service api backend with the allocated nodePort. Will use user-specified nodePort value if specified by the client. Only effects when Kind is set to LoadBalancer and ExternalTrafficPolicy is set to Local. &#42;int32 false publishNotReadyAddresses publishNotReadyAddresses, when set to true, indicates that DNS implementations must publish the notReadyAddresses of subsets for the Endpoints associated with the Service. The default value is false. The primary use case for setting this field is to use a StatefulSet&#8217;s Headless Service to propagate SRV records for its Pods without respect to their readiness for purpose of peer discovery. &#42;bool false sessionAffinityConfig sessionAffinityConfig contains the configurations of session affinity. &#42; corev1.SessionAffinityConfig false ipFamily ipFamily specifies whether this Service has a preference for a particular IP family (e.g. IPv4 vs. IPv6). If a specific IP family is requested, the clusterIP field will be allocated from that family, if it is available in the cluster. If no IP family is requested, the cluster&#8217;s primary IP family will be used. Other IP fields (loadBalancerIP, loadBalancerSourceRanges, externalIPs) and controllers which allocate external load-balancers should use the same IP family. Endpoints for this Service will be of this family. This field is immutable after creation. Assigning a ServiceIPFamily not available in the cluster (e.g. IPv6 in IPv4 only cluster) is an error condition and will fail during clusterIP assignment. &#42; corev1.IPFamily false Back to TOC ",
            "title": "ServiceSpec"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " StartQuorum defines the order that deployments will be started in a Coherence cluster made up of multiple deployments. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false Back to TOC ",
            "title": "StartQuorum"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " StartQuorumStatus tracks the state of a deployment&#8217;s start quorums. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false ready Whether this quorum&#8217;s condition has been met bool true Back to TOC ",
            "title": "StartQuorumStatus"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " Coherence is the Schema for the Coherence API. Field Description Type Required metadata &#160; metav1.ObjectMeta false spec &#160; CoherenceResourceSpec false status &#160; CoherenceResourceStatus false Back to TOC ",
            "title": "Coherence"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " CoherenceList contains a list of Coherence resources. Field Description Type Required metadata &#160; metav1.ListMeta false items &#160; [] Coherence true Back to TOC ",
            "title": "CoherenceList"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " CoherenceResourceStatus defines the observed state of Coherence resource. Field Description Type Required phase The phase of a Coherence resource is a simple, high-level summary of where the Coherence resource is in its lifecycle. The conditions array, the reason and message fields, and the individual container status arrays contain more detail about the pod&#8217;s status. There are eight possible phase values: Initialized: The deployment has been accepted by the Kubernetes system. Created: The deployments secondary resources, (e.g. the StatefulSet, Services etc) have been created. Ready: The StatefulSet for the deployment has the correct number of replicas and ready replicas. Waiting: The deployment&#8217;s start quorum conditions have not yet been met. Scaling: The number of replicas in the deployment is being scaled up or down. RollingUpgrade: The StatefulSet is performing a rolling upgrade. Stopped: The replica count has been set to zero. Failed: An error occurred reconciling the deployment and its secondary resources. status.ConditionType false coherenceCluster The name of the Coherence cluster that this deployment is part of. string false replicas Replicas is the desired number of members in the Coherence deployment represented by the Coherence resource. int32 false currentReplicas CurrentReplicas is the current number of members in the Coherence deployment represented by the Coherence resource. int32 false readyReplicas ReadyReplicas is the number of number of members in the Coherence deployment represented by the Coherence resource that are in the ready state. int32 false role The effective role name for this deployment. This will come from the Spec.Role field if set otherwise the deployment name will be used for the role name string false selector label query over deployments that should match the replicas count. This is same as the label selector but in the string format to avoid introspection by clients. The string will be in the same format as the query-param syntax. More info about label selectors: http://kubernetes.io/docs/user-guide/labels#label-selectors string false conditions The status conditions. status.Conditions false Back to TOC ",
            "title": "CoherenceResourceStatus"
        },
        {
            "location": "/about/04_coherence_spec",
            "text": " This is a reference for the Coherence Operator API types. These are all the types and fields that are used in the Coherence CRD. This document was generated from comments in the Go structs in the pkg/api/ directory. Table of Contents CoherenceResourceSpec ApplicationSpec CoherenceSpec CoherenceTracingSpec CoherenceWKASpec ConfigMapVolumeSpec ImageSpec JVMSpec JvmDebugSpec JvmGarbageCollectorSpec JvmJmxmpSpec JvmMemorySpec JvmOutOfMemorySpec LocalObjectReference NamedPortSpec NetworkSpec PersistenceSpec PersistentStorageSpec PodDNSConfig PortSpecWithSSL ProbeHandler ReadinessProbeSpec Resource Resources SSLSpec ScalingProbe ScalingSpec SecretVolumeSpec ServiceMonitorSpec ServiceSpec StartQuorum StartQuorumStatus Coherence CoherenceList CoherenceResourceStatus CoherenceResourceSpec CoherenceResourceSpec defines the specification of a Coherence resource. A Coherence resource is typically one or more Pods that perform the same functionality, for example storage members. Field Description Type Required image The name of the image. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; corev1.PullPolicy false imagePullSecrets ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec. If specified, these secrets will be passed to individual puller implementations for them to use. For example, in the case of docker, only DockerConfig type secrets are honored. More info: https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod [] LocalObjectReference false replicas The desired number of cluster members of this deployment. This is a pointer to distinguish between explicit zero and not specified. If not specified a default value of 3 will be used. This field cannot be negative. &#42;int32 false cluster The optional name of the Coherence cluster that this Coherence resource belongs to. If this value is set the Pods controlled by this Coherence resource will form a cluster with other Pods controlled by Coherence resources with the same cluster name. If not set the Coherence resource&#8217;s name will be used as the cluster name. &#42;string false role The name of the role that this deployment represents in a Coherence cluster. This value will be used to set the Coherence role property for all members of this role string false coherence The optional settings specific to Coherence functionality. &#42; CoherenceSpec false application The optional application specific settings. &#42; ApplicationSpec false jvm The JVM specific options &#42; JVMSpec false ports Ports specifies additional port mappings for the Pod and additional Services for those ports. [] NamedPortSpec false scaling The configuration to control safe scaling. &#42; ScalingSpec false startQuorum StartQuorum controls the start-up order of this Coherence resource in relation to other Coherence resources. [] StartQuorum false env Env is additional environment variable mappings that will be passed to the Coherence container in the Pod. To specify extra variables add them as name value pairs the same as they would be added to a Pod containers spec. [] corev1.EnvVar false labels The extra labels to add to the all of the Pods in this deployments. Labels here will add to or override those defined for the cluster. More info: http://kubernetes.io/docs/user-guide/labels map[string]string false annotations Annotations are free-form yaml that will be added to the store release as annotations Any annotations should be placed BELOW this annotations: key. For example if we wanted to include annotations for Prometheus it would look like this: annotations: prometheus.io/scrape: \\\"true\\\" + prometheus.io/port: \\\"2408\\\" map[string]string false initContainers List of additional initialization containers to add to the deployment&#8217;s Pod. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ [] corev1.Container false sideCars List of additional side-car containers to add to the deployment&#8217;s Pod. [] corev1.Container false configMapVolumes A list of ConfigMaps to add as volumes. Each entry in the list will be added as a ConfigMap Volume to the deployment&#8217;s Pods and as a VolumeMount to all of the containers and init-containers in the Pod. see: Add ConfigMap Volumes [] ConfigMapVolumeSpec false secretVolumes A list of Secrets to add as volumes. Each entry in the list will be added as a Secret Volume to the deployment&#8217;s Pods and as a VolumeMount to all of the containers and init-containers in the Pod. see: Add Secret Volumes [] SecretVolumeSpec false volumes Volumes defines extra volume mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumes section of a Pod definition + as described in https://kubernetes.io/docs/concepts/storage/volumes/ [] corev1.Volume false volumeClaimTemplates VolumeClaimTemplates defines extra PVC mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumeClaimTemplates section of a Pod definition + as described in https://kubernetes.io/docs/concepts/storage/persistent-volumes/ [] corev1.PersistentVolumeClaim false volumeMounts VolumeMounts defines extra volume mounts to map to the additional volumes or PVCs declared above in store.volumes and store.volumeClaimTemplates [] corev1.VolumeMount false healthPort The port that the health check endpoint will bind to. &#42;int32 false readinessProbe The readiness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false livenessProbe The liveness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false resources Resources is the optional resource requests and limits for the containers ref: http://kubernetes.io/docs/user-guide/compute-resources/ + By default the cpu requests is set to zero and the cpu limit set to 32. This is because it appears that K8s defaults cpu to one and since Java 10 the JVM now correctly picks up cgroup cpu limits then the JVM will only see one cpu. By setting resources.requests.cpu=0 and resources.limits.cpu=32 it ensures that the JVM will see the either the number of cpus on the host if this is &#8656; 32 or the JVM will see 32 cpus if the host has &gt; 32 cpus. The limit is set to zero so that there is no hard-limit applied. No default memory limits are applied. &#42; corev1.ResourceRequirements false affinity Affinity controls Pod scheduling preferences. ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity &#42; corev1.Affinity false nodeSelector NodeSelector is the Node labels for pod assignment ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector map[string]string false tolerations Tolerations is for nodes that have taints on them. Useful if you want to dedicate nodes to just run the coherence container + For example: tolerations: + - key: \\\"key\\\" + operator: \\\"Equal\\\" + value: \\\"value\\\" + effect: \\\"NoSchedule\\\" + ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] corev1.Toleration false securityContext SecurityContext is the PodSecurityContext that will be added to all of the Pods in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ &#42; corev1.PodSecurityContext false shareProcessNamespace Share a single process namespace between all of the containers in a pod. When this is set containers will be able to view and signal processes from other containers in the same pod, and the first process in each container will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be set. Optional: Default to false. &#42;bool false hostIPC Use the host&#8217;s ipc namespace. Optional: Default to false. &#42;bool false network Configure various networks and DNS settings for Pods in this rolw. &#42; NetworkSpec false coherenceUtils The configuration for the Coherence utils image &#42; ImageSpec false serviceAccountName The name to use for the service account to use when RBAC is enabled The role bindings must already have been created as this chart does not create them it just sets the serviceAccountName value in the Pod spec. string false automountServiceAccountToken Whether or not to auto-mount the Kubernetes API credentials for a service account &#42;bool false operatorRequestTimeout The timeout to apply to REST requests made back to the Operator from Coherence Pods. These requests are typically to obtain site and rack information for the Pod. &#42;int32 false Back to TOC ApplicationSpec The specification of the application deployed into the Coherence. Field Description Type Required type The application type to execute. This field would be set if using the Coherence Graal image and running a none-Java application. For example if the application was a Node application this field would be set to \\\"node\\\". The default is to run a plain Java application. &#42;string false main Class is the Coherence container main class. The default value is com.tangosol.net.DefaultCacheServer. If the application type is non-Java this would be the name of the corresponding language specific runnable, for example if the application type is \\\"node\\\" the main may be a Javascript file. &#42;string false args Args is the optional arguments to pass to the main class. []string false workingDir The application folder in the custom artifacts Docker image containing application artifacts. This will effectively become the working directory of the Coherence container. If not set the application directory default value is \\\"/app\\\". &#42;string false Back to TOC CoherenceSpec This section of the CRD configures settings specific to Coherence. see: Coherence Configuration Field Description Type Required cacheConfig CacheConfig is the name of the cache configuration file to use see: Configure Cache Config File &#42;string false overrideConfig OverrideConfig is name of the Coherence operational configuration override file, the default is tangosol-coherence-override.xml see: Configure Operational Config File &#42;string false storageEnabled A boolean flag indicating whether members of this deployment are storage enabled. This value will set the corresponding coherence.distributed.localstorage System property. If not specified the default value is true. This flag is also used to configure the ScalingPolicy value if a value is not specified. If the StorageEnabled field is not specified or is true the scaling will be safe, if StorageEnabled is set to false scaling will be parallel. see: Configure Storage Enabled &#42;bool false persistence Persistence values configure the on-disc data persistence settings. The bool Enabled enables or disabled on disc persistence of data. see: Configure Persistence &#42; PersistenceSpec false logLevel The Coherence log level, default being 5 (info level). see: Configure Coherence log level &#42;int32 false management Management configures Coherence management over REST Note: Coherence management over REST will is available in Coherence version &gt;= 12.2.1.4. see: Management &amp; Diagnostics &#42; PortSpecWithSSL false metrics Metrics configures Coherence metrics publishing Note: Coherence metrics publishing will is available in Coherence version &gt;= 12.2.1.4. see: Metrics &#42; PortSpecWithSSL false tracing Tracing is used to configure Coherence distributed tracing functionality. &#42; CoherenceTracingSpec false allowEndangeredForStatusHA AllowEndangeredForStatusHA is a list of Coherence partitioned cache service names that are allowed to be in an endangered state when testing for StatusHA. Instances where a StatusHA check is performed include the readiness probe and when scaling a deployment. This field would not typically be used except in cases where a cache service is configured with a backup count greater than zero but it does not matter if caches in those services loose data due to member departure. Normally, such cache services would have a backup count of zero, which would automatically excluded them from the StatusHA check. []string false excludeFromWKA Exclude members of this deployment from being part of the cluster&#8217;s WKA list. see: Well Known Addressing &#42;bool false wka Specify an existing Coherence deployment to be used for WKA. If an existing deployment is to be used for WKA the ExcludeFromWKA is implicitly set to true. see: Well Known Addressing &#42; CoherenceWKASpec false skipVersionCheck Certain features rely on a version check prior to starting the server, e.g. metrics requires &gt;= 12.2.1.4. The version check relies on the ability of the start script to find coherence.jar but if due to how the image has been built this check is failing then setting this flag to true will skip version checking and assume that the latest coherence.jar is being used. &#42;bool false Back to TOC CoherenceTracingSpec CoherenceTracingSpec configures Coherence tracing. Field Description Type Required ratio Ratio is the tracing sampling-ratio, which controls the likelihood of a tracing span being collected. For instance, a value of 1.0 will result in all tracing spans being collected, while a value of 0.1 will result in roughly 1 out of every 10 tracing spans being collected. A value of 0 indicates that tracing spans should only be collected if they are already in the context of another tracing span. With such a configuration, Coherence will not initiate tracing on its own, and it is up to the application to start an outer tracing span, from which Coherence will then collect inner tracing spans. A value of -1 disables tracing completely. The Coherence default is -1 if not overridden. For values other than -1, numbers between 0 and 1 are acceptable. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. &#42;resource.Quantity false Back to TOC CoherenceWKASpec Field Description Type Required deployment The name of the existing Coherence deployment to use for WKA. string true namespace The optional namespace of the existing Coherence deployment to use for WKA if different from this deployment&#8217;s namespace. string false Back to TOC ConfigMapVolumeSpec Represents a ConfigMap that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add ConfigMap Volumes Field Description Type Required name The name of the ConfigMap to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the ConfigMap name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \\\"\\\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; corev1.MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \\\"\\\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the ConfigMap or its keys must be defined &#42;bool false Back to TOC ImageSpec ImageSpec defines the settings for a Docker image Field Description Type Required image The image name. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; corev1.PullPolicy false Back to TOC JVMSpec The JVM configuration. Field Description Type Required classpath Classpath specifies additional items to add to the classpath of the JVM. []string false args Args specifies the options (System properties, -XX: args etc) to pass to the JVM. []string false debug The settings for enabling debug mode in the JVM. &#42; JvmDebugSpec false useContainerLimits If set to true Adds the -XX:+UseContainerSupport JVM option to ensure that the JVM respects any container resource limits. The default value is true &#42;bool false gc Set JVM garbage collector options. &#42; JvmGarbageCollectorSpec false diagnosticsVolume &#160; &#42; corev1.VolumeSource false memory Configure the JVM memory options. &#42; JvmMemorySpec false jmxmp Configure JMX using JMXMP. &#42; JvmJmxmpSpec false useJibClasspath A flag indicating whether to automatically add the default classpath for images created by the JIB tool https://github.com/GoogleContainerTools/jib If true then the /app/lib/* /app/classes and /app/resources entries are added to the JVM classpath. The default value fif not specified is true. &#42;bool false Back to TOC JvmDebugSpec The JVM Debug specific configuration. Field Description Type Required enabled Enabled is a flag to enable or disable running the JVM in debug mode. Default is disabled. &#42;bool false suspend A boolean true if the target VM is to be suspended immediately before the main class is loaded; false otherwise. The default value is false. &#42;bool false attach Attach specifies the address of the debugger that the JVM should attempt to connect back to instead of listening on a port. &#42;string false port The port that the debugger will listen on; the default is 5005. &#42;int32 false Back to TOC JvmGarbageCollectorSpec Options for managing the JVM garbage collector. Field Description Type Required collector The name of the JVM garbage collector to use. G1 - adds the -XX:+UseG1GC option CMS - adds the -XX:+UseConcMarkSweepGC option Parallel - adds the -XX:+UseParallelGC Default - use the JVMs default collector The field value is case insensitive If not set G1 is used. If set to a value other than those above then the default collector for the JVM will be used. &#42;string false args Args specifies the GC options to pass to the JVM. []string false logging Enable the following GC logging args -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime Default is true &#42;bool false Back to TOC JvmJmxmpSpec Options for configuring JMX using JMXMP. Field Description Type Required enabled If set to true the JMXMP support will be enabled. Default is false &#42;bool false port The port tht the JMXMP MBeanServer should bind to. If not set the default port is 9099 &#42;int32 false Back to TOC JvmMemorySpec Options for managing the JVM memory. Field Description Type Required heapSize HeapSize is the min/max heap value to pass to the JVM. The format should be the same as that used for Java&#8217;s -Xms and -Xmx JVM options. If not set the JVM defaults are used. &#42;string false maxRAM Sets the JVM option -XX:MaxRAM=N which sets the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). &#42;string false initialRAMPercentage Set initial heap size as a percentage of total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:InitialRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false maxRAMPercentage Set maximum heap size as a percentage of total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:MaxRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false minRAMPercentage Set the minimal JVM Heap size as a percentage of the total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:MinRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false stackSize StackSize is the stack size value to pass to the JVM. The format should be the same as that used for Java&#8217;s -Xss JVM option. If not set the JVM defaults are used. &#42;string false metaspaceSize MetaspaceSize is the min/max metaspace size to pass to the JVM. This sets the -XX:MetaspaceSize and -XX:MaxMetaspaceSize=size JVM options. If not set the JVM defaults are used. &#42;string false directMemorySize DirectMemorySize sets the maximum total size (in bytes) of the New I/O (the java.nio package) direct-buffer allocations. This value sets the -XX:MaxDirectMemorySize JVM option. If not set the JVM defaults are used. &#42;string false nativeMemoryTracking Adds the -XX:NativeMemoryTracking=mode JVM options where mode is on of \\\"off\\\", \\\"summary\\\" or \\\"detail\\\", the default is \\\"summary\\\" If not set to \\\"off\\\" also add -XX:+PrintNMTStatistics &#42;string false onOutOfMemory Configure the JVM behaviour when an OutOfMemoryError occurs. &#42; JvmOutOfMemorySpec false Back to TOC JvmOutOfMemorySpec Options for managing the JVM behaviour when an OutOfMemoryError occurs. Field Description Type Required exit If set to true the JVM will exit when an OOM error occurs. Default is true &#42;bool false heapDump If set to true adds the -XX:+HeapDumpOnOutOfMemoryError JVM option to cause a heap dump to be created when an OOM error occurs. Default is true &#42;bool false Back to TOC LocalObjectReference LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Field Description Type Required name Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names string true Back to TOC NamedPortSpec Field Description Type Required name Name specifies the name of the port. string true port Port specifies the port used. int32 false protocol Protocol for container port. Must be UDP or TCP. Defaults to \\\"TCP\\\" &#42; corev1.Protocol false nodePort The port on each node on which this service is exposed when type=NodePort or LoadBalancer. Usually assigned by the system. If specified, it will be allocated to the service if unused or else creation of the service will fail. Default is to auto-allocate a port if the ServiceType of this Service requires one. More info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport &#42;int32 false hostPort Number of port to expose on the host. If specified, this must be a valid port number, 0 &lt; x &lt; 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. &#42;int32 false hostIP What host IP to bind the external port to. &#42;string false service Service configures the Kubernetes Service used to expose the port. &#42; ServiceSpec false serviceMonitor The specification of a Prometheus ServiceMonitor resource that will be created for the Service being exposed for this port. &#42; ServiceMonitorSpec false Back to TOC NetworkSpec NetworkSpec configures various networking and DNS settings for Pods in a deployment. Field Description Type Required dnsConfig Specifies the DNS parameters of a pod. Parameters specified here will be merged to the generated DNS configuration based on DNSPolicy. &#42; PodDNSConfig false dnsPolicy Set DNS policy for the pod. Defaults to \\\"ClusterFirst\\\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. &#42; corev1.DNSPolicy false hostAliases HostAliases is an optional list of hosts and IPs that will be injected into the pod&#8217;s hosts file if specified. This is only valid for non-hostNetwork pods. [] corev1.HostAlias false hostNetwork Host networking requested for this pod. Use the host&#8217;s network namespace. If this option is set, the ports that will be used must be specified. Default to false. &#42;bool false hostname Specifies the hostname of the Pod If not specified, the pod&#8217;s hostname will be set to a system-defined value. &#42;string false Back to TOC PersistenceSpec The spec for Coherence persistence. Field Description Type Required mode The persistence mode to use. Valid choices are \\\"on-demand\\\", \\\"active\\\", \\\"active-async\\\". This field will set the coherence.distributed.persistence-mode System property to \\\"default-\\\" + Mode. &#42;string false persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; corev1.VolumeSource false snapshots Snapshot values configure the on-disc persistence data snapshot (backup) settings. These settings enable a different location for persistence snapshot data. If not set then snapshot files will be written to the same volume configured for persistence data in the Persistence section. &#42; PersistentStorageSpec false Back to TOC PersistentStorageSpec PersistenceStorageSpec defines the persistence settings for the Coherence Field Description Type Required persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; corev1.VolumeSource false Back to TOC PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Field Description Type Required nameservers A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. []string false searches A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed. []string false options A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. [] corev1.PodDNSConfigOption false Back to TOC PortSpecWithSSL PortSpecWithSSL defines a port with SSL settings for a Coherence component Field Description Type Required enabled Enable or disable flag. &#42;bool false port The port to bind to. &#42;int32 false ssl SSL configures SSL settings for a Coherence component &#42; SSLSpec false Back to TOC ProbeHandler The definition of a probe handler. Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false Back to TOC ReadinessProbeSpec ReadinessProbeSpec defines the settings for the Coherence Pod readiness probe Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false timeoutSeconds Number of seconds after which the probe times out. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false periodSeconds How often (in seconds) to perform the probe. &#42;int32 false successThreshold Minimum consecutive successes for the probe to be considered successful after having failed. &#42;int32 false failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. &#42;int32 false Back to TOC Resource Field Description Type Required kind &#160; ResourceType true name &#160; string true spec &#160; runtime.Object true Back to TOC Resources Field Description Type Required version &#160; int32 true items &#160; [] Resource true Back to TOC SSLSpec SSLSpec defines the SSL settings for a Coherence component over REST endpoint. Field Description Type Required enabled Enabled is a boolean flag indicating whether enables or disables SSL on the Coherence management over REST endpoint, the default is false (disabled). &#42;bool false secrets Secrets is the name of the k8s secrets containing the Java key stores and password files. This value MUST be provided if SSL is enabled on the Coherence management over ReST endpoint. &#42;string false keyStore Keystore is the name of the Java key store file in the k8s secret to use as the SSL keystore when configuring component over REST to use SSL. &#42;string false keyStorePasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the keystore password when configuring component over REST to use SSL. &#42;string false keyPasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the key password when configuring component over REST to use SSL. &#42;string false keyStoreAlgorithm KeyStoreAlgorithm is the name of the keystore algorithm for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509 &#42;string false keyStoreProvider KeyStoreProvider is the name of the keystore provider for the keystore in the k8s secret used when configuring component over REST to use SSL. &#42;string false keyStoreType KeyStoreType is the name of the Java keystore type for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false trustStore TrustStore is the name of the Java trust store file in the k8s secret to use as the SSL trust store when configuring component over REST to use SSL. &#42;string false trustStorePasswordFile TrustStorePasswordFile is the name of the file in the k8s secret containing the trust store password when configuring component over REST to use SSL. &#42;string false trustStoreAlgorithm TrustStoreAlgorithm is the name of the keystore algorithm for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509. &#42;string false trustStoreProvider TrustStoreProvider is the name of the keystore provider for the trust store in the k8s secret used when configuring component over REST to use SSL. &#42;string false trustStoreType TrustStoreType is the name of the Java keystore type for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false requireClientCert RequireClientCert is a boolean flag indicating whether the client certificate will be authenticated by the server (two-way SSL) when configuring component over REST to use SSL. + If not set the default is false &#42;bool false Back to TOC ScalingProbe ScalingProbe is the handler that will be used to determine how to check for StatusHA in a Coherence. StatusHA checking is primarily used during scaling of a deployment, a deployment must be in a safe Phase HA state before scaling takes place. If StatusHA handler is disabled for a deployment (by specifically setting Enabled to false then no check will take place and a deployment will be assumed to be safe). Field Description Type Required timeoutSeconds Number of seconds after which the handler times out (only applies to http and tcp handlers). Defaults to 1 second. Minimum value is 1. &#42;int false Back to TOC ScalingSpec The configuration to control safe scaling. Field Description Type Required policy ScalingPolicy describes how the replicas of the deployment will be scaled. The default if not specified is based upon the value of the StorageEnabled field. If StorageEnabled field is not specified or is true the default scaling will be safe, if StorageEnabled is set to false the default scaling will be parallel. &#42;ScalingPolicy false probe The probe to use to determine whether a deployment is Phase HA. If not set the default handler will be used. In most use-cases the default handler would suffice but in advanced use-cases where the application code has a different concept of Phase HA to just checking Coherence services then a different handler may be specified. &#42; ScalingProbe false Back to TOC SecretVolumeSpec Represents a Secret that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add Secret Volumes Field Description Type Required name The name of the Secret to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the Secret name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \\\"\\\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; corev1.MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \\\"\\\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the Secret or its keys must be defined &#42;bool false Back to TOC ServiceMonitorSpec The ServiceMonitor spec for a port service. Field Description Type Required enabled Enabled is a flag to enable or disable creation of a Prometheus ServiceMonitor for a port. If Prometheus ServiceMonitor CR is not installed no ServiceMonitor then even if this flag is true no ServiceMonitor will be created. &#42;bool false labels Additional labels to add to the ServiceMonitor. More info: http://kubernetes.io/docs/user-guide/labels map[string]string false jobLabel The label to use to retrieve the job name from. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec string false targetLabels TargetLabels transfers labels on the Kubernetes Service onto the target. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec []string false podTargetLabels PodTargetLabels transfers labels on the Kubernetes Pod onto the target. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec []string false sampleLimit SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. See https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec uint64 false path HTTP path to scrape for metrics. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false scheme HTTP scheme to use for scraping. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false params Optional HTTP URL parameters See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint map[string][]string false interval Interval at which metrics should be scraped See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false scrapeTimeout Timeout after which the scrape is ended See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false tlsConfig TLS configuration to use when scraping the endpoint See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;monitoringv1.TLSConfig false bearerTokenFile File to read bearer token for scraping targets. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint string false bearerTokenSecret Secret to mount to read bearer token for scraping targets. The secret needs to be in the same namespace as the service monitor and accessible by the Prometheus Operator. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint corev1.SecretKeySelector false honorLabels HonorLabels chooses the metric&#8217;s labels on collisions with target labels. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint bool false honorTimestamps HonorTimestamps controls whether Prometheus respects the timestamps present in scraped data. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;bool false basicAuth BasicAuth allow an endpoint to authenticate over basic authentication More info: https://prometheus.io/docs/operating/configuration/#endpoints See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;monitoringv1.BasicAuth false metricRelabelings MetricRelabelings to apply to samples before ingestion. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint []&#42;monitoringv1.RelabelConfig false relabelings Relabelings to apply to samples before scraping. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint []&#42;monitoringv1.RelabelConfig false proxyURL ProxyURL eg http://proxyserver:2195 Directs scrapes to proxy through this endpoint. See https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint &#42;string false Back to TOC ServiceSpec Field Description Type Required enabled Enabled controls whether to create the service yaml or not &#42;bool false name An optional name to use to override the generated service name. &#42;string false port The service port value &#42;int32 false type Kind is the K8s service type (typically ClusterIP or LoadBalancer) The default is \\\"ClusterIP\\\". &#42; corev1.ServiceType false externalIPs externalIPs is a list of IP addresses for which nodes in the cluster will also accept traffic for this service. These IPs are not managed by Kubernetes. The user is responsible for ensuring that traffic arrives at a node with this IP. A common example is external load-balancers that are not part of the Kubernetes system. []string false clusterIP clusterIP is the IP address of the service and is usually assigned randomly by the master. If an address is specified manually and is not in use by others, it will be allocated to the service; otherwise, creation of the service will fail. This field can not be changed through updates. Valid values are \\\"None\\\", empty string (\\\"\\\"), or a valid IP address. \\\"None\\\" can be specified for headless services when proxying is not required. Only applies to types ClusterIP, NodePort, and LoadBalancer. Ignored if type is ExternalName. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42;string false loadBalancerIP LoadBalancerIP is the IP address of the load balancer &#42;string false labels The extra labels to add to the service. More info: http://kubernetes.io/docs/user-guide/labels map[string]string false annotations Annotations is free form yaml that will be added to the service annotations map[string]string false sessionAffinity Supports \\\"ClientIP\\\" and \\\"None\\\". Used to maintain session affinity. Enable client IP based session affinity. Must be ClientIP or None. Defaults to None. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42; corev1.ServiceAffinity false loadBalancerSourceRanges If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature.\\\" More info: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/ []string false externalName externalName is the external reference that kubedns or equivalent will return as a CNAME record for this service. No proxying will be involved. Must be a valid RFC-1123 hostname ( https://tools.ietf.org/html/rfc1123 ) and requires Kind to be ExternalName. &#42;string false externalTrafficPolicy externalTrafficPolicy denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints. \\\"Local\\\" preserves the client source IP and avoids a second hop for LoadBalancer and Nodeport type services, but risks potentially imbalanced traffic spreading. \\\"Cluster\\\" obscures the client source IP and may cause a second hop to another node, but should have good overall load-spreading. &#42; corev1.ServiceExternalTrafficPolicyType false healthCheckNodePort healthCheckNodePort specifies the healthcheck nodePort for the service. If not specified, HealthCheckNodePort is created by the service api backend with the allocated nodePort. Will use user-specified nodePort value if specified by the client. Only effects when Kind is set to LoadBalancer and ExternalTrafficPolicy is set to Local. &#42;int32 false publishNotReadyAddresses publishNotReadyAddresses, when set to true, indicates that DNS implementations must publish the notReadyAddresses of subsets for the Endpoints associated with the Service. The default value is false. The primary use case for setting this field is to use a StatefulSet&#8217;s Headless Service to propagate SRV records for its Pods without respect to their readiness for purpose of peer discovery. &#42;bool false sessionAffinityConfig sessionAffinityConfig contains the configurations of session affinity. &#42; corev1.SessionAffinityConfig false ipFamily ipFamily specifies whether this Service has a preference for a particular IP family (e.g. IPv4 vs. IPv6). If a specific IP family is requested, the clusterIP field will be allocated from that family, if it is available in the cluster. If no IP family is requested, the cluster&#8217;s primary IP family will be used. Other IP fields (loadBalancerIP, loadBalancerSourceRanges, externalIPs) and controllers which allocate external load-balancers should use the same IP family. Endpoints for this Service will be of this family. This field is immutable after creation. Assigning a ServiceIPFamily not available in the cluster (e.g. IPv6 in IPv4 only cluster) is an error condition and will fail during clusterIP assignment. &#42; corev1.IPFamily false Back to TOC StartQuorum StartQuorum defines the order that deployments will be started in a Coherence cluster made up of multiple deployments. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false Back to TOC StartQuorumStatus StartQuorumStatus tracks the state of a deployment&#8217;s start quorums. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false ready Whether this quorum&#8217;s condition has been met bool true Back to TOC Coherence Coherence is the Schema for the Coherence API. Field Description Type Required metadata &#160; metav1.ObjectMeta false spec &#160; CoherenceResourceSpec false status &#160; CoherenceResourceStatus false Back to TOC CoherenceList CoherenceList contains a list of Coherence resources. Field Description Type Required metadata &#160; metav1.ListMeta false items &#160; [] Coherence true Back to TOC CoherenceResourceStatus CoherenceResourceStatus defines the observed state of Coherence resource. Field Description Type Required phase The phase of a Coherence resource is a simple, high-level summary of where the Coherence resource is in its lifecycle. The conditions array, the reason and message fields, and the individual container status arrays contain more detail about the pod&#8217;s status. There are eight possible phase values: Initialized: The deployment has been accepted by the Kubernetes system. Created: The deployments secondary resources, (e.g. the StatefulSet, Services etc) have been created. Ready: The StatefulSet for the deployment has the correct number of replicas and ready replicas. Waiting: The deployment&#8217;s start quorum conditions have not yet been met. Scaling: The number of replicas in the deployment is being scaled up or down. RollingUpgrade: The StatefulSet is performing a rolling upgrade. Stopped: The replica count has been set to zero. Failed: An error occurred reconciling the deployment and its secondary resources. status.ConditionType false coherenceCluster The name of the Coherence cluster that this deployment is part of. string false replicas Replicas is the desired number of members in the Coherence deployment represented by the Coherence resource. int32 false currentReplicas CurrentReplicas is the current number of members in the Coherence deployment represented by the Coherence resource. int32 false readyReplicas ReadyReplicas is the number of number of members in the Coherence deployment represented by the Coherence resource that are in the ready state. int32 false role The effective role name for this deployment. This will come from the Spec.Role field if set otherwise the deployment name will be used for the role name string false selector label query over deployments that should match the replicas count. This is same as the label selector but in the string format to avoid introspection by clients. The string will be in the same format as the query-param syntax. More info about label selectors: http://kubernetes.io/docs/user-guide/labels#label-selectors string false conditions The status conditions. status.Conditions false Back to TOC ",
            "title": "Coherence Operator API Docs"
        },
        {
            "location": "/jvm/050_memory",
            "text": " The JVM has an option -XX:MaxRAM=N the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). When using resource limited containers it is useful to set the max RAM option to avoid the JVM exceeding the container limits. The Coherence CRD allows the max RAM option to be set using the jvm.memory.maxRAM field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: maxRAM: 10g ",
            "title": "Max RAM"
        },
        {
            "location": "/jvm/050_memory",
            "text": " There are three JVM options that can be used to control the JVM heap as a percentage of the available memory. These options can be useful when controlling memory as a percentage of container memory in combination with rescource limits on containers. JVM Option Description -XX:InitialRAMPercentage=N Sets the initial amount of memory that the JVM will use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 1.5625 percent. '-XX:MaxRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 25 percent. Specifying this option disables automatic use of compressed oops if the combined result of this and other options influencing the maximum amount of memory is larger than the range of memory addressable by compressed oops. '-XX:MinRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option for small heaps. A small heap is a heap of approximately 125 MB. The default value is 50 percent. Where N is a decimal value between 0 and 100. For example, 12.3456. When running in a container, and the -XX:+UseContainerSupport is set (which it is by default for the Coherence container), both the default heap size for containers, the -XX:InitialRAMPercentage option, the -XX:MaxRAMPercentage option, and the -XX:MaxRAMPercentage option, will be based on the available container memory. Some JVMs may not support these options. The Coherence CRD allows these options to be set with the jvm.memory.initialRAMPercentage , jvm.memory.minRAMPercentage , and jvm.memory.maxRAMPercentage fields. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: initialRAMPercentage: 10 minRAMPercentage: 5.75 maxRAMPercentage: 75 Setting the jvm.memory.heapSize field will cause RAM percentage fields to be ignored. Due to CRDs not supporting decimal fields the RAM percentage fields are of type resource.Quantity, see the Kubernetes Quantity API docs for details of the different number formats allowed. ",
            "title": "Heap Size as a Percentage of Container Memory"
        },
        {
            "location": "/jvm/050_memory",
            "text": " To set the JVM heap size set the jvm.memory.heapSize field. The value of the field can be any value that can be used with the JVM -Xmx and -Xms arguments. The value of the jvm.memory.heapSize field will be used to set both the -Xms and -Xmx arguments, so the heap will be a fixed size. For example setting jvm.memory.heapSize to 5g will effectively pass -Xms5g -Xmx5g to the JVM. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: heapSize: 10g This example sets the heap size to 10g . ",
            "title": "Heap Size"
        },
        {
            "location": "/jvm/050_memory",
            "text": " Direct memory size is used to limit on memory that can be reserved for all Direct Byte Buffers. If a value is set for this option, the sum of all Direct Byte Buffer sizes cannot exceed the limit. After the limit is reached, a new Direct Byte Buffer can be allocated only when enough old buffers are freed to provide enough space to allocate the new buffer. By default, the VM limits the amount of heap memory used for Direct Byte Buffers to approximately 85% of the maximum heap size. To set a value for the direct memory size use the jvm.memory.directMemorySize field. This wil set the value of the -XX:MaxDirectMemorySize JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: directMemorySize: 10g The direct memory size is set to 10g which will pass -XX:MaxDirectMemorySize=10g to the JVM. ",
            "title": "Direct Memory Size (NIO Memory)"
        },
        {
            "location": "/jvm/050_memory",
            "text": " Metaspace is memory the VM uses to store class metadata. Class metadata are the runtime representation of java classes within a JVM process - basically any information the JVM needs to work with a Java class. That includes, but is not limited to, runtime representation of data from the JVM class file format. To set the size of the metaspace use the jvm.memory.metaspaceSize field in the Coherence CRD. Setting this field sets both the -XX:MetaspaceSize and -XX:MaxMetaspaceSize JVM options to this value giving a fixed size metaspace. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: metaspaceSize: 100m Set the metaspace size to 100m which will pass -XX:MetaspaceSize=100m -XX:MaxMetaspaceSize=100m to the JVM. ",
            "title": "Metaspace Size"
        },
        {
            "location": "/jvm/050_memory",
            "text": " Thread stacks are memory areas allocated for each Java thread for their internal use. This is where the thread stores its local execution state. The current default size for a linux JVM is 1MB. To set the stack size use the jvm.memory.stackSize field in the Coherence CRD. Setting this value sets the -Xss JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: stackSize: 500k The stack size will be set to 500k , passing -Xss500k to the JVM. ",
            "title": "Stack Size"
        },
        {
            "location": "/jvm/050_memory",
            "text": " The Coherence CRD allows two optional behaviours to be specified if the JVM throws an out of memory error. The jvm.memory.onOutOfMemory.heapDump is a bool field that when set to true will pass the -XX:+HeapDumpOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off heap dumps on OOM set the specifically field to be false . The jvm.memory.onOutOfMemory.exit is a bool field that when set to true will pass the -XX:+ExitOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off killing the JVM on OOM set the specifically field to be false . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: onOutOfMemory: heapDump: true exit: true The JVM will create a heap dump on OOM The JVM will exit on OOM ",
            "title": "Out Of Memory Behaviour"
        },
        {
            "location": "/jvm/050_memory",
            "text": " The Native Memory Tracking (NMT) is a Java VM feature that tracks internal memory usage for a JVM. The Coherence CRD allows native memory tracking to be configured using the jvm.memory.nativeMemoryTracking field. Setting this field sets the -XX:NativeMemoryTracking JVM option. There are three valid values, off , summary or detail . If not specified the default value used by the operator is summary <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: nativeMemoryTracking: detail Native memory tracking is set to detail which will pass the -XX:NativeMemoryTracking=detail option to the JVM. ",
            "title": "Native Memory Tracking"
        },
        {
            "location": "/jvm/050_memory",
            "text": " The JVM has a number of arguments that set the sizes of different memory regions; the most commonly set is the heap size but there are a number of others. The Coherence CRD spec has fields that allow some of these to sizes to be set. The Coherence CRD also has settings to control the behaviour of the JVM if an out of memory error occurs. For example, killing the container, creating a heap dump etc. Max RAM The JVM has an option -XX:MaxRAM=N the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). When using resource limited containers it is useful to set the max RAM option to avoid the JVM exceeding the container limits. The Coherence CRD allows the max RAM option to be set using the jvm.memory.maxRAM field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: maxRAM: 10g Heap Size as a Percentage of Container Memory There are three JVM options that can be used to control the JVM heap as a percentage of the available memory. These options can be useful when controlling memory as a percentage of container memory in combination with rescource limits on containers. JVM Option Description -XX:InitialRAMPercentage=N Sets the initial amount of memory that the JVM will use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 1.5625 percent. '-XX:MaxRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 25 percent. Specifying this option disables automatic use of compressed oops if the combined result of this and other options influencing the maximum amount of memory is larger than the range of memory addressable by compressed oops. '-XX:MinRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option for small heaps. A small heap is a heap of approximately 125 MB. The default value is 50 percent. Where N is a decimal value between 0 and 100. For example, 12.3456. When running in a container, and the -XX:+UseContainerSupport is set (which it is by default for the Coherence container), both the default heap size for containers, the -XX:InitialRAMPercentage option, the -XX:MaxRAMPercentage option, and the -XX:MaxRAMPercentage option, will be based on the available container memory. Some JVMs may not support these options. The Coherence CRD allows these options to be set with the jvm.memory.initialRAMPercentage , jvm.memory.minRAMPercentage , and jvm.memory.maxRAMPercentage fields. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: initialRAMPercentage: 10 minRAMPercentage: 5.75 maxRAMPercentage: 75 Setting the jvm.memory.heapSize field will cause RAM percentage fields to be ignored. Due to CRDs not supporting decimal fields the RAM percentage fields are of type resource.Quantity, see the Kubernetes Quantity API docs for details of the different number formats allowed. Heap Size To set the JVM heap size set the jvm.memory.heapSize field. The value of the field can be any value that can be used with the JVM -Xmx and -Xms arguments. The value of the jvm.memory.heapSize field will be used to set both the -Xms and -Xmx arguments, so the heap will be a fixed size. For example setting jvm.memory.heapSize to 5g will effectively pass -Xms5g -Xmx5g to the JVM. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: heapSize: 10g This example sets the heap size to 10g . Direct Memory Size (NIO Memory) Direct memory size is used to limit on memory that can be reserved for all Direct Byte Buffers. If a value is set for this option, the sum of all Direct Byte Buffer sizes cannot exceed the limit. After the limit is reached, a new Direct Byte Buffer can be allocated only when enough old buffers are freed to provide enough space to allocate the new buffer. By default, the VM limits the amount of heap memory used for Direct Byte Buffers to approximately 85% of the maximum heap size. To set a value for the direct memory size use the jvm.memory.directMemorySize field. This wil set the value of the -XX:MaxDirectMemorySize JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: directMemorySize: 10g The direct memory size is set to 10g which will pass -XX:MaxDirectMemorySize=10g to the JVM. Metaspace Size Metaspace is memory the VM uses to store class metadata. Class metadata are the runtime representation of java classes within a JVM process - basically any information the JVM needs to work with a Java class. That includes, but is not limited to, runtime representation of data from the JVM class file format. To set the size of the metaspace use the jvm.memory.metaspaceSize field in the Coherence CRD. Setting this field sets both the -XX:MetaspaceSize and -XX:MaxMetaspaceSize JVM options to this value giving a fixed size metaspace. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: metaspaceSize: 100m Set the metaspace size to 100m which will pass -XX:MetaspaceSize=100m -XX:MaxMetaspaceSize=100m to the JVM. Stack Size Thread stacks are memory areas allocated for each Java thread for their internal use. This is where the thread stores its local execution state. The current default size for a linux JVM is 1MB. To set the stack size use the jvm.memory.stackSize field in the Coherence CRD. Setting this value sets the -Xss JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: stackSize: 500k The stack size will be set to 500k , passing -Xss500k to the JVM. Out Of Memory Behaviour The Coherence CRD allows two optional behaviours to be specified if the JVM throws an out of memory error. The jvm.memory.onOutOfMemory.heapDump is a bool field that when set to true will pass the -XX:+HeapDumpOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off heap dumps on OOM set the specifically field to be false . The jvm.memory.onOutOfMemory.exit is a bool field that when set to true will pass the -XX:+ExitOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off killing the JVM on OOM set the specifically field to be false . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: onOutOfMemory: heapDump: true exit: true The JVM will create a heap dump on OOM The JVM will exit on OOM Native Memory Tracking The Native Memory Tracking (NMT) is a Java VM feature that tracks internal memory usage for a JVM. The Coherence CRD allows native memory tracking to be configured using the jvm.memory.nativeMemoryTracking field. Setting this field sets the -XX:NativeMemoryTracking JVM option. There are three valid values, off , summary or detail . If not specified the default value used by the operator is summary <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: nativeMemoryTracking: detail Native memory tracking is set to detail which will pass the -XX:NativeMemoryTracking=detail option to the JVM. ",
            "title": "Heap &amp; Memory Settings"
        },
        {
            "location": "/management/100_tmb_test",
            "text": " Coherence provides utilities that can be used to test network performance, which obviously has a big impact on a distributed system such as Coherence. The documentation for these utilities can be found in the official Coherence Documentation . Whilst generally these tests would be run on server hardware, with more and more Coherence deployments moving into the cloud and into Kubernetes these tests can also be performed in Pods to measure inter-Pod network performance. This test can be used to see the impact of running Pods across different zones, or on different types of Kubernetes networks, with different Pod resource settings, etc. ",
            "title": "Coherence Network Testing"
        },
        {
            "location": "/management/100_tmb_test",
            "text": " Create a yaml file that will create the Service and Pod for the listener: <markup lang=\"yaml\" title=\"message-bus-listener.yaml\" >apiVersion: v1 kind: Service metadata: name: message-bus-listener spec: selector: app: message-bus-listener ports: - protocol: TCP port: 8000 targetPort: mbus --- apiVersion: v1 kind: Pod metadata: name: message-bus-listener labels: app: message-bus-listener spec: restartPolicy: Never containers: - name: coherence image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0 ports: - name: mbus containerPort: 8000 protocol: TCP command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 This example uses the Coherence image from Oracle container registry, but any image with coherence.jar in it could be used. The command line that the container will execute is exactly the same as that for the listener process in the Coherence Documentation . Start the listener Pod : <markup lang=\"bash\" >kubectl create -f message-bus-listener.yaml Retrieving the logs for the listener Pod the messages should show that the Pod has started: <markup lang=\"bash\" >kubectl logs pod/message-bus-listener OPEN event for tmb://message-bus-listener:8000 ",
            "title": "Run the Listener Pod"
        },
        {
            "location": "/management/100_tmb_test",
            "text": "<markup lang=\"yaml\" title=\"message-bus-sender.yaml\" >apiVersion: v1 kind: Pod metadata: name: message-bus-sender labels: app: message-bus-sender spec: restartPolicy: Never containers: - name: coherence image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0 command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 - -peer - tmb://message-bus-listener:8000 Again, the command line is the same as that for the sender process in the Coherence Documentation . The peer address uses the Service name message-bus-listener from the sender yaml . Start the sender Pod : <markup lang=\"bash\" >kubectl create -f message-bus-sender.yaml Retrieving the logs for the sender Pod the messages should show that the Pod has started and show the test results: <markup lang=\"bash\" >kubectl logs pod/message-bus-sender OPEN event for tmb://message-bus-sender:8000 CONNECT event for tmb://message-bus-listener:8000 on tmb://message-bus-sender:8000 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 Note Don&#8217;t forget to stop the Pods after obtaining the results: <markup lang=\"bash\" >kubectl delete -f message-bus-sender.yaml kubectl delete -f message-bus-listener.yaml ",
            "title": "Run the Sender Pod"
        },
        {
            "location": "/management/100_tmb_test",
            "text": " In the example above the Pods will be scheduled wherever Kubernetes decides to put them. This could have a big impact on the test result for different test runs. For example in a Kubernetes cluster that spans zones and data centres, if the two Pods get scheduled in different data centres this will have worse results than if the two Pods get scheduled onto the same node. To get consistent results add node selectors, taints, tolerations etc, as covered in the Kubernetes assign Pods to Nodes documentation. ",
            "title": "Run Pods on Specific Nodes"
        },
        {
            "location": "/management/100_tmb_test",
            "text": " The message bus test can easily be run using Pods in Kubernetes. Using the example from the Coherence documentation there will need to be two Pods , a listener and a sender. This example will create a Service for the listener so that the sender Pod can use the Service name to resolve the listener Pod address. Run the Listener Pod Create a yaml file that will create the Service and Pod for the listener: <markup lang=\"yaml\" title=\"message-bus-listener.yaml\" >apiVersion: v1 kind: Service metadata: name: message-bus-listener spec: selector: app: message-bus-listener ports: - protocol: TCP port: 8000 targetPort: mbus --- apiVersion: v1 kind: Pod metadata: name: message-bus-listener labels: app: message-bus-listener spec: restartPolicy: Never containers: - name: coherence image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0 ports: - name: mbus containerPort: 8000 protocol: TCP command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 This example uses the Coherence image from Oracle container registry, but any image with coherence.jar in it could be used. The command line that the container will execute is exactly the same as that for the listener process in the Coherence Documentation . Start the listener Pod : <markup lang=\"bash\" >kubectl create -f message-bus-listener.yaml Retrieving the logs for the listener Pod the messages should show that the Pod has started: <markup lang=\"bash\" >kubectl logs pod/message-bus-listener OPEN event for tmb://message-bus-listener:8000 Run the Sender Pod <markup lang=\"yaml\" title=\"message-bus-sender.yaml\" >apiVersion: v1 kind: Pod metadata: name: message-bus-sender labels: app: message-bus-sender spec: restartPolicy: Never containers: - name: coherence image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0 command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 - -peer - tmb://message-bus-listener:8000 Again, the command line is the same as that for the sender process in the Coherence Documentation . The peer address uses the Service name message-bus-listener from the sender yaml . Start the sender Pod : <markup lang=\"bash\" >kubectl create -f message-bus-sender.yaml Retrieving the logs for the sender Pod the messages should show that the Pod has started and show the test results: <markup lang=\"bash\" >kubectl logs pod/message-bus-sender OPEN event for tmb://message-bus-sender:8000 CONNECT event for tmb://message-bus-listener:8000 on tmb://message-bus-sender:8000 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 Note Don&#8217;t forget to stop the Pods after obtaining the results: <markup lang=\"bash\" >kubectl delete -f message-bus-sender.yaml kubectl delete -f message-bus-listener.yaml Run Pods on Specific Nodes In the example above the Pods will be scheduled wherever Kubernetes decides to put them. This could have a big impact on the test result for different test runs. For example in a Kubernetes cluster that spans zones and data centres, if the two Pods get scheduled in different data centres this will have worse results than if the two Pods get scheduled onto the same node. To get consistent results add node selectors, taints, tolerations etc, as covered in the Kubernetes assign Pods to Nodes documentation. ",
            "title": "Run the Message Bus Test in Pods"
        },
        {
            "location": "/jvm/080_jmx",
            "text": " The Java Management Extensions (JMX) are a common way to connect to a JVM and retrieve information from MBeans attributes or trigger operations by calling MBean methods. By default, the JVM uses RMI as the transport layer for JMX but RMI can be notoriously tricky to make work in a container environment due to the address and port NAT&#8217;ing that is typical with containers or clouds. For this reason the Operator supports an alternative transport called JMXMP. The difference is that JMXMP only requires a single port for communications and this port is simple to configure. JMXMP is configured using the fields in the jvm.jmxmp section of the Coherence CRD spec. Enabling JMXMP support adds the opendmk_jmxremote_optional_jar.jar JMXMP library to the classpath and sets the the Coherence MBean server factory to produce a JMXMP MBean server. By default, the JMXMP server will bind to port 9099 in the container but this can be configured to bind to a different port. Using a custom transport for JMX, such as JMXMP, requires any JMX client that will connect to the JMX server to also have a JMXMP library on its classpath. See the VisualVM Example for a detailed example of how to configure JMX and connect to a server in a Coherence resource. Example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: jmxmp: enabled: true port: 9099 JMXMP is enabled so that a JMXMP server will be started in the Coherence container&#8217;s JVM The port that the JMX server will bind to in the container is 9099 ",
            "title": "Using JMX"
        },
        {
            "location": "/metrics/050_ssl",
            "text": " It is possible to configure metrics endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.metrics.ssl section of the CRD spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: metrics: enabled: true ssl: enabled: true keyStore: metrics-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: metrics-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: metrics-secret The enabled field when set to true enables SSL for metrics or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. The various files and keystores referred to in the configuration above can be any location accessible in the image used by the coherence container in the deployment&#8217;s Pods . Typically, for things such as SSL keys and certs, these would be provided by obtained from Secrets loaded as additional Pod Volumes . See Add Secrets Volumes for the documentation on how to specify secrets as additional volumes. ",
            "title": "SSL with Metrics"
        },
        {
            "location": "/about/05_upgrade",
            "text": " Version 3 of the Coherence Operator is very different to version 2. There is only a single CRD named Coherence instead of the three CRDs used by v2, and the operator no longer uses Helm internally to install the Kubernetes resources. In terms of usage and concepts, the biggest change is that there are no longer clusters and roles. The Coherence CRD represents what would previously in v2 have been a role. A Coherence cluster that is made up of multiple roles will just require multiple Coherence resources deploying to Kubernetes. The simplification of the operator, and consequently the better reliability, far outweigh any advantage of being able to put multiple roles in a single yaml file. If this is desire just put multiple Coherence resource definitions in a single yaml file with the --- separator. For example: In Operator v2 a cluster may have been defined with two roles, storage and proxy like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: storage replicas: 3 - role: proxy replicas: 2 In Operator v3 this needs to be two separate`Coherence` resources. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster-storage spec: - role: storage replicas: 3 cluster: my-cluster --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster-proxy spec: - role: proxy replicas: 2 cluster: my-cluster To make both Coherence resources part of the same cluster the cluster field must now be set in both resources to the same value, in this case my-cluster . ",
            "title": "Upgrading from Operator v2"
        },
        {
            "location": "/about/05_upgrade",
            "text": " In Operator v2 there were multiple images defined, one for Coherence and one used to provide application artifacts. Because of the application changes described only a single image now needs to be specified in the image field of the CRD spec. See the Applications section of the doecumentation for more details. ",
            "title": "Images"
        },
        {
            "location": "/about/05_upgrade",
            "text": " Coherence applications in Operator v2 worked by application resources (jar files etc) being provided in an image that was loaded as an init-container in the Pod , and the application artifacts copied to the classpath of the Coherence container. In version 3 of the Operator there is only one image required that should contain all of the resources required for the application, including Coherence jar. This gives the application developer much more control over how the image is built and what resources it contains, as well as making it more obvious what is going to be run when the container starts. Images In Operator v2 there were multiple images defined, one for Coherence and one used to provide application artifacts. Because of the application changes described only a single image now needs to be specified in the image field of the CRD spec. See the Applications section of the doecumentation for more details. ",
            "title": "Applications"
        },
        {
            "location": "/about/05_upgrade",
            "text": " A lot of the fields in the Coherence CRD are the same as when defining a role in version 2. Whilst a number of new fields and features have been added in version 3, a handful of fields have moved, and a small number, that no longer made sense, have been removed. The Coherence Spec page documents the full Coherence CRD, so it is simple to locate where a field might have moved to. ",
            "title": "CRD Differences"
        },
        {
            "location": "/about/05_upgrade",
            "text": " Version 3 of the operator no longer has fields to configure a Fluentd side-car container. There are a lot of different ways to configure Fluentd and making the Operator accomodate all of these was becoming too much of a head-ache to do in a backwards compatible way. If a Fluentd side-car is required it can just be added to the Coherence resource spec as an additional container, so there is no limitation on the Fluentd configuration. See the Logging documentation for more examples. ",
            "title": "Logging and Fluentd"
        },
        {
            "location": "/about/05_upgrade",
            "text": " Version 3 of the Operator no longer comes with the option to install Prometheus and/or Elasticsearch. This feature was only ever intended to make it easier to demo features that required Prometheus and Elasticsearch and keeping this up to date was a headache nobody needed. Both Prometheus and Elasticsearch have operators of their own which make installing them simple and importing the dashboards provided by the Coherence Operator simple too. ",
            "title": "Prometheus and Elasticsearch"
        },
        {
            "location": "/coherence/070_wka",
            "text": " A Coherence cluster is made up of one or more JVMs. In order for these JVMs to form a cluster they need to be able to discover other cluster members. The default mechanism for discovery is multicast broadcast but this does not work in most container environments. Coherence provides an alternative mechanism where the addresses of the hosts where the members of the cluster will run is provided in the form of a \"well known address\" (or WKA) list . This address list is then used by Coherence when it starts in a JVM to discover other cluster members running on the hosts in the WKA list. When running in containers each container is effectively a host and has its own host name and IP address (or addresses) and in Kubernetes it is the Pod that is effectively a host. When starting a container it is usually not possible to know in advance what the host names of the containers or Pods will be so there needs to be another solution to providing the WKA list. Coherence processes a WKA list it by performing a DNS lookup for each host name in the list. If a host name resolves to more than one IP address then all of those IP addresses will be used in cluster discovery. This feature of Coherence when combined with Kubernetes Services allows discovery of cluster members without resorting to a custom discovery mechanism. A Kubernetes Service has a DNS name and that name will resolve to all the IP addresses of the Pods that match that Service selector. This means that a Coherence JVM only needs to be given the DNS name of a Service as the single host name in its WKA list so that it will form a cluster with any other JVM using in a Pod matching the selector. When the Coherence Operator creates reconciles a Coherence CRD configuration to create a running set of Pods it creates a headless service specifically for the purposes of WKA for that Coherence resource with a selector that matches any Pod with the same cluster name. For example, if a Coherence resource is created with the following yaml: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: cluster: test-cluster In this yaml the Coherence resource has a cluster name of test-cluster The Operator will create a Service for the Coherence resource using the same name as the deployment with a -wka suffix. So in the example above the Operator would create a Service with the name storage-wka . The yaml for the WKA Service would look like the following: <markup lang=\"yaml\" title=\"wka-service.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-wka labels: coherenceCluster: test-cluster component: coherenceWkaService spec: clusterIP: None publishNotReadyAddresses: true ports: - name: coherence protocol: TCP port: 7 targetPort: 7 selector: coherenceCluster: test-cluster component: coherencePod The Service name is made up of the cluster name with the suffix -wka so in this case storage-wka The service has a clusterIP of None so it is headless The Service is configured to allow unready Pods so that all Pods matching the selector will be resolved as members of this service regardless of their ready state. This is important so that Coherence JVMs can discover other members before they are fully ready. A single port is exposed, in this case the echo port (7), even though nothing in the Coherence Pods binds to this port. Ideally no port would be included, but a Kubernetes service has to have at least one port defined. The selector will match all Pods with the labels coherenceCluster=test-cluster and component=coherencePod which are labels that the Coherence Operator will assign to all Pods in this cluster Because this Service is created in the same Namespace as the deployment&#8217;s Pods the JVMs can use the raw Service name as the WKA list, in the example above the WKA list would just be test-cluster-wka . ",
            "title": "Well Known Addressing and Cluster Discovery"
        },
        {
            "location": "/coherence/070_wka",
            "text": " In some situations it may be desirable to exclude the Pods belonging to certain deployments in the cluster from being members of the well known address list. For example certain K8s network configurations such as host networking can cause issues with WKA if other deployments in the cluster are using host networking. A role can be excluded from the WKA list by setting the excludeFromWKA field of the coherence section of the deployment&#8217;s spec to true . <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-client spec: cluster: `my-cluster` coherence: excludeFromWKA: true The cluster field is set to the name of the Coherence cluster that this deployment wil be part of (there is no point in excluding a deployment from WKA unless it is part of a wider cluster). The excludeFromWKA field is true so that Pods in the test-client deployment will not form part of the WKA list for the Coherence cluster. The operator does not validate the excludeFromWKA field for a deployment so it is possible to try to create a cluster where all of the deployment have excludeFromWKA set to true which will cause the cluster fail to start. When excluding a deployment from WKA it is important that at least one deployment that is part of the WKA list has been started first otherwise the non-WKA role members cannot start.Eventually the K8s readiness probe for these Pods would time-out causing K8s to restart them but this would not be a desirable way to start a cluster. The start-up order can be controlled by configuring the deployment&#8217;s startQuorum list, as described in the documentation section on deployment start-up ordering . ",
            "title": "Exclude a Deployment From WKA"
        },
        {
            "location": "/coherence/070_wka",
            "text": " It is possible to configure a Coherence cluster made up of multiple Coherence deployments that are deployed into different namespaces in the same Kubernetes cluster (with some caveats). The coherence.wka section of the Coherence CRD spec can be used to override the default WKA behaviour. For example, suppose that there is a Coherence deployment named data that is the storage enabled cluster members holding data for an online store. This data deployment will be deployed into the back-end namespace in a Kubernetes cluster. Another Coherence deployment of storage disabled members will provide the front end REST API for the online store. This will be named web-store and deployed in the front-end namespace. Although both the data and web-store deployments are in different namespaces they need to form a single Coherence cluster. <markup lang=\"yaml\" title=\"data-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data namespace: back-end spec: cluster: `shop` The data deployment is deployed into the back-end namespace The Coherence cluster name is set to shop <markup lang=\"yaml\" title=\"web-store-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web-store namespace: front-end spec: cluster: `shop` coherence: wka: deployment: data namespace: back-end The web-store deployment is deployed into the front-end namespace. The Coherence cluster name is set to shop to match the data deployment The coherence.wka section specifies the name of the Coherence deployment to use for WKA so in this case the data deployment in the back-end namespace. As described already above the data deployment will have a headless Service created for WKA named data-wka , which will be in the back-end namespace. The full name of this Service in Kubernetes will be data-wka.back-end.svc.cluster.local and this will be the name that the members of the web-store deployment will be configured to use for WKA. When using WKA in this way the Coherence deployment that is providing the WKA Service should be running before any deployment that depends on it is deployed. ",
            "title": "Multi-Namespace Clusters"
        },
        {
            "location": "/jvm/010_overview",
            "text": " Classpath Default classpath settings and options for setting a custom classpath. JVM Arguments Adding arbitrary JVM arguments and system properties. Garbage Collection Configuring the garbage collector. Heap & Memory Settings Configuring the heap size and other memory settings. Debugger Using debugger settings. JMX Using JMX. Use Container Limits Configuring the JVM to respect container resource limits. ",
            "title": "Guides to JVM Settings"
        },
        {
            "location": "/jvm/010_overview",
            "text": " The Coherence Operator allows full control over the configuration of the JVM used to run the Coherence application. The jvm section of the Coherence CRD spec has a number of fields to easily configure specific aspects of the JVM as well as a catch-all jvm.args list that allows any arbitrary argument to be passed to the JVM. Whilst every configuration setting could, in theory, be set only by specifying JVM arguments in the jvm.args field of the Coherence CRD, the other configuration fields provide simpler means to set configuration without having to remember specific JVM argument names or system property names to set. You are, of course, free to use whichever approach best suits your requirements; but obviously it is better to choose one approach and be consistent. Guides to JVM Settings Classpath Default classpath settings and options for setting a custom classpath. JVM Arguments Adding arbitrary JVM arguments and system properties. Garbage Collection Configuring the garbage collector. Heap & Memory Settings Configuring the heap size and other memory settings. Debugger Using debugger settings. JMX Using JMX. Use Container Limits Configuring the JVM to respect container resource limits. ",
            "title": "Overview"
        },
        {
            "location": "/applications/020_build_application",
            "text": " The image does not need to have an EntryPoint or command specified, it does not need to actually be executable. If the image does have an EntryPoint , it will just be ignored. The Coherence Operator actually injects its own runner executable into the container which the container runs and which in turn builds the Java command line to execute. The runner process looks at arguments and environment variables configured for the Coherence container and from these constructs a Java comamnd line that it then executes. The default command might look something like this: <markup lang=\"bash\" >java -cp `/app/resources:/app/classes:/app/libs/*` \\ &lt;JVM args&gt; \\ &lt;System Properties&gt; \\ com.tangosol.net.DefaultCacheServer The runner will work out the JVM&#8217;s classpath, args and system properties to add to the command line and execute the main class com.tangosol.net.DefaultCacheServer . All these are configurable in the Coherence resource spec. ",
            "title": "Image EntryPoint - What Does the Operator Run?"
        },
        {
            "location": "/applications/020_build_application",
            "text": " If the CLASSPATH environment variable has been set in an image that classpath will be used when running the Coherence container. Other elements may also be added to the classpath depending on the configuration of the Coherence resource. ",
            "title": "Optional CLASSPATH Environment Variable"
        },
        {
            "location": "/applications/020_build_application",
            "text": " An application image contains .jar files (at least coherence.jar ), possibly Java class files, also possibly other ad-hoc files, all of which need to be on the application&#8217;s classpath. There are certain classpath values that the operator supports out of the box without needing any extra configuration, but for occasions where the location of files in the image does not match the defaults a classpath can be specified. Images built with JIB have a default classpath of /app/resources:/app/classes:/app/libs/* . When the Coherence container starts if the directories /app/resources , /app/classes or /app/libs/ exist in the image they will automatically be added to the classpath of the JVM. In this way the Operator supports standard JIB images without requiring additional configuration. If the image is not a JIB image, or is a JIB image without the standard classpath but one or more of the /app/resources , /app/classes or /app/libs/ directories exist they will still be added to the classpath. This may be desired or in some cases it may cause issues. It is possible to disable automatically adding these directories in the Coherence resource spec by setting the jvm.useJibClasspath field to false (the default value of the field is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. If the image is not a JIB image, or is a JIB image without the standard classpath, then additional classpath entries can be configured as described in the setting the classpath documentation. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/libs/*\" - \"/data/config\" The jvm.classpath field will be used to add additional items to the classpath, the field is a list of strings. Each entry in the jvm.classpath will be appended to the classpath exactly as it is declared, so in this case the classpath will be /data/libs/*:/data/config ",
            "title": "Setting the Classpath"
        },
        {
            "location": "/applications/020_build_application",
            "text": " The JAVA_HOME environment variable does not have to be set in the image. If it is set the JVM at that location will be used to run the application. If it is not set then the java executable must be on the PATH in the image. ",
            "title": "Optional JAVA_HOME Environment Variable"
        },
        {
            "location": "/applications/020_build_application",
            "text": " The COHERENCE_HOME environment variable does not have to be set in an image. Typically, all the jar files, including coherence.jar would be packaged into a single directory which is then used as the classpath. It is possible to run official Coherence images published by Oracle, which have COHERENCE_HOME set, which is then used by the Operator to set the classpath. If the COHERENCE_HOME environment variable is set in an image the following entries will be added to the end of the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf ",
            "title": "Optional COHERENCE_HOME Environment Variable"
        },
        {
            "location": "/applications/020_build_application",
            "text": " If the application requires access to external storage volumes in Kubernetes it is possible to add additional Volumes and VolumeMappings to the Pod and containers. There are three ways to add additional volumes: ConfigMaps - easily add a ConfigMap volume and volume mapping see: Add ConfigMap Volumes Secrets - easily add a Secret volume and volume mapping see: Add Secret Volumes Volumes - easily add any additional volume and volume mapping see: Add Volumes Both of ConfigMaps and Secrets have been treated as a special case because they are quite commonly used to provide configurations to Pods, so the Coherence spec provides a simpler way to declare them than for ad-hoc Volumes . ",
            "title": "Additional Data Volumes"
        },
        {
            "location": "/applications/020_build_application",
            "text": " To deploy a Coherence application using the operator the application code must be packaged into an image that the Coherence container in the Pods will run. This image can be any image that contains a JVM as well as the application&#8217;s jar files, including obviously coherence.jar . There are many ways to build an image for a Java application so it would be of little value to document the exact steps for one of them here that might turn out to be used by very few people. One of the simplest ways to build a Java image is to use JIB . The Operator supports JIB images automatically but any image that meets the requirements of having a JVM and coherence.jar will be supported. Any version of Java which works with the version of coherence.jar in the image will be suitable. This can be a JRE, it does not need to be a full JDK. At a bare minimum the directories in an image might look like this example (obviously there would be more O/S related files and more JVM files, but they are not relevant for the example): <markup >/ |-- app | |-- libs | |-- application.jar | |-- coherence.jar |-- usr |-- bin | |-- java | |-- lib |-- jvm |-- java-11-openjdk The /app/libs directory contains the application jar files. This will be the classpath used to run the application. The /usr/bin/java file is the Java executable and on the PATH in the image (this would be a link to the actual Java executable location, in this example to /usr/lib/jvm/java-11-openjdk/bin/java . The /usr/lib/jvm/java-11-openjdk/ is the actual JVM install location. Image EntryPoint - What Does the Operator Run? The image does not need to have an EntryPoint or command specified, it does not need to actually be executable. If the image does have an EntryPoint , it will just be ignored. The Coherence Operator actually injects its own runner executable into the container which the container runs and which in turn builds the Java command line to execute. The runner process looks at arguments and environment variables configured for the Coherence container and from these constructs a Java comamnd line that it then executes. The default command might look something like this: <markup lang=\"bash\" >java -cp `/app/resources:/app/classes:/app/libs/*` \\ &lt;JVM args&gt; \\ &lt;System Properties&gt; \\ com.tangosol.net.DefaultCacheServer The runner will work out the JVM&#8217;s classpath, args and system properties to add to the command line and execute the main class com.tangosol.net.DefaultCacheServer . All these are configurable in the Coherence resource spec. Optional CLASSPATH Environment Variable If the CLASSPATH environment variable has been set in an image that classpath will be used when running the Coherence container. Other elements may also be added to the classpath depending on the configuration of the Coherence resource. Setting the Classpath An application image contains .jar files (at least coherence.jar ), possibly Java class files, also possibly other ad-hoc files, all of which need to be on the application&#8217;s classpath. There are certain classpath values that the operator supports out of the box without needing any extra configuration, but for occasions where the location of files in the image does not match the defaults a classpath can be specified. Images built with JIB have a default classpath of /app/resources:/app/classes:/app/libs/* . When the Coherence container starts if the directories /app/resources , /app/classes or /app/libs/ exist in the image they will automatically be added to the classpath of the JVM. In this way the Operator supports standard JIB images without requiring additional configuration. If the image is not a JIB image, or is a JIB image without the standard classpath but one or more of the /app/resources , /app/classes or /app/libs/ directories exist they will still be added to the classpath. This may be desired or in some cases it may cause issues. It is possible to disable automatically adding these directories in the Coherence resource spec by setting the jvm.useJibClasspath field to false (the default value of the field is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. If the image is not a JIB image, or is a JIB image without the standard classpath, then additional classpath entries can be configured as described in the setting the classpath documentation. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/libs/*\" - \"/data/config\" The jvm.classpath field will be used to add additional items to the classpath, the field is a list of strings. Each entry in the jvm.classpath will be appended to the classpath exactly as it is declared, so in this case the classpath will be /data/libs/*:/data/config Optional JAVA_HOME Environment Variable The JAVA_HOME environment variable does not have to be set in the image. If it is set the JVM at that location will be used to run the application. If it is not set then the java executable must be on the PATH in the image. Optional COHERENCE_HOME Environment Variable The COHERENCE_HOME environment variable does not have to be set in an image. Typically, all the jar files, including coherence.jar would be packaged into a single directory which is then used as the classpath. It is possible to run official Coherence images published by Oracle, which have COHERENCE_HOME set, which is then used by the Operator to set the classpath. If the COHERENCE_HOME environment variable is set in an image the following entries will be added to the end of the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf Additional Data Volumes If the application requires access to external storage volumes in Kubernetes it is possible to add additional Volumes and VolumeMappings to the Pod and containers. There are three ways to add additional volumes: ConfigMaps - easily add a ConfigMap volume and volume mapping see: Add ConfigMap Volumes Secrets - easily add a Secret volume and volume mapping see: Add Secret Volumes Volumes - easily add any additional volume and volume mapping see: Add Volumes Both of ConfigMaps and Secrets have been treated as a special case because they are quite commonly used to provide configurations to Pods, so the Coherence spec provides a simpler way to declare them than for ad-hoc Volumes . ",
            "title": "Build Custom Application Images"
        },
        {
            "location": "/jvm/070_debugger",
            "text": " One scenario for debugging is for the Coherence JVM to open a port and listen for a debugger connection request. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true port: 5005 suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.port field specifies the port the JVM will listen on for a debugger connection. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 The address=*:5005 value comes from the jvm.debug.port field The suspend=n value comes from the jvm.debug.suspend field If the jvm.debug.port is not specified the default value used by the Operator will be 5005 . ",
            "title": "Listening for a Debugger Connection"
        },
        {
            "location": "/jvm/070_debugger",
            "text": " Another scenario for debugging is for the Coherence JVM to connect out to a listening debugger. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true attach: \"10.10.100.2:5000\" suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.attach field specifies the address of the debugger that the JVM will connect to. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=n,address=10.10.100.2:5000,suspend=n,timeout=10000 ",
            "title": "Attaching to a Debugger Connection"
        },
        {
            "location": "/jvm/070_debugger",
            "text": " Occasionally it is useful to be able to connect a debugger to a JVM, and the Coherence CRD spec has fields to configure the Coherence container&#8217;s JVM to work with a debugger. The fields in the CRD will ultimately result in arguments being passed to the JVM and could have been added as plain JVM arguments, but having specific fields in the CRD makes it simpler to configure and the intention more obvious. The fields to control debug settings of the JVM are in the jvm.debug section of the CRD spec. Listening for a Debugger Connection One scenario for debugging is for the Coherence JVM to open a port and listen for a debugger connection request. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true port: 5005 suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.port field specifies the port the JVM will listen on for a debugger connection. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 The address=*:5005 value comes from the jvm.debug.port field The suspend=n value comes from the jvm.debug.suspend field If the jvm.debug.port is not specified the default value used by the Operator will be 5005 . Attaching to a Debugger Connection Another scenario for debugging is for the Coherence JVM to connect out to a listening debugger. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true attach: \"10.10.100.2:5000\" suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.attach field specifies the address of the debugger that the JVM will connect to. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=n,address=10.10.100.2:5000,suspend=n,timeout=10000 ",
            "title": "Debugger Configuration"
        },
        {
            "location": "/about/02_introduction",
            "text": " The Coherence Operator is a Kubernetes Operator that is used to manage Oracle Coherence clusters in Kubernetes. The Coherence Operator takes on the tasks of that human Dev Ops resource might carry out when managing Coherence clusters, such as configuration, installation, safe scaling, management and metrics. The Coherence Operator is a Go based application built using the Operator SDK . It is distributed as a Docker image and Helm chart for easy installation and configuration. ",
            "title": "What is the Coherence Operator?"
        },
        {
            "location": "/about/02_introduction",
            "text": " A Coherence cluster is a number of distributed Java Virtual Machines (JVMs) that communicate to form a single coherent cluster. In Kubernetes, this concept can be related to a number of Pods that form a single cluster. In each Pod is a JVM running a Coherence DefaultCacheServer , or a custom application using Coherence. The operator uses a Kubernetes Custom Resource Definition (CRD) to represent a group of members in a Coherence cluster. Typically, a deployment would be used to configure one or more members of a specific role in a cluster. Every field in the Coherence CRD Spec is optional, so a simple cluster can be defined in yaml as: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster In this case the metadata.name field in the Coherence resource yaml will be used as the Coherence cluster name. The operator will use default values for fields that have not been entered, so the above yaml will create a Coherence deployment using a StatefulSet with a replica count of three, which means that will be three storage enabled Coherence Pods . See the Coherence CRD spec page for details of all the fields in the CRD. ",
            "title": "Coherence Clusters"
        },
        {
            "location": "/other/020_environment",
            "text": " Environment variables can be added to the Coherence container in the Pods managed by the Operator. Additional variables should be added to the env list in the Coherence CRD spec. The entries in the env list are Kubernetes EnvVar values, exactly the same as when adding environment variables to a container spec. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: env: - name: VAR_ONE value: VALUE_ONE - name: VAR_TWO valueFrom: secretKeyRef: name: test-secret key: secret-key The VAR_ONE environment variable is a simple variable with a value of VALUE_ONE The VAR_TWO environment variable is variable that is loaded from a secret. ",
            "title": "Environment Variables"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " The Coherence Operator provides detailed Grafana dashboards to provide insight into your running Coherence Clusters. ",
            "title": "preambule"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. ",
            "title": "Grafana Dashboards"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Navigation Dashboards Coherence Dashboard Main Members Summary &amp; Details Dashboards Services Summary &amp; Details Dashboards Caches Summary &amp; Detail Dashboards Proxy Servers Summary &amp; Detail Dashboards Persistence Summary Dashboard Federation Summary &amp; Details Dashboards Machines Summary Dashboard HTTP Servers Summary Dashboard Elastic Data Summary Dashboard ",
            "title": "Table of Contents"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Allows for selection of information to be displayed where there is more than one item. Cluster Name - Allows selection of the cluster to view metrics for Top N Limit - Limits the display of Top values for tables that support it Service Name, Member Name, Cache Name - These will appear on various dashboards See the Grafana Documetation for more information on Variables. ",
            "title": "Variables"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Vertical red lines on a graph to indicate a change in a key markers such as: Show Cluster Size Changes - Displays when the cluster size has changed Show Partition Transfers - Displays when partition transfers have occurred See the Grafana Documetation for more information on Annotations. ",
            "title": "Annotations"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Select Dashboard - In the top right a drop down list of dashboards is available selection Drill Through - Ability to drill through based upon service, member, node, etc. ",
            "title": "Navigation"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " The pre-loaded Coherence Dashboards provide a number of common features and navigation capabilities that appear at the top of most dashboards. Variables Allows for selection of information to be displayed where there is more than one item. Cluster Name - Allows selection of the cluster to view metrics for Top N Limit - Limits the display of Top values for tables that support it Service Name, Member Name, Cache Name - These will appear on various dashboards See the Grafana Documetation for more information on Variables. Annotations Vertical red lines on a graph to indicate a change in a key markers such as: Show Cluster Size Changes - Displays when the cluster size has changed Show Partition Transfers - Displays when partition transfers have occurred See the Grafana Documetation for more information on Annotations. Navigation Select Dashboard - In the top right a drop down list of dashboards is available selection Drill Through - Ability to drill through based upon service, member, node, etc. ",
            "title": "Navigation"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows a high-level overview of the selected Coherence cluster including metrics such as: Cluster member count, services, memory and health Top N loaded members, Top N heap usage and GC activity Service backlogs and endangered or vulnerable services Top query times, non-optimized queries Guardian recoveries and terminations ",
            "title": "1. Coherence Dashboard Main"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Members Summary"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Member Details"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows an overview of all cluster members that are enabled for metrics capture including metrics such as: Member list include heap usage Top N members for GC time and count Total GC collection count and time by Member Publisher and Receiver success rates Guardian recoveries and send queue size Members Summary Member Details ",
            "title": "2. Members Summary &amp; Details Dashboards"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Services Summary"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Service Details"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows an overview of all cluster services including metrics such as: Service members for storage and non-storage services Service task count StatusHA values as well as endangered, vulnerable and unbalanced partitions Top N services by task count and backlog Task rates, request pending counts and task and request averages Services Summary Service Details ",
            "title": "3. Services Summary &amp; Details Dashboards"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Caches Summary"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Cache Details"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows an overview of all caches including metrics such as: Cache entries, memory and index usage Cache access counts including gets, puts and removed, max query times Front cache hit and miss rates Caches Summary Cache Details ",
            "title": "4. Caches Summary &amp; Detail Dashboards"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Proxy Servers Summary"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Proxy Servers Detail"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows and overview of Proxy servers including metrics such as: Active connection count and service member count Total messages sent/ received Proxy server data rates Individual connection details abd byte backlogs Proxy Servers Summary Proxy Servers Detail ",
            "title": "5. Proxy Servers Summary &amp; Detail Dashboards"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows and overview of Persistence including metrics such as: Persistence enabled services Maximum active persistence latency Active space total usage and by service ",
            "title": "6. Persistence Summary Dashboard"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Federation Summary"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " ",
            "title": "Federation Details"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows overview of Federation including metrics such as: Destination and Origins details Entries, records and bytes send and received Federation Summary Federation Details ",
            "title": "7. Federation Summary &amp; Details Dashboards"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows an overview of all machines that make up the Kubernetes cluster underlying the Coherence cluster including metrics such as: Machine processors, free swap space and physical memory Load averages ",
            "title": "8. Machines Summary Dashboard"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows an overview of all HTTP Servers running in the cluster including metrics such as: Service member count, requests, error count and average request time HTTP Request rates and response codes ",
            "title": "9. HTTP Servers Summary Dashboard"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " Shows an overview of all HTTP Servers running in the cluster including metrics such as: RAM and Flash journal files in use RAM and Flash compactions ",
            "title": "10. Elastic Data Summary Dashboard"
        },
        {
            "location": "/metrics/040_dashboards",
            "text": " 1. Coherence Dashboard Main Shows a high-level overview of the selected Coherence cluster including metrics such as: Cluster member count, services, memory and health Top N loaded members, Top N heap usage and GC activity Service backlogs and endangered or vulnerable services Top query times, non-optimized queries Guardian recoveries and terminations 2. Members Summary &amp; Details Dashboards Shows an overview of all cluster members that are enabled for metrics capture including metrics such as: Member list include heap usage Top N members for GC time and count Total GC collection count and time by Member Publisher and Receiver success rates Guardian recoveries and send queue size Members Summary Member Details 3. Services Summary &amp; Details Dashboards Shows an overview of all cluster services including metrics such as: Service members for storage and non-storage services Service task count StatusHA values as well as endangered, vulnerable and unbalanced partitions Top N services by task count and backlog Task rates, request pending counts and task and request averages Services Summary Service Details 4. Caches Summary &amp; Detail Dashboards Shows an overview of all caches including metrics such as: Cache entries, memory and index usage Cache access counts including gets, puts and removed, max query times Front cache hit and miss rates Caches Summary Cache Details 5. Proxy Servers Summary &amp; Detail Dashboards Shows and overview of Proxy servers including metrics such as: Active connection count and service member count Total messages sent/ received Proxy server data rates Individual connection details abd byte backlogs Proxy Servers Summary Proxy Servers Detail 6. Persistence Summary Dashboard Shows and overview of Persistence including metrics such as: Persistence enabled services Maximum active persistence latency Active space total usage and by service 7. Federation Summary &amp; Details Dashboards Shows overview of Federation including metrics such as: Destination and Origins details Entries, records and bytes send and received Federation Summary Federation Details 8. Machines Summary Dashboard Shows an overview of all machines that make up the Kubernetes cluster underlying the Coherence cluster including metrics such as: Machine processors, free swap space and physical memory Load averages 9. HTTP Servers Summary Dashboard Shows an overview of all HTTP Servers running in the cluster including metrics such as: Service member count, requests, error count and average request time HTTP Request rates and response codes 10. Elastic Data Summary Dashboard Shows an overview of all HTTP Servers running in the cluster including metrics such as: RAM and Flash journal files in use RAM and Flash compactions ",
            "title": "Dashboards"
        },
        {
            "location": "/jvm/090_container_limits",
            "text": " The JVM can be configured to respect container limits set, for example cpu and memory limits. This can be important if container limits have been set for the container in the resources section as a JVM that does not respect these limits can cause the Pod to be killed. This is done by adding the -XX:+UseContainerSupport JVM option. It is possible to control this using the jvm.useContainerLimits field in the Coherence CRD spec. If the field is not set, the operator adds the -XX:+UseContainerSupport option by default. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useContainerLimits: false The useContainerLimits field is set to false, so the -XX:+UseContainerSupport will not be passed to the JVM. See the Resource Limits documentation on how to specify resource limits for the Coherence container. ",
            "title": "Respect Container Resource Limits"
        }
 ]
}