{
    "docs": [
        {
            "location": "/metrics/020_metrics",
            "text": " Coherence clusters can be deployed with a metrics endpoint enabled that can be scraped by common metrics applications such as Prometheus. ",
            "title": "preambule"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. The Coherence Operator can be installed with a demo Prometheus installation using embedded Prometheus Operator and Grafana Helm charts. This Prometheus deployment is not intended for production use but is useful for development, testing and demo purposes. ",
            "title": "Deploying Coherence Clusters with Metrics Enabled"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " To enable Prometheus, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ coherence/coherence-operator Set &lt;namespace&gt; to the Kubernetes namespace that the Coherence Operator should be installed into. After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s The Coherence Operator Pod The Grafana Pod The Prometheus Pod The demo install of Prometheus in the Operator configures Prometheus to use service monitors to work out which Pods to scrape metrics from. A ServiceMonitor in Prometheus will scrape from a port defined in a Kubernetes Service from all Pods that match that service&#8217;s selector. ",
            "title": "1. Install the Coherence Operator with Prometheus"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " Now that Prometheus is running Coherence clusters can be created that expose metrics on a port on each Pod and also deploy a Service to expose the metrics that Prometheus can use. Deploy a simple metrics enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: storage replicas: 2 coherence: metrics: enabled: true port: 9612 ports: - name: metrics port: 9612 This cluster will have a single role called storage The cluster will have two replicas ( Pods ) The port field sets the port that Coherence will use to expose metrics in the container. The port field is is optional, the default port value is 9612 . The metrics port must be added to the additional ports list so that it is exposed on a service to allow the default Prometheus ServiceMonitor installed by the operator to work; the port should be named metrics . The port field must be set to the same value as the port field in the coherence.metrics section or if no coherence.metrics.port was specified it should be set to the default value of 9612 The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f metrics-cluster.yaml The Coherence Operator will see the new CoherenceCluster resource and create the cluster with two Pods . If kubectl get pods -n &lt;namespace&gt; is run again it should now look something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s test-cluster-storage-0 1/1 Running 0 70s test-cluster-storage-1 1/1 Running 0 70s Pod one of the Coherence cluster. Pod two of the Coherence cluster. If the services are listed for the namespace: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get svc The list of services will look something like this. <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE operator-grafana ClusterIP 10.104.251.51 &lt;none&gt; 80/TCP 31m operator-kube-state-metrics ClusterIP 10.110.18.78 &lt;none&gt; 8080/TCP 31m operator-prometheus-node-exporter ClusterIP 10.102.181.6 &lt;none&gt; 9100/TCP 31m operator-prometheusoperato-operator ClusterIP 10.107.59.229 &lt;none&gt; 8080/TCP 31m operator-prometheusoperato-prometheus ClusterIP 10.99.208.18 &lt;none&gt; 9090/TCP 31m prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 31m test-cluster-storage-headless ClusterIP None &lt;none&gt; 30000/TCP 16m test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 16m test-cluster-wka ClusterIP None &lt;none&gt; 30000/TCP 16m One of the services will be the service exposing the Coherence metrics. The service name is typically in the format &lt;cluster-name&gt;-&lt;role-name&gt;-&lt;port-name&gt; The Prometheus ServiceMonitor installed by the Coherence Operator is configured to look for services with the label component=coherence-service-metrics . When ports are exposed in a CoherenceCluster , as has been done here for metrics, the service created will have a label of the format component=coherence-service-&lt;port-name&gt; , so in this case the test-cluster-storage-metrics service above will have the label component=coherence-service-metrics . The labels for the service can be displayed: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt;&gt; get svc/test-cluster-storage-metrics --label-columns=component <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE COMPONENT test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 26m coherence-service-metrics Which shows that the service does indeed have the required label. ",
            "title": "2. Install a Coherence Cluster with Metrics Enabled"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " Now that Prometheus is running and is able to scrape metrics from the Coherence cluster it should be possible to access those metrics in Prometheus. First find the Prometheus Pod name using kubectl <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name Using the Pod name use kubectl to create a port forward session to the Prometheus Pod so that the Prometheus API on port 9090 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name) \\ 9090:9090 It is now possible to access the Prometheus API on localhost port 9090. This can be used to directly retrieve Coherence metrics using curl , for example to obtain the cluster size metric: <markup lang=\"bash\" >curl -w '' -X GET http://127.0.0.1:9090/api/v1/query?query=vendor:coherence_cluster_size It is also possible to browse directly to the Prometheus web UI at http://127.0.0.1:9090 ",
            "title": "3. Access Prometheus"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " There are a number of dashboard created by default: Coherence Dashboard main for inspecting coherence clusters Coherence Cluster Members Summary and Details Coherence Cluster Members Machines Summary Coherence Cache Summary and Details Coherence Services Summary and Details Coherence Proxy Servers Summary and Details Coherence Elastic Data Summary Coherence Cache Persistence Summary Coherence Http Servers Summary ",
            "title": "a. Default Dashboards"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " By default when the Coherence Operator configured to install Prometheus the Prometheus Operator also install a Grafana Pod and the Coherence Operator imports into Grafana a number of custom dashboards for displaying Coherence metrics. Grafana can be accessed by using port forwarding in the same way that was done for Prometheus First find the Grafana Pod : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name Using the Pod name use kubectl to create a port forward session to the Grafana Pod so that the Grafana API on port 3000 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name) \\ 3000:3000 The custom Coherence dashboards can be accessed by pointing a browser to http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main The Grafana credentials are username admin password prom-operator a. Default Dashboards There are a number of dashboard created by default: Coherence Dashboard main for inspecting coherence clusters Coherence Cluster Members Summary and Details Coherence Cluster Members Machines Summary Coherence Cache Summary and Details Coherence Services Summary and Details Coherence Proxy Servers Summary and Details Coherence Elastic Data Summary Coherence Cache Persistence Summary Coherence Http Servers Summary ",
            "title": "3. Access Grafana"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f metrics-cluster.yaml The Coherence Operator, along with Prometheus and Grafana can be removed using Helm: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "4. Clean Up"
        },
        {
            "location": "/metrics/020_metrics",
            "text": " Install the Coherence Operator with Prometheus Install a Coherence Cluster with Metrics Enabled Access Prometheus Access Grafana Default Dashboards Clean Up 1. Install the Coherence Operator with Prometheus To enable Prometheus, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ coherence/coherence-operator Set &lt;namespace&gt; to the Kubernetes namespace that the Coherence Operator should be installed into. After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s The Coherence Operator Pod The Grafana Pod The Prometheus Pod The demo install of Prometheus in the Operator configures Prometheus to use service monitors to work out which Pods to scrape metrics from. A ServiceMonitor in Prometheus will scrape from a port defined in a Kubernetes Service from all Pods that match that service&#8217;s selector. 2. Install a Coherence Cluster with Metrics Enabled Now that Prometheus is running Coherence clusters can be created that expose metrics on a port on each Pod and also deploy a Service to expose the metrics that Prometheus can use. Deploy a simple metrics enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: storage replicas: 2 coherence: metrics: enabled: true port: 9612 ports: - name: metrics port: 9612 This cluster will have a single role called storage The cluster will have two replicas ( Pods ) The port field sets the port that Coherence will use to expose metrics in the container. The port field is is optional, the default port value is 9612 . The metrics port must be added to the additional ports list so that it is exposed on a service to allow the default Prometheus ServiceMonitor installed by the operator to work; the port should be named metrics . The port field must be set to the same value as the port field in the coherence.metrics section or if no coherence.metrics.port was specified it should be set to the default value of 9612 The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f metrics-cluster.yaml The Coherence Operator will see the new CoherenceCluster resource and create the cluster with two Pods . If kubectl get pods -n &lt;namespace&gt; is run again it should now look something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s test-cluster-storage-0 1/1 Running 0 70s test-cluster-storage-1 1/1 Running 0 70s Pod one of the Coherence cluster. Pod two of the Coherence cluster. If the services are listed for the namespace: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get svc The list of services will look something like this. <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE operator-grafana ClusterIP 10.104.251.51 &lt;none&gt; 80/TCP 31m operator-kube-state-metrics ClusterIP 10.110.18.78 &lt;none&gt; 8080/TCP 31m operator-prometheus-node-exporter ClusterIP 10.102.181.6 &lt;none&gt; 9100/TCP 31m operator-prometheusoperato-operator ClusterIP 10.107.59.229 &lt;none&gt; 8080/TCP 31m operator-prometheusoperato-prometheus ClusterIP 10.99.208.18 &lt;none&gt; 9090/TCP 31m prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 31m test-cluster-storage-headless ClusterIP None &lt;none&gt; 30000/TCP 16m test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 16m test-cluster-wka ClusterIP None &lt;none&gt; 30000/TCP 16m One of the services will be the service exposing the Coherence metrics. The service name is typically in the format &lt;cluster-name&gt;-&lt;role-name&gt;-&lt;port-name&gt; The Prometheus ServiceMonitor installed by the Coherence Operator is configured to look for services with the label component=coherence-service-metrics . When ports are exposed in a CoherenceCluster , as has been done here for metrics, the service created will have a label of the format component=coherence-service-&lt;port-name&gt; , so in this case the test-cluster-storage-metrics service above will have the label component=coherence-service-metrics . The labels for the service can be displayed: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt;&gt; get svc/test-cluster-storage-metrics --label-columns=component <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE COMPONENT test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 26m coherence-service-metrics Which shows that the service does indeed have the required label. 3. Access Prometheus Now that Prometheus is running and is able to scrape metrics from the Coherence cluster it should be possible to access those metrics in Prometheus. First find the Prometheus Pod name using kubectl <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name Using the Pod name use kubectl to create a port forward session to the Prometheus Pod so that the Prometheus API on port 9090 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name) \\ 9090:9090 It is now possible to access the Prometheus API on localhost port 9090. This can be used to directly retrieve Coherence metrics using curl , for example to obtain the cluster size metric: <markup lang=\"bash\" >curl -w '' -X GET http://127.0.0.1:9090/api/v1/query?query=vendor:coherence_cluster_size It is also possible to browse directly to the Prometheus web UI at http://127.0.0.1:9090 3. Access Grafana By default when the Coherence Operator configured to install Prometheus the Prometheus Operator also install a Grafana Pod and the Coherence Operator imports into Grafana a number of custom dashboards for displaying Coherence metrics. Grafana can be accessed by using port forwarding in the same way that was done for Prometheus First find the Grafana Pod : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name Using the Pod name use kubectl to create a port forward session to the Grafana Pod so that the Grafana API on port 3000 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name) \\ 3000:3000 The custom Coherence dashboards can be accessed by pointing a browser to http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main The Grafana credentials are username admin password prom-operator a. Default Dashboards There are a number of dashboard created by default: Coherence Dashboard main for inspecting coherence clusters Coherence Cluster Members Summary and Details Coherence Cluster Members Machines Summary Coherence Cache Summary and Details Coherence Services Summary and Details Coherence Proxy Servers Summary and Details Coherence Elastic Data Summary Coherence Cache Persistence Summary Coherence Http Servers Summary 4. Clean Up After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f metrics-cluster.yaml The Coherence Operator, along with Prometheus and Grafana can be removed using Helm: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Table of Contents"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " The Coherence Operator provides detailed Grafana dashboards to provide insight into your running Coherence Clusters. ",
            "title": "preambule"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. ",
            "title": "Grafana Dashboards"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Navigation Dashboards Coherence Dashboard Main Members Summary &amp; Details Dashboards Services Summary &amp; Details Dashboards Caches Summary &amp; Detail Dashboards Proxy Servers Summary &amp; Detail Dashboards Persistence Summary Dashboard Federation Summary &amp; Details Dashboards Machines Summary Dashboard HTTP Servers Summary Dashboard Elastic Data Summary Dashboard ",
            "title": "Table of Contents"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Allows for selection of information to be displayed where there is more than one item. Cluster Name - Allows selection of the cluster to view metrics for Top N Limit - Limits the display of Top values for tables that support it Service Name, Member Name, Cache Name - These will appear on various dashboards See the Grafana Documetation for more information on Variables. ",
            "title": "Variables"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Vertical red lines on a graph to indicate a change in a key markers such as: Show Cluster Size Changes - Displays when the cluster size has changed Show Partition Transfers - Displays when partition transfers have occurred See the Grafana Documetation for more information on Annotations. ",
            "title": "Annotations"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Select Dashboard - In the top right a drop down list of dashboards is available selection Drill Through - Ability to drill through based upon service, member, node, etc. ",
            "title": "Navigation"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " The pre-loaded Coherence Dashboards provide a number of common features and navigation capabilities that appear at the top of most dashboards. Variables Allows for selection of information to be displayed where there is more than one item. Cluster Name - Allows selection of the cluster to view metrics for Top N Limit - Limits the display of Top values for tables that support it Service Name, Member Name, Cache Name - These will appear on various dashboards See the Grafana Documetation for more information on Variables. Annotations Vertical red lines on a graph to indicate a change in a key markers such as: Show Cluster Size Changes - Displays when the cluster size has changed Show Partition Transfers - Displays when partition transfers have occurred See the Grafana Documetation for more information on Annotations. Navigation Select Dashboard - In the top right a drop down list of dashboards is available selection Drill Through - Ability to drill through based upon service, member, node, etc. ",
            "title": "Navigation"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows a high-level overview of the selected Coherence cluster including metrics such as: Cluster member count, services, memory and health Top N loaded members, Top N heap usage and GC activity Service backlogs and endangered or vulnerable services Top query times, non-optimized queries Guardian recoveries and terminations ",
            "title": "1. Coherence Dashboard Main"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Members Summary"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Member Details"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows an overview of all cluster members that are enabled for metrics capture including metrics such as: Member list include heap usage Top N members for GC time and count Total GC collection count and time by Member Publisher and Receiver success rates Guardian recoveries and send queue size Members Summary Member Details ",
            "title": "2. Members Summary &amp; Details Dashboards"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Services Summary"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Service Details"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows an overview of all cluster services including metrics such as: Service members for storage and non-storage services Service task count StatusHA values as well as endangered, vulnerable and unbalanced partitions Top N services by task count and backlog Task rates, request pending counts and task and request averages Services Summary Service Details ",
            "title": "3. Services Summary &amp; Details Dashboards"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Caches Summary"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Cache Details"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows an overview of all caches including metrics such as: Cache entries, memory and index usage Cache access counts including gets, puts and removed, max query times Front cache hit and miss rates Caches Summary Cache Details ",
            "title": "4. Caches Summary &amp; Detail Dashboards"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Proxy Servers Summary"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Proxy Servers Detail"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows and overview of Proxy servers including metrics such as: Active connection count and service member count Total messages sent/ received Proxy server data rates Individual connection details abd byte backlogs Proxy Servers Summary Proxy Servers Detail ",
            "title": "5. Proxy Servers Summary &amp; Detail Dashboards"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows and overview of Persistence including metrics such as: Persistence enabled services Maximum active persistence latency Active space total usage and by service ",
            "title": "6. Persistence Summary Dashboard"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Federation Summary"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " ",
            "title": "Federation Details"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows overview of Federation including metrics such as: Destination and Origins details Entries, records and bytes send and received Federation Summary Federation Details ",
            "title": "7. Federation Summary &amp; Details Dashboards"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows an overview of all machines that make up the Kubernetes cluster underlying the Coherence cluster including metrics such as: Machine processors, free swap space and physical memory Load averages ",
            "title": "8. Machines Summary Dashboard"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows an overview of all HTTP Servers running in the cluster including metrics such as: Service member count, requests, error count and average request time HTTP Request rates and response codes ",
            "title": "9. HTTP Servers Summary Dashboard"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " Shows an overview of all HTTP Servers running in the cluster including metrics such as: RAM and Flash journal files in use RAM and Flash compactions ",
            "title": "10. Elastic Data Summary Dashboard"
        },
        {
            "location": "/metrics/050_dashboards",
            "text": " 1. Coherence Dashboard Main Shows a high-level overview of the selected Coherence cluster including metrics such as: Cluster member count, services, memory and health Top N loaded members, Top N heap usage and GC activity Service backlogs and endangered or vulnerable services Top query times, non-optimized queries Guardian recoveries and terminations 2. Members Summary &amp; Details Dashboards Shows an overview of all cluster members that are enabled for metrics capture including metrics such as: Member list include heap usage Top N members for GC time and count Total GC collection count and time by Member Publisher and Receiver success rates Guardian recoveries and send queue size Members Summary Member Details 3. Services Summary &amp; Details Dashboards Shows an overview of all cluster services including metrics such as: Service members for storage and non-storage services Service task count StatusHA values as well as endangered, vulnerable and unbalanced partitions Top N services by task count and backlog Task rates, request pending counts and task and request averages Services Summary Service Details 4. Caches Summary &amp; Detail Dashboards Shows an overview of all caches including metrics such as: Cache entries, memory and index usage Cache access counts including gets, puts and removed, max query times Front cache hit and miss rates Caches Summary Cache Details 5. Proxy Servers Summary &amp; Detail Dashboards Shows and overview of Proxy servers including metrics such as: Active connection count and service member count Total messages sent/ received Proxy server data rates Individual connection details abd byte backlogs Proxy Servers Summary Proxy Servers Detail 6. Persistence Summary Dashboard Shows and overview of Persistence including metrics such as: Persistence enabled services Maximum active persistence latency Active space total usage and by service 7. Federation Summary &amp; Details Dashboards Shows overview of Federation including metrics such as: Destination and Origins details Entries, records and bytes send and received Federation Summary Federation Details 8. Machines Summary Dashboard Shows an overview of all machines that make up the Kubernetes cluster underlying the Coherence cluster including metrics such as: Machine processors, free swap space and physical memory Load averages 9. HTTP Servers Summary Dashboard Shows an overview of all HTTP Servers running in the cluster including metrics such as: Service member count, requests, error count and average request time HTTP Request rates and response codes 10. Elastic Data Summary Dashboard Shows an overview of all HTTP Servers running in the cluster including metrics such as: RAM and Flash journal files in use RAM and Flash compactions ",
            "title": "Dashboards"
        },
        {
            "location": "/developer/09_useful",
            "text": " For local testing, for example in Docker Desktop it is useful to add the zone label to your local K8s node with the fault domain that is then used by the Coherence Pods to set their site property. For example, if your local node is called docker-desktop you can use the following command to set the zone name to twilight-zone : <markup lang=\"bash\" >kubectl label node docker-desktop failure-domain.beta.kubernetes.io/zone=twilight-zone With this label set all Coherence Pods installed by the Coherence Operator on that node will be running in the twilight-zone . ",
            "title": "Labeling Your K8s Node"
        },
        {
            "location": "/developer/09_useful",
            "text": " Assuming that you have the Kubernetes Dashboard then you can easily start the local proxy and display the required login token by running: <markup lang=\"bash\" >./hack/kube-dash.sh This will display the authentication token, the local k8s dashboard URL and then start kubectl proxy . ",
            "title": "Kubernetes Dashboard"
        },
        {
            "location": "/developer/09_useful",
            "text": " Sometimes a CoherenceInternal resource becomes stuck in k8s. This is because the operator adds finalizers to the resources causing k8s to be unable to delete them. The simplest way to delete them is to use the kubectl patch command to remove the finalizer. For example, if there was a CoherenceInternal resource called test-role in namespace testing then the following command could be used. <markup lang=\"bash\" >kubectl -n testing patch coherenceinternal/test-role \\ -p '{\"metadata\":{\"finalizers\": []}}' \\ --type=merge; Alternatively there is a make target that wil clean up and remove all CoherenceCLuster, CoherenceRole and CoherenceInternal resources from the test namespace. <markup lang=\"bash\" >make delete-coherence-clusters ",
            "title": "Stuck CoherenceInternal Resources"
        },
        {
            "location": "/developer/09_useful",
            "text": " Labeling Your K8s Node For local testing, for example in Docker Desktop it is useful to add the zone label to your local K8s node with the fault domain that is then used by the Coherence Pods to set their site property. For example, if your local node is called docker-desktop you can use the following command to set the zone name to twilight-zone : <markup lang=\"bash\" >kubectl label node docker-desktop failure-domain.beta.kubernetes.io/zone=twilight-zone With this label set all Coherence Pods installed by the Coherence Operator on that node will be running in the twilight-zone . Kubernetes Dashboard Assuming that you have the Kubernetes Dashboard then you can easily start the local proxy and display the required login token by running: <markup lang=\"bash\" >./hack/kube-dash.sh This will display the authentication token, the local k8s dashboard URL and then start kubectl proxy . Stuck CoherenceInternal Resources Sometimes a CoherenceInternal resource becomes stuck in k8s. This is because the operator adds finalizers to the resources causing k8s to be unable to delete them. The simplest way to delete them is to use the kubectl patch command to remove the finalizer. For example, if there was a CoherenceInternal resource called test-role in namespace testing then the following command could be used. <markup lang=\"bash\" >kubectl -n testing patch coherenceinternal/test-role \\ -p '{\"metadata\":{\"finalizers\": []}}' \\ --type=merge; Alternatively there is a make target that wil clean up and remove all CoherenceCLuster, CoherenceRole and CoherenceInternal resources from the test namespace. <markup lang=\"bash\" >make delete-coherence-clusters ",
            "title": "Useful Info"
        },
        {
            "location": "/clusters/115_environment_variables",
            "text": " It is possible to pass arbitrary environment variables to the Pods that are created for a Coherence cluster. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/115_environment_variables",
            "text": " If configuring a single implicit role environment variables are set in the spec.env section; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" - name: BAR value: \"bar-val\" The FOO environment variable with a value of foo-val will be passed to the coherence container in the Pods created for the implicit role. The BAR environment variable with a value of bar-val will be passed to the coherence container in the Pods created for the implicit role. ",
            "title": "Environment Variables in the Implicit Role"
        },
        {
            "location": "/clusters/115_environment_variables",
            "text": " When configuring one or more explicit roles in the roles section of the spec environment variables can be configured for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data env: - name: FOO value: \"foo-val\" - role: proxy env: - name: BAR value: \"bar-val\" All Pods created for the data role will have the FOO environment variable set to foo-val All Pods created for the proxy role will have the BAR environment variable set to bar-val ",
            "title": "Environment Variables in Explicit Roles"
        },
        {
            "location": "/clusters/115_environment_variables",
            "text": " When configuring one or more explicit roles it is also possible to configure environment variables at the defaults level. These environment variables will be shared by all Pods in all roles unless specifically overridden for a role. An environment variable is only overridden in a role by declaring a role level environment variable with the same name. When creating the Pods configuration the Coherence Operator will merge the list of default environment variables with the role&#8217;s list of environment variables. Where an environment variable in a role has the same key as an environment variable in the defaults section the value in the role will take precedence. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" roles: - role: data - role: proxy env: - name: BAR value: \"bar-val\" - role: web env: - name: FOO value: \"foo-web\" - name: BAR value: \"bar-web\" The data role does not have any environment variables configured so it will just inherit the FOO=foo-val environment variable from the defaults. The proxy role has the BAR=bar-val environment variables configured and will also inherit the FOO=foo-val environment variable from the defaults. The web role has will override the FOO environment variable from the default with FOO=foo-web . It also has its own BAR=bar-web environment variable. ",
            "title": "Environment Variables in Explicit Roles With Defaults"
        },
        {
            "location": "/clusters/115_environment_variables",
            "text": " Environment variables can be configured in a CoherenceCluster and will be passed through to the Coherence Pods created for the roles in the cluster. Environment variables are configured in the env field of the spec. The format for setting environment variables is exactly the same as when configuring them in a Kubernetes Container . Environment Variables in the Implicit Role If configuring a single implicit role environment variables are set in the spec.env section; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" - name: BAR value: \"bar-val\" The FOO environment variable with a value of foo-val will be passed to the coherence container in the Pods created for the implicit role. The BAR environment variable with a value of bar-val will be passed to the coherence container in the Pods created for the implicit role. Environment Variables in Explicit Roles When configuring one or more explicit roles in the roles section of the spec environment variables can be configured for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data env: - name: FOO value: \"foo-val\" - role: proxy env: - name: BAR value: \"bar-val\" All Pods created for the data role will have the FOO environment variable set to foo-val All Pods created for the proxy role will have the BAR environment variable set to bar-val Environment Variables in Explicit Roles With Defaults When configuring one or more explicit roles it is also possible to configure environment variables at the defaults level. These environment variables will be shared by all Pods in all roles unless specifically overridden for a role. An environment variable is only overridden in a role by declaring a role level environment variable with the same name. When creating the Pods configuration the Coherence Operator will merge the list of default environment variables with the role&#8217;s list of environment variables. Where an environment variable in a role has the same key as an environment variable in the defaults section the value in the role will take precedence. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" roles: - role: data - role: proxy env: - name: BAR value: \"bar-val\" - role: web env: - name: FOO value: \"foo-web\" - name: BAR value: \"bar-web\" The data role does not have any environment variables configured so it will just inherit the FOO=foo-val environment variable from the defaults. The proxy role has the BAR=bar-val environment variables configured and will also inherit the FOO=foo-val environment variable from the defaults. The web role has will override the FOO environment variable from the default with FOO=foo-web . It also has its own BAR=bar-web environment variable. ",
            "title": "Environment Variables"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " The Coherence persistence feature is used to save a cache to disk and ensures that cache data can always be recovered especially in the case of a full cluster restart or re-creation. ",
            "title": "preambule"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " When enabling Persistence in the Coherence Operator, you have two options: Use the default Persistent Volume Claim (PVC) - PVC&#8217;s will be automatically be created and bound to pods on startup Specify existing persistent volumes - allows full control of the underlying allocated volumes For more information on Coherence Persistence, please see the Coherence Documentation . ",
            "title": "Using Coherence Persistence"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " Prerequisites Use Default Persistent Volume Claim Use Specific Persistent Volumes ",
            "title": "Table of Contents"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " Install the Coherence Operator Create any secrets required to pull Docker images Create a new working directory and change to that directory ",
            "title": "Prerequisites"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " In your working directory directory create a file called persistence-cluster.yaml with the following contents: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: persistence-cluster spec: jvm: memory: heapSize: 512m replicas: 3 coherence: persistence: enabled: true persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Enables Active Persistence Specifies that the volume can be mounted as read-write by a single node Sets the size of the Persistent Volume Add an imagePullSecrets entry if required to pull images from a private repository. ",
            "title": "1. Create the Coherence cluster yaml"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; persistence-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. ",
            "title": "3. Connect to the Coherence Console to add data"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " This will not delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f persistence-cluster.yaml Use kubectl get pods -n &lt;namespace&gt; to confirm the pods have terminated. ",
            "title": "4. Delete the cluster"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": "<markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-persistence-cluster-storage-0 Bound pvc-060c61d6-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-1 Bound pvc-061204e8-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-2 Bound pvc-06205b32-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s ",
            "title": "5. Confirm the PVC&#8217;s are still present"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": "<markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f persistence-cluster.yaml coherencecluster.coherence.oracle.com/persistence-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=persistence-cluster NAME READY STATUS RESTARTS AGE persistence-cluster-storage-0 1/1 Running 0 79s persistence-cluster-storage-1 0/1 Running 0 79s persistence-cluster-storage-2 0/1 Running 0 79s Wait until the pods are Running and Ready, then confirm the data is still present by using the cache test and size commands only as in step 3 above. ",
            "title": "6. Re-install the Coherence cluster"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " Issue the following to delete the Coherence cluster. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f persistence-cluster.yaml Ensure all the pods have all terminated before you delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n &lt;namespace&gt; ",
            "title": "7. Uninstall the Cluster and PVC&#8217;s"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " Issue the following to install the cluster: <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f persistence-cluster.yaml coherencecluster.coherence.oracle.com/persistence-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=persistence-cluster NAME READY STATUS RESTARTS AGE persistence-cluster-storage-0 1/1 Running 0 79s persistence-cluster-storage-1 0/1 Running 0 79s persistence-cluster-storage-2 0/1 Running 0 79s Check the Persistent Volumes and PVC are automatically created. <markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-persistence-cluster-storage-0 Bound pvc-060c61d6-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-1 Bound pvc-061204e8-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-2 Bound pvc-06205b32-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s Wait until all nodes are Running and READY before continuing. 3. Connect to the Coherence Console to add data <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; persistence-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. 4. Delete the cluster This will not delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f persistence-cluster.yaml Use kubectl get pods -n &lt;namespace&gt; to confirm the pods have terminated. 5. Confirm the PVC&#8217;s are still present <markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-persistence-cluster-storage-0 Bound pvc-060c61d6-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-1 Bound pvc-061204e8-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-2 Bound pvc-06205b32-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s 6. Re-install the Coherence cluster <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f persistence-cluster.yaml coherencecluster.coherence.oracle.com/persistence-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=persistence-cluster NAME READY STATUS RESTARTS AGE persistence-cluster-storage-0 1/1 Running 0 79s persistence-cluster-storage-1 0/1 Running 0 79s persistence-cluster-storage-2 0/1 Running 0 79s Wait until the pods are Running and Ready, then confirm the data is still present by using the cache test and size commands only as in step 3 above. 7. Uninstall the Cluster and PVC&#8217;s Issue the following to delete the Coherence cluster. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f persistence-cluster.yaml Ensure all the pods have all terminated before you delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n &lt;namespace&gt; ",
            "title": "2. Install the Coherence Cluster"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " By default, when you enable Coherence Persistence, the required infrastructure in terms of persistent volumes (PV) and persistent volume claims (PVC) is set up automatically. Also, the persistence-mode is set to active . This allows the Coherence cluster to be restarted and the data to be retained. This example shows how to enable Persistence with all the defaults. 1. Create the Coherence cluster yaml In your working directory directory create a file called persistence-cluster.yaml with the following contents: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: persistence-cluster spec: jvm: memory: heapSize: 512m replicas: 3 coherence: persistence: enabled: true persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Enables Active Persistence Specifies that the volume can be mounted as read-write by a single node Sets the size of the Persistent Volume Add an imagePullSecrets entry if required to pull images from a private repository. 2. Install the Coherence Cluster Issue the following to install the cluster: <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f persistence-cluster.yaml coherencecluster.coherence.oracle.com/persistence-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=persistence-cluster NAME READY STATUS RESTARTS AGE persistence-cluster-storage-0 1/1 Running 0 79s persistence-cluster-storage-1 0/1 Running 0 79s persistence-cluster-storage-2 0/1 Running 0 79s Check the Persistent Volumes and PVC are automatically created. <markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-persistence-cluster-storage-0 Bound pvc-060c61d6-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-1 Bound pvc-061204e8-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-2 Bound pvc-06205b32-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s Wait until all nodes are Running and READY before continuing. 3. Connect to the Coherence Console to add data <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; persistence-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. 4. Delete the cluster This will not delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f persistence-cluster.yaml Use kubectl get pods -n &lt;namespace&gt; to confirm the pods have terminated. 5. Confirm the PVC&#8217;s are still present <markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-persistence-cluster-storage-0 Bound pvc-060c61d6-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-1 Bound pvc-061204e8-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s persistence-volume-persistence-cluster-storage-2 Bound pvc-06205b32-ee2d-11e9-aa71-025000000001 1Gi RWO hostpath 2m32s 6. Re-install the Coherence cluster <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f persistence-cluster.yaml coherencecluster.coherence.oracle.com/persistence-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=persistence-cluster NAME READY STATUS RESTARTS AGE persistence-cluster-storage-0 1/1 Running 0 79s persistence-cluster-storage-1 0/1 Running 0 79s persistence-cluster-storage-2 0/1 Running 0 79s Wait until the pods are Running and Ready, then confirm the data is still present by using the cache test and size commands only as in step 3 above. 7. Uninstall the Cluster and PVC&#8217;s Issue the following to delete the Coherence cluster. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f persistence-cluster.yaml Ensure all the pods have all terminated before you delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl get pvc -n &lt;namespace&gt; | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n &lt;namespace&gt; ",
            "title": "Use Default Persistent Volume Claim"
        },
        {
            "location": "/app-deployment/060_persistence",
            "text": " This example shows how to use specific persistent volumes (PV) for Coherence when using active persistence mode. Local storage is the recommended storage type for achieving the best performance for active persistence, but this sample can be modified to use any storage class. TBD ",
            "title": "Use Specific Persistent Volumes"
        },
        {
            "location": "/clusters/054_coherence_storage_enabled",
            "text": " A Coherence cluster member can be storage enabled or storage disabled and hence a CoherenceCluster role can be configured to be storage enabled or disabled. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/054_coherence_storage_enabled",
            "text": " Coherence has a default System property that configures cache services to be storage enabled (i.e. that JVM will be manage data for caches) or storage disabled (i.e. that member will be not manage data for caches). A role in a CoherenceCluster can be set as storage enabled or disabled using the storageEnabled field; the value is a boolean true or false. Setting this property sets the Coherence JVM system property coherence.distributed.localstorage to true or false. If the storageEnabled field is not specifically set for a role then the coherence.distributed.localstorage property will not be set in the JVMs for that role and Coherence&#8217;s default behaviour will apply. If a custom application is deployed into the Coherence container that specifies a custom cache configuration file or custom operational configuration file it is entirely possible for the coherence.distributed.localstorage system property to be ignored if the application configuration files override this value. If this is the case then the settings described below will have no effect. ",
            "title": "Storage Enabled or Disabled Roles"
        },
        {
            "location": "/clusters/054_coherence_storage_enabled",
            "text": " When creating a CoherenceCluster with the single implicit role the storageEnabled field is set in the CoherenceCluster spec.coherence field. For example <markup lang=\"yaml\" title=\"Storage Enabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: storageEnabled: true The implicit role will be storage enabled <markup lang=\"yaml\" title=\"Storage Disabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: storageEnabled: false The implicit role will be storage disabled ",
            "title": "Storage Enabled or Disabled Implicit Role"
        },
        {
            "location": "/clusters/054_coherence_storage_enabled",
            "text": " When creating a CoherenceCluster with the explicit roles the storageEnabled field is set for each role in the CoherenceCluster roles list. <markup lang=\"yaml\" title=\"Storage Enabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: storageEnabled: true - role: proxy coherence: storageEnabled: false The data role will be storage enabled The proxy role will be storage disabled ",
            "title": "Storage Enabled or Disabled Explicit Roles"
        },
        {
            "location": "/clusters/054_coherence_storage_enabled",
            "text": " When creating a CoherenceCluster with the explicit roles the storageEnabled field is set for each role in the CoherenceCluster roles list and a default can be set in the CoherenceCluster spec . <markup lang=\"yaml\" title=\"Storage Enabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: storageEnabled: false roles: - role: data coherence: storageEnabled: true - role: proxy - role: web The default value will be storage disabled The data role overrides the default and will be storage enabled The proxy role does not have a specific storageEnabled so will be storage disabled The web roles does not have a specific storageEnabled so will be storage disabled ",
            "title": "Storage Enabled or Disabled Explicit Roles With Defaults"
        },
        {
            "location": "/clusters/010_introduction",
            "text": " Creating a Coherence cluster using the Coherence Operator is as simple as creating any other Kubernetes resource. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/010_introduction",
            "text": " The Coherence Operator uses a Kubernetes custom resource definition, (CRD) named CoherenceCluster to define the configuration for a Coherence cluster. All of the fields in the CoherenceCluster CRD are optional and a Coherence cluster can be created with a simple yaml file: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster The metadata.name field will be used as the Coherence cluster name. The yaml above will create a Coherence cluster with three storage enabled members. There is not much that can actually be achived with this cluster because no ports are exposed outside of Kubernetes so the cluster is inaccessible. It could be possibly be accessed by other Pods in the same Kubernetes cluster but in most use cases additional configuration would be required. ",
            "title": "CoherenceCluster CRD Overview"
        },
        {
            "location": "/clusters/010_introduction",
            "text": " A role is what is actually configured in the CoherenceCluster spec. In a traditional Coherence application that may have had a number of storage enabled members and a number of storage disable Coherence*Extend proxy members this cluster would have effectively had two roles, \"storage\" and \"proxy\". Some clusters may simply have just a storage role and some complex Coherence applications and clusters may have many roles and even different roles storage enabled for different caches/services within the same cluster. A role in a CoherenceCluster is either configured as a single implicit role or one or more explicit roles . <markup lang=\"yaml\" title=\"Single Implicit Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: replicas: 6 The configuration for the role (in this case just the replicas field) is added directly to the spec section of the CoherenceCluster . <markup lang=\"yaml\" title=\"Single Explicit Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: data replicas: 6 The configuration for a single explicit role named data is added to the roles list. of the CoherenceCluster . <markup lang=\"yaml\" title=\"Multiple Explicit Roles\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: data replicas: 6 - role: data replicas: 3 The first role in the roles list is named data with a replicas value of 6 The second role in the roles list is named proxy with a replicas value of 3 ",
            "title": "Coherence Roles"
        },
        {
            "location": "/clusters/010_introduction",
            "text": " The specification for a role in the CoherenceCluster CRD (both implicit or expilict) has the following top level fields that may be configured: <markup lang=\"yaml\" > role: replicas: application: {} coherence: {} jvm: {} scaling: {} ports: [] logging: {} volumes: [] volumeClaimTemplates: [] volumeMounts: [] env: [] annotations: {} labels: [] nodeSelector: {} tolerations: [] affinity: {} resources: {} readinessProbe: {} livenessProbe: {} The role field sets the name of the role, if omitted the default name of storage will be used. If configuring multiple roles in a CoherenceCluster each role must have a unique name. See Define Coherence Roles for more details. The replicas field sets the number of replicas ( Pods ) that will be vreated for the role. If not specified the default value is 3 . See configuring a Role&#8217;s Replica Count for more details. The application section contains fields for configuring custom application code. See Configure Applications for more details. The coherence section contains fields for configuring Coherence specific settings. See Configure Coherence for more details. The jvm section contains fields for configuring how the JVM behaves. See Configure the JVM for more details. The scaling section contains fields for configuring how the number of replicas in a role is safely scaled up and down. See Configure Safe Scaling for more details. The ports section contains fields for configuring how ports are exposed via services. See Expose Ports and Services for more details. The logging section contains fields for configuring logging. See Configure Logging for more details. The volumes section contains fields for configuring additional volumes to add to the Pods for a role. See Configure Additional Volumes for more details. The volumeClaimTemplates section contains fields for configuring additional PVCs to add to the Pods for a role. See Configure Additional Volumes for more details. The volumeMounts section contains fields for configuring additional volume mounts to add to the Pods for a role. See Configure Additional Volumes for more details. The env section contains extra environment variables to add to the Coherence container. See Environment Variables for more details. The annotations map contains extra annotations to add to the Pods for the role. See Configure Pod Annotations for more details. The labels map contains extra labels to add to the Pods for the role. See Configure Pod Labels for more details. The nodeSelector map contains node selectors to determine how Kubernetes schedules the Pods in the role. See Configure Pod Scheduling for more details. The tolerations array contains taints and tolerations to determine how Kubernetes schedules the Pods in the role. See Configure Pod Scheduling for more details. The affinity contains Pod affinity fields to determine how Kubernetes schedules the Pods in the role. See Configure Pod Scheduling for more details. The resources contains configures resource limits for the Coherence containers. See Configure Container Resource Limits for more details. The readinessProbe section configures the readiness probe for the Coherence containers. See Readiness &amp; Liveness Probes for more details. The livenessProbe section configures the liveness probe for the Coherence containers. See Readiness &amp; Liveness Probes for more details. ",
            "title": "The Coherence Role Specification"
        },
        {
            "location": "/clusters/020_k8s_resources",
            "text": " When a CoherenceCluster is deployed into Kubernetes the Coherence Operator will create a number of other resources in Kubernetes. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/020_k8s_resources",
            "text": " A CoherenceCluster is made up of one or more roles. In theory a CoherenceCluster could have zero roles but this would not by typical. A role maps to zero or more Pods that will all share the same specification and hence typically take on the same business role within an application. In Kubernetes a Coherence role is represented by a CoherenceRole and a CoherenceInternal crd although it is not expected that these crds are modified directly, they are purely used to allow roles in the same cluster to be managed as independent entities by the Coherence Operator. When a resource of type CoherenceCluster is created in Kubernetes the Coherence Operator will create the other resources. A Service will be created for every CoherenceCluster to be used for Coherence WKA (cluster membership discovery). Every Pod that is created as part of this cluster will have a label coherenceCluster=&lt;cluster-name&gt; and the WKA Service uses this label to identify all of the Pods in the same Coherence cluster. The Pods then use the Service as their WKA address. A CoherenceRole resource will be created for each role in the CoherenceCluster spec that has a replica count greater than zero. The name of the CoherenceRole will be in the form &lt;cluster-name&gt;-&lt;role-name&gt; Each CoherenceRole will have a related CoherenceInternal resource. The name of the CoherenceInternal will be the same as the CoherenceRole resource. Each CoherenceRole will have a related StatefulSet with corresponding Pods and headless Service required by the StatefulSet . The name of the StatefulSet will be the same as the CoherenceRole For each port that a role in a CoherenceCluster is configured to expose a corresponding Service will be created for that port. The name of the Service will be &lt;cluster-name&gt;-&lt;role-name&gt;-&lt;port-name&gt; (although this can be overridden when specifying the port in the CoherenceCLuster spec for that role and port). ",
            "title": "Kubernetes Resource Relationships When Creating Coherence Clusters"
        },
        {
            "location": "/clusters/125_labels",
            "text": " Labels can be added to the Pods of a role in a CoherenceCluster . ",
            "title": "preambule"
        },
        {
            "location": "/clusters/125_labels",
            "text": " The Coherence Operator applies the following labels to a role. These labels should not be overridden as they are used by the Coherence Operator. Label Description coherenceCluster This label will be set to the owning CoherenceCluster name coherenceRole This label will be set to the role name coherenceDeployment This label will be the concatenated cluster name and role name in the format of the format ClusterName-RoleName component This label is always coherencePod The default labels above make it simple to find all Pods for a Coherence cluster or for a role when querying Kubernetes (for example with kubectl get ). ",
            "title": "Default Labels"
        },
        {
            "location": "/clusters/125_labels",
            "text": " When creating a CoherenceCluster with a single implicit role labels can be defined at the spec level. Labels are defined as a map of string key value pairs, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: labels: key1 : value1 key2 : value2 The implicit role will have the labels key1=value1 and key2=value2 which will result in all Pods for the role also having those same labels. ",
            "title": "Configure Pod Labels for the Implicit Role"
        },
        {
            "location": "/clusters/125_labels",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list labels can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data labels: key1 : value1 - role: proxy labels: key2 : value2 The data role will have the label key1=value1 The proxy role will have the labels key2=value2 ",
            "title": "Configure Pod Labels for Explicit Roles"
        },
        {
            "location": "/clusters/125_labels",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list labels can be defined as defaults applied to all roles and also for each role. The default labels will be merged with the role labels. Where labels exist with the same key in both the defaults and the role then the labels in the role will take precedence. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: labels: key1 : value1 key2 : value2 roles: - role: data - role: proxy labels: key3 : value3 - role: web labels: key2 : value-two key3 : value3 There are two default labels key1=value1 and key2=value2 that will apply to all Pods in all roles unless specifically overridden. The data role has no other labels defined so will just have the default labels key1=value1 and key2=value2 The proxy role specified an labels key3=value3 so will have this labels as well as the default labels key1=value1 and key2=value2 The web role specifies the key3=value3 labels and also overrides the key2 label with the value value-two so it will have three labels, key1=value1 , key2=value-two and key3=value3 ",
            "title": "Configure Pod Labels for Explicit Roles With Defaults"
        },
        {
            "location": "/clusters/125_labels",
            "text": " Custom Pod labels can be added to the spec of a role which will then be added to all Pods for that role created by the Coherence Operator. Default Labels The Coherence Operator applies the following labels to a role. These labels should not be overridden as they are used by the Coherence Operator. Label Description coherenceCluster This label will be set to the owning CoherenceCluster name coherenceRole This label will be set to the role name coherenceDeployment This label will be the concatenated cluster name and role name in the format of the format ClusterName-RoleName component This label is always coherencePod The default labels above make it simple to find all Pods for a Coherence cluster or for a role when querying Kubernetes (for example with kubectl get ). Configure Pod Labels for the Implicit Role When creating a CoherenceCluster with a single implicit role labels can be defined at the spec level. Labels are defined as a map of string key value pairs, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: labels: key1 : value1 key2 : value2 The implicit role will have the labels key1=value1 and key2=value2 which will result in all Pods for the role also having those same labels. Configure Pod Labels for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list labels can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data labels: key1 : value1 - role: proxy labels: key2 : value2 The data role will have the label key1=value1 The proxy role will have the labels key2=value2 Configure Pod Labels for Explicit Roles With Defaults When creating a CoherenceCluster with explicit roles in the roles list labels can be defined as defaults applied to all roles and also for each role. The default labels will be merged with the role labels. Where labels exist with the same key in both the defaults and the role then the labels in the role will take precedence. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: labels: key1 : value1 key2 : value2 roles: - role: data - role: proxy labels: key3 : value3 - role: web labels: key2 : value-two key3 : value3 There are two default labels key1=value1 and key2=value2 that will apply to all Pods in all roles unless specifically overridden. The data role has no other labels defined so will just have the default labels key1=value1 and key2=value2 The proxy role specified an labels key3=value3 so will have this labels as well as the default labels key1=value1 and key2=value2 The web role specifies the key3=value3 labels and also overrides the key2 label with the value value-two so it will have three labels, key1=value1 , key2=value-two and key3=value3 ",
            "title": "Configure Pod Labels"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The Coherence Operator is a Go based project built using the Operator SDK . ",
            "title": "preambule"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The following prerequisites are required to build and test the operator (the prerequisites to just run the operator are obviously a sub-set of these). operator-sdk version v0.11.0 git go version v1.12+. mercurial version 3.9+ docker version 17.03+. kubectl version v1.11.3+. Access to a Kubernetes v1.11.3+ cluster. Java 8+ JDK Maven version 3.5+ Access to a Maven repository containing Oracle Coherence 12.2.1.4 (for the exact GAV see the pom.xml file in the java/ directory) Optional: delve version 1.2.0+ (for local debugging with operator-sdk up local --enable-delve ). This project uses make for building, which should already be installed on most systems This project currently uses the Operator SDK v0.11.0 so make sure you install the correct version of the Operator SDK CLI. As stated above this project requires K8s v1.11.3+ so if using Docker on MacOS you need at least version 2.1.0.0 ",
            "title": "Development Prerequisites"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The project also contains a Java sub-project that is used to create Coherence utilities that the Operator relies on to work correctly with Coherence clusters that it is managing. This project was initially generated using the Operator SDK and this dictates the structure of the project which means that files and directories should not be moved arbitrarily. ",
            "title": "Project Structure"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The following should not be moved: File Description bin/ scripts used in the Operator Docker image build/Dockerfile the Dockerfile used by the Operator SDK to build the Docker image cmd/manager/main.go The Operator main generated by the Operator SDK deploy/ Yaml files generated and maintained by the Operator SDK deploy/crds The CRD files generated and maintained by the Operator SDK helm-charts/ The Helm charts used by the Operator pkg/apis The API struct code generated by the Operator SDK and used to generate the CRD files pkg/controller The controller code generated by the Operator SDK watches.yaml The Helm Operator configuration generated by the Operator SDK local-watches.yaml The Helm Operator configuration used when running the operator locally ",
            "title": "Operator SDK Files"
        },
        {
            "location": "/developer/01_introduction",
            "text": " Development Prerequisites The following prerequisites are required to build and test the operator (the prerequisites to just run the operator are obviously a sub-set of these). operator-sdk version v0.11.0 git go version v1.12+. mercurial version 3.9+ docker version 17.03+. kubectl version v1.11.3+. Access to a Kubernetes v1.11.3+ cluster. Java 8+ JDK Maven version 3.5+ Access to a Maven repository containing Oracle Coherence 12.2.1.4 (for the exact GAV see the pom.xml file in the java/ directory) Optional: delve version 1.2.0+ (for local debugging with operator-sdk up local --enable-delve ). This project uses make for building, which should already be installed on most systems This project currently uses the Operator SDK v0.11.0 so make sure you install the correct version of the Operator SDK CLI. As stated above this project requires K8s v1.11.3+ so if using Docker on MacOS you need at least version 2.1.0.0 Project Structure The project also contains a Java sub-project that is used to create Coherence utilities that the Operator relies on to work correctly with Coherence clusters that it is managing. This project was initially generated using the Operator SDK and this dictates the structure of the project which means that files and directories should not be moved arbitrarily. Operator SDK Files The following should not be moved: File Description bin/ scripts used in the Operator Docker image build/Dockerfile the Dockerfile used by the Operator SDK to build the Docker image cmd/manager/main.go The Operator main generated by the Operator SDK deploy/ Yaml files generated and maintained by the Operator SDK deploy/crds The CRD files generated and maintained by the Operator SDK helm-charts/ The Helm charts used by the Operator pkg/apis The API struct code generated by the Operator SDK and used to generate the CRD files pkg/controller The controller code generated by the Operator SDK watches.yaml The Helm Operator configuration generated by the Operator SDK local-watches.yaml The Helm Operator configuration used when running the operator locally ",
            "title": "Coherence Operator Development"
        },
        {
            "location": "/about/01_overview",
            "text": " fa-rocket Quick Start Quick start guide to running your first Coherence cluster using the Coherence Operator. settings Install Installing and running the Coherence Operator. extension Application Deployment Deploying Coherence Applications using the Coherence Operator. list Examples Step-by-step examples with code for various tasks with the Coherence Operator. ",
            "title": "Get Going"
        },
        {
            "location": "/about/01_overview",
            "text": " av_timer Metrics Enabling and working with Metrics in the Coherence Operator. donut_large Logging Viewing and managing log files within using the Coherence Operator. cloud Management and Diagnostics Management and Diagnostics in the Coherence Operator. widgets CoherenceCluster CRD Reference In depth CoherenceCluster CRD reference documentation. ",
            "title": "In Depth"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " When using the implicit role configuration the replicas count is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 In this case a cluster will be created with a single implicit role named storage with a replica count of six. This will result in a StatefulSet with six Pods . ",
            "title": "Implicit Role Replicas"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " When using the explicit role configuration the replicas count is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The data role has a replica count of six The proxy role has a replic count of three ",
            "title": "Explicit Role Replicas"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " When using the explicit role configuration a value for replicas count can be set in the CoherenceCluster spec section that will be used as the default replicas value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 3 The default replicas value is set to six. The data and proxy roles do not have a replicas value so will use this default value and so will each have a StatefulSet with a replica count of six The web role has an explicit replicas value of three so will have three replicas in its StatefulSet ",
            "title": "Explicit Roles with Default Replicas"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " The replica count for a role in a CoherenceCluster is set using the replicas field of a role spec. Implicit Role Replicas When using the implicit role configuration the replicas count is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 In this case a cluster will be created with a single implicit role named storage with a replica count of six. This will result in a StatefulSet with six Pods . Explicit Role Replicas When using the explicit role configuration the replicas count is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The data role has a replica count of six The proxy role has a replic count of three Explicit Roles with Default Replicas When using the explicit role configuration a value for replicas count can be set in the CoherenceCluster spec section that will be used as the default replicas value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 3 The default replicas value is set to six. The data and proxy roles do not have a replicas value so will use this default value and so will each have a StatefulSet with a replica count of six The web role has an explicit replicas value of three so will have three replicas in its StatefulSet ",
            "title": "Setting the Replica Count for a Role"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " The different configuration files commonly used by Coherence can be specified for a role in the role&#8217;s spec. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " There are three Coherence configuration files that can be set in a role&#8217;s specification: The Coherence Cache Configuration file The Coherence Operational Override file Oracle Coherence provides a number of ways to specify the configuration files to be used. If deploying application code alongside the Coherence JVM it is entirely possible to use the default configuration file names in the application or to hard code one or more of the configuration file names in the application itself. This section of the documentation describes CoherenceCluster role configuration that will result in passing the -Dcoherence.cache.config and the coherence.override system properties to the Coherence JVM. If the deployed application overrides or ignores these properties then setting the configurations described below will have no effect. ",
            "title": "Coherence Config Files"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " When using the implicit role configuration the cacheConfig value is set directly in the CoherenceCluster spec.coherence section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: cacheConfig: application-cache-config.xml In this case a cluster will be created with a single implicit role named storage where the coherence.cache.config system property and hence the cache configuration file used will be application-cache-config.xml ",
            "title": "Set the Cache Configuration for an Implicit Role"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " When using the explicit role configuration the cacheConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: cacheConfig: data-cache-config.xml - role: proxy coherence: cacheConfig: proxy-cache-config.xml The data role will use the data-cache-config.xml cache configuration file The proxy role will use the proxy-cache-config.xml cache configuration file ",
            "title": "Set the Cache Configuration for Explicit Role"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " When using the explicit role configuration a value for cacheConfig value can be set in the CoherenceCluster spec section that will be used as the default cacheConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: cacheConfig: application-cache-config.xml roles: - role: data - role: proxy - role: web coherence: cacheConfig: web-cache-config.xml The default cacheConfig value is set to application-cache-config.xml . The data and proxy roles do not have a cacheConfig value so will use this default value and will each have use the application-cache-config.xml file The web role has an explicit cacheConfig value of web-cache-config.xml so will use the web-cache-config.xml cache configuration file ",
            "title": "Set the Cache Configuration for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " The Coherence cache configuration file for a role in a CoherenceCluster is set using the cacheConfig field of a role spec. The value of this field will end up being passed to the Coherence JVM as the coherence.cache.config system property and will hence set the value of the cache configuration file used as described in the Coherence documentation. Set the Cache Configuration for an Implicit Role When using the implicit role configuration the cacheConfig value is set directly in the CoherenceCluster spec.coherence section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: cacheConfig: application-cache-config.xml In this case a cluster will be created with a single implicit role named storage where the coherence.cache.config system property and hence the cache configuration file used will be application-cache-config.xml Set the Cache Configuration for Explicit Role When using the explicit role configuration the cacheConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: cacheConfig: data-cache-config.xml - role: proxy coherence: cacheConfig: proxy-cache-config.xml The data role will use the data-cache-config.xml cache configuration file The proxy role will use the proxy-cache-config.xml cache configuration file Set the Cache Configuration for Explicit Roles with a Default When using the explicit role configuration a value for cacheConfig value can be set in the CoherenceCluster spec section that will be used as the default cacheConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: cacheConfig: application-cache-config.xml roles: - role: data - role: proxy - role: web coherence: cacheConfig: web-cache-config.xml The default cacheConfig value is set to application-cache-config.xml . The data and proxy roles do not have a cacheConfig value so will use this default value and will each have use the application-cache-config.xml file The web role has an explicit cacheConfig value of web-cache-config.xml so will use the web-cache-config.xml cache configuration file ",
            "title": "Setting the Coherence Cache Configuration File"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " When using the implicit role configuration the overrideConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: overrideConfig: application-override.xml In this case a cluster will be created with a single implicit role named storage where the coherence.override system property and hence the operational override file used will be application-override.xml ",
            "title": "Set the Operational Override for an Implicit Role"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " When using the explicit role configuration the overrideConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: overrideConfig: data-override.xml - role: proxy coherence: overrideConfig: proxy-override.xml The data role will use the data-override.xml operational override file The proxy role will use the proxy-override.xml operational override file ",
            "title": "Set the Operational Override for Explicit Role"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " When using the explicit role configuration a value for overrideConfig value can be set in the CoherenceCluster spec section that will be used as the default overrideConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: overrideConfig: application-override.xml roles: - role: data - role: proxy - role: web coherence: overrideConfig: web-override.xml The default overrideConfig value is set to application-override.xml . The data and proxy roles do not have an overrideConfig value so will use this default value and will each have use the application-override.xml file The web role has an explicit overrideConfig value of web-override.xml so will use the web-override.xml operational override file ",
            "title": "Set the Operational Override for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/052_coherence_config_files",
            "text": " The Coherence operational override file for a role in a CoherenceCluster is set using the overrideConfig field of a role spec. The value of this field will end up being passed to the Coherence JVM as the coherence.override system property and will hence set the value of the operational override file used as described in the Coherence documentation. Set the Operational Override for an Implicit Role When using the implicit role configuration the overrideConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: overrideConfig: application-override.xml In this case a cluster will be created with a single implicit role named storage where the coherence.override system property and hence the operational override file used will be application-override.xml Set the Operational Override for Explicit Role When using the explicit role configuration the overrideConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: overrideConfig: data-override.xml - role: proxy coherence: overrideConfig: proxy-override.xml The data role will use the data-override.xml operational override file The proxy role will use the proxy-override.xml operational override file Set the Operational Override for Explicit Roles with a Default When using the explicit role configuration a value for overrideConfig value can be set in the CoherenceCluster spec section that will be used as the default overrideConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: overrideConfig: application-override.xml roles: - role: data - role: proxy - role: web coherence: overrideConfig: web-override.xml The default overrideConfig value is set to application-override.xml . The data and proxy roles do not have an overrideConfig value so will use this default value and will each have use the application-override.xml file The web role has an explicit overrideConfig value of web-override.xml so will use the web-override.xml operational override file ",
            "title": "Setting the Coherence Operational Override File"
        },
        {
            "location": "/management/040_visualvm",
            "text": " VisualVM is a visual tool integrating commandline JDK tools and lightweight profiling capabilities. Designed for both development and production time use. ",
            "title": "preambule"
        },
        {
            "location": "/management/040_visualvm",
            "text": " Coherence management is implemented using Java Management Extensions (JMX). JMX is a Java standard for managing and monitoring Java applications and services. VisualVM and other JMX tools can be used to manage and monitor Coherence Clusters via JMX. This example shows how to connect to a cluster via VisualVM over JMXMP. Please see Management over ReST for how to connect to a cluster via the VisualVM plugin using ReST. See the Coherence Management Documentation for more information on JMX and Management. ",
            "title": "Access the Coherence cluster via VisualVM"
        },
        {
            "location": "/clusters/190_service_account",
            "text": " In Kubernetes clusters that have RBAC enabled it may be a requirement to set the service account that will be used by the Pods created for a CoherenceCluster The service account name is set for the CoherenceCluster as a whole and will be applied to all Pods . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: serviceAccountName: foo All Pods in the test-cluster will use the service account foo ",
            "title": "Configure the Kubernetes Service Account"
        },
        {
            "location": "/management/010_overview",
            "text": " Management Over ReST Management Over ReST. Using VisualVM Access the Coherence cluster via VisualVM. ",
            "title": "Management"
        },
        {
            "location": "/management/010_overview",
            "text": " Generating Heap Dumps Produce and extract a heap dump. Accessing the Console Accessing the Coherence Console. Accessing CohQL Accessing the CohQL client. ",
            "title": "Diagnostics"
        },
        {
            "location": "/install/01_installation",
            "text": " The Coherence Operator is available as a Docker image oracle/coherence-operator:2.0.0-1910171100 that can easily be installed into a Kubernetes cluster. ",
            "title": "preambule"
        },
        {
            "location": "/install/01_installation",
            "text": " In order for the Coherence Operator to be able to install Coherence clusters it needs to be able to pull Coherence Docker images. These images are not available in public Docker repositories and will typically Kubernetes will need authentication to be able to pull them. This is achived by creating pull secrets. Pull secrets are not global and hence secrets will be required in the namespace(s) that Coherence clusters will be installed into. see Obtain Coherence Images ",
            "title": "Image Pull Secrets"
        },
        {
            "location": "/install/01_installation",
            "text": " Access to a Kubernetes v1.11.3+ cluster. Access to Oracle Coherence Docker images. Image Pull Secrets In order for the Coherence Operator to be able to install Coherence clusters it needs to be able to pull Coherence Docker images. These images are not available in public Docker repositories and will typically Kubernetes will need authentication to be able to pull them. This is achived by creating pull secrets. Pull secrets are not global and hence secrets will be required in the namespace(s) that Coherence clusters will be installed into. see Obtain Coherence Images ",
            "title": "Prerequisites"
        },
        {
            "location": "/install/01_installation",
            "text": " Add the coherence helm repository using the following commands: <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update ",
            "title": "Add the Coherence Helm Repository"
        },
        {
            "location": "/install/01_installation",
            "text": " To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Uninstall the Coherence Operator Helm chart"
        },
        {
            "location": "/install/01_installation",
            "text": " Once the Coherence Helm repo is configured the Coherence Operator can be installed using a normal Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ coherence/coherence-operator where &lt;namespace&gt; is the namespace that the Coherence Operator will be installed into and the namespace where it will manage CoherenceClusters Uninstall the Coherence Operator Helm chart To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Install the Coherence Operator Helm chart"
        },
        {
            "location": "/install/01_installation",
            "text": " The simplest way to install the Coherence Operator is to use the Helm chart. This will ensure that all of the correct resources are created in Kubernetes. Add the Coherence Helm Repository Add the coherence helm repository using the following commands: <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update Install the Coherence Operator Helm chart Once the Coherence Helm repo is configured the Coherence Operator can be installed using a normal Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ coherence/coherence-operator where &lt;namespace&gt; is the namespace that the Coherence Operator will be installed into and the namespace where it will manage CoherenceClusters Uninstall the Coherence Operator Helm chart To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Installing With Helm"
        },
        {
            "location": "/clusters/140_resource_constraints",
            "text": " When using the implicit role configuration of the resource limits is set directly in the CoherenceCluster spec resources section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" The coherence container in the implicit role&#8217;s Pods has a request of 0.25 cpu and 64MiB (226 bytes) of memory. The coherence container has a limit of 0.5 cpu and 128MiB of memory. ",
            "title": "Configure Resource Limits for the Single Implicit Role"
        },
        {
            "location": "/clusters/140_resource_constraints",
            "text": " When using the explicit roles in a CoherenceCluster roles list the Coherence image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data resources: requests: memory: \"10Gi\" cpu: \"4\" limits: memory: \"15Gi\" cpu: \"4\" - role: proxy resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" The coherence container in the data role&#8217;s Pods has a request of 4 cpu and 10GiB of memory. The coherence container has a limit of 4 cpu and 15GiB of memory. The coherence container in the proxy role&#8217;s Pods has a request of 0.25 cpu and 64MiB of memory. The coherence container has a limit of 0.5 cpu and 128MiB of memory. ",
            "title": "Configure Resource Limits for Explicit Roles"
        },
        {
            "location": "/clusters/140_resource_constraints",
            "text": " When using the explicit roles in a CoherenceCluster roles list the resource limits to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" roles: - role: data resources: requests: memory: \"10Gi\" cpu: \"4\" limits: memory: \"15Gi\" cpu: \"4\" - role: proxy - role: web The default resource limits has a request of 0.25 cpu and 64MiB (226 bytes) of memory and has a limit of 0.5 cpu and 128MiB of memory. The data role overrides the defaults and specifies a request of 4 cpu and 10GiB of memory. The coherence container has a limit of 4 cpu and 15GiB of memory. The proxy role and the web role do not specify resource limits so the defaults will apply so that Pods in the proxy and web roles have a request of 0.25 cpu and 64MiB (226 bytes) of memory and has a limit of 0.5 cpu and 128MiB of memory. ",
            "title": "Configure Resource Limits for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/140_resource_constraints",
            "text": " When creating a CoherenceCluster you can optionally specify how much CPU and memory (RAM) each Coherence Container needs. The container resources are specified in the resources section of a role in a CoherenceCluster , the format is exactly the same as documented in the Kubernetes documentation Managing Compute Resources for Containers . When setting resource limits, in particular memory limits, for a container it is important to ensure that the Coherence JVM is properly configured so that it does not consume more memory than the limits. If the JVM attempts to consume more memory than the resource limits allow the Pod can be killed by Kubernetes. See Configuring the JVM for details on the different memory settings. Configure Resource Limits for the Single Implicit Role When using the implicit role configuration of the resource limits is set directly in the CoherenceCluster spec resources section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" The coherence container in the implicit role&#8217;s Pods has a request of 0.25 cpu and 64MiB (226 bytes) of memory. The coherence container has a limit of 0.5 cpu and 128MiB of memory. Configure Resource Limits for Explicit Roles When using the explicit roles in a CoherenceCluster roles list the Coherence image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data resources: requests: memory: \"10Gi\" cpu: \"4\" limits: memory: \"15Gi\" cpu: \"4\" - role: proxy resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" The coherence container in the data role&#8217;s Pods has a request of 4 cpu and 10GiB of memory. The coherence container has a limit of 4 cpu and 15GiB of memory. The coherence container in the proxy role&#8217;s Pods has a request of 0.25 cpu and 64MiB of memory. The coherence container has a limit of 0.5 cpu and 128MiB of memory. Configure Resource Limits for Explicit Roles with a Default When using the explicit roles in a CoherenceCluster roles list the resource limits to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" roles: - role: data resources: requests: memory: \"10Gi\" cpu: \"4\" limits: memory: \"15Gi\" cpu: \"4\" - role: proxy - role: web The default resource limits has a request of 0.25 cpu and 64MiB (226 bytes) of memory and has a limit of 0.5 cpu and 128MiB of memory. The data role overrides the defaults and specifies a request of 4 cpu and 10GiB of memory. The coherence container has a limit of 4 cpu and 15GiB of memory. The proxy role and the web role do not specify resource limits so the defaults will apply so that Pods in the proxy and web roles have a request of 0.25 cpu and 64MiB (226 bytes) of memory and has a limit of 0.5 cpu and 128MiB of memory. ",
            "title": "Container Resource Limits"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " Annotations can be added to the Pods of a role in a CoherenceCluster . ",
            "title": "preambule"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " When creating a CoherenceCluster with a single implicit role annotations can be defined at the spec level. Annotations are defined as a map of string key value pairs, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back The implicit role will have the annotation acme.com/layer : back This will result in a StatefulSet for the role with the annotation added to the PodSpec . <markup lang=\"yaml\" title=\"StatefulSet snippet\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster-storage spec: replicas: 3 selector: matchLabels: coherenceDeployment: test-cluster-storage component: coherencePod serviceName: test-cluster-storage template: metadata: annotations: acme.com/layer: back The annotation acme.com/layer: back has been applied to the StatefulSet Pod template. ",
            "title": "Configure Pod Annotations for the Implicit Role"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data annotations: acme.com/layer: back - role: proxy annotations: acme.com/layer: front The data role will have the annotation acme.com/layer: back The proxy role will have the annotation acme.com/layer: front ",
            "title": "Configure Pod Annotations for Explicit Roles"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined as defaults applied to all roles and also for each role. Where annotations exist with the same key in both the defaults and the role then the annotation in the role will take precedence. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back acme.com/app: orders roles: - role: data - role: proxy annotations: acme.com/state: none - role: web annotations: acme.com/three: none acme.com/layer: front There are two default annotations acme.com/layer : back and acme.com/app : orders that will apply to all Pods in all roles unless specifically overridden. The data role has no other annotations defined so will just have the default annotations acme.com/layer : back and acme.com/app : orders The proxy role specified an annotation acme.com/state : none so will have this annotation as well as the default annotations acme.com/layer : back and acme.com/app : orders The web role specifies the acme.com/three: none annotation and also overrides the acme.com/layer annotation with the value front so it will have three annotations, acme.com/three: none , acme.com/layer : front and acme.com/app : orders ",
            "title": "Configure Pod Annotations for Explicit Roles With Defaults"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " Custom annotations can be added to the spec of a role which will then be added to all Pods for that role created by the Coherence Operator. Configure Pod Annotations for the Implicit Role When creating a CoherenceCluster with a single implicit role annotations can be defined at the spec level. Annotations are defined as a map of string key value pairs, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back The implicit role will have the annotation acme.com/layer : back This will result in a StatefulSet for the role with the annotation added to the PodSpec . <markup lang=\"yaml\" title=\"StatefulSet snippet\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster-storage spec: replicas: 3 selector: matchLabels: coherenceDeployment: test-cluster-storage component: coherencePod serviceName: test-cluster-storage template: metadata: annotations: acme.com/layer: back The annotation acme.com/layer: back has been applied to the StatefulSet Pod template. Configure Pod Annotations for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data annotations: acme.com/layer: back - role: proxy annotations: acme.com/layer: front The data role will have the annotation acme.com/layer: back The proxy role will have the annotation acme.com/layer: front Configure Pod Annotations for Explicit Roles With Defaults When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined as defaults applied to all roles and also for each role. Where annotations exist with the same key in both the defaults and the role then the annotation in the role will take precedence. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back acme.com/app: orders roles: - role: data - role: proxy annotations: acme.com/state: none - role: web annotations: acme.com/three: none acme.com/layer: front There are two default annotations acme.com/layer : back and acme.com/app : orders that will apply to all Pods in all roles unless specifically overridden. The data role has no other annotations defined so will just have the default annotations acme.com/layer : back and acme.com/app : orders The proxy role specified an annotation acme.com/state : none so will have this annotation as well as the default annotations acme.com/layer : back and acme.com/app : orders The web role specifies the acme.com/three: none annotation and also overrides the acme.com/layer annotation with the value front so it will have three annotations, acme.com/three: none , acme.com/layer : front and acme.com/app : orders ",
            "title": "Configure Pod Annotations"
        },
        {
            "location": "/metrics/010_overview",
            "text": " Enabling Metrics Deploying Coherence clusters with metrics enabled. Enabling SSL Enabling SSL for metrics capture. Using Your Own Prometheus Scraping metrics from your own Prometheus instance. Grafana Dashboards Details on the Grafana Dashboards available. ",
            "title": "Metrics"
        },
        {
            "location": "/app-deployment/010_overview",
            "text": " Packaging Applications Adding application jars/config to a Coherence deployment. Using Application Roles Defining and using application roles. Persistence Using Coherence Persistence. Rolling Upgrades Changing an image version for Coherence or applications using rolling upgrade. ",
            "title": "Coherence Application Deployment"
        },
        {
            "location": "/about/03_quickstart",
            "text": " This guide is a simple set of steps to install the Coherence Operator and then use that to install a simple Coherence cluster. ",
            "title": "preambule"
        },
        {
            "location": "/about/03_quickstart",
            "text": " Ensure that the Coherence Operator prerequisites are available. ",
            "title": "Prerequisites"
        },
        {
            "location": "/about/03_quickstart",
            "text": "<markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update ",
            "title": "1.1 Add the Coherence Operator Helm repository"
        },
        {
            "location": "/about/03_quickstart",
            "text": "<markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name &lt;release-name&gt; \\ coherence/coherence-operator Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be helm install --namespace coherence --name operator coherence/coherence-operator See the full install guide for more details. ",
            "title": "1.2. Install the Coherence Operator Helm chart"
        },
        {
            "location": "/about/03_quickstart",
            "text": " 1.1 Add the Coherence Operator Helm repository <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update 1.2. Install the Coherence Operator Helm chart <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name &lt;release-name&gt; \\ coherence/coherence-operator Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be helm install --namespace coherence --name operator coherence/coherence-operator See the full install guide for more details. ",
            "title": "1. Install the Coherence Operator"
        },
        {
            "location": "/about/03_quickstart",
            "text": " The minimal required yaml to create a CoherenceCluster resource is shown below. <markup lang=\"yaml\" title=\"my-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster The only required field is metadata.name which will be used as the Coherence cluster name, in this case my-cluster <markup >kubectl -n &lt;namespace&gt; apply -f my-cluster.yaml Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence create -f my-cluster.yaml ",
            "title": "2.1 Install a Coherence cluster using the minimal required configuration."
        },
        {
            "location": "/about/03_quickstart",
            "text": " After installing the my-cluster.yaml above here should be a single coherencecluster resource named my-cluster and a single coherencerole resource named my-cluster-storage created in the Coherence Operator namespace. <markup >kubectl -n &lt;namespace&gt; get coherencecluster Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence get coherence <markup >NAME AGE coherencerole.coherence.oracle.com/my-cluster-storage 19s NAME AGE coherencecluster.coherence.oracle.com/my-cluster 19s See the in-depth documentation on the Kubernetes resources created by the Coherence Operator. ",
            "title": "2.2 List the Coherence Resources"
        },
        {
            "location": "/about/03_quickstart",
            "text": " The Coherence Operator applies a coherenceCluster label to all of the Pods so this label can be used with the kubectl command to find Pods for a Coherence cluster. <markup >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence get pod -l coherenceCluster=my-cluster <markup >NAME READY STATUS RESTARTS AGE my-cluster-storage-0 1/1 Running 0 2m58s my-cluster-storage-1 1/1 Running 0 2m58s my-cluster-storage-2 1/1 Running 0 2m58s The default cluster size is three so there should be three Pods ",
            "title": "2.3 List all of the Pods for the Coherence cluster."
        },
        {
            "location": "/about/03_quickstart",
            "text": " Ensure that the Coherence images can be pulled by the Kubernetes cluster see Obtain Coherence Images By default a CoherenceCluster will use images from Oracle Container Registry. If a different registry is used the image name will need to be specified in the CoherenceCluster yaml, see Setting the Coherence Image for documentation on how to specify a different Coherence image to the default. 2.1 Install a Coherence cluster using the minimal required configuration. The minimal required yaml to create a CoherenceCluster resource is shown below. <markup lang=\"yaml\" title=\"my-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster The only required field is metadata.name which will be used as the Coherence cluster name, in this case my-cluster <markup >kubectl -n &lt;namespace&gt; apply -f my-cluster.yaml Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence create -f my-cluster.yaml 2.2 List the Coherence Resources After installing the my-cluster.yaml above here should be a single coherencecluster resource named my-cluster and a single coherencerole resource named my-cluster-storage created in the Coherence Operator namespace. <markup >kubectl -n &lt;namespace&gt; get coherencecluster Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence get coherence <markup >NAME AGE coherencerole.coherence.oracle.com/my-cluster-storage 19s NAME AGE coherencecluster.coherence.oracle.com/my-cluster 19s See the in-depth documentation on the Kubernetes resources created by the Coherence Operator. 2.3 List all of the Pods for the Coherence cluster. The Coherence Operator applies a coherenceCluster label to all of the Pods so this label can be used with the kubectl command to find Pods for a Coherence cluster. <markup >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence get pod -l coherenceCluster=my-cluster <markup >NAME READY STATUS RESTARTS AGE my-cluster-storage-0 1/1 Running 0 2m58s my-cluster-storage-1 1/1 Running 0 2m58s my-cluster-storage-2 1/1 Running 0 2m58s The default cluster size is three so there should be three Pods ",
            "title": "2. Install a Coherence Cluster"
        },
        {
            "location": "/about/03_quickstart",
            "text": " Using the kubectl scale command a specific CoherenceRole can be scaled up or down. <markup >kubectl -n &lt;namespace&gt; scale coherencerole/storage --replicas=6 Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence scale coherencerole/my-cluster-storage --replicas=6 ",
            "title": "3.1 Use kubectl to Scale Up"
        },
        {
            "location": "/about/03_quickstart",
            "text": "<markup >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence get pod -l coherenceCluster=my-cluster <markup >NAME READY STATUS RESTARTS AGE my-cluster-storage-0 1/1 Running 0 4m23s my-cluster-storage-1 1/1 Running 0 4m23s my-cluster-storage-2 1/1 Running 0 4m23s my-cluster-storage-3 1/1 Running 0 1m19s my-cluster-storage-4 1/1 Running 0 1m19s my-cluster-storage-5 1/1 Running 0 1m19s There should eventually be six running Pods . ",
            "title": "3.2 List all of the Pods fo the Coherence cluster"
        },
        {
            "location": "/about/03_quickstart",
            "text": " 3.1 Use kubectl to Scale Up Using the kubectl scale command a specific CoherenceRole can be scaled up or down. <markup >kubectl -n &lt;namespace&gt; scale coherencerole/storage --replicas=6 Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence scale coherencerole/my-cluster-storage --replicas=6 3.2 List all of the Pods fo the Coherence cluster <markup >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster Use the same namespace that the operator was installed into, e.g. if the namespace is coherence the command would be kubectl -n coherence get pod -l coherenceCluster=my-cluster <markup >NAME READY STATUS RESTARTS AGE my-cluster-storage-0 1/1 Running 0 4m23s my-cluster-storage-1 1/1 Running 0 4m23s my-cluster-storage-2 1/1 Running 0 4m23s my-cluster-storage-3 1/1 Running 0 1m19s my-cluster-storage-4 1/1 Running 0 1m19s my-cluster-storage-5 1/1 Running 0 1m19s There should eventually be six running Pods . ",
            "title": "3. Scale the Coherence Cluster"
        },
        {
            "location": "/management/050_console",
            "text": " The Coherence Console is a useful debugging and diagnosis tool usually used by administrators. ",
            "title": "preambule"
        },
        {
            "location": "/management/050_console",
            "text": " Deploy a simple CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"example-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: example-cluster spec: role: storage replicas: 3 Add an imagePullSecrets entry if required to pull images from a private repository. <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f example-cluster.yaml coherencecluster.coherence.oracle.com/example-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=example-cluster NAME READY STATUS RESTARTS AGE example-cluster-storage-0 1/1 Running 0 59s example-cluster-storage-1 1/1 Running 0 59s example-cluster-storage-2 1/1 Running 0 59s ",
            "title": "1. Install a Coherence Cluster"
        },
        {
            "location": "/management/050_console",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; example-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Issue the command size to verify the cache entry count. Lastly issue the help command to show all available commands. Type bye to exit the console. ",
            "title": "2. Connect to the Coherence Console to add data"
        },
        {
            "location": "/management/050_console",
            "text": " After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f example-cluster.yaml ",
            "title": "3. Clean Up"
        },
        {
            "location": "/management/050_console",
            "text": " The example shows how to access the Coherence Console in a running cluster. The Coherence Console is for advanced Coherence users and use-cases and care should be taken when using it. 1. Install a Coherence Cluster Deploy a simple CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"example-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: example-cluster spec: role: storage replicas: 3 Add an imagePullSecrets entry if required to pull images from a private repository. <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f example-cluster.yaml coherencecluster.coherence.oracle.com/example-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=example-cluster NAME READY STATUS RESTARTS AGE example-cluster-storage-0 1/1 Running 0 59s example-cluster-storage-1 1/1 Running 0 59s example-cluster-storage-2 1/1 Running 0 59s 2. Connect to the Coherence Console to add data <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; example-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Issue the command size to verify the cache entry count. Lastly issue the help command to show all available commands. Type bye to exit the console. 3. Clean Up After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f example-cluster.yaml ",
            "title": "Accessing the Coherence Console"
        },
        {
            "location": "/clusters/200_private_repos",
            "text": " Sometimes the images used by a Coherence cluster need to be pulled from a private image registry that requires credentials. The Coherence Operator supports supplying credentials in the CoherenceCluster configuration. The Kubernetes documentation on using a private registries gives a number of options for supplying credentials. ",
            "title": "Using Private Image Registries"
        },
        {
            "location": "/clusters/200_private_repos",
            "text": " Kubernetes supports configuring pods to use imagePullSecrets for pulling images. If possible, this is the preferable and most portable route. See the kubernetes docs for this. Once secrets have been created in the namespace that the CoherenceCluster is to be installed in then the secret name can be specified in the CoherenceCluster spec . It is possible to specify multiple secrets in the case where the different images being used are pulled from different registries. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: imagePullSecrets: - name: coherence-secret The coherence-secret will be used for pulling images from the registry associated to the secret The imagePullSecrets field is a list of values in the same format that they would be specified in Kubernetes Pod specs, so multiple secrets can be specified for different authenticated registries in the case where the Coherence cluster will use images from different authenticated registries.. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: imagePullSecrets: - name: coherence-secret - name: ocr-secret The imagePullSecrets list specifies two secrets to use coherence-secret and ocr-secret Image pull secrets are only specified for the CoherenceCluster as a whole as there is no benefit to being able to specify different secrets for different roles within a cluster. ",
            "title": "Use ImagePullSecrets"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " Since version 12.2.1.4 Coherence has had functionality to expose metrics via a http endpoint. This endpoint is disabled by default in Coherence clusters but can be enabled and configured by setting the relevant fields in the CoherenceCluster resource. ",
            "title": "Coherence Metrics"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " When configuring a single implicit role in a CoherenceCluster metrics can be enabled by setting the coherence.metrics.enabled to true in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true Metrics will be enabled and the http endpoint will bind to port 9612 in the container. The port is not exposed in a Service . ",
            "title": "Enabling Metrics for the Implicit Role"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " When configuring a explicit roles in the roles list of a CoherenceCluster metrics can be enabled or disabled by setting the coherence.metrics.enabled for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: metrics: enabled: true - role: proxy coherence: metrics: enabled: false The data role has the metrics enabled. The proxy role has the metrics disabled. ",
            "title": "Enabling Metrics for Explicit Roles"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " When configuring a explicit roles in the roles list of a CoherenceCluster a default value for the coherence.metrics.enabled field can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true roles: - role: data - role: proxy coherence: metrics: enabled: false The default value for enabling metrics is true which will apply to all roles in the roles list unless the field is specifically overridden. The data role does not specify a value for the coherence.metrics.enabled field so it will use the default value of true so metrics will be enabled. The proxy role overrides the default value for the coherence.metrics.enabled field and sets it to false so metrics will be disabled. ",
            "title": "Enabling Metrics for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " Enabling metrics only enables the http server so that the endpoint is available in the container. If external access to the API is required via a service then the port needs to be exposed just like any other additional ports as described in Expose Ports and Services . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true ports: - name: metrics port: 9612 metrics will be enabled and the default port value will be used so that the http endpoint will bind to port 9612 in the container. An additional port named metrics is added to the ports array which will cause the metrics port to be exposed on a service. The port specified is 9612 as that is the default port that metrics will bind to. ",
            "title": "Exposing Metrics via a Service"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " The default port in the container that metrics uses is 9612. It is possible to change ths port using the coherence.metrics.port field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true port: 9000 ports: - name: metrics port: 9000 metrics is enabled and configured to bind to port 9000 in the container. The corresponding port value of 9000 must be used when exposing the port on a Service . ",
            "title": "Expose metrics on a Different Port"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " It is possible to configure metrics endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.metrics.ssl section of the spec. See metrics for a more in depth guide to configuring SSL. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true ssl: enabled: true keyStore: metrics-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: metrics-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: metrics-secret The enabled field when set to true enables SSL for metrics or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. ",
            "title": "Configuring metrics With SSL"
        },
        {
            "location": "/clusters/060_coherence_metrics",
            "text": " Coherence metrics can be enabled or disabled by setting the coherence.metrics.enabled field. Note Enabling metrics will add a number of .jar files to the classpath of the Coherence JVM. In Coherence 12.2.1.4 those .jar file are: <markup >org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34 org.glassfish.hk2:hk2-api:jar:2.4.0-b34 org.glassfish.hk2:hk2-locator:jar:2.4.0-b34 org.glassfish.hk2:hk2-utils:jar:2.4.0-b34 org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34 com.fasterxml.jackson.core:jackson-annotations:jar:2.9.9 com.fasterxml.jackson.core:jackson-core:jar:2.9.9 com.fasterxml.jackson.core:jackson-databind:jar:2.9.9.2 com.fasterxml.jackson.jaxrs:jackson-jaxrs-base:jar:2.9.9 com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:jar:2.9.9 com.fasterxml.jackson.module:jackson-module-jaxb-annotations:jar:2.9.9 javax.annotation:javax.annotation-api:jar:1.2 javax.validation:validation-api:jar:1.1.0.Final javax.ws.rs:javax.ws.rs-api:jar:2.0.1 org.glassfish.jersey.core:jersey-client:jar:2.22.4 org.glassfish.jersey.core:jersey-common:jar:2.22.4 org.glassfish.jersey.ext:jersey-entity-filtering:jar:2.22.4 org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.22.4 org.glassfish.jersey.media:jersey-media-json-jackson:jar:2.22.4 org.glassfish.jersey.core:jersey-server:jar:2.22.4 org.glassfish.hk2:osgi-resource-locator:jar:1.0.1 If adding additional application .jar files care should be taken that there are no version conflicts. If conflicts are an issue there are alternative approaches available to exposing the metrics. The list above is subject to change in later Coherence patches and version. Enabling Metrics for the Implicit Role When configuring a single implicit role in a CoherenceCluster metrics can be enabled by setting the coherence.metrics.enabled to true in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true Metrics will be enabled and the http endpoint will bind to port 9612 in the container. The port is not exposed in a Service . Enabling Metrics for Explicit Roles When configuring a explicit roles in the roles list of a CoherenceCluster metrics can be enabled or disabled by setting the coherence.metrics.enabled for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: metrics: enabled: true - role: proxy coherence: metrics: enabled: false The data role has the metrics enabled. The proxy role has the metrics disabled. Enabling Metrics for Explicit Roles with a Default When configuring a explicit roles in the roles list of a CoherenceCluster a default value for the coherence.metrics.enabled field can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true roles: - role: data - role: proxy coherence: metrics: enabled: false The default value for enabling metrics is true which will apply to all roles in the roles list unless the field is specifically overridden. The data role does not specify a value for the coherence.metrics.enabled field so it will use the default value of true so metrics will be enabled. The proxy role overrides the default value for the coherence.metrics.enabled field and sets it to false so metrics will be disabled. Exposing Metrics via a Service Enabling metrics only enables the http server so that the endpoint is available in the container. If external access to the API is required via a service then the port needs to be exposed just like any other additional ports as described in Expose Ports and Services . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true ports: - name: metrics port: 9612 metrics will be enabled and the default port value will be used so that the http endpoint will bind to port 9612 in the container. An additional port named metrics is added to the ports array which will cause the metrics port to be exposed on a service. The port specified is 9612 as that is the default port that metrics will bind to. Expose metrics on a Different Port The default port in the container that metrics uses is 9612. It is possible to change ths port using the coherence.metrics.port field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true port: 9000 ports: - name: metrics port: 9000 metrics is enabled and configured to bind to port 9000 in the container. The corresponding port value of 9000 must be used when exposing the port on a Service . Configuring metrics With SSL It is possible to configure metrics endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.metrics.ssl section of the spec. See metrics for a more in depth guide to configuring SSL. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: metrics: enabled: true ssl: enabled: true keyStore: metrics-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: metrics-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: metrics-secret The enabled field when set to true enables SSL for metrics or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. ",
            "title": "Enabling Coherence Metrics"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " The Coherence Operator provides a number of Kibana dashboards and querires to allow you to view and analyze logs from your Coherence clusters. ",
            "title": "preambule"
        },
        {
            "location": "/logging/040_dashboards",
            "text": "",
            "title": "Kibana Dashboards &amp; Searches"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " Dashboards Coherence Cluster - All Messages Coherence Cluster - Errors and Warnings Coherence Cluster - Persistence Coherence Cluster - Configuration Messages Coherence Cluster - Network Coherence Cluster - Partitions Coherence Cluster - Message Sources Searches ",
            "title": "Table of Contents"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. ",
            "title": "1. Coherence Cluster - All Messages"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. ",
            "title": "2. Coherence Cluster - Errors and Warnings"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " This dashboard shows Persistence related messages including failed and successful operations. ",
            "title": "3. Coherence Cluster - Persistence"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. ",
            "title": "4. Coherence Cluster - Configuration Messages"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " This dashboard hows network related messages, such as communication delays and TCP ring disconnects. ",
            "title": "5. Coherence Cluster - Network"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " Shows partition transfer and partition loss messages. ",
            "title": "6. Coherence Cluster - Partitions"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " Shows the source (thread) for messages Users can drill-down by cluster, host and message level. ",
            "title": "7. Coherence Cluster - Message Sources"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " Information from all dashboards (and queries) can be filtered using the standard Kibana date/time filtering in the top right of the UI, as well as the Add a filter button. 1. Coherence Cluster - All Messages This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. 2. Coherence Cluster - Errors and Warnings This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. 3. Coherence Cluster - Persistence This dashboard shows Persistence related messages including failed and successful operations. 4. Coherence Cluster - Configuration Messages This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. 5. Coherence Cluster - Network This dashboard hows network related messages, such as communication delays and TCP ring disconnects. 6. Coherence Cluster - Partitions Shows partition transfer and partition loss messages. 7. Coherence Cluster - Message Sources Shows the source (thread) for messages Users can drill-down by cluster, host and message level. ",
            "title": "Dashboards"
        },
        {
            "location": "/logging/040_dashboards",
            "text": " A number of searches are automatically includes which can help assist in diagnosis and troubleshooting a Coherence cluster. They can be accessed via the Discover side-bar and selecting `Open . These are grouped into the following general categories: Cluster - Cluster join, discovery, heartbeat, member joining and stopping messages Cache - Cache restarting, exceptions and index exception messages Configuration - Configuration loading and not loading messages Persistence - Persistence success and failure messages Network - Network communications delays, disconnects, timeouts and terminations Partition - Partition loss, ownership and transfer related messages Member - Member thread dump, join and leave messages Errors - All Error messages only Federation - Federation participant, disconnection, connection, errors and other messages ",
            "title": "Searches"
        },
        {
            "location": "/clusters/130_pod_scheduling",
            "text": " In Kubernetes Pods can be configured to control how and onto which nodes Kubernetes will schedule those Pods ; the Coherence Operator allows the same control for Pods in roles in a CoherenceCluster resource. The following settings can be configured: Field Description nodeSelector nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of role spec, it specifies a map of key-value pairs. For the Pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). See Assigning Pods to Nodes in the Kubernetes documentation affinity The affinity/anti-affinity feature, greatly expands the types of constraints you can express over just using labels in a nodeSelector . See Assigning Pods to Nodes in the Kubernetes documentation tolerations nodeSelector and affinity are properties of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite  they allow a node to repel a set of Pods . Taints and tolerations work together to ensure that Pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any Pods that do not tolerate the taints. Tolerations are applied to Pods , and allow (but do not require) the Pods to schedule onto nodes with matching taints. See Taints and Tolerations in the Kubernetes documentation. &#8230;&#8203; The nodeSelector , affinity and tolerations fields are all part of the role spec and like any other role spec field can be configured at different levels depending on whether the CoherenceCluster has implicit or explicit roles. The format of the fields is that same as documented in the Kubernetes documentation Assigning Pods to Nodes and Taints and Tolerations ",
            "title": "Configure Pod Scheduling"
        },
        {
            "location": "/clusters/130_pod_scheduling",
            "text": " When configuring a CoherenceCluster with a single implicit role the scheduling fields are configured directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: tolerations: - key: \"example-key\" operator: \"Exists\" effect: \"NoSchedule\" nodeSelector: - disktype: ssd affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 The tolerations are set for the implicit storage role A nodeSelector is set for the implicit storage role affinity is set for the implicit storage role ",
            "title": "Pod Scheduling for a Single Implicit Role"
        },
        {
            "location": "/clusters/130_pod_scheduling",
            "text": " When configuring one or more explicit roles in a CoherenceCluster the scheduling fields are configured for each role in the roles list. For example <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data nodeSelector: - disktype: ssd - role: proxy tolerations: - key: \"example-key\" operator: \"Exists\" effect: \"NoSchedule\" affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 The data role has a nodeSelector configured The proxy role has tolerations and affinity configured ",
            "title": "Pod Scheduling for Explicit Roles"
        },
        {
            "location": "/clusters/130_pod_scheduling",
            "text": " When configuring one or more explicit roles in a CoherenceCluster default values for the scheduling fields may be configured directly in the spec section of the CoherenceCluster that will apply to all roles in the roles list unless specifically overridden for a role. Values specified for a role fully override the default values, so even though nodeSelector is a map the default and role values are not merged. For example <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: nodeSelector: - disktype: ssd roles: - role: data nodeSelector: - shape: massive - role: proxy - role: web The default scheduling configuration specified a node selector label of disktype=ssd The data role overrides the nodeSelector to be shape=massive The proxy and web roles do not specify any scheduling fields so they will just ue the default node selector label of disktype=ssd The tolerations and affinity fields may be used in the same way. ",
            "title": "Pod Scheduling for Explicit Roles with Defaults"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " In the Operator SDK framework a controller is responsible for managing a specific CRD. A single controller could, in theory, manage multiple CRDs but it is clearer and simpler to keep them separate. The Coherence Operator has three controllers, two are part of the operator source code and one is provided by the Operator SDK framework. All controllers have a Reconcile function that is triggered by events from Kubernetes for resources that the controller is listening to. ",
            "title": "Controllers"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The CoherenceCluster controller manages instances of the CoherenceCluster CRD. The source for this controller is in the pkg/controller/coherencecluster/coherencecluster_controller.go file. The CoherenceCluster controller listens for events related to CoherenceCluster CRDs created or modified in the namespace that the operator is running in. It also listens to events for any CoherenceRole CRD that it owns. When a CoherenceCluster resource is created or modified a CoherenceRole is created (or modified or deleted) for each role in the CoherenceCluster spec. Each time a k8s event is raised for a CoherenceCluster or CoherenceRole resource the Reconcile method on the CoherenceCluster controller is called. Create - When a CoherenceCluster is created the controller will work out how many roles are present in the spec. For each role that has a Replica count greater than zero a CoherenceRole is created in k8s. When a CoherenceRole is created it is associated to the parent CoherenceCluster so that k8s can track ownership of related resources (this is used for cascade delete - see below). Update - When a CoherenceCluster is updated the controller will work out what the roles in the updated spec should be. It then compares these roles to the currently deployed CoherenceRoles for that cluster. It then creates, updates or deletes CoherenceRoles as required. Delete - When a CoherenceCluster is deleted the controller does not currently need to do anything. This is because k8s has cascade delete functionality that allows related resources to be deleted together (a little like cascade delete in a database). When a CoherenceCluster is deleted then any related CoherenceRoles will be deleted and also any resources that have those CoherenceRoles as owners (i.e. the corresponding CoherenceInternal resources) ",
            "title": "CoherenceCluster Controller"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The CoherenceRole controller manages instances of the CoherenceRole CRD. The source for this controller is in the pkg/controller/coherencerole/coherencerole_controller.go file. The CoherenceRole controller listens for events related to CoherenceRole CRDs created or modified in the namespace that the operator is running in. It also listens to events for any StatefulSet resources that were created by the corresponding Helm install for the role. When a CoherenceRole resource is created or modified a corresponding CoherenceInternal resource is created (or modified or deleted) from the role&#8217;s spec. Creation of a CoherenceInternal resource will trigger a Helm install of the Coherence Helm chart by the Helm Controller. Each time a k8s event is raised for a CoherenceRole or for a StatefulSet resource related to the role the Reconcile method on the CoherenceRole controller is called. The StatefulSet resource is listened to as a way to keep track of the state fo the role, i.e how many replicas are actually running and ready compared to the desired state. The StatefulSet is also used to obtain references to the Pods that make up the role when performing a StatusHA check prior to scaling. Create - When a CoherenceRole is created a corresponding CoherenceInternal resource will be created in k8s. Update - When a CoherenceRole is updated one of three actions can take place. Scale Up - If the update increases the role&#8217;s replica count then the role is being scaled up. The role&#8217;s spec is first checked to determine whether anything else has changed, if it has a rolling upgrade is performed first to bring the existing members up to the desired spec. After any possible the upgrade then the role&#8217;s member count is scaled up. Scale Down - If the update decreases the role&#8217;s replica count then the role is being scaled down. The member count of the role is scaled down and then the role&#8217;s spec is checked to determine whether anything else has changed, if it has a rolling upgrade is performed to bring the remaining members up to the desired spec. Update Only - If the changes to the role&#8217;s spec do not include a change to the replica count then a rolling upgrade is performed of the existing cluster members. Rolling Upgrade - A rolling upgrade is actually performed out of the box by the StatefulSet associated to the role. To upgrade the members of a role the CoherenceRole controller only has to update the CoherenceInternal spec. This will cause the Helm controller to update the associated Helm install whivh in turn causes the StatefulSet to perform a rolling update of the associated Pods. Scaling - The CoherenceOperator supports safe scaling of the members of a role. This means that a scaling operation will not take place unless the members of the role are Status HA. Safe scaling means that the number of replicas is scaled one at a time untile the desired size is reached with a Status HA check being performed before each member is added or removed. The exact action is controlled by a customer defined scaling policy that is part of the role&#8217;s spec. There are three policy types: SafeScaling - the safe scaling policy means that regardless of whether a role is being scaled up or down the size is always scaled one at a time with a Status HA check before each member is added or removed. ParallelScaling - with parallel scaling no Status HA check is performed, a role is scaled to the desired size by adding or removing the required number of members at the same time. For a storage enabled role with this policy scaling down could result in data loss. Ths policy is intended for storage disabled roles where it allows for fatser start and scaling times. ParallelUpSafeDownScaling - this policy is the default scaling policy. It means that when scaling up the required number of members is added all at once but when scaling down members are removed one at a time with a Status HA check before each removal. This policy allows clusters to start and scale up fatser whilst protecting from data loss when scaling down. Delete - As with a CoherenceCluster, when a CoherenceRole is deleted its corresponding CoherenceInternal resource is also deleted by a cascading delete in k8s. The CoherenceRole controller does not need to take any action on deletion. ",
            "title": "CoherenceRole Controller"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The final controller in the Coherence Operator is the Helm controller. This controller is actually part of the Operator SDK and the source is not in the Coherence Operator&#8217;s source code tree. The Helm controller is configured to watch for a particular CRD and performs Helm install, delete and upgrades as resources based on that CRD are created, deleted or updated. In the case of the Coherence Operator the Helm controller is watching for instances of the CoherenceInternal CRD that are created, updated or deleted by the CoherenceRole controller. When this occurs the Helm controller uses the spec of the CoherenceInternal resource as the values file to install or upgrade the Coherence Helm chart. The Coherence Helm chart used by the operator is actually embedded in the Coherence Operator Docker image so there is no requirement for the customer to have access to a chart repository. The Helm operator also uses an embedded helm and tiller so there is no requirement for the customer to install Helm in their k8s cluster. A customer can have Helm installed but it will never be used by the operator so there is no version conflict. If a customer were to perform a helm ls operation in their cluster they would not see the installs controlled by the Coherence Operator. ",
            "title": "Helm Controller"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The high level operation of the Coherence Operator can be seen in the diagram below. The entry point to the operator is the`main()` function in the cmd/manager/main.go file. This function performs the creation and initialisation of the three controllers and the ReST server. It also creates a configuration k8s secret that is used by Coherence Pods. The Coherence Operator works in a single namespace, that is it manages CRDs and hence Coherence clusters only in the same namespace that it is installed into. Controllers In the Operator SDK framework a controller is responsible for managing a specific CRD. A single controller could, in theory, manage multiple CRDs but it is clearer and simpler to keep them separate. The Coherence Operator has three controllers, two are part of the operator source code and one is provided by the Operator SDK framework. All controllers have a Reconcile function that is triggered by events from Kubernetes for resources that the controller is listening to. CoherenceCluster Controller The CoherenceCluster controller manages instances of the CoherenceCluster CRD. The source for this controller is in the pkg/controller/coherencecluster/coherencecluster_controller.go file. The CoherenceCluster controller listens for events related to CoherenceCluster CRDs created or modified in the namespace that the operator is running in. It also listens to events for any CoherenceRole CRD that it owns. When a CoherenceCluster resource is created or modified a CoherenceRole is created (or modified or deleted) for each role in the CoherenceCluster spec. Each time a k8s event is raised for a CoherenceCluster or CoherenceRole resource the Reconcile method on the CoherenceCluster controller is called. Create - When a CoherenceCluster is created the controller will work out how many roles are present in the spec. For each role that has a Replica count greater than zero a CoherenceRole is created in k8s. When a CoherenceRole is created it is associated to the parent CoherenceCluster so that k8s can track ownership of related resources (this is used for cascade delete - see below). Update - When a CoherenceCluster is updated the controller will work out what the roles in the updated spec should be. It then compares these roles to the currently deployed CoherenceRoles for that cluster. It then creates, updates or deletes CoherenceRoles as required. Delete - When a CoherenceCluster is deleted the controller does not currently need to do anything. This is because k8s has cascade delete functionality that allows related resources to be deleted together (a little like cascade delete in a database). When a CoherenceCluster is deleted then any related CoherenceRoles will be deleted and also any resources that have those CoherenceRoles as owners (i.e. the corresponding CoherenceInternal resources) CoherenceRole Controller The CoherenceRole controller manages instances of the CoherenceRole CRD. The source for this controller is in the pkg/controller/coherencerole/coherencerole_controller.go file. The CoherenceRole controller listens for events related to CoherenceRole CRDs created or modified in the namespace that the operator is running in. It also listens to events for any StatefulSet resources that were created by the corresponding Helm install for the role. When a CoherenceRole resource is created or modified a corresponding CoherenceInternal resource is created (or modified or deleted) from the role&#8217;s spec. Creation of a CoherenceInternal resource will trigger a Helm install of the Coherence Helm chart by the Helm Controller. Each time a k8s event is raised for a CoherenceRole or for a StatefulSet resource related to the role the Reconcile method on the CoherenceRole controller is called. The StatefulSet resource is listened to as a way to keep track of the state fo the role, i.e how many replicas are actually running and ready compared to the desired state. The StatefulSet is also used to obtain references to the Pods that make up the role when performing a StatusHA check prior to scaling. Create - When a CoherenceRole is created a corresponding CoherenceInternal resource will be created in k8s. Update - When a CoherenceRole is updated one of three actions can take place. Scale Up - If the update increases the role&#8217;s replica count then the role is being scaled up. The role&#8217;s spec is first checked to determine whether anything else has changed, if it has a rolling upgrade is performed first to bring the existing members up to the desired spec. After any possible the upgrade then the role&#8217;s member count is scaled up. Scale Down - If the update decreases the role&#8217;s replica count then the role is being scaled down. The member count of the role is scaled down and then the role&#8217;s spec is checked to determine whether anything else has changed, if it has a rolling upgrade is performed to bring the remaining members up to the desired spec. Update Only - If the changes to the role&#8217;s spec do not include a change to the replica count then a rolling upgrade is performed of the existing cluster members. Rolling Upgrade - A rolling upgrade is actually performed out of the box by the StatefulSet associated to the role. To upgrade the members of a role the CoherenceRole controller only has to update the CoherenceInternal spec. This will cause the Helm controller to update the associated Helm install whivh in turn causes the StatefulSet to perform a rolling update of the associated Pods. Scaling - The CoherenceOperator supports safe scaling of the members of a role. This means that a scaling operation will not take place unless the members of the role are Status HA. Safe scaling means that the number of replicas is scaled one at a time untile the desired size is reached with a Status HA check being performed before each member is added or removed. The exact action is controlled by a customer defined scaling policy that is part of the role&#8217;s spec. There are three policy types: SafeScaling - the safe scaling policy means that regardless of whether a role is being scaled up or down the size is always scaled one at a time with a Status HA check before each member is added or removed. ParallelScaling - with parallel scaling no Status HA check is performed, a role is scaled to the desired size by adding or removing the required number of members at the same time. For a storage enabled role with this policy scaling down could result in data loss. Ths policy is intended for storage disabled roles where it allows for fatser start and scaling times. ParallelUpSafeDownScaling - this policy is the default scaling policy. It means that when scaling up the required number of members is added all at once but when scaling down members are removed one at a time with a Status HA check before each removal. This policy allows clusters to start and scale up fatser whilst protecting from data loss when scaling down. Delete - As with a CoherenceCluster, when a CoherenceRole is deleted its corresponding CoherenceInternal resource is also deleted by a cascading delete in k8s. The CoherenceRole controller does not need to take any action on deletion. Helm Controller The final controller in the Coherence Operator is the Helm controller. This controller is actually part of the Operator SDK and the source is not in the Coherence Operator&#8217;s source code tree. The Helm controller is configured to watch for a particular CRD and performs Helm install, delete and upgrades as resources based on that CRD are created, deleted or updated. In the case of the Coherence Operator the Helm controller is watching for instances of the CoherenceInternal CRD that are created, updated or deleted by the CoherenceRole controller. When this occurs the Helm controller uses the spec of the CoherenceInternal resource as the values file to install or upgrade the Coherence Helm chart. The Coherence Helm chart used by the operator is actually embedded in the Coherence Operator Docker image so there is no requirement for the customer to have access to a chart repository. The Helm operator also uses an embedded helm and tiller so there is no requirement for the customer to install Helm in their k8s cluster. A customer can have Helm installed but it will never be used by the operator so there is no version conflict. If a customer were to perform a helm ls operation in their cluster they would not see the installs controlled by the Coherence Operator. ",
            "title": "How The Operator Works"
        },
        {
            "location": "/developer/05_building",
            "text": " The Operator SDK generates Go projects that use Go Modules and hence the Coherence Operator uses Go Modules too. The Coherence Operator can be checked out from Git to any location, it does not have to be under your $GOPATH . The first time that the project is built may require Go to fetch a number of dependencies and may take longer than usual to complete. The easiest way to build the whole project is using make . To build the Coherence Operator, package the Helm charts and create the various Docker images run the following command: <markup lang=\"bash\" >make all The all make target will build the Go and Java parts of the Operator and create all of the images required. There have been issues with Go not being able to resolve all of the module dependencies required to build the Coherence Operator. This can be resolved by setting the GOPROXY environment variable GOPROXY=https://proxy.golang.org ",
            "title": "How to Build the Coherence Operator"
        },
        {
            "location": "/developer/05_building",
            "text": " The Coherence Operator contains tests that can be executed using make . The tests are plain Go tests and also Ginkgo test suites. To execute the unit and functional tests that do not require a k8s cluster you can execute the following command: <markup lang=\"bash\" >make test-all This will build and execute all of the Go and Java tests, you do not need to have run a make build first. ",
            "title": "Unit Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " To only tun the Go tests use: <markup lang=\"bash\" >make test-operator ",
            "title": "Go Unit Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " To only tun the Java tests use: <markup lang=\"bash\" >make test-mvn ",
            "title": "Java Unit Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " End to end tests require the Operator to be running. There are three types of end-to-end tests, Helm tests, local tests and remote tests. Helm tests are tests that install the Coherence Operator Helm chart and then make assertions about the state fo the resulting install. These tests do not test functionality of the Operator itself. The Helm tests suite is run using make: make helm-test Local tests, which is the majority ot the tests, can be executed with a locally running operator (i.e. the operator does not need to be deployed in a container in k8s). This makes the tests faster to run and also makes it possible to run the operator in a debugger while the test is executing The local end-to-end test suite is run using make: make e2e-local-test It is possible to run a sub-set of the tests or an individual test by using the GO_TEST_FLAGS=&lt;regex&gt; parameter. For example, to just run the TestMinimalCoherenceCluster clustering test in the test/e2e/local/clustering_test.go file: <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' The reg-ex above matches exactly the TestMinimalCoherenceCluster test name because it uses the reg-ex start ^ and end $ characters. For example, to run all of the clustering tests where the test name starts with TestOneRole we can use the reg-ex ^TestOneRole.*' <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestOneRole.*' Note Any $ signs in the reg-ex need to be escaped by using a double dollar sign $$ . The GO_TEST_FLAGS parameter can actually consist of any valid argument to be passed to the go test command. There is plenty of documentation on go test Remote tests require the operator to actually be installed in a container in k8s. An example of this is the scaling tests because the operator needs to be able to directly reach the Pods. Very few end-to-end tests fall into this categrory. The local end-to-end test suite is run using make: make e2e-test As with local tests the GO_TEST_FLAGS parameter can be used to execute a sub-set of tests or a single test. ",
            "title": "End-to-End Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " Unit Tests The Coherence Operator contains tests that can be executed using make . The tests are plain Go tests and also Ginkgo test suites. To execute the unit and functional tests that do not require a k8s cluster you can execute the following command: <markup lang=\"bash\" >make test-all This will build and execute all of the Go and Java tests, you do not need to have run a make build first. Go Unit Tests To only tun the Go tests use: <markup lang=\"bash\" >make test-operator Java Unit Tests To only tun the Java tests use: <markup lang=\"bash\" >make test-mvn End-to-End Tests End to end tests require the Operator to be running. There are three types of end-to-end tests, Helm tests, local tests and remote tests. Helm tests are tests that install the Coherence Operator Helm chart and then make assertions about the state fo the resulting install. These tests do not test functionality of the Operator itself. The Helm tests suite is run using make: make helm-test Local tests, which is the majority ot the tests, can be executed with a locally running operator (i.e. the operator does not need to be deployed in a container in k8s). This makes the tests faster to run and also makes it possible to run the operator in a debugger while the test is executing The local end-to-end test suite is run using make: make e2e-local-test It is possible to run a sub-set of the tests or an individual test by using the GO_TEST_FLAGS=&lt;regex&gt; parameter. For example, to just run the TestMinimalCoherenceCluster clustering test in the test/e2e/local/clustering_test.go file: <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' The reg-ex above matches exactly the TestMinimalCoherenceCluster test name because it uses the reg-ex start ^ and end $ characters. For example, to run all of the clustering tests where the test name starts with TestOneRole we can use the reg-ex ^TestOneRole.*' <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestOneRole.*' Note Any $ signs in the reg-ex need to be escaped by using a double dollar sign $$ . The GO_TEST_FLAGS parameter can actually consist of any valid argument to be passed to the go test command. There is plenty of documentation on go test Remote tests require the operator to actually be installed in a container in k8s. An example of this is the scaling tests because the operator needs to be able to directly reach the Pods. Very few end-to-end tests fall into this categrory. The local end-to-end test suite is run using make: make e2e-test As with local tests the GO_TEST_FLAGS parameter can be used to execute a sub-set of tests or a single test. ",
            "title": "Testing"
        },
        {
            "location": "/developer/05_building",
            "text": " By default the version number used to tag the Docker images and Helm charts is set in the VERSION property in the Makefile and in the pom.xml files in the java/ directory. The Makefile also contains a VERSION_SUFFIX variable that is used to add a suffix to the build. By default this suffix is ci so the default version of the build artifacts is 2.0.0-ci . Change this suffix, for example when building a release candidate or a full release. For example, if building a release called alpha2 the following command can be used: <markup lang=\"bash\" >make build-all-images VERSION_SUFFIX=alpha2 If building a full release without a suffix the following command can be used <markup lang=\"bash\" >make build-all-images VERSION_SUFFIX=\"\" Testing Unit Tests The Coherence Operator contains tests that can be executed using make . The tests are plain Go tests and also Ginkgo test suites. To execute the unit and functional tests that do not require a k8s cluster you can execute the following command: <markup lang=\"bash\" >make test-all This will build and execute all of the Go and Java tests, you do not need to have run a make build first. Go Unit Tests To only tun the Go tests use: <markup lang=\"bash\" >make test-operator Java Unit Tests To only tun the Java tests use: <markup lang=\"bash\" >make test-mvn End-to-End Tests End to end tests require the Operator to be running. There are three types of end-to-end tests, Helm tests, local tests and remote tests. Helm tests are tests that install the Coherence Operator Helm chart and then make assertions about the state fo the resulting install. These tests do not test functionality of the Operator itself. The Helm tests suite is run using make: make helm-test Local tests, which is the majority ot the tests, can be executed with a locally running operator (i.e. the operator does not need to be deployed in a container in k8s). This makes the tests faster to run and also makes it possible to run the operator in a debugger while the test is executing The local end-to-end test suite is run using make: make e2e-local-test It is possible to run a sub-set of the tests or an individual test by using the GO_TEST_FLAGS=&lt;regex&gt; parameter. For example, to just run the TestMinimalCoherenceCluster clustering test in the test/e2e/local/clustering_test.go file: <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' The reg-ex above matches exactly the TestMinimalCoherenceCluster test name because it uses the reg-ex start ^ and end $ characters. For example, to run all of the clustering tests where the test name starts with TestOneRole we can use the reg-ex ^TestOneRole.*' <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestOneRole.*' Note Any $ signs in the reg-ex need to be escaped by using a double dollar sign $$ . The GO_TEST_FLAGS parameter can actually consist of any valid argument to be passed to the go test command. There is plenty of documentation on go test Remote tests require the operator to actually be installed in a container in k8s. An example of this is the scaling tests because the operator needs to be able to directly reach the Pods. Very few end-to-end tests fall into this categrory. The local end-to-end test suite is run using make: make e2e-test As with local tests the GO_TEST_FLAGS parameter can be used to execute a sub-set of tests or a single test. ",
            "title": "Build Versions"
        },
        {
            "location": "/logging/010_overview",
            "text": " Enabling Log Capture Capturing and viewing Coherence cluster Logs. Using Your Own Elasticsearch Pushing logs to your own Elasticsearch instance. Kibana Dashboards Details on the Kibana Dashboards available.. ",
            "title": "Logging"
        },
        {
            "location": "/about/05_kubernetes",
            "text": " For development and testing of the Coherence Operator it&#8217;s often convenient to run Kubernetes on your desktop. Some ways to do this are: Kubernetes support in Docker for Desktop Kind Kubernetes Minikube ",
            "title": "preambule"
        },
        {
            "location": "/about/05_kubernetes",
            "text": " Install Docker for Mac or Docker for Windows . Starting with version 18.06 Docker for Desktop includes Kubernetes support. ",
            "title": "Install"
        },
        {
            "location": "/about/05_kubernetes",
            "text": " Enable Kubernetes Support for Mac or Kubernetes Support for Windows . Once Kubernetes installation is complete, make sure you have your context set correctly to use docker-for-desktop. <markup lang=\"bash\" title=\"Make sure K8s context is set to docker-for-desktop\" >kubectl config get-contexts kubectl config use-context docker-for-desktop kubectl cluster-info kubectl version --short kubectl get nodes ",
            "title": "Enable Kubernetes Support"
        },
        {
            "location": "/about/05_kubernetes",
            "text": " Install Install Docker for Mac or Docker for Windows . Starting with version 18.06 Docker for Desktop includes Kubernetes support. Enable Kubernetes Support Enable Kubernetes Support for Mac or Kubernetes Support for Windows . Once Kubernetes installation is complete, make sure you have your context set correctly to use docker-for-desktop. <markup lang=\"bash\" title=\"Make sure K8s context is set to docker-for-desktop\" >kubectl config get-contexts kubectl config use-context docker-for-desktop kubectl cluster-info kubectl version --short kubectl get nodes ",
            "title": "Docker for Desktop."
        },
        {
            "location": "/about/05_kubernetes",
            "text": " If trying to use Kind to run the Coherence Operator using locally built images these images need to be added to the Kubernetes cluster using the Kind CLI because the local images will obviously not be in a repository that the nodes can pull from. Although a Kind cluster is running in Docker it does not appear to have access to any local Docker images so all images either need to be pull-able or loaded via the Kind CLI. For example if the Operator has been built with make all there will be the following local images <markup lang=\"bash\" >docker images --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/operator-test-image 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci-utils These images can be added to the Kind cluster with the commands: <markup lang=\"bash\" >kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci-utils kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/operator-test-image:2.0.0-ci ",
            "title": "A Word About Kind and Docker Images"
        },
        {
            "location": "/about/05_kubernetes",
            "text": " Install Kind as described in the Kind Quick Start To create a Kubernetes three node cluster in Kind you need a configuration file <markup lang=\"yaml\" title=\"kind-config.yaml\" >kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 nodes: - role: control-plane - role: worker - role: worker - role: worker Then create a Kind Kubernetes cluster with the following command <markup lang=\"bash\" >kind create cluster --config kind-config.yaml After a short while (depending on how long images take to download) there should be a Kubernetes cluster with a master and three worker nodes running in Docker containers. <markup lang=\"bash\" >docker ps d790d6b779ff kindest/node:v1.15.3 \"/usr/local/bin/entr\" 23 hours ago Up 23 hours kind-worker2 a096c8bf0c1a kindest/node:v1.15.3 \"/usr/local/bin/entr\" 23 hours ago Up 23 hours kind-worker3 4c01d94c29b7 kindest/node:v1.15.3 \"/usr/local/bin/entr\" 23 hours ago Up 23 hours 56603/tcp, 127.0.0.1:56603-&gt;6443/tcp kind-control-plane 8f62284be151 kindest/node:v1.15.3 \"/usr/local/bin/entr\" 23 hours ago Up 23 hours kind-worker As described in the Kind documentation now export KUBECONFIG for the Kind cluster <markup lang=\"bash\" >export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\" To be able to use this cluster with the Operator Helm chart Helm will need to be initialised. <markup lang=\"bash\" >helm init The Kind cluster has RBAC enabled so Helm&#8217;s Tiller will now need to be patched with a role: <markup lang=\"bash\" >kubectl create serviceaccount \\ --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule \\ --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system \\ tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' A Word About Kind and Docker Images If trying to use Kind to run the Coherence Operator using locally built images these images need to be added to the Kubernetes cluster using the Kind CLI because the local images will obviously not be in a repository that the nodes can pull from. Although a Kind cluster is running in Docker it does not appear to have access to any local Docker images so all images either need to be pull-able or loaded via the Kind CLI. For example if the Operator has been built with make all there will be the following local images <markup lang=\"bash\" >docker images --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/operator-test-image 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci-utils These images can be added to the Kind cluster with the commands: <markup lang=\"bash\" >kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci-utils kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/operator-test-image:2.0.0-ci ",
            "title": "Kind"
        },
        {
            "location": "/app-deployment/030_roles",
            "text": " TBC ",
            "title": "preambule"
        },
        {
            "location": "/app-deployment/030_roles",
            "text": "",
            "title": "Defining and using application roles."
        },
        {
            "location": "/metrics/030_ssl",
            "text": " Coherence clusters can be deployed with a metrics endpoint enabled that can be scraped by common metrics applications such as Prometheus. ",
            "title": "preambule"
        },
        {
            "location": "/metrics/030_ssl",
            "text": " Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. Please see Coherence Metrics Documentation for information on how to enable SSL. ",
            "title": "Enabling SSL for metrics capture"
        },
        {
            "location": "/logging/030_own",
            "text": " The Coherence Operator can be configured to instruct Fluentd to push logs to a separate Elasticsearch instance rather thatn the in-built one. ",
            "title": "preambule"
        },
        {
            "location": "/logging/030_own",
            "text": " To enable an different Elasticsearch endpoint, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set elasticsearchEndpoint.host=your-es-host --set elasticsearchEndpoint.port=your-es-host You can also set the user and password if you Elasticsearch instance requires it: <markup lang=\"bash\" >--set elasticsearchEndpoint.user=user --set elasticsearchEndpoint.password=password For this example we have used the Stable ELK Stack at https://github.com/helm/charts/tree/master/stable/elastic-stack to install the required components and have Elasticsearch runing on coherence-example-elastic-stack.default.svc.cluster.local:9200 A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set elasticsearchEndpoint.host=coherence-example-elastic-stack.default.svc.cluster.local \\ --set elasticsearchEndpoint.port=9200 \\ coherence/coherence-operator After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-66c6d868b9-rd429 1/1 Running 0 8m coherence-operator-grafana-8454698bcf-v5kxw 2/2 Running 0 8m coherence-operator-kube-state-metrics-6dc8675d87-qnfdw 1/1 Running 0 8m coherence-operator-prometh-operator-58d94ffbb8-94d4m 1/1 Running 0 8m coherence-operator-prometheus-node-exporter-vpjjt 1/1 Running 0 8m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 0 8m You will notice that there are no Kibana and Elasticsearch pods. ",
            "title": "1. Install the Coherence Operator with custom Elasticsearch endpoint"
        },
        {
            "location": "/logging/030_own",
            "text": " From this point on there is no difference in installation from when EFK is installed by the Coherence Operator. This is because when Coherence is installed it will querying the Coherence Operator to receive the new Elasticsearch endpoint. . Deploy a simple logging enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"logging-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: logging-cluster spec: role: storage replicas: 3 logging: fluentd: enabled: true Enables log capture via Fluentd The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f logging-cluster.yaml coherencecluster.coherence.oracle.com/logging-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=logging-cluster NAME READY STATUS RESTARTS AGE logging-cluster-storage-0 2/2 Running 0 86s logging-cluster-storage-1 2/2 Running 0 86s logging-cluster-storage-2 2/2 Running 0 86s ",
            "title": "2. Install a Coherence Cluster with Logging Enabled"
        },
        {
            "location": "/logging/030_own",
            "text": " Issue the following to view the logs for the Fluentd container on the first Pod: <markup lang=\"bash\" >kubectl logs -n &lt;namespace&gt; logging-cluster-storage-0 -c fluentd In the output you will see something similar to the following indicating your Fluentd container will send data to your own Elasticsearch. <markup lang=\"bash\" > &lt;match coherence-cluster&gt; @type elasticsearch host \"coherence-example-elastic-stack.default.svc.cluster.local\" port 9020 user \"\" password xxxxxx logstash_format true logstash_prefix \"coherence-cluster\" &lt;/match&gt; ",
            "title": "3. Inspect the Fluentd container logs"
        },
        {
            "location": "/logging/030_own",
            "text": " Connect to your Kibana UI and create an index pattern called coherence-cluster-* to view the incoming logs. ",
            "title": "4. Connect to your Kibana UI"
        },
        {
            "location": "/logging/030_own",
            "text": " After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f logging-cluster.yaml ",
            "title": "5. Clean Up"
        },
        {
            "location": "/logging/030_own",
            "text": " This example shows how to instruct Fluentd to push data to your own Elasticsearch instance. 1. Install the Coherence Operator with custom Elasticsearch endpoint To enable an different Elasticsearch endpoint, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set elasticsearchEndpoint.host=your-es-host --set elasticsearchEndpoint.port=your-es-host You can also set the user and password if you Elasticsearch instance requires it: <markup lang=\"bash\" >--set elasticsearchEndpoint.user=user --set elasticsearchEndpoint.password=password For this example we have used the Stable ELK Stack at https://github.com/helm/charts/tree/master/stable/elastic-stack to install the required components and have Elasticsearch runing on coherence-example-elastic-stack.default.svc.cluster.local:9200 A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set elasticsearchEndpoint.host=coherence-example-elastic-stack.default.svc.cluster.local \\ --set elasticsearchEndpoint.port=9200 \\ coherence/coherence-operator After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-66c6d868b9-rd429 1/1 Running 0 8m coherence-operator-grafana-8454698bcf-v5kxw 2/2 Running 0 8m coherence-operator-kube-state-metrics-6dc8675d87-qnfdw 1/1 Running 0 8m coherence-operator-prometh-operator-58d94ffbb8-94d4m 1/1 Running 0 8m coherence-operator-prometheus-node-exporter-vpjjt 1/1 Running 0 8m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 0 8m You will notice that there are no Kibana and Elasticsearch pods. 2. Install a Coherence Cluster with Logging Enabled From this point on there is no difference in installation from when EFK is installed by the Coherence Operator. This is because when Coherence is installed it will querying the Coherence Operator to receive the new Elasticsearch endpoint. . Deploy a simple logging enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"logging-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: logging-cluster spec: role: storage replicas: 3 logging: fluentd: enabled: true Enables log capture via Fluentd The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f logging-cluster.yaml coherencecluster.coherence.oracle.com/logging-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=logging-cluster NAME READY STATUS RESTARTS AGE logging-cluster-storage-0 2/2 Running 0 86s logging-cluster-storage-1 2/2 Running 0 86s logging-cluster-storage-2 2/2 Running 0 86s 3. Inspect the Fluentd container logs Issue the following to view the logs for the Fluentd container on the first Pod: <markup lang=\"bash\" >kubectl logs -n &lt;namespace&gt; logging-cluster-storage-0 -c fluentd In the output you will see something similar to the following indicating your Fluentd container will send data to your own Elasticsearch. <markup lang=\"bash\" > &lt;match coherence-cluster&gt; @type elasticsearch host \"coherence-example-elastic-stack.default.svc.cluster.local\" port 9020 user \"\" password xxxxxx logstash_format true logstash_prefix \"coherence-cluster\" &lt;/match&gt; 4. Connect to your Kibana UI Connect to your Kibana UI and create an index pattern called coherence-cluster-* to view the incoming logs. 5. Clean Up After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f logging-cluster.yaml ",
            "title": "Pushing logs to your own Elasticsearch instance"
        },
        {
            "location": "/clusters/050_coherence",
            "text": " The coherence section of the role spec in a CoherenceCluster contains the following fields and sections that may be configured: <markup lang=\"yaml\" >coherence: cacheConfig: coherence-cache-config.xml overrideConfig: tangosol-coherence-override.xml logLevel: 5 storageEnabled: true imageSpec: {} management: {} metrics: {} persistence: {} snapshot: {} The cacheConfig field sets the name of the Coherence cache configuration file to use. See Coherence Config Files for more details. The overrideConfig field sets the name of the Coherence operational override configuration file to use. See Coherence Config Files for more details. The logLevel field sets the log level that Coherence should use. See Logging Configuration for more details. The storageEnabled field sets whether the role is storage enabled or not. See Storage Enabled or Disabled Roles for more details. The imageSpec section configures the Coherence image details such as image name, pull policy etc. See Setting the Coherence Image for more details. The management configures how Coherence management over ReST behaves, whether it is enabled, etc. See Coherence Management Over ReST for more details. The metrics configures how Coherence metrics behaves, whether it is enabled, etc. See Coherence Metrics for more details. The persistence configures how Coherence management over ReST behaves, whether it is enabled, etc. See Coherence Persistence for more details. The snapshot configures how Coherence management over ReST behaves, whether it is enabled, etc. See Coherence Snapshots for more details. ",
            "title": "Configure Coherence"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " There are a number of fields in the CoherenceCluster CRD that can be used to configure the JVM. These fields are all in the jvm section when configuring a role in the CRD. JVM Arguments Garbage Collector Configuration Configuring the Garbage Collector to Use Configuring the Garbage Collector Arguments Configuring Garbage Collector Logging Memory Configuration Heap Size Metaspace size Stack size Native Memory Size Native Memory Tracking Behaviour on Out Of Memory Error Container Resource Limits Flight Recorder Diagnostic Volume JVM Debug Arguments The following sections describe the different JVM configuration options available in the CoherenceCluster CRD. These CRD fields all result in the addition or omission of various JVM arguments. A number of arguments are always passed to the Coherence container&#8217;s JVM: <markup >-XX:HeapDumpPath=/jvm/${POD_NAME}/${POD_UID}/heap-dumps/${POD_NAME}-${POD_UID}.hprof -XX:ErrorFile=/jvm/${POD_NAME}/${POD_UID}/hs-err-${POD_NAME}-${POD_UID}.log -Dcoherence.ttl=0 -XshowSettings:all -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions -XX:+UnlockCommercialFeatures -XX:+UnlockExperimentalVMOptions Any heap dumps created by the JVM when an out of memory error occurs will be written to a file called /jvm/${POD_NAME}/${POD_UID}/heap-dumps/${POD_NAME}-${POD_UID}.hprof Any error files created by a JVM crash will be written to a file called /jvm/${POD_NAME}/${POD_UID}/hs-err-${POD_NAME}-${POD_UID}.log Coherence multicast discovery is disabled as multicast cannot be relied on in containers The /jvm root directory used for heap dumps and error files can be mounted to an external volume to allow easier access to these files. ",
            "title": "Configure the JVM"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role the args is set in the spec.jvm section of the configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: args: - \"-XX:G1HeapRegionSize=16m\" - \"-Dfoo=bar\" The -XX:G1HeapRegionSize=16m JVM option and the -Dfoo=bar system property will be passed as arguments to the JVM for the implicit storage role. ",
            "title": "Setting the JVM Arguments for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles the args are set in the jvm section of the configuration for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: args: - \"-XX:G1HeapRegionSize=16m\" - \"-Dcoherence.pof.config=storage-pof-config.xml\" - role: proxy jvm: args: - \"-XX:MaxGCPauseMillis=500\" - \"-Dcoherence.pof.config=proxy-pof-config.xml\" The -XX:G1HeapRegionSize=16m -Dcoherence.pof.config=storage-pof-config.xml arguments will be passed to the JVM for the explicit data role. The -XX:MaxGCPauseMillis=500 coherence.pof.config=proxy-pof-config.xml argument will be passed to the JVM for the explicit proxy role. ",
            "title": "Setting the JVM Arguments for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default args value can be set in the CoherenceCluster spec section that will apply to all of the roles in the roles list. Any args set explicitly in the jvm.args field for a role will be merged with those in the defaults section. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1HeapRegionSize=16m\" roles: - role: data jvm: args: - \"-XX:+AggressiveHeap\" - role: proxy The default JVM args of -XX:MaxGCPauseMillis=500 and -XX:G1HeapRegionSize=16m will be passed to the JVM for all roles. The data role adds an additional argument -XX:+AggressiveHeap so the JVM will be passed three arguments: -XX:MaxGCPauseMillis=500 -XX:G1HeapRegionSize=16m -XX:+AggressiveHeap The proxy role does not specify any additional args so will just use the two default JVM arguments -XX:MaxGCPauseMillis=500 -XX:G1HeapRegionSize=16m ",
            "title": "Setting the JVM Arguments for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " The jvm.args field is a string array of arbitrary JVM options. Any valid JVM option or system property argument may be passed to the JVM in the Coherence container by setting the value in this field. Setting the JVM Arguments for the Implicit Role When creating a CoherenceCluster with a single implicit role the args is set in the spec.jvm section of the configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: args: - \"-XX:G1HeapRegionSize=16m\" - \"-Dfoo=bar\" The -XX:G1HeapRegionSize=16m JVM option and the -Dfoo=bar system property will be passed as arguments to the JVM for the implicit storage role. Setting the JVM Arguments for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the args are set in the jvm section of the configuration for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: args: - \"-XX:G1HeapRegionSize=16m\" - \"-Dcoherence.pof.config=storage-pof-config.xml\" - role: proxy jvm: args: - \"-XX:MaxGCPauseMillis=500\" - \"-Dcoherence.pof.config=proxy-pof-config.xml\" The -XX:G1HeapRegionSize=16m -Dcoherence.pof.config=storage-pof-config.xml arguments will be passed to the JVM for the explicit data role. The -XX:MaxGCPauseMillis=500 coherence.pof.config=proxy-pof-config.xml argument will be passed to the JVM for the explicit proxy role. Setting the JVM Arguments for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default args value can be set in the CoherenceCluster spec section that will apply to all of the roles in the roles list. Any args set explicitly in the jvm.args field for a role will be merged with those in the defaults section. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1HeapRegionSize=16m\" roles: - role: data jvm: args: - \"-XX:+AggressiveHeap\" - role: proxy The default JVM args of -XX:MaxGCPauseMillis=500 and -XX:G1HeapRegionSize=16m will be passed to the JVM for all roles. The data role adds an additional argument -XX:+AggressiveHeap so the JVM will be passed three arguments: -XX:MaxGCPauseMillis=500 -XX:G1HeapRegionSize=16m -XX:+AggressiveHeap The proxy role does not specify any additional args so will just use the two default JVM arguments -XX:MaxGCPauseMillis=500 -XX:G1HeapRegionSize=16m ",
            "title": "JVM Arguments"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role the garbage collector to use is set in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: collector: CMS The implicit storage role will use the CMS garbage collector. ",
            "title": "Setting the Garbage Collector for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles the garbage collector to use is set in the jvm.gc.collector section for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: collector: G1 - role: proxy jvm: gc: collector: CMS The JVMs for the data role will use the G1 garbage collector The JVMs for the proxy role will use the CMS garbage collector ",
            "title": "Setting the Garbage Collector for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default garbage collector can be set in the spec.jvm.gc.collector field of the CRD. This value can then be overridden for specific roles in the jvm.gc.collector field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: collector: CMS roles: - role: data jvm: gc: collector: G1 - role: proxy The default garbage collector us set to CMS which will be used by all roles in the roles list that do not set a specific collector to use. The data role overrides the default collector so that the JVMs for the data role will use the G1 garbage collector The proxy role does not specify a collector to use so that JVMs for the proxy role will use the CMS garbage collector ",
            "title": "Setting the Garbage Collector for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " The CoherenceCluster CRD supports setting the garbage collectors to use automatically. The supported collectors are G1 , CMS , Parallel or the JVM default. The garbage collector to use is set using the jvm.gc.collector field. The value sould be one of: Value Description G1 Enables the G1 garbage collector by adding the -XX:+UseG1GC JVM option CMS Enables the CMS garbage collector by adding the -XX:+UseConcMarkSweepGC JVM option Parallel Enables the parallel garbage collector by adding the -XX:+UseParallelGC JVM option Default Deos not add any extra GC parameter; the JVM will use its default garbage collector &#8230;&#8203; The jvm.gc.collector value is not case sensitive so for example CMS , cms and CmS will all enable the CMS collector. The contents of the jvm.gc.collector are not validated, any value other than those described above will be treated as Default enabling the JVMs default garbage collector. The default value for jvm.gc.collector is G1 which will enable the recommended G1 garbage collector. Setting the Garbage Collector for the Implicit Role When creating a CoherenceCluster with a single implicit role the garbage collector to use is set in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: collector: CMS The implicit storage role will use the CMS garbage collector. Setting the Garbage Collector for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the garbage collector to use is set in the jvm.gc.collector section for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: collector: G1 - role: proxy jvm: gc: collector: CMS The JVMs for the data role will use the G1 garbage collector The JVMs for the proxy role will use the CMS garbage collector Setting the Garbage Collector for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default garbage collector can be set in the spec.jvm.gc.collector field of the CRD. This value can then be overridden for specific roles in the jvm.gc.collector field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: collector: CMS roles: - role: data jvm: gc: collector: G1 - role: proxy The default garbage collector us set to CMS which will be used by all roles in the roles list that do not set a specific collector to use. The data role overrides the default collector so that the JVMs for the data role will use the G1 garbage collector The proxy role does not specify a collector to use so that JVMs for the proxy role will use the CMS garbage collector ",
            "title": "Configuring the Garbage Collector to Use"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role the GC arguments are set in the spec.jvm.gc.args field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" The implicit storage role will have the additional GC arguments -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 passed to the JVM. ",
            "title": "Setting Garbage Collector Arguments for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles the GC arguments are set in the jvm.gc.args field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" - role: proxy jvm: gc: args: - \"-XX:MaxGCPauseMillis=1000\" The explicit data role will have the additional GC arguments -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 passed to the JVM. The explicit proxy role will have the additional GC argument -XX:MaxGCPauseMillis=1000 passed to the JVM. ",
            "title": "Setting Garbage Collector Arguments for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default GC arguments are set in the spec.jvm.gc.args field and will be applied to all roles in the roles list that do not set specific GC arguments. GC arguments set for explicit roles override the defaults. The role&#8217;s GC arguments are not merged with the default GC arguments. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" roles: - role: data - role: proxy jvm: gc: args: - \"-XX:MaxGCPauseMillis=1000\" The default GC arguments are -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 The data role does not specify any GC arguments so the default arguments of -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 will be passed to the data role JVMs. The proxy role specifies the GC arguments -XX:MaxGCPauseMillis=1000 which will override the defaults so only -XX:MaxGCPauseMillis=1000 will be passed to the proxy role JVMs. ",
            "title": "Setting Garbage Collector Arguments for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " Arbitrary GC arguments can be passed to the JVM in the jvm.gc.args field. This field is a string array where each argument to be passed to the JVM is a separate string value. Setting Garbage Collector Arguments for the Implicit Role When creating a CoherenceCluster with a single implicit role the GC arguments are set in the spec.jvm.gc.args field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" The implicit storage role will have the additional GC arguments -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 passed to the JVM. Setting Garbage Collector Arguments for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the GC arguments are set in the jvm.gc.args field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" - role: proxy jvm: gc: args: - \"-XX:MaxGCPauseMillis=1000\" The explicit data role will have the additional GC arguments -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 passed to the JVM. The explicit proxy role will have the additional GC argument -XX:MaxGCPauseMillis=1000 passed to the JVM. Setting Garbage Collector Arguments for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default GC arguments are set in the spec.jvm.gc.args field and will be applied to all roles in the roles list that do not set specific GC arguments. GC arguments set for explicit roles override the defaults. The role&#8217;s GC arguments are not merged with the default GC arguments. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" roles: - role: data - role: proxy jvm: gc: args: - \"-XX:MaxGCPauseMillis=1000\" The default GC arguments are -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 The data role does not specify any GC arguments so the default arguments of -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 will be passed to the data role JVMs. The proxy role specifies the GC arguments -XX:MaxGCPauseMillis=1000 which will override the defaults so only -XX:MaxGCPauseMillis=1000 will be passed to the proxy role JVMs. ",
            "title": "Configuring Garbage Collector Arguments"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role GC logging can be enabled or disabled in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: logging: true The implicit storage role has GC logging explicitly enabled so that the JVM arguments listed above will be added to the JVM&#8217;s command line. ",
            "title": "Configuring Garbage Collector Logging for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles GC logging can be enabled or disabled in the jvm.gc.logging field of each role in the roles list. section of the yaml For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: logging: true - role: proxy jvm: gc: logging: false The data role has GC logging explicitly enabled so that the JVM arguments listed above will be added to the JVM&#8217;s command line The proxy role has GC logging explicitly disabled so that the JVM arguments listed above will not be added to the JVM&#8217;s command line ",
            "title": "Configuring Garbage Collector Logging for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default GC logging setting can be specified in the spec section of the CRD which can then be overridden for individual roles in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: logging: false roles: - role: data jvm: gc: logging: true - role: proxy The default value for jvm.gc.logging is false, which will disable GC logging. The data role overrides the default and sets GC logging to true The proxy role does not specify a value for jvm.gc.logging so it will use the default, which will disable GC logging. ",
            "title": "Configuring Garbage Collector Logging for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " The Coherence documentation recommends enabling GC logging for Coherence JVMs. To this end the CoherenceCluster CRD has a boolean field jvm.gc.logging to enable or disable default GC logging JVM arguments. By default the value of this field is set to true if it is not specified for a CoherenceCluster . The following GC logging JVM arguments are added if the jvm.gc.logging field is omitted or explicitly set to true : <markup >-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime Configuring Garbage Collector Logging for the Implicit Role When creating a CoherenceCluster with a single implicit role GC logging can be enabled or disabled in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: logging: true The implicit storage role has GC logging explicitly enabled so that the JVM arguments listed above will be added to the JVM&#8217;s command line. Configuring Garbage Collector Logging for Explicit Roles When creating a CoherenceCluster with one or more explicit roles GC logging can be enabled or disabled in the jvm.gc.logging field of each role in the roles list. section of the yaml For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: logging: true - role: proxy jvm: gc: logging: false The data role has GC logging explicitly enabled so that the JVM arguments listed above will be added to the JVM&#8217;s command line The proxy role has GC logging explicitly disabled so that the JVM arguments listed above will not be added to the JVM&#8217;s command line Configuring Garbage Collector Logging for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default GC logging setting can be specified in the spec section of the CRD which can then be overridden for individual roles in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: logging: false roles: - role: data jvm: gc: logging: true - role: proxy The default value for jvm.gc.logging is false, which will disable GC logging. The data role overrides the default and sets GC logging to true The proxy role does not specify a value for jvm.gc.logging so it will use the default, which will disable GC logging. ",
            "title": "Configuring Garbage Collector Logging"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " The CoherenceCluster CRD allows garbage collector settings to be applied to the Coherence JVMs. Whilst any GC parameters could actually be applied using the jvm.args field these GC specific fields allow options to be set without having to look up and remember specific GC options. The garbage collector configuration is set in the jvm.gc section of the CRD. Configuring the Garbage Collector to Use Configuring the Garbage Collector Arguments Configuring Garbage Collector Logging Configuring the Garbage Collector to Use The CoherenceCluster CRD supports setting the garbage collectors to use automatically. The supported collectors are G1 , CMS , Parallel or the JVM default. The garbage collector to use is set using the jvm.gc.collector field. The value sould be one of: Value Description G1 Enables the G1 garbage collector by adding the -XX:+UseG1GC JVM option CMS Enables the CMS garbage collector by adding the -XX:+UseConcMarkSweepGC JVM option Parallel Enables the parallel garbage collector by adding the -XX:+UseParallelGC JVM option Default Deos not add any extra GC parameter; the JVM will use its default garbage collector &#8230;&#8203; The jvm.gc.collector value is not case sensitive so for example CMS , cms and CmS will all enable the CMS collector. The contents of the jvm.gc.collector are not validated, any value other than those described above will be treated as Default enabling the JVMs default garbage collector. The default value for jvm.gc.collector is G1 which will enable the recommended G1 garbage collector. Setting the Garbage Collector for the Implicit Role When creating a CoherenceCluster with a single implicit role the garbage collector to use is set in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: collector: CMS The implicit storage role will use the CMS garbage collector. Setting the Garbage Collector for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the garbage collector to use is set in the jvm.gc.collector section for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: collector: G1 - role: proxy jvm: gc: collector: CMS The JVMs for the data role will use the G1 garbage collector The JVMs for the proxy role will use the CMS garbage collector Setting the Garbage Collector for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default garbage collector can be set in the spec.jvm.gc.collector field of the CRD. This value can then be overridden for specific roles in the jvm.gc.collector field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: collector: CMS roles: - role: data jvm: gc: collector: G1 - role: proxy The default garbage collector us set to CMS which will be used by all roles in the roles list that do not set a specific collector to use. The data role overrides the default collector so that the JVMs for the data role will use the G1 garbage collector The proxy role does not specify a collector to use so that JVMs for the proxy role will use the CMS garbage collector Configuring Garbage Collector Arguments Arbitrary GC arguments can be passed to the JVM in the jvm.gc.args field. This field is a string array where each argument to be passed to the JVM is a separate string value. Setting Garbage Collector Arguments for the Implicit Role When creating a CoherenceCluster with a single implicit role the GC arguments are set in the spec.jvm.gc.args field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" The implicit storage role will have the additional GC arguments -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 passed to the JVM. Setting Garbage Collector Arguments for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the GC arguments are set in the jvm.gc.args field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" - role: proxy jvm: gc: args: - \"-XX:MaxGCPauseMillis=1000\" The explicit data role will have the additional GC arguments -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 passed to the JVM. The explicit proxy role will have the additional GC argument -XX:MaxGCPauseMillis=1000 passed to the JVM. Setting Garbage Collector Arguments for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default GC arguments are set in the spec.jvm.gc.args field and will be applied to all roles in the roles list that do not set specific GC arguments. GC arguments set for explicit roles override the defaults. The role&#8217;s GC arguments are not merged with the default GC arguments. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=500\" - \"-XX:G1ReservePercent=20\" roles: - role: data - role: proxy jvm: gc: args: - \"-XX:MaxGCPauseMillis=1000\" The default GC arguments are -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 The data role does not specify any GC arguments so the default arguments of -XX:MaxGCPauseMillis=500 and -XX:G1ReservePercent=20 will be passed to the data role JVMs. The proxy role specifies the GC arguments -XX:MaxGCPauseMillis=1000 which will override the defaults so only -XX:MaxGCPauseMillis=1000 will be passed to the proxy role JVMs. Configuring Garbage Collector Logging The Coherence documentation recommends enabling GC logging for Coherence JVMs. To this end the CoherenceCluster CRD has a boolean field jvm.gc.logging to enable or disable default GC logging JVM arguments. By default the value of this field is set to true if it is not specified for a CoherenceCluster . The following GC logging JVM arguments are added if the jvm.gc.logging field is omitted or explicitly set to true : <markup >-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime Configuring Garbage Collector Logging for the Implicit Role When creating a CoherenceCluster with a single implicit role GC logging can be enabled or disabled in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: logging: true The implicit storage role has GC logging explicitly enabled so that the JVM arguments listed above will be added to the JVM&#8217;s command line. Configuring Garbage Collector Logging for Explicit Roles When creating a CoherenceCluster with one or more explicit roles GC logging can be enabled or disabled in the jvm.gc.logging field of each role in the roles list. section of the yaml For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: gc: logging: true - role: proxy jvm: gc: logging: false The data role has GC logging explicitly enabled so that the JVM arguments listed above will be added to the JVM&#8217;s command line The proxy role has GC logging explicitly disabled so that the JVM arguments listed above will not be added to the JVM&#8217;s command line Configuring Garbage Collector Logging for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default GC logging setting can be specified in the spec section of the CRD which can then be overridden for individual roles in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: gc: logging: false roles: - role: data jvm: gc: logging: true - role: proxy The default value for jvm.gc.logging is false, which will disable GC logging. The data role overrides the default and sets GC logging to true The proxy role does not specify a value for jvm.gc.logging so it will use the default, which will disable GC logging. ",
            "title": "Garbage Collector Configuration"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role the heapSize is set in the spec.jvm section of the configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: heapSize: 10g The Coherence JVM for the implicit role defined above will have a 10 GB heap. Equivalent to passing -Xms10g -Xmx10g to the JVM. ",
            "title": "Setting the JVM Heap Size for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles the heapSize is set in the jvm section of the configuration for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: heapSize: 10g - role: proxy jvm: memory: heapSize: 500m The Coherence JVM for the data role defined above will have a 10 GB heap. Equivalent to passing -Xms10g -Xmx10g to the JVM. The Coherence JVM for the proxy role defined above will have a 500 MB heap. Equivalent to passing -Xms500m -Xmx500m to the JVM. ",
            "title": "Setting the JVM Heap Size for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default heapSize value can be set in the CoherenceCluster spec section that will apply to all of the roles in the roles list unless specifically overridden by a role&#8217;s jvm.heapSize field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: heapSize: 500m roles: - role: data jvm: memory: heapSize: 10g - role: proxy - role: web The default max heap size of 500 MB will be applied to all of the roles in the cluster unless overridden for a specific role. The data role overrides the default value to set the max heap for all JVMs in the data role to 10 GB. Equivalent to passing -Xms10g -Xmx10g to the JVM. The proxy role does not specify a heapSize value so it will use the default value of 500 MB. The web role does not specify a heapSize value so it will use the default value of 500 MB. ",
            "title": "Setting the JVM Heap Size for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " It is good practice to fix the Coherence JVM heap size and to set both the JVM -Xmx and -Xms options to the same value. The heap size of the JVM can be configured for roles in the jvm.heapSize field of a role spec. If the heapSize value is configured then that value is applied to bot the JVMs minimum and maximum heap sizes (i.e. used to set both -Xms and - Xmx ). The format of the value of the heapSize field is any valid value that can be used when setting the -Xmx JVM option, for example 10G would set a 10 GB heap. Setting the JVM Heap Size for the Implicit Role When creating a CoherenceCluster with a single implicit role the heapSize is set in the spec.jvm section of the configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: heapSize: 10g The Coherence JVM for the implicit role defined above will have a 10 GB heap. Equivalent to passing -Xms10g -Xmx10g to the JVM. Setting the JVM Heap Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the heapSize is set in the jvm section of the configuration for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: heapSize: 10g - role: proxy jvm: memory: heapSize: 500m The Coherence JVM for the data role defined above will have a 10 GB heap. Equivalent to passing -Xms10g -Xmx10g to the JVM. The Coherence JVM for the proxy role defined above will have a 500 MB heap. Equivalent to passing -Xms500m -Xmx500m to the JVM. Setting the JVM Heap Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default heapSize value can be set in the CoherenceCluster spec section that will apply to all of the roles in the roles list unless specifically overridden by a role&#8217;s jvm.heapSize field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: heapSize: 500m roles: - role: data jvm: memory: heapSize: 10g - role: proxy - role: web The default max heap size of 500 MB will be applied to all of the roles in the cluster unless overridden for a specific role. The data role overrides the default value to set the max heap for all JVMs in the data role to 10 GB. Equivalent to passing -Xms10g -Xmx10g to the JVM. The proxy role does not specify a heapSize value so it will use the default value of 500 MB. The web role does not specify a heapSize value so it will use the default value of 500 MB. ",
            "title": "JVM Heap Size"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role the metaspace size can be set in the spec section of the CRD. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: metaspaceSize: 256m The metaspace size will for the implicit storage role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m ",
            "title": "Configuring the JVM Metaspace Size for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles the metaspace size can be set in the jvm.memory.metaspaceSize field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: metaspaceSize: 256m - role: proxy jvm: memory: metaspaceSize: 512m The metaspace size will for the data role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m The metaspace size will for the proxy role will be set to 512m by setting the JVM arguments -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m ",
            "title": "Configuring the JVM Metaspace Size for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: metaspaceSize: 512m roles: - role: data jvm: memory: metaspaceSize: 256m - role: proxy The metaspace size will for the data role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m The metaspace size will for the proxy role will be set to 512m by setting the JVM arguments -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m ",
            "title": "Configuring the JVM Metaspace Size for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " The metaspace size is the amount of native memory that can be allocated for class metadata. By default the JVM does not limit this size. When running in size limited containers this size may be set to ensure that the JVM does not cause the container to exceed its configured memory limits. The metaspace size is set using the jvm.memory.metaspaceSize field. Setting this field causes the -XX:MetaspaceSize and -XX:MaxMetaspaceSize JVM arguments to be set. There is no default value for the metaspaceSize field so if it is omitted the JVMs default behaviour will control the metaspace size. Configuring the JVM Metaspace Size for the Implicit Role When creating a CoherenceCluster with a single implicit role the metaspace size can be set in the spec section of the CRD. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: metaspaceSize: 256m The metaspace size will for the implicit storage role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m Configuring the JVM Metaspace Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the metaspace size can be set in the jvm.memory.metaspaceSize field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: metaspaceSize: 256m - role: proxy jvm: memory: metaspaceSize: 512m The metaspace size will for the data role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m The metaspace size will for the proxy role will be set to 512m by setting the JVM arguments -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m Configuring the JVM Metaspace Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: metaspaceSize: 512m roles: - role: data jvm: memory: metaspaceSize: 256m - role: proxy The metaspace size will for the data role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m The metaspace size will for the proxy role will be set to 512m by setting the JVM arguments -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m ",
            "title": "JVM Metaspace Size"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role the stack size can be set in the spec section of CRD. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: stackSize: 1024k The stack size for the implicit storage role is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM. ",
            "title": "Configuring the JVM Stack Size for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: stackSize: 512k - role: proxy jvm: memory: stackSize: 1024k The stack size for the data role is set to 512k which will cause the -Xss512k argument to be passed to the JVM. The stack size for the proxy role is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM. ",
            "title": "Configuring the JVM Stack Size for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default stack size can be set in the spec section of the yaml that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: stackSize: 1024k roles: - role: data jvm: memory: stackSize: 512k - role: proxy The default stack size is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM for all roles in the roles list unless overridden. The stack size for the data role is specifically set to 512k which will cause the -Xss512k argument to be passed to the JVMs for the data role. The stack size for the proxy role is not configured so the default value will be used which will cause the -Xss1024k argument to be passed to the JVMs for the proxy role. ",
            "title": "Configuring the JVM Stack Size for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " Setting the stack size sets the thread stack size (in bytes) used by the JVM. The stack size is configured in for roles in a CoherenceCluster by setitng the jvm.memory`stackSize field. Setting this fields sets the -Xss JVM argument. Omitting this fields does not set the -Xss argument leaving the JVM to its default configuration which sets the stack size based on the O/S being used. Configuring the JVM Stack Size for the Implicit Role When creating a CoherenceCluster with a single implicit role the stack size can be set in the spec section of CRD. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: stackSize: 1024k The stack size for the implicit storage role is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM. Configuring the JVM Stack Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: stackSize: 512k - role: proxy jvm: memory: stackSize: 1024k The stack size for the data role is set to 512k which will cause the -Xss512k argument to be passed to the JVM. The stack size for the proxy role is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM. Configuring the JVM Stack Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default stack size can be set in the spec section of the yaml that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: stackSize: 1024k roles: - role: data jvm: memory: stackSize: 512k - role: proxy The default stack size is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM for all roles in the roles list unless overridden. The stack size for the data role is specifically set to 512k which will cause the -Xss512k argument to be passed to the JVMs for the data role. The stack size for the proxy role is not configured so the default value will be used which will cause the -Xss1024k argument to be passed to the JVMs for the proxy role. ",
            "title": "JVM Stack Size"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: directMemorySize: 2g the maximum direct memory size for the implicit storage role is set to 2g causing the -XX:MaxDirectMemorySize=2g argument to be passed to the JVM. ",
            "title": "Configuring the JVM Native Memory Size for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: directMemorySize: 2g - role: proxy jvm: memory: directMemorySize: 1g the maximum direct memory size for the data role is set to 2g causing the -XX:MaxDirectMemorySize=2g argument to be passed to the JVM. the maximum direct memory size for the proxy role is set to 1g causing the -XX:MaxDirectMemorySize=1g argument to be passed to the JVM. ",
            "title": "Configuring the JVM Native Memory Size for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: directMemorySize: 1g roles: - role: data jvm: memory: directMemorySize: 2g - role: proxy ",
            "title": "Configuring the JVM Native Memory Size for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " Native memory is used by the JVM and by Coherence for a number of reasons. In a resource limited container it may be useful to limit the amount of nio memory available to the JVM to stop the JVM exceeding the containers memory limits. The nio size is set using the jvm.directMemorySize field which will cause the -XX:MaxDirectMemorySize JVM argument to be set. There is no default value for the jvm.directMemorySize field so if it is omitted the JVM&#8217;s default size will be used. Configuring the JVM Native Memory Size for the Implicit Role When creating a CoherenceCluster with a single implicit role <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: directMemorySize: 2g the maximum direct memory size for the implicit storage role is set to 2g causing the -XX:MaxDirectMemorySize=2g argument to be passed to the JVM. Configuring the JVM Native Memory Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: directMemorySize: 2g - role: proxy jvm: memory: directMemorySize: 1g the maximum direct memory size for the data role is set to 2g causing the -XX:MaxDirectMemorySize=2g argument to be passed to the JVM. the maximum direct memory size for the proxy role is set to 1g causing the -XX:MaxDirectMemorySize=1g argument to be passed to the JVM. Configuring the JVM Native Memory Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: directMemorySize: 1g roles: - role: data jvm: memory: directMemorySize: 2g - role: proxy ",
            "title": "JVM Native Memory Size"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role native memory tracking can be configured in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: nativeMemoryTracking: detail The native memory tracking mode for the JVMs in the implicit storage role will be set to detail causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs. ",
            "title": "Configuring Native Memory Tracking for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles native memory tracking can br configured specifically for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: nativeMemoryTracking: detail - role: proxy jvm: memory: nativeMemoryTracking: summary The native memory tracking mode for the JVMs in the data role will be set to detail causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs. The native memory tracking mode for the JVMs in the proxy role will be set to summary causing the -XX:NativeMemoryTracking=summary to be passed to the JVMs. ",
            "title": "Configuring Native Memory Tracking for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default native memory tracking mode can be set in the spec section which will apply to all roles in the roles list unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: nativeMemoryTracking: off roles: - role: data jvm: memory: nativeMemoryTracking: detail - role: proxy The default native memory tracking mode is set to off for all roles in the roles list unless specifically overridden. This will cause the -XX:NativeMemoryTracking=off to be passed to the JVMs. The native memory tracking mode is specifically set to detail for the data role causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs in the data role. The native memory tracking mode is not set for the proxy role so it will use the default value of off causing the -XX:NativeMemoryTracking=off to be passed to the JVMs in the proxy role. ",
            "title": "Configuring Native Memory Tracking for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " The Native memory tracking mode can be configured for JVMs using the jvm.memory.nativeMemoryTracking field to track JVM nio memory usage, which can be useful when debugging nio memory issues. Setting the nativeMemoryTracking value causes the -XX:NativeMemoryTracking JVM argument to be set. If the jvm.memory.nativeMemoryTracking field is not specified a value of summary is used passing -XX:NativeMemoryTracking=summary to the JVM. See the native memory tracking documentation. Configuring Native Memory Tracking for the Implicit Role When creating a CoherenceCluster with a single implicit role native memory tracking can be configured in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: nativeMemoryTracking: detail The native memory tracking mode for the JVMs in the implicit storage role will be set to detail causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs. Configuring Native Memory Tracking for Explicit Roles When creating a CoherenceCluster with one or more explicit roles native memory tracking can br configured specifically for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: nativeMemoryTracking: detail - role: proxy jvm: memory: nativeMemoryTracking: summary The native memory tracking mode for the JVMs in the data role will be set to detail causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs. The native memory tracking mode for the JVMs in the proxy role will be set to summary causing the -XX:NativeMemoryTracking=summary to be passed to the JVMs. Configuring Native Memory Tracking for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default native memory tracking mode can be set in the spec section which will apply to all roles in the roles list unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: nativeMemoryTracking: off roles: - role: data jvm: memory: nativeMemoryTracking: detail - role: proxy The default native memory tracking mode is set to off for all roles in the roles list unless specifically overridden. This will cause the -XX:NativeMemoryTracking=off to be passed to the JVMs. The native memory tracking mode is specifically set to detail for the data role causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs in the data role. The native memory tracking mode is not set for the proxy role so it will use the default value of off causing the -XX:NativeMemoryTracking=off to be passed to the JVMs in the proxy role. ",
            "title": "Native Memory Tracking"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role native memory tracking can be configured in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: onOutOfMemory: exit: true heapDump: true The implicit storage role will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The implicit storage role will generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM ",
            "title": "Configuring OOM Behaviour for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles native memory tracking can br configured specifically for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: onOutOfMemory: exit: true heapDump: true - role: proxy jvm: memory: onOutOfMemory: exit: false heapDump: false The data role will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The data role will generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM The proxy role will not exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be not passed to the JVM The proxy role will not generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will not be passed to the JVM ",
            "title": "Configuring OOM Behaviour for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default native memory tracking mode can be set in the spec section which will apply to all roles in the roles list unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: onOutOfMemory: exit: false heapDump: false roles: - role: data jvm: memory: onOutOfMemory: exit: true heapDump: true - role: proxy The default setting for exit on out of memory error is false The default setting for generating a heap dump on out of memory error is false The data role overrides the default jvm.memory.onOutOfMemory.exit value to true and will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The data role overrides the default jvm.memory.onOutOfMemory.heapDump value to true and will generate a heap dump if an out of memory error occurs, -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM The proxy role does not specify any values for jvm.memory.onOutOfMemory.exit or jvm.memory.onOutOfMemory.heapDump so it will use the default values of false , the -XX:+ExitOnOutOfMemoryError and -XX:+HeapDumpOnOutOfMemoryError\" arguments will not be passed to the JVM ",
            "title": "Configuring OOM Behaviour for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " It is an important recommendation in the Coherence documentation to specifically set the behaviour of a JVM when it encounters an out of memory error. The JVM should be set to exit and generate a heap dump. A JVM that encounters an OOM error is left in an undefined state and this can cause a Coherence cluster to become unstable if the JVM does not exit. Generating a heap dump is useful to diagnose why the JVM had the OOM error. There are two boolean fields in the CoherenceCluster CRD that control this behaviour: jvm.memory.onOutOfMemory.exit which determines whether the JVM will exit on an OOM error; the default value if the field is not specified is true . A value of true causes the -XX:+ExitOnOutOfMemoryError argument to be passed to the JVM. jvm.memory.onOutOfMemory.heapDump which determines whether the JVM will generate a heap dump on an OOM error; the default value if the field is not specified is true . Heap dumps will be written to a file /jvm/${POD_NAME}/${POD_UID}/heap-dumps/${POD_NAME}-${POD_UID}.hprof . The root /jvm directory can be mapped to an external volume for easier access to the heap dumps (see: setting the disgnostic volume ) Configuring OOM Behaviour for the Implicit Role When creating a CoherenceCluster with a single implicit role native memory tracking can be configured in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: onOutOfMemory: exit: true heapDump: true The implicit storage role will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The implicit storage role will generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM Configuring OOM Behaviour for Explicit Roles When creating a CoherenceCluster with one or more explicit roles native memory tracking can br configured specifically for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: onOutOfMemory: exit: true heapDump: true - role: proxy jvm: memory: onOutOfMemory: exit: false heapDump: false The data role will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The data role will generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM The proxy role will not exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be not passed to the JVM The proxy role will not generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will not be passed to the JVM Configuring OOM Behaviour for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default native memory tracking mode can be set in the spec section which will apply to all roles in the roles list unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: onOutOfMemory: exit: false heapDump: false roles: - role: data jvm: memory: onOutOfMemory: exit: true heapDump: true - role: proxy The default setting for exit on out of memory error is false The default setting for generating a heap dump on out of memory error is false The data role overrides the default jvm.memory.onOutOfMemory.exit value to true and will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The data role overrides the default jvm.memory.onOutOfMemory.heapDump value to true and will generate a heap dump if an out of memory error occurs, -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM The proxy role does not specify any values for jvm.memory.onOutOfMemory.exit or jvm.memory.onOutOfMemory.heapDump so it will use the default values of false , the -XX:+ExitOnOutOfMemoryError and -XX:+HeapDumpOnOutOfMemoryError\" arguments will not be passed to the JVM ",
            "title": "JVM Behaviour on Out Of Memory"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " The JVM has a number of options that can be set to fix the size of different memory regions. The CoherenceCluster CRD provides fields to set that most common values. None of these fields have default values so if they are not specified the JVMs default behaviour will apply. The memory options that can be configured are: Heap Size Metaspace size Stack size Max Native Memory Native Memory Tracking Behaviour on Out Of Memory Error If the Pod resource limits are being set to limit memory usage of a Pod it is recommended that some of the JVM memory regions are fixed to ensure that the JVM does not exceed the container&#8217;s resource limits in a JVM before Java 10. Prior to Java 10 the JVM could see all of the memory available to a machine regardless of any Pod limits. The JVM could then easily attempt to consume more memory that the Pod or Container was allowed and consequently crashing the Pod . With Coherence images that use a version of Java above 10 this issue is less of a problem. Even so if using the resources section of the configuration to limit a Pod or Containers memory it is a good idea to limit the JVM heap. Also see the useContainerLimits setting . JVM Heap Size It is good practice to fix the Coherence JVM heap size and to set both the JVM -Xmx and -Xms options to the same value. The heap size of the JVM can be configured for roles in the jvm.heapSize field of a role spec. If the heapSize value is configured then that value is applied to bot the JVMs minimum and maximum heap sizes (i.e. used to set both -Xms and - Xmx ). The format of the value of the heapSize field is any valid value that can be used when setting the -Xmx JVM option, for example 10G would set a 10 GB heap. Setting the JVM Heap Size for the Implicit Role When creating a CoherenceCluster with a single implicit role the heapSize is set in the spec.jvm section of the configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: heapSize: 10g The Coherence JVM for the implicit role defined above will have a 10 GB heap. Equivalent to passing -Xms10g -Xmx10g to the JVM. Setting the JVM Heap Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the heapSize is set in the jvm section of the configuration for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: heapSize: 10g - role: proxy jvm: memory: heapSize: 500m The Coherence JVM for the data role defined above will have a 10 GB heap. Equivalent to passing -Xms10g -Xmx10g to the JVM. The Coherence JVM for the proxy role defined above will have a 500 MB heap. Equivalent to passing -Xms500m -Xmx500m to the JVM. Setting the JVM Heap Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default heapSize value can be set in the CoherenceCluster spec section that will apply to all of the roles in the roles list unless specifically overridden by a role&#8217;s jvm.heapSize field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: heapSize: 500m roles: - role: data jvm: memory: heapSize: 10g - role: proxy - role: web The default max heap size of 500 MB will be applied to all of the roles in the cluster unless overridden for a specific role. The data role overrides the default value to set the max heap for all JVMs in the data role to 10 GB. Equivalent to passing -Xms10g -Xmx10g to the JVM. The proxy role does not specify a heapSize value so it will use the default value of 500 MB. The web role does not specify a heapSize value so it will use the default value of 500 MB. JVM Metaspace Size The metaspace size is the amount of native memory that can be allocated for class metadata. By default the JVM does not limit this size. When running in size limited containers this size may be set to ensure that the JVM does not cause the container to exceed its configured memory limits. The metaspace size is set using the jvm.memory.metaspaceSize field. Setting this field causes the -XX:MetaspaceSize and -XX:MaxMetaspaceSize JVM arguments to be set. There is no default value for the metaspaceSize field so if it is omitted the JVMs default behaviour will control the metaspace size. Configuring the JVM Metaspace Size for the Implicit Role When creating a CoherenceCluster with a single implicit role the metaspace size can be set in the spec section of the CRD. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: metaspaceSize: 256m The metaspace size will for the implicit storage role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m Configuring the JVM Metaspace Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the metaspace size can be set in the jvm.memory.metaspaceSize field for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: metaspaceSize: 256m - role: proxy jvm: memory: metaspaceSize: 512m The metaspace size will for the data role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m The metaspace size will for the proxy role will be set to 512m by setting the JVM arguments -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m Configuring the JVM Metaspace Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: metaspaceSize: 512m roles: - role: data jvm: memory: metaspaceSize: 256m - role: proxy The metaspace size will for the data role will be set to 256m by setting the JVM arguments -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m The metaspace size will for the proxy role will be set to 512m by setting the JVM arguments -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m JVM Stack Size Setting the stack size sets the thread stack size (in bytes) used by the JVM. The stack size is configured in for roles in a CoherenceCluster by setitng the jvm.memory`stackSize field. Setting this fields sets the -Xss JVM argument. Omitting this fields does not set the -Xss argument leaving the JVM to its default configuration which sets the stack size based on the O/S being used. Configuring the JVM Stack Size for the Implicit Role When creating a CoherenceCluster with a single implicit role the stack size can be set in the spec section of CRD. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: stackSize: 1024k The stack size for the implicit storage role is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM. Configuring the JVM Stack Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: stackSize: 512k - role: proxy jvm: memory: stackSize: 1024k The stack size for the data role is set to 512k which will cause the -Xss512k argument to be passed to the JVM. The stack size for the proxy role is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM. Configuring the JVM Stack Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default stack size can be set in the spec section of the yaml that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: stackSize: 1024k roles: - role: data jvm: memory: stackSize: 512k - role: proxy The default stack size is set to 1024k which will cause the -Xss1024k argument to be passed to the JVM for all roles in the roles list unless overridden. The stack size for the data role is specifically set to 512k which will cause the -Xss512k argument to be passed to the JVMs for the data role. The stack size for the proxy role is not configured so the default value will be used which will cause the -Xss1024k argument to be passed to the JVMs for the proxy role. JVM Native Memory Size Native memory is used by the JVM and by Coherence for a number of reasons. In a resource limited container it may be useful to limit the amount of nio memory available to the JVM to stop the JVM exceeding the containers memory limits. The nio size is set using the jvm.directMemorySize field which will cause the -XX:MaxDirectMemorySize JVM argument to be set. There is no default value for the jvm.directMemorySize field so if it is omitted the JVM&#8217;s default size will be used. Configuring the JVM Native Memory Size for the Implicit Role When creating a CoherenceCluster with a single implicit role <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: directMemorySize: 2g the maximum direct memory size for the implicit storage role is set to 2g causing the -XX:MaxDirectMemorySize=2g argument to be passed to the JVM. Configuring the JVM Native Memory Size for Explicit Roles When creating a CoherenceCluster with one or more explicit roles For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: directMemorySize: 2g - role: proxy jvm: memory: directMemorySize: 1g the maximum direct memory size for the data role is set to 2g causing the -XX:MaxDirectMemorySize=2g argument to be passed to the JVM. the maximum direct memory size for the proxy role is set to 1g causing the -XX:MaxDirectMemorySize=1g argument to be passed to the JVM. Configuring the JVM Native Memory Size for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: directMemorySize: 1g roles: - role: data jvm: memory: directMemorySize: 2g - role: proxy Native Memory Tracking The Native memory tracking mode can be configured for JVMs using the jvm.memory.nativeMemoryTracking field to track JVM nio memory usage, which can be useful when debugging nio memory issues. Setting the nativeMemoryTracking value causes the -XX:NativeMemoryTracking JVM argument to be set. If the jvm.memory.nativeMemoryTracking field is not specified a value of summary is used passing -XX:NativeMemoryTracking=summary to the JVM. See the native memory tracking documentation. Configuring Native Memory Tracking for the Implicit Role When creating a CoherenceCluster with a single implicit role native memory tracking can be configured in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: nativeMemoryTracking: detail The native memory tracking mode for the JVMs in the implicit storage role will be set to detail causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs. Configuring Native Memory Tracking for Explicit Roles When creating a CoherenceCluster with one or more explicit roles native memory tracking can br configured specifically for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: nativeMemoryTracking: detail - role: proxy jvm: memory: nativeMemoryTracking: summary The native memory tracking mode for the JVMs in the data role will be set to detail causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs. The native memory tracking mode for the JVMs in the proxy role will be set to summary causing the -XX:NativeMemoryTracking=summary to be passed to the JVMs. Configuring Native Memory Tracking for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default native memory tracking mode can be set in the spec section which will apply to all roles in the roles list unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: nativeMemoryTracking: off roles: - role: data jvm: memory: nativeMemoryTracking: detail - role: proxy The default native memory tracking mode is set to off for all roles in the roles list unless specifically overridden. This will cause the -XX:NativeMemoryTracking=off to be passed to the JVMs. The native memory tracking mode is specifically set to detail for the data role causing the -XX:NativeMemoryTracking=detail to be passed to the JVMs in the data role. The native memory tracking mode is not set for the proxy role so it will use the default value of off causing the -XX:NativeMemoryTracking=off to be passed to the JVMs in the proxy role. JVM Behaviour on Out Of Memory It is an important recommendation in the Coherence documentation to specifically set the behaviour of a JVM when it encounters an out of memory error. The JVM should be set to exit and generate a heap dump. A JVM that encounters an OOM error is left in an undefined state and this can cause a Coherence cluster to become unstable if the JVM does not exit. Generating a heap dump is useful to diagnose why the JVM had the OOM error. There are two boolean fields in the CoherenceCluster CRD that control this behaviour: jvm.memory.onOutOfMemory.exit which determines whether the JVM will exit on an OOM error; the default value if the field is not specified is true . A value of true causes the -XX:+ExitOnOutOfMemoryError argument to be passed to the JVM. jvm.memory.onOutOfMemory.heapDump which determines whether the JVM will generate a heap dump on an OOM error; the default value if the field is not specified is true . Heap dumps will be written to a file /jvm/${POD_NAME}/${POD_UID}/heap-dumps/${POD_NAME}-${POD_UID}.hprof . The root /jvm directory can be mapped to an external volume for easier access to the heap dumps (see: setting the disgnostic volume ) Configuring OOM Behaviour for the Implicit Role When creating a CoherenceCluster with a single implicit role native memory tracking can be configured in the spec section of the yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: onOutOfMemory: exit: true heapDump: true The implicit storage role will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The implicit storage role will generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM Configuring OOM Behaviour for Explicit Roles When creating a CoherenceCluster with one or more explicit roles native memory tracking can br configured specifically for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: memory: onOutOfMemory: exit: true heapDump: true - role: proxy jvm: memory: onOutOfMemory: exit: false heapDump: false The data role will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The data role will generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM The proxy role will not exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be not passed to the JVM The proxy role will not generate a heap dump if an out of memory error occurs, the -XX:+HeapDumpOnOutOfMemoryError\" argument will not be passed to the JVM Configuring OOM Behaviour for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default native memory tracking mode can be set in the spec section which will apply to all roles in the roles list unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: memory: onOutOfMemory: exit: false heapDump: false roles: - role: data jvm: memory: onOutOfMemory: exit: true heapDump: true - role: proxy The default setting for exit on out of memory error is false The default setting for generating a heap dump on out of memory error is false The data role overrides the default jvm.memory.onOutOfMemory.exit value to true and will exit if an out of memory error occurs, the -XX:+ExitOnOutOfMemoryError argument will be passed to the JVM The data role overrides the default jvm.memory.onOutOfMemory.heapDump value to true and will generate a heap dump if an out of memory error occurs, -XX:+HeapDumpOnOutOfMemoryError\" argument will be passed to the JVM The proxy role does not specify any values for jvm.memory.onOutOfMemory.exit or jvm.memory.onOutOfMemory.heapDump so it will use the default values of false , the -XX:+ExitOnOutOfMemoryError and -XX:+HeapDumpOnOutOfMemoryError\" arguments will not be passed to the JVM ",
            "title": "Memory Configuration"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with a single implicit role the useContainerLimits is set in the spec.jvm section of the configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: useContainerLimits: true The -XX:+UseContainerSupport JVM option will be passed as arguments to the JVM for the implicit storage role. ",
            "title": "Setting Container Resource Limits for the Implicit Role"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles the useContainerLimits are set in the jvm section of the configuration for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: useContainerLimits: true - role: proxy jvm: useContainerLimits: false The -XX:+UseContainerSupport JVM option will be passed as arguments to the JVM for the explicit data role. The -XX:+UseContainerSupport JVM option will not be passed as arguments to the JVM for the explicit proxy role. ",
            "title": "Setting Container Resource Limits for Explicit Roles"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When creating a CoherenceCluster with one or more explicit roles a default useContainerLimits value can be set in the CoherenceCluster spec section that will apply to all of the roles in the roles list unless explicitly overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: useContainerLimits: true roles: - role: data jvm: useContainerLimits: false - role: proxy The default useContainerLimits is set to true . The data role overrides the default useContainerLimits and sets it to false . The proxy role does not specify any useContainerLimits value so will use the default of true . ",
            "title": "Setting Container Resource Limits for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " When running JVMs inside containers it is recommended to configure the JVM to respect the memory and CPU resource limits that are configured for the container. This is especially important in Kubernetes where the Pod may be terminated if a container exceeds the configured resource limits. The jvm.useContainerLimits field is used to either add or omit the -XX:+UseContainerSupport JVM argument. If useContainerLimits is set to true then -XX:+UseContainerSupport is added to the JVM arguments, if useContainerLimits is set to false then -XX:+UseContainerSupport is not added to the JVM arguments. The default value of useContainerLimits if not specified is true so -XX:+UseContainerSupport will always be added to the JVM arguments unless useContainerLimits is explicitly set to false . It is recommended that this value be left unspecified as the default true unless other arguments are being passed to the JVM to limit its resource usage. Setting Container Resource Limits for the Implicit Role When creating a CoherenceCluster with a single implicit role the useContainerLimits is set in the spec.jvm section of the configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: useContainerLimits: true The -XX:+UseContainerSupport JVM option will be passed as arguments to the JVM for the implicit storage role. Setting Container Resource Limits for Explicit Roles When creating a CoherenceCluster with one or more explicit roles the useContainerLimits are set in the jvm section of the configuration for each role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data jvm: useContainerLimits: true - role: proxy jvm: useContainerLimits: false The -XX:+UseContainerSupport JVM option will be passed as arguments to the JVM for the explicit data role. The -XX:+UseContainerSupport JVM option will not be passed as arguments to the JVM for the explicit proxy role. Setting Container Resource Limits for Explicit Roles with a Default When creating a CoherenceCluster with one or more explicit roles a default useContainerLimits value can be set in the CoherenceCluster spec section that will apply to all of the roles in the roles list unless explicitly overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: jvm: useContainerLimits: true roles: - role: data jvm: useContainerLimits: false - role: proxy The default useContainerLimits is set to true . The data role overrides the default useContainerLimits and sets it to false . The proxy role does not specify any useContainerLimits value so will use the default of true . ",
            "title": "Container Resource Limits"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " Flight Recorder is a useful tool to use when diagnosing issues with a Coherence application or as an aid to performance and GC tuning. By default the JVMs in a CoherenceCluster are configured to produce a continual flight recording that will be dumped to a file when the JVM exits. The /jvm root directory used for .jfr files can be mounted to an external volume to allow easier access to these files. ",
            "title": "Flight Recorder"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " By default the Coherence JVMs are configured to write heap dumps, error logs and flight recordings to directories in the container under the root /jvm directory. The /jvm directory is mapped to volumeMount named jvm which is in turn mapped to a volume named jvm . The default configuration for the jvm volume in the Coherence Pods is an empty directory. <markup lang=\"yaml\" >volumeMounts: - name: jvm mountPath: /jvm volumes: - name: jvm emptyDir: {} The default may be changed to map the jvm volume to any supported Kubernetes VolumeSource . ",
            "title": "Diagnostic Volume"
        },
        {
            "location": "/clusters/080_jvm",
            "text": " Sometimes attaching a debugger to a JVM is the best way to track down the cause of an issue. The CoherenceCluster CRD has a number of fields that can be used to configure how the JVM can be started in debug mode. ",
            "title": "JVM Debug Arguments"
        },
        {
            "location": "/about/02_concepts",
            "text": " The Coherence Operator is a Kubernetes Operator that is used to manage Oracle Coherence clusters in Kubernetes. The Coherence Operator takes on the tasks of that human Dev Ops resource might carry out when managing Coherence clusters, such as configuration, installation, safe scaling, management and metrics. The Coherence Operator is a Go based application built using the Operator SDK . It is distributed as a Docker image and Helm chart for easy installation and configuration. ",
            "title": "What is the Coherence Operator?"
        },
        {
            "location": "/about/02_concepts",
            "text": " A Coherence cluster is a number of distributed Java Virtual Machines (JVMs) that communicate to form a single coherent cluster. In Kubernetes, this concept can be related to a number of Pods that form a single cluster. In each Pod is a JVM running a Coherence DefaultCacheServer , or a custom application using Coherence. The operator uses a Kubernetes Custom Resource Definition (CRD) to represent a Coherence cluster and the roles within it. Every field in the CoherenceCluster CRD spec is optional so a simple cluster can be defined in yaml as: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster The metadata.name field in the CoherenceCluster yaml will be used as the Coherence cluster name and must be unique in a given Kubernetes namespace. The operator will use default values for fields that have not been entered, so the above yaml will create a Coherence cluster using a StatefulSet with a replica count of three, which means that will be three storage enabled Coherence Pods . ",
            "title": "Coherence Clusters"
        },
        {
            "location": "/about/02_concepts",
            "text": " The operator implies that a single role is required when the roles list in the CoherenceCluster CRD yaml is either empty or missing. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: role: data replicas: 6 The yaml above does not include any roles defined in the roles list of the spec section. When all of the role configuration is in fields directly in the spec section like this the operator implies that a single role is required and will use the values defined in the spec section to create a single StatefulSet . ",
            "title": "A Single Implied Role"
        },
        {
            "location": "/about/02_concepts",
            "text": " Roles can be defined explicitly by adding the configuration of each role to the roles list in the CoherenceCluster CRD spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: data replicas: 6 In the example above there is explicitly one role defined in the roles list. ",
            "title": "A Single Explicit Role"
        },
        {
            "location": "/about/02_concepts",
            "text": " To define a Coherence cluster with multiple roles each role is configured as a separate entry in the roles list. For example, if a cluster requires two roles, one named storage and another named web the configuration may look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: storage replicas: 3 - role: web replicas: 2 The storage role is explicitly defined in the roles list The web role is explicitly defined in the roles list This will result in a Coherence cluster made up of two StatefulSets . The storage role will have a StatefulSet with three Pods and the web role will have a StatefulSet with two Pods . The Coherence cluster will have a total of five Pods . ",
            "title": "Multiple Explicit Role"
        },
        {
            "location": "/about/02_concepts",
            "text": " When defining explicit roles in the roles list and field added directly to the CoherenceCluster spec section becomes a default value that is applied to all of the roles in the roles list unless the value is overridden in the configuration for a specific role. This allows common configuration shared by multiple roles to be maintained in a single place instead of being duplicated for every role. For example, if a cluster requires three roles, one named storage with a 5g JVM heap, one named proxy with a 5g JVM heap and another named web with a 1g JVM heap the configuration may look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: jvm: memory: heapSize: 5g roles: - role: storage replicas: 6 - role: storage replicas: 3 - role: web replicas: 2 jvm: memory: heapSize: 1g The jvm.memory.heapSize value of 5g is added directly under the spec section so this value will apply to all roles meaning all roles will have the JVM options -Xms5g -Xmx5g unless overridden. In this case the storage and the proxy roles do not set the jvm.memory.heapSize field so they will have a 5g JVM heap. The web role overrides the jvm.memory.heapSize field with a value of 1g so the JVMs in the web role will have the JVM options -Xms1g -Xmx1g When using default values some default values are overridden by values in a role and sometimes the default and role values are merged. When the field is a single intrinsic value, for example a number or a string the role value overrides the default. Where the field is an array/slice or a map it may be merged. The CoherenceCluster CRD section documents how fields are overridden or merged. ",
            "title": "Explicit Roles with Default Values"
        },
        {
            "location": "/about/02_concepts",
            "text": " A Coherence cluster can be made up of a number of Pods that perform different roles. All of the Pods in a given role share the same configuration. A cluster usually has at least one role where Pods are storage enabled. Each role in a Coherence cluster has a name and configuration. A cluster can have zero or many roles defined in the CoherenceCluster CRD Spec . You can define common configuration shared by all roles to save duplicating configuration multiple times in the yaml. The Coherence Operator will create a StatefulSet for each role defined in the CoherenceCluster CRD yaml. This separation allows roles to be managed and scaled independently from each other. All of the Pods in the different StatefulSets will form a single Coherence cluster. There are two ways to describe the specification of a role in a CoherenceCluster CRD depending on whether the cluster has a single implied role or has one or more explicit roles. A Single Implied Role The operator implies that a single role is required when the roles list in the CoherenceCluster CRD yaml is either empty or missing. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: role: data replicas: 6 The yaml above does not include any roles defined in the roles list of the spec section. When all of the role configuration is in fields directly in the spec section like this the operator implies that a single role is required and will use the values defined in the spec section to create a single StatefulSet . A Single Explicit Role Roles can be defined explicitly by adding the configuration of each role to the roles list in the CoherenceCluster CRD spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: data replicas: 6 In the example above there is explicitly one role defined in the roles list. Multiple Explicit Role To define a Coherence cluster with multiple roles each role is configured as a separate entry in the roles list. For example, if a cluster requires two roles, one named storage and another named web the configuration may look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: storage replicas: 3 - role: web replicas: 2 The storage role is explicitly defined in the roles list The web role is explicitly defined in the roles list This will result in a Coherence cluster made up of two StatefulSets . The storage role will have a StatefulSet with three Pods and the web role will have a StatefulSet with two Pods . The Coherence cluster will have a total of five Pods . Explicit Roles with Default Values When defining explicit roles in the roles list and field added directly to the CoherenceCluster spec section becomes a default value that is applied to all of the roles in the roles list unless the value is overridden in the configuration for a specific role. This allows common configuration shared by multiple roles to be maintained in a single place instead of being duplicated for every role. For example, if a cluster requires three roles, one named storage with a 5g JVM heap, one named proxy with a 5g JVM heap and another named web with a 1g JVM heap the configuration may look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: jvm: memory: heapSize: 5g roles: - role: storage replicas: 6 - role: storage replicas: 3 - role: web replicas: 2 jvm: memory: heapSize: 1g The jvm.memory.heapSize value of 5g is added directly under the spec section so this value will apply to all roles meaning all roles will have the JVM options -Xms5g -Xmx5g unless overridden. In this case the storage and the proxy roles do not set the jvm.memory.heapSize field so they will have a 5g JVM heap. The web role overrides the jvm.memory.heapSize field with a value of 1g so the JVMs in the web role will have the JVM options -Xms1g -Xmx1g When using default values some default values are overridden by values in a role and sometimes the default and role values are merged. When the field is a single intrinsic value, for example a number or a string the role value overrides the default. Where the field is an array/slice or a map it may be merged. The CoherenceCluster CRD section documents how fields are overridden or merged. ",
            "title": "Coherence Roles"
        },
        {
            "location": "/clusters/085_safe_scaling",
            "text": " The Coherence Operator uses a scaling policy to determine how the StatefulSet that makes up a role withing a cluster is scaled. Scaling policy has the following values: Value Description ParallelUpSafeDown This is the default scaling policy. With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ) and when scaling down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the role have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when adding members, but offers safe, albeit slower, scaling down as Pods are removed one by one. Parallel With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ). With this policy no StatusHA check is performed either when scaling up or when scaling down. This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for roles that are storage disabled. Safe With this policy when scaling up and down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the role have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy is slow to start, scale up and scale down. &#8230;&#8203; The scaling policy is set in the scaling.policy section of the configuration of a role. ",
            "title": "Scaling Policy"
        },
        {
            "location": "/clusters/085_safe_scaling",
            "text": " When creating a CoherenceCluster with a single implicit role the scaling policy can be defined at the spec level. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: scaling: policy: Safe The implicit role will have a scaling policy of Safe ",
            "title": "Configure Scaling Policy for a Single Implicit Role"
        },
        {
            "location": "/clusters/085_safe_scaling",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list scaling policy can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data scaling: policy: ParallelUpSafeDown - role: proxy scaling: policy: Parallel The data role will have the scaling policy ParallelUpSafeDown The proxy role will have the scaling policy Parallel ",
            "title": "Configure Scaling Policy for Explicit Roles"
        },
        {
            "location": "/clusters/085_safe_scaling",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list scaling policy can be defined as defaults applied to all roles unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: scaling: policy: Parallel roles: - role: data scaling: policy: ParallelUpSafeDown - role: proxy - role: web Thedefault scaling policy is Parallel that will apply to all roles unless specifically overridden. The data role overrides the default and specifies a scaling policy of ParallelUpSafeDown The proxy role does not specify a scaling policy so will use the defautl of Parallel The web role does not specify a scaling policy so will use the defautl of Parallel ",
            "title": "Configure Pod Labels for Explicit Roles With Defaults"
        },
        {
            "location": "/clusters/085_safe_scaling",
            "text": " The Coherence Operator contains functionality to allow it to safely scale a role within a Coherence cluster without losing data. Scaling can be configured in the scaling section of the CoherenceCluster CRD. A role in a CoherenceCluster can be scaled by changing the replica count in the role&#8217;s spec or by using the kubectl scale command. Scaling Policy The Coherence Operator uses a scaling policy to determine how the StatefulSet that makes up a role withing a cluster is scaled. Scaling policy has the following values: Value Description ParallelUpSafeDown This is the default scaling policy. With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ) and when scaling down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the role have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when adding members, but offers safe, albeit slower, scaling down as Pods are removed one by one. Parallel With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ). With this policy no StatusHA check is performed either when scaling up or when scaling down. This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for roles that are storage disabled. Safe With this policy when scaling up and down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the role have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy is slow to start, scale up and scale down. &#8230;&#8203; The scaling policy is set in the scaling.policy section of the configuration of a role. Configure Scaling Policy for a Single Implicit Role When creating a CoherenceCluster with a single implicit role the scaling policy can be defined at the spec level. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: scaling: policy: Safe The implicit role will have a scaling policy of Safe Configure Scaling Policy for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list scaling policy can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data scaling: policy: ParallelUpSafeDown - role: proxy scaling: policy: Parallel The data role will have the scaling policy ParallelUpSafeDown The proxy role will have the scaling policy Parallel Configure Pod Labels for Explicit Roles With Defaults When creating a CoherenceCluster with explicit roles in the roles list scaling policy can be defined as defaults applied to all roles unless specifically overridden for a role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: scaling: policy: Parallel roles: - role: data scaling: policy: ParallelUpSafeDown - role: proxy - role: web Thedefault scaling policy is Parallel that will apply to all roles unless specifically overridden. The data role overrides the default and specifies a scaling policy of ParallelUpSafeDown The proxy role does not specify a scaling policy so will use the defautl of Parallel The web role does not specify a scaling policy so will use the defautl of Parallel ",
            "title": "Configure Safe Scaling"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Since version 12.2.1.4 Coherence has had functionality to expose a management API over ReST. This API is disabled by default in Coherence clusters but can be enabled and configured by setting the relevant fields in the CoherenceCluster resource. ",
            "title": "preambule"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Deploy a simple management enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: management-cluster spec: role: storage replicas: 3 coherence: management: enabled: true ports: - name: management port: 30000 Indicates to enable Management over ReST The management port must be added to the additional ports list so that it is exposed on a service The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f management-cluster.yaml coherencecluster.coherence.oracle.com/management-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=management-cluster NAME READY STATUS RESTARTS AGE management-cluster-storage-0 1/1 Running 0 36s management-cluster-storage-1 1/1 Running 0 36s management-cluster-storage-2 1/1 Running 0 36s ",
            "title": "1. Install a Coherence cluster with Management over ReST enabled"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example port-forward management-cluster-storage-0 30000:30000 Forwarding from [::1]:30000 -&gt; 30000 Forwarding from 127.0.0.1:30000 -&gt; 30000 ",
            "title": "2. Port-forward the Management over ReST port"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Issue the following to access the ReST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/ | jq <markup lang=\"json\" >{ \"links\": [ { \"rel\": \"parent\", \"href\": \"http://127.0.0.1:30000/management/coherence\" }, { \"rel\": \"self\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"canonical\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"services\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/services\" }, { \"rel\": \"caches\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/caches\" }, { \"rel\": \"members\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/members\" }, { \"rel\": \"management\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/management\" }, { \"rel\": \"journal\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/journal\" }, { \"rel\": \"hotcache\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/hotcache\" }, { \"rel\": \"reporters\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\": \"webApplications\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/webApplications\" } ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2019-10-15T03:55:46.461Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"12.2.1.4.0\", \"running\": true, \"clusterName\": \"management-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2019-10-15 03:46:15.848, Address=10.1.2.184:36531, MachineId=49519, Location=site:coherence.coherence-example.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-storage-1, Role=storage)\", \"Member(Id=2, Timestamp=2019-10-15 03:46:19.405, Address=10.1.2.183:40341, MachineId=49519, Location=site:coherence.coherence-example.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-storage-2, Role=storage)\", \"Member(Id=3, Timestamp=2019-10-15 03:46:19.455, Address=10.1.2.185:38719, MachineId=49519, Location=site:coherence.coherence-example.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-storage-0, Role=storage)\" ], \"type\": \"Cluster\" } The jq utility is used to format the JSON, and may not be available on all platforms. ",
            "title": "3. Access the REST endpoint"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Issue the following to access the Sagger endpoint which documents all the API&#8217;s available. <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog | jq <markup lang=\"json\" >{ \"swagger\": \"2.0\", \"info\": { \"title\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"description\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"version\": \"12.2.1.4.0\" }, \"schemes\": [ \"http\", \"https\" ], ... The above output has been truncated due to the large size. ",
            "title": "3. Access the Swagger endpoint"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " Management over ReST can be used for all management functions, as one would with standard MBean access over JMX. Please see the Coherence REST API for more information on these features. Connecting JVisualVM to Management over ReST Enabling SSL Produce and extract a Java Flight Recorder (JFR) file Access the Reporter ",
            "title": "4. Other Resources"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f management-cluster.yaml Stop the port-forward command using CTRL-C . ",
            "title": "5. Clean Up"
        },
        {
            "location": "/management/020_management_over_rest",
            "text": " This example shows how to enable and access Coherence MBeans using Management over ReST. Once the Management port is exposed via a load balancer or port-forward command the ReEST endpoint is available at http://host:port/management/coherence/cluster and the Swagger JSON document is available at http://host:port/management/coherence/cluster/metadata-catalog . See REST API for Managing Oracle Coherence for full details on each of the endpoints. For more details on enabling Management over ReST including enabling SSL, please see the Coherence Operator documentation . See the Coherence Management documentation for more information. Note: Use of Management over ReST is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. 1. Install a Coherence cluster with Management over ReST enabled Deploy a simple management enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: management-cluster spec: role: storage replicas: 3 coherence: management: enabled: true ports: - name: management port: 30000 Indicates to enable Management over ReST The management port must be added to the additional ports list so that it is exposed on a service The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f management-cluster.yaml coherencecluster.coherence.oracle.com/management-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=management-cluster NAME READY STATUS RESTARTS AGE management-cluster-storage-0 1/1 Running 0 36s management-cluster-storage-1 1/1 Running 0 36s management-cluster-storage-2 1/1 Running 0 36s 2. Port-forward the Management over ReST port <markup lang=\"bash\" >kubectl -n coherence-example port-forward management-cluster-storage-0 30000:30000 Forwarding from [::1]:30000 -&gt; 30000 Forwarding from 127.0.0.1:30000 -&gt; 30000 3. Access the REST endpoint Issue the following to access the ReST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/ | jq <markup lang=\"json\" >{ \"links\": [ { \"rel\": \"parent\", \"href\": \"http://127.0.0.1:30000/management/coherence\" }, { \"rel\": \"self\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"canonical\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"services\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/services\" }, { \"rel\": \"caches\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/caches\" }, { \"rel\": \"members\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/members\" }, { \"rel\": \"management\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/management\" }, { \"rel\": \"journal\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/journal\" }, { \"rel\": \"hotcache\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/hotcache\" }, { \"rel\": \"reporters\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\": \"webApplications\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/webApplications\" } ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2019-10-15T03:55:46.461Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"12.2.1.4.0\", \"running\": true, \"clusterName\": \"management-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2019-10-15 03:46:15.848, Address=10.1.2.184:36531, MachineId=49519, Location=site:coherence.coherence-example.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-storage-1, Role=storage)\", \"Member(Id=2, Timestamp=2019-10-15 03:46:19.405, Address=10.1.2.183:40341, MachineId=49519, Location=site:coherence.coherence-example.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-storage-2, Role=storage)\", \"Member(Id=3, Timestamp=2019-10-15 03:46:19.455, Address=10.1.2.185:38719, MachineId=49519, Location=site:coherence.coherence-example.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-storage-0, Role=storage)\" ], \"type\": \"Cluster\" } The jq utility is used to format the JSON, and may not be available on all platforms. 3. Access the Swagger endpoint Issue the following to access the Sagger endpoint which documents all the API&#8217;s available. <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog | jq <markup lang=\"json\" >{ \"swagger\": \"2.0\", \"info\": { \"title\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"description\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"version\": \"12.2.1.4.0\" }, \"schemes\": [ \"http\", \"https\" ], ... The above output has been truncated due to the large size. 4. Other Resources Management over ReST can be used for all management functions, as one would with standard MBean access over JMX. Please see the Coherence REST API for more information on these features. Connecting JVisualVM to Management over ReST Enabling SSL Produce and extract a Java Flight Recorder (JFR) file Access the Reporter 5. Clean Up After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f management-cluster.yaml Stop the port-forward command using CTRL-C . ",
            "title": "Management over ReST"
        },
        {
            "location": "/management/030_heapdump",
            "text": " Some of the debugging techniques described in Debugging in Coherence require the creation of files, such as log files and JVM heap dumps, for analysis. You can also create and extract these files in the Coherence Operator. ",
            "title": "preambule"
        },
        {
            "location": "/management/030_heapdump",
            "text": " Deploy a simple CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"heapdump-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: heapdump-cluster spec: role: storage replicas: 3 Add an imagePullSecrets entry if required to pull images from a private repository. <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f heapdump-cluster.yaml coherencecluster.coherence.oracle.com/heapdump-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=heapdump-cluster NAME READY STATUS RESTARTS AGE heapdump-cluster-storage-0 1/1 Running 0 59s heapdump-cluster-storage-1 1/1 Running 0 59s heapdump-cluster-storage-2 1/1 Running 0 59s ",
            "title": "1. Install a Coherence Cluster"
        },
        {
            "location": "/management/030_heapdump",
            "text": " Obtain the PID of the Coherence process. Generally, the PID is 1. You can also use jps to get the actual PID. <markup lang=\"bash\" >kubectl exec -it -n coherence-example heapdump-cluster-storage-0 -- bash $ jps 1 Main 153 Jps The process with Main is the main process that calls DefaultCacheServer to start a cluster node. ",
            "title": "2. Obtain the PID of the Coherence process"
        },
        {
            "location": "/management/030_heapdump",
            "text": "<markup lang=\"bash\" >$ rm -f /tmp/heap.hprof $ /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof $ exit ",
            "title": "3. Use the jcmd command to extract the heap dump"
        },
        {
            "location": "/management/030_heapdump",
            "text": "<markup lang=\"bash\" >kubectl cp &lt;namespace&gt;/heapdump-cluster-storage-0:/tmp/heap.hprof heap.hprof tar: Removing leading `/' from member names ls -l heap.hprof -rw-r--r-- 1 user staff 21113314 15 Oct 08:50 heap.hprof Depending upon whether the Kubernetes cluster is local or remote, this might take some time. ",
            "title": "4. Copy the heap dump to local machine"
        },
        {
            "location": "/management/030_heapdump",
            "text": " Assuming that the Coherence PID is 1, you can use this repeatable single-command technique to extract the heap dump: <markup lang=\"bash\" >(kubectl exec heapdump-cluster-storage-0 -n &lt;namespace&gt; -- /bin/bash -c \\ \"rm -f /tmp/heap.hprof; /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof; cat /tmp/heap.hprof &gt; /dev/stderr\" ) 2&gt; heap.hprof ",
            "title": "5. Single command usage"
        },
        {
            "location": "/management/030_heapdump",
            "text": " After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f heapdump-cluster.yaml ",
            "title": "6. Clean Up"
        },
        {
            "location": "/management/030_heapdump",
            "text": " This example shows how to collect a .hprof file for a heap dump. A single-command technique is also included at the end of this sample. Coherence Pods are configured to produce a heap dump on OOM error by default. See Configure The JVM for more information. You cal also trigger a heap dump via the Management over ReST API . 1. Install a Coherence Cluster Deploy a simple CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"heapdump-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: heapdump-cluster spec: role: storage replicas: 3 Add an imagePullSecrets entry if required to pull images from a private repository. <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f heapdump-cluster.yaml coherencecluster.coherence.oracle.com/heapdump-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=heapdump-cluster NAME READY STATUS RESTARTS AGE heapdump-cluster-storage-0 1/1 Running 0 59s heapdump-cluster-storage-1 1/1 Running 0 59s heapdump-cluster-storage-2 1/1 Running 0 59s 2. Obtain the PID of the Coherence process Obtain the PID of the Coherence process. Generally, the PID is 1. You can also use jps to get the actual PID. <markup lang=\"bash\" >kubectl exec -it -n coherence-example heapdump-cluster-storage-0 -- bash $ jps 1 Main 153 Jps The process with Main is the main process that calls DefaultCacheServer to start a cluster node. 3. Use the jcmd command to extract the heap dump <markup lang=\"bash\" >$ rm -f /tmp/heap.hprof $ /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof $ exit 4. Copy the heap dump to local machine <markup lang=\"bash\" >kubectl cp &lt;namespace&gt;/heapdump-cluster-storage-0:/tmp/heap.hprof heap.hprof tar: Removing leading `/' from member names ls -l heap.hprof -rw-r--r-- 1 user staff 21113314 15 Oct 08:50 heap.hprof Depending upon whether the Kubernetes cluster is local or remote, this might take some time. 5. Single command usage Assuming that the Coherence PID is 1, you can use this repeatable single-command technique to extract the heap dump: <markup lang=\"bash\" >(kubectl exec heapdump-cluster-storage-0 -n &lt;namespace&gt; -- /bin/bash -c \\ \"rm -f /tmp/heap.hprof; /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof; cat /tmp/heap.hprof &gt; /dev/stderr\" ) 2&gt; heap.hprof 6. Clean Up After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f heapdump-cluster.yaml ",
            "title": "Produce and extract a heap dump"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": " Whilst it is simple to deploy a Coherence cluster into Kubernetes in most cases there is also a requirement to add application code and configuration to the Coherence JVMs class path. ",
            "title": "preambule"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": " In your working directory directory create a file called Dockerfile with the following contents: <markup lang=\"dockerfile\" >FROM scratch COPY files/lib/ /app/lib/ COPY files/conf/ /app/conf/ ",
            "title": "2. Create the Dockerfile"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": "<markup lang=\"bash\" >mkdir -p files/lib files/conf Add the following content to a file in /files/conf called storage-cache-config.xml . <markup lang=\"xml\" >&lt;?xml version='1.0'?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;ExamplePartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "3. Add the required config files"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": " In your working directory , issue the following: <markup lang=\"bash\" >docker build -t packaging-example:1.0.0 . Step 1/3 : FROM scratch ---&gt; Step 2/3 : COPY files/lib/ /app/lib/ ---&gt; c91db5a34f5c Step 3/3 : COPY files/conf/ /app/conf/ ---&gt; 7dd0b5f3e37a Successfully built 7dd0b5f3e37a Successfully tagged packaging-example:1.0.0 In this example we have created but not populated the lib directory which would be used for application classes. ",
            "title": "4. Build the Docker image"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": " Create the file packaging-cluster.yaml with the following contents. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: packaging-cluster spec: replicas: 3 coherence: cacheConfig: storage-cache-config.xml application: image: packaging-example:1.0.0 Add an imagePullSecrets entry if required to pull images from a private repository. ",
            "title": "5. Create the Coherence cluster yaml"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": " Issue the following to install the cluster: <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f packaging-cluster.yaml coherencecluster.coherence.oracle.com/packaging-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=packaging-cluster NAME READY STATUS RESTARTS AGE packaging-cluster-storage-0 1/1 Running 0 58s packaging-cluster-storage-1 1/1 Running 0 58s packaging-cluster-storage-2 1/1 Running 0 58s ",
            "title": "6. Install the Coherence Cluster"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; packaging-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt, type cache test and you will notice the following indicating your cache configuration file with the service name of ExamplePartitionedCache is being loaded. <markup lang=\"bash\" >... Cache Configuration: test SchemeName: server AutoStart: true ServiceName: ExamplePartitionedCache .. ",
            "title": "7. Add Data to the Coherence Cluster via the Console"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": "<markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f packaging-cluster.yaml coherencecluster.coherence.oracle.com \"packaging-cluster\" deleted ",
            "title": "8. Uninstall the Coherence Cluster"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": " Install the Coherence Operator Create any secrets required to pull Docker images Create a new working directory and change to that directory 2. Create the Dockerfile In your working directory directory create a file called Dockerfile with the following contents: <markup lang=\"dockerfile\" >FROM scratch COPY files/lib/ /app/lib/ COPY files/conf/ /app/conf/ 3. Add the required config files <markup lang=\"bash\" >mkdir -p files/lib files/conf Add the following content to a file in /files/conf called storage-cache-config.xml . <markup lang=\"xml\" >&lt;?xml version='1.0'?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;ExamplePartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; 4. Build the Docker image In your working directory , issue the following: <markup lang=\"bash\" >docker build -t packaging-example:1.0.0 . Step 1/3 : FROM scratch ---&gt; Step 2/3 : COPY files/lib/ /app/lib/ ---&gt; c91db5a34f5c Step 3/3 : COPY files/conf/ /app/conf/ ---&gt; 7dd0b5f3e37a Successfully built 7dd0b5f3e37a Successfully tagged packaging-example:1.0.0 In this example we have created but not populated the lib directory which would be used for application classes. 5. Create the Coherence cluster yaml Create the file packaging-cluster.yaml with the following contents. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: packaging-cluster spec: replicas: 3 coherence: cacheConfig: storage-cache-config.xml application: image: packaging-example:1.0.0 Add an imagePullSecrets entry if required to pull images from a private repository. 6. Install the Coherence Cluster Issue the following to install the cluster: <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f packaging-cluster.yaml coherencecluster.coherence.oracle.com/packaging-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=packaging-cluster NAME READY STATUS RESTARTS AGE packaging-cluster-storage-0 1/1 Running 0 58s packaging-cluster-storage-1 1/1 Running 0 58s packaging-cluster-storage-2 1/1 Running 0 58s 7. Add Data to the Coherence Cluster via the Console <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; packaging-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt, type cache test and you will notice the following indicating your cache configuration file with the service name of ExamplePartitionedCache is being loaded. <markup lang=\"bash\" >... Cache Configuration: test SchemeName: server AutoStart: true ServiceName: ExamplePartitionedCache .. 8. Uninstall the Coherence Cluster <markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f packaging-cluster.yaml coherencecluster.coherence.oracle.com \"packaging-cluster\" deleted ",
            "title": "1. Prerequisites"
        },
        {
            "location": "/app-deployment/020_packaging",
            "text": " A common scenario for Coherence deployments is to include specific user artefacts such as cache and operational configuration files as well as user classes. This can be achieved with Coherence Operator by specifying the application configuration in the application section of the spec or for an individual role. The image field specifies a Docker image from which the configuration and classes are copied and added to the JVM classpath at runtime. The libDir and configDir are optional fields below application and are described below: libDir - contains application classes, default value is /app/lib configDir - contains cache and operational configuration files, default value is /app/conf The example yaml below instructs the Coherence Operator to attach a Docker image called acme/orders-data:1.0.0 at Pod startup and copy the artefacts in the libDir and configDir to the Pod and add to the JVM classpath. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 Since we are using the default directories, we would expect that the Docker image referenced above would include two directories /app/lib and /app/conf containing the appropriate files. The following Dockerfile could be used to create such an image, assuming the the directories files/lib and files/conf contain the files to copy. <markup lang=\"dockerfile\" >FROM scratch COPY files/lib/ /app/lib/ COPY files/conf/ /app/conf/ Is is recommended to use the scratch image in the FROM clause to minimize the size of the resultant image. See the Coherence Applications section for full details on each of the fields in the application section. 1. Prerequisites Install the Coherence Operator Create any secrets required to pull Docker images Create a new working directory and change to that directory 2. Create the Dockerfile In your working directory directory create a file called Dockerfile with the following contents: <markup lang=\"dockerfile\" >FROM scratch COPY files/lib/ /app/lib/ COPY files/conf/ /app/conf/ 3. Add the required config files <markup lang=\"bash\" >mkdir -p files/lib files/conf Add the following content to a file in /files/conf called storage-cache-config.xml . <markup lang=\"xml\" >&lt;?xml version='1.0'?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;ExamplePartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; 4. Build the Docker image In your working directory , issue the following: <markup lang=\"bash\" >docker build -t packaging-example:1.0.0 . Step 1/3 : FROM scratch ---&gt; Step 2/3 : COPY files/lib/ /app/lib/ ---&gt; c91db5a34f5c Step 3/3 : COPY files/conf/ /app/conf/ ---&gt; 7dd0b5f3e37a Successfully built 7dd0b5f3e37a Successfully tagged packaging-example:1.0.0 In this example we have created but not populated the lib directory which would be used for application classes. 5. Create the Coherence cluster yaml Create the file packaging-cluster.yaml with the following contents. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: packaging-cluster spec: replicas: 3 coherence: cacheConfig: storage-cache-config.xml application: image: packaging-example:1.0.0 Add an imagePullSecrets entry if required to pull images from a private repository. 6. Install the Coherence Cluster Issue the following to install the cluster: <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f packaging-cluster.yaml coherencecluster.coherence.oracle.com/packaging-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=packaging-cluster NAME READY STATUS RESTARTS AGE packaging-cluster-storage-0 1/1 Running 0 58s packaging-cluster-storage-1 1/1 Running 0 58s packaging-cluster-storage-2 1/1 Running 0 58s 7. Add Data to the Coherence Cluster via the Console <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; packaging-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt, type cache test and you will notice the following indicating your cache configuration file with the service name of ExamplePartitionedCache is being loaded. <markup lang=\"bash\" >... Cache Configuration: test SchemeName: server AutoStart: true ServiceName: ExamplePartitionedCache .. 8. Uninstall the Coherence Cluster <markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f packaging-cluster.yaml coherencecluster.coherence.oracle.com \"packaging-cluster\" deleted ",
            "title": "Introduction"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " If required, you can scrape the metrics from your own Prometheus Operator instance rather than using the prometheusopeartor subchart included with the Coherence Operator. ",
            "title": "preambule"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ coherence/coherence-operator Set &lt;namespace&gt; to the Kubernetes namespace that the Coherence Operator should be installed into. After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should only show the Coherence Operator. <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s ",
            "title": "1. Install the Coherence Operator with Prometheus disabled"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " Id you do not already have a Prometheus environment installed, you can use the Prometheus Operator chart from https://github.com/helm/charts/tree/master/stable/prometheus-operator using the following: <markup lang=\"bash\" >helm install stable/prometheus-operator --namespace &lt;namespace&gt; --name prometheus \\ --set prometheusOperator.createCustomResource=false ",
            "title": "2. Install Prometheus Operator (Optional)"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " Create a ServiceMonitor with the following configuration to instruct Prometheus to scrape the Coherence Pods. <markup lang=\"yaml\" title=\"service-monitor.yaml\" >apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: monitoring-coherence namespace: coherence-example labels: release: prometheus spec: selector: matchLabels: component: coherencePod endpoints: - port: metrics interval: 30s namespaceSelector: matchNames: - coherence-example Match the Prometheus Operator release name Scrape all Pods that match component= coherencePod The metrics Pod to Scrape The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt;-f service-monitor.yaml See the Prometheus Operator documentation for more information on ServiceMonitors usage. ",
            "title": "3. Create a ServiceMonitor"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " Now that Prometheus is running Coherence clusters can be created that expose metrics on a port on each Pod . Deploy a simple metrics enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: metrics-cluster spec: role: storage replicas: 3 coherence: metrics: enabled: true port: 9612 ports: - name: metrics port: 9612 The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f metrics-cluster.yaml <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods NAME READY STATUS RESTARTS AGE alertmanager-prometheus-prometheus-oper-alertmanager-0 2/2 Running 0 51m coherence-operator-8465cf7d88-7hw4g 1/1 Running 0 81m metrics-cluster-storage-0 1/1 Running 0 12m metrics-cluster-storage-1 1/1 Running 0 12m metrics-cluster-storage-2 1/1 Running 0 12m prometheus-grafana-757f7c9f6d-brqvb 2/2 Running 0 51m prometheus-kube-state-metrics-5ffdf76ddd-86qg4 1/1 Running 0 51m prometheus-prometheus-node-exporter-4d9qx 1/1 Running 0 51m prometheus-prometheus-oper-operator-64cd6c6c45-5ql2k 2/2 Running 0 51m prometheus-prometheus-prometheus-oper-prometheus-0 3/3 Running 1 51m ",
            "title": "4. Install a Coherence Cluster with Metrics Enabled"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " Port-forward the Prometheus port using the following: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward prometheus-prometheus-prometheus-oper-prometheus-0 9090:9090 Forwarding from 127.0.0.1:9090 -&gt; 9090 Forwarding from [::1]:9090 -&gt; 9090 Access the following endpoint to confirm that Prometheus is scraping the pods: http://127.0.0.1:9090/targets The following should be displayed indicating the Coherence Pods are being scraped. ",
            "title": "5. Validate that Prometheus can see the Pods"
        },
        {
            "location": "/metrics/040_scraping",
            "text": "<markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f metrics-cluster.yaml coherencecluster.coherence.oracle.com \"metrics-cluster\" deleted helm delete prometheus --purge release \"prometheus\" deleted ",
            "title": "7. Uninstall the Coherence Cluster &amp; Prometheus"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " Port-forward the Grafana port using the following, replacing the grafana pod <markup lang=\"bash\" >kubectl port-forward $(kubectl get pod -n &lt;namespace&gt; -l app=grafana -o name) -n &lt;namespace&gt; 3000:3000 Forwarding from 127.0.0.1:9090 -&gt; 9090 Forwarding from [::1]:9090 -&gt; 9090 Access Grafana via the following URL: http://127.0.0.1:3000/ The Grafana credentials are username admin password prom-operator Once logged in, highlight the + icon and select Import . Import all of the Grafana dashboards from the following location: https://github.com/oracle/coherence-operator/helm-charts/coherence-operator/dashboards Once all the dashboards have been loaded, you can access the Main dasbohard via: http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main 7. Uninstall the Coherence Cluster &amp; Prometheus <markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f metrics-cluster.yaml coherencecluster.coherence.oracle.com \"metrics-cluster\" deleted helm delete prometheus --purge release \"prometheus\" deleted ",
            "title": "6. Access Grafana and Load the Dashboards"
        },
        {
            "location": "/metrics/040_scraping",
            "text": " Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. This example shows you how to scrape metrics from your own Prometheus instance. 1. Install the Coherence Operator with Prometheus disabled A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ coherence/coherence-operator Set &lt;namespace&gt; to the Kubernetes namespace that the Coherence Operator should be installed into. After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should only show the Coherence Operator. <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s 2. Install Prometheus Operator (Optional) Id you do not already have a Prometheus environment installed, you can use the Prometheus Operator chart from https://github.com/helm/charts/tree/master/stable/prometheus-operator using the following: <markup lang=\"bash\" >helm install stable/prometheus-operator --namespace &lt;namespace&gt; --name prometheus \\ --set prometheusOperator.createCustomResource=false 3. Create a ServiceMonitor Create a ServiceMonitor with the following configuration to instruct Prometheus to scrape the Coherence Pods. <markup lang=\"yaml\" title=\"service-monitor.yaml\" >apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: monitoring-coherence namespace: coherence-example labels: release: prometheus spec: selector: matchLabels: component: coherencePod endpoints: - port: metrics interval: 30s namespaceSelector: matchNames: - coherence-example Match the Prometheus Operator release name Scrape all Pods that match component= coherencePod The metrics Pod to Scrape The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt;-f service-monitor.yaml See the Prometheus Operator documentation for more information on ServiceMonitors usage. 4. Install a Coherence Cluster with Metrics Enabled Now that Prometheus is running Coherence clusters can be created that expose metrics on a port on each Pod . Deploy a simple metrics enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: metrics-cluster spec: role: storage replicas: 3 coherence: metrics: enabled: true port: 9612 ports: - name: metrics port: 9612 The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f metrics-cluster.yaml <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods NAME READY STATUS RESTARTS AGE alertmanager-prometheus-prometheus-oper-alertmanager-0 2/2 Running 0 51m coherence-operator-8465cf7d88-7hw4g 1/1 Running 0 81m metrics-cluster-storage-0 1/1 Running 0 12m metrics-cluster-storage-1 1/1 Running 0 12m metrics-cluster-storage-2 1/1 Running 0 12m prometheus-grafana-757f7c9f6d-brqvb 2/2 Running 0 51m prometheus-kube-state-metrics-5ffdf76ddd-86qg4 1/1 Running 0 51m prometheus-prometheus-node-exporter-4d9qx 1/1 Running 0 51m prometheus-prometheus-oper-operator-64cd6c6c45-5ql2k 2/2 Running 0 51m prometheus-prometheus-prometheus-oper-prometheus-0 3/3 Running 1 51m 5. Validate that Prometheus can see the Pods Port-forward the Prometheus port using the following: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward prometheus-prometheus-prometheus-oper-prometheus-0 9090:9090 Forwarding from 127.0.0.1:9090 -&gt; 9090 Forwarding from [::1]:9090 -&gt; 9090 Access the following endpoint to confirm that Prometheus is scraping the pods: http://127.0.0.1:9090/targets The following should be displayed indicating the Coherence Pods are being scraped. 6. Access Grafana and Load the Dashboards Port-forward the Grafana port using the following, replacing the grafana pod <markup lang=\"bash\" >kubectl port-forward $(kubectl get pod -n &lt;namespace&gt; -l app=grafana -o name) -n &lt;namespace&gt; 3000:3000 Forwarding from 127.0.0.1:9090 -&gt; 9090 Forwarding from [::1]:9090 -&gt; 9090 Access Grafana via the following URL: http://127.0.0.1:3000/ The Grafana credentials are username admin password prom-operator Once logged in, highlight the + icon and select Import . Import all of the Grafana dashboards from the following location: https://github.com/oracle/coherence-operator/helm-charts/coherence-operator/dashboards Once all the dashboards have been loaded, you can access the Main dasbohard via: http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main 7. Uninstall the Coherence Cluster &amp; Prometheus <markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f metrics-cluster.yaml coherencecluster.coherence.oracle.com \"metrics-cluster\" deleted helm delete prometheus --purge release \"prometheus\" deleted ",
            "title": "Scraping metrics from your own Prometheus instance"
        },
        {
            "location": "/developer/06_debugging",
            "text": " To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >make debug-stop ",
            "title": "Stopping the Debug Session"
        },
        {
            "location": "/developer/06_debugging",
            "text": " To debug the operator while running a particular tests first start the debugger as described above. Then use the debug make test target to execute the test. For example to debug the TestMinimalCoherenceCluster test first start the debug session: <markup lang=\"bash\" >make run-debug Then execute the test with the debug-e2e-local-test make target: <markup lang=\"bash\" >make debug-e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' ",
            "title": "Debugging Tests"
        },
        {
            "location": "/developer/06_debugging",
            "text": " Assuming that you have an IDE capable of debugging Go and have delve installed you can debug the operator. When debugging an instance of the operator is run locally so functionality that will only work when the operator is deployed into k8s cannot be properly debugged. To start an instance of the operator that can be debugged use the make target run-debug , for example: <markup lang=\"bash\" >make run-debug This will start the operator and listen for a debugger to connect on the default delve port 2345 . The operator will connect to whichever k8s cluster the current environment is configured to point to. Stopping the Debug Session To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >make debug-stop Debugging Tests To debug the operator while running a particular tests first start the debugger as described above. Then use the debug make test target to execute the test. For example to debug the TestMinimalCoherenceCluster test first start the debug session: <markup lang=\"bash\" >make run-debug Then execute the test with the debug-e2e-local-test make target: <markup lang=\"bash\" >make debug-e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' ",
            "title": "Debugging the Coherence Operator"
        },
        {
            "location": "/logging/020_logging",
            "text": " The Coherence Operator manages data logging through the Elasticsearch, Fluentd and Kibana (EFK) stack. ",
            "title": "preambule"
        },
        {
            "location": "/logging/020_logging",
            "text": " To enable the EFK stack, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set installEFK=true A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set installEFK=true \\ coherence/coherence-operator After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-66c6d868b9-rd429 1/1 Running 0 8m coherence-operator-grafana-8454698bcf-v5kxw 2/2 Running 0 8m coherence-operator-kube-state-metrics-6dc8675d87-qnfdw 1/1 Running 0 8m coherence-operator-prometh-operator-58d94ffbb8-94d4m 1/1 Running 0 8m coherence-operator-prometheus-node-exporter-vpjjt 1/1 Running 0 8m elasticsearch-f978d6fdd-dw7qg 1/1 Running 0 8m kibana-9964496fd-5tpv9 1/1 Running 0 8m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 0 8m The Elasticsearch Pod The Kibana Pod ",
            "title": "1. Install the Coherence Operator with Fluentd logging enabled"
        },
        {
            "location": "/logging/020_logging",
            "text": " Deploy a simple logging enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"logging-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: logging-cluster spec: role: storage replicas: 3 logging: fluentd: enabled: true Enables log capture via Fluentd The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f logging-cluster.yaml coherencecluster.coherence.oracle.com/logging-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=logging-cluster NAME READY STATUS RESTARTS AGE logging-cluster-storage-0 2/2 Running 0 86s logging-cluster-storage-1 2/2 Running 0 86s logging-cluster-storage-2 2/2 Running 0 86s Notice that under the Ready column it shows 2/2 . This means that there are two containers for this Pod, Coherence and Fluentd, and they are both ready. The Fluentd container will capture the logs, parse them and send them to Elasticsearch. Kibana can then be used to view the logs. ",
            "title": "2. Install a Coherence Cluster with Logging Enabled"
        },
        {
            "location": "/logging/020_logging",
            "text": " First find the Kibana Pod : <markup lang=\"bash\" >kubectl -n coherence-example get pod -l component=kibana -o name Using the Pod name use kubectl to create a port forward session to the Kibana Pod so that the Kibana API on port 5601 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l component=kibana -o name) \\ 5601:5601 Forwarding from [::1]:5601 -&gt; 5601 Forwarding from 127.0.0.1:5601 -&gt; 5601 ",
            "title": "3. Port-forward the Kibana pod"
        },
        {
            "location": "/logging/020_logging",
            "text": " Coherence Cluster - All Messages : Shows all messages Coherence Cluster - Errors and Warnings : Shows errors and warning messages only Coherence Cluster - Persistence : Shows Persistence related messages Coherence Cluster - Configuration Messages: Shows configuration related messages Coherence Cluster - Network : Shows network related messages, such as communication delays and TCP ring disconnects Coherence Cluster - Partitions : Shows partition transfer and loss messages Coherence Cluster - Message Sources : Shows the source (thread) for messages ",
            "title": "Default Dashboards"
        },
        {
            "location": "/logging/020_logging",
            "text": " There are many searches related to common Coherence messages, warnings, and errors that are loaded and can be accessed via the Discover side-bar and selecting `Open . See here for more information on the default dashboards and searches. ",
            "title": "Default Queries"
        },
        {
            "location": "/logging/020_logging",
            "text": " Access Kibana using the following URL: http://127.0.0.1:5601/ It may take approximately 2-3 minutes for the first logs to reach the Elasticsearch instance. Default Dashboards Coherence Cluster - All Messages : Shows all messages Coherence Cluster - Errors and Warnings : Shows errors and warning messages only Coherence Cluster - Persistence : Shows Persistence related messages Coherence Cluster - Configuration Messages: Shows configuration related messages Coherence Cluster - Network : Shows network related messages, such as communication delays and TCP ring disconnects Coherence Cluster - Partitions : Shows partition transfer and loss messages Coherence Cluster - Message Sources : Shows the source (thread) for messages Default Queries There are many searches related to common Coherence messages, warnings, and errors that are loaded and can be accessed via the Discover side-bar and selecting `Open . See here for more information on the default dashboards and searches. ",
            "title": "4. Access the Kibana Application UI"
        },
        {
            "location": "/logging/020_logging",
            "text": " After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f logging-cluster.yaml ",
            "title": "4. Clean Up"
        },
        {
            "location": "/logging/020_logging",
            "text": " This example shows how to enable log capture and access the Kibana user interface (UI) to view the captured logs. Logs are scraped via a Fluentd sidecar image, parsed and sent to Elasticsearch. A default index pattern called coherence-cluster-* is created which holds all captured logs. 1. Install the Coherence Operator with Fluentd logging enabled To enable the EFK stack, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set installEFK=true A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set installEFK=true \\ coherence/coherence-operator After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-66c6d868b9-rd429 1/1 Running 0 8m coherence-operator-grafana-8454698bcf-v5kxw 2/2 Running 0 8m coherence-operator-kube-state-metrics-6dc8675d87-qnfdw 1/1 Running 0 8m coherence-operator-prometh-operator-58d94ffbb8-94d4m 1/1 Running 0 8m coherence-operator-prometheus-node-exporter-vpjjt 1/1 Running 0 8m elasticsearch-f978d6fdd-dw7qg 1/1 Running 0 8m kibana-9964496fd-5tpv9 1/1 Running 0 8m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 0 8m The Elasticsearch Pod The Kibana Pod 2. Install a Coherence Cluster with Logging Enabled Deploy a simple logging enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"logging-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: logging-cluster spec: role: storage replicas: 3 logging: fluentd: enabled: true Enables log capture via Fluentd The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f logging-cluster.yaml coherencecluster.coherence.oracle.com/logging-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=logging-cluster NAME READY STATUS RESTARTS AGE logging-cluster-storage-0 2/2 Running 0 86s logging-cluster-storage-1 2/2 Running 0 86s logging-cluster-storage-2 2/2 Running 0 86s Notice that under the Ready column it shows 2/2 . This means that there are two containers for this Pod, Coherence and Fluentd, and they are both ready. The Fluentd container will capture the logs, parse them and send them to Elasticsearch. Kibana can then be used to view the logs. 3. Port-forward the Kibana pod First find the Kibana Pod : <markup lang=\"bash\" >kubectl -n coherence-example get pod -l component=kibana -o name Using the Pod name use kubectl to create a port forward session to the Kibana Pod so that the Kibana API on port 5601 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l component=kibana -o name) \\ 5601:5601 Forwarding from [::1]:5601 -&gt; 5601 Forwarding from 127.0.0.1:5601 -&gt; 5601 4. Access the Kibana Application UI Access Kibana using the following URL: http://127.0.0.1:5601/ It may take approximately 2-3 minutes for the first logs to reach the Elasticsearch instance. Default Dashboards Coherence Cluster - All Messages : Shows all messages Coherence Cluster - Errors and Warnings : Shows errors and warning messages only Coherence Cluster - Persistence : Shows Persistence related messages Coherence Cluster - Configuration Messages: Shows configuration related messages Coherence Cluster - Network : Shows network related messages, such as communication delays and TCP ring disconnects Coherence Cluster - Partitions : Shows partition transfer and loss messages Coherence Cluster - Message Sources : Shows the source (thread) for messages Default Queries There are many searches related to common Coherence messages, warnings, and errors that are loaded and can be accessed via the Discover side-bar and selecting `Open . See here for more information on the default dashboards and searches. 4. Clean Up After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f logging-cluster.yaml ",
            "title": "Capturing and viewing Coherence cluster Logs"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The CoherenceCluster CRD is the main CRD that defines what a Coherence cluster looks like. This is the CRD that a customer creates and manges through the normal kubernetes commands and APIs. A CoherenceCluster is made up of one or more roles. Each role defines a sub-set of the members of a Coherence cluster (or all of the members in the case of a cluster with a single role). The yaml for the CoherenceCluster CRD is in the file deploy/crds/coherence_v1_coherencecluster_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceCluster struct in the pkg/apis/coherence/v1/coherencecluster_types.go source file. ",
            "title": "CoherenceCluster CRD"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The CoherenceRole CRD is a definition of a role within a CoherenceCluster. A role is a sub-set of the members of a cluster that all share the same configuration. A customer should not interact directly with a CoherenceRole other than when scaling (for example using kubectl scale commands). The reason that a cluster is split into roles represented by a different CRD is to allow more fine grained control over different parts of the cluster, especially for operations such as scaling. By having a separate CRD for a role allows a customer to update or scale each role individually. The yaml for the CoherenceRole CRD is in the file deploy/crds/coherence_v1_coherencerole_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceRole struct in the pkg/apis/coherence/v1/coherencerole_types.go source file. ",
            "title": "CoherenceRole CRD"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The CoherenceInternal CRD is (as the name suggests) entirely internal to the Coherence Operator and a customer should not interact with it at all. The CoherenceInternal CRD is a representation of the values file used to install the Coherence Helm chart. The yaml for the CoherenceInternal CRD is in the file deploy/crds/coherence_v1_coherenceinternal_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceInternal struct in the pkg/apis/coherence/v1/coherenceinternal_types.go source file. ",
            "title": "CoherenceInternal CRD"
        },
        {
            "location": "/developer/03_high_level",
            "text": " In Kubernetes a CRD is a yaml (or json) file that defines the structure of a custom resource. When building operators using the Operator SDK the yaml files are not edited directly, they are generated from the Go structs in the source code. The Coherence Operator has three CRDs: CoherenceCluster CoherenceRole CoherenceInternal CoherenceCluster CRD The CoherenceCluster CRD is the main CRD that defines what a Coherence cluster looks like. This is the CRD that a customer creates and manges through the normal kubernetes commands and APIs. A CoherenceCluster is made up of one or more roles. Each role defines a sub-set of the members of a Coherence cluster (or all of the members in the case of a cluster with a single role). The yaml for the CoherenceCluster CRD is in the file deploy/crds/coherence_v1_coherencecluster_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceCluster struct in the pkg/apis/coherence/v1/coherencecluster_types.go source file. CoherenceRole CRD The CoherenceRole CRD is a definition of a role within a CoherenceCluster. A role is a sub-set of the members of a cluster that all share the same configuration. A customer should not interact directly with a CoherenceRole other than when scaling (for example using kubectl scale commands). The reason that a cluster is split into roles represented by a different CRD is to allow more fine grained control over different parts of the cluster, especially for operations such as scaling. By having a separate CRD for a role allows a customer to update or scale each role individually. The yaml for the CoherenceRole CRD is in the file deploy/crds/coherence_v1_coherencerole_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceRole struct in the pkg/apis/coherence/v1/coherencerole_types.go source file. CoherenceInternal CRD The CoherenceInternal CRD is (as the name suggests) entirely internal to the Coherence Operator and a customer should not interact with it at all. The CoherenceInternal CRD is a representation of the values file used to install the Coherence Helm chart. The yaml for the CoherenceInternal CRD is in the file deploy/crds/coherence_v1_coherenceinternal_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceInternal struct in the pkg/apis/coherence/v1/coherenceinternal_types.go source file. ",
            "title": "Custom Resource Definitions (CRDs)"
        },
        {
            "location": "/developer/03_high_level",
            "text": " To modify the contents of a CRD (for example to add a new field) the corresponding Go struct needs to be updated. For backwards compatibility between released versions we should ensure that we do not delete fields. After any of the structs have been modified the new CRD files need to be generated, this is done by running the Operator SDK generator using the Makefile. If the generate step is not run the code will not work properly. <markup lang=\"bash\" >make generate ",
            "title": "Modifying CRDs"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The Coherence Operator has been built using the Operator SDK and hence the design is based on how the framework works. Custom Resource Definitions (CRDs) In Kubernetes a CRD is a yaml (or json) file that defines the structure of a custom resource. When building operators using the Operator SDK the yaml files are not edited directly, they are generated from the Go structs in the source code. The Coherence Operator has three CRDs: CoherenceCluster CoherenceRole CoherenceInternal CoherenceCluster CRD The CoherenceCluster CRD is the main CRD that defines what a Coherence cluster looks like. This is the CRD that a customer creates and manges through the normal kubernetes commands and APIs. A CoherenceCluster is made up of one or more roles. Each role defines a sub-set of the members of a Coherence cluster (or all of the members in the case of a cluster with a single role). The yaml for the CoherenceCluster CRD is in the file deploy/crds/coherence_v1_coherencecluster_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceCluster struct in the pkg/apis/coherence/v1/coherencecluster_types.go source file. CoherenceRole CRD The CoherenceRole CRD is a definition of a role within a CoherenceCluster. A role is a sub-set of the members of a cluster that all share the same configuration. A customer should not interact directly with a CoherenceRole other than when scaling (for example using kubectl scale commands). The reason that a cluster is split into roles represented by a different CRD is to allow more fine grained control over different parts of the cluster, especially for operations such as scaling. By having a separate CRD for a role allows a customer to update or scale each role individually. The yaml for the CoherenceRole CRD is in the file deploy/crds/coherence_v1_coherencerole_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceRole struct in the pkg/apis/coherence/v1/coherencerole_types.go source file. CoherenceInternal CRD The CoherenceInternal CRD is (as the name suggests) entirely internal to the Coherence Operator and a customer should not interact with it at all. The CoherenceInternal CRD is a representation of the values file used to install the Coherence Helm chart. The yaml for the CoherenceInternal CRD is in the file deploy/crds/coherence_v1_coherenceinternal_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceInternal struct in the pkg/apis/coherence/v1/coherenceinternal_types.go source file. Modifying CRDs To modify the contents of a CRD (for example to add a new field) the corresponding Go struct needs to be updated. For backwards compatibility between released versions we should ensure that we do not delete fields. After any of the structs have been modified the new CRD files need to be generated, this is done by running the Operator SDK generator using the Makefile. If the generate step is not run the code will not work properly. <markup lang=\"bash\" >make generate ",
            "title": "High Level Design"
        },
        {
            "location": "/clusters/100_logging",
            "text": " There are various settings in a Coherence role that control different aspects of logging, including the Coherence log level, configuration files and whether Fluentd log capture is enabled. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The Coherence log level is set with the coherence.logLevel field. This field is an integer value between zero and nine (see the Coherence documentation for a fuller explanation). To set the Coherence log level when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: logLevel: 5 The implicit role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: logLevel: 9 - role: proxy coherence: logLevel: 5 The data role will have a Coherence log level of 9 The proxy role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: logLevel: 9 roles: - role: data - role: proxy - role: web coherence: logLevel: 5 The data and proxy roles will use the default Coherence log level of 9 The web role overrides the default Coherence log level setting it to 5 ",
            "title": "Coherence Log Level"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The default logging configuration for Coherence clusters started by the Coherence Operator is to set Coherence to used JDK logging; the JDK logger is then configured with a configuration file. The default configuration file is embedded into the Pod by the Coherence Operator but this default my be overridden; for example an application deployed into the cluster may require different logging configurations. The name of the file is provided in the logging.configFile field. The logging configuration file must be available to the JVM when it starts, either by providing it in application code or by mounting a volume containing the file, or by using a ConfigMap . To set the logging configuration file when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties The implicit role will use the app-logging.properties logging configuration file To set the logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data logging: configFile: data-logging.properties - role: proxy logging: configFile: proxy-logging.properties The data role will use the data-logging.properties logging configuration file The proxy role will use the proxy-logging.properties logging configuration file To set a default logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties roles: - role: data - role: proxy - role: web logging: configFile: web-logging.properties The app-logging.properties logging configuration file is set as the default ans will be used by the data and proxy roles. The web role has a specific configuration file set and will use the web-logging.properties file ",
            "title": "Logging Config File"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The logging.ConfigMap field can be used to specify the name of a ConfigMap that contains the logging configuration file to use. The ConfigMap should exist in the same namespace as the Coherence cluster. TBD&#8230;&#8203; ",
            "title": "Logging ConfigMap"
        },
        {
            "location": "/clusters/100_logging",
            "text": " Logging configuration for a role is defined in the logging section of the role&#8217;s spec . There are a number of different fields used to configure different logging features. The logging configuration can be set at different places depending on whether the implicit role or explicit roles are being configured. See the in-depth logging guide for more details on configuring application logging. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: logLevel: 9 logging: configFile: app-logging.properties configMapName: logging-cm fluentd: enabled: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent configFile: fluentd-config.yaml tag: test-cluster The fields in the example above are described in detail in the following sections. Coherence Log Level The Coherence log level is set with the coherence.logLevel field. This field is an integer value between zero and nine (see the Coherence documentation for a fuller explanation). To set the Coherence log level when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: logLevel: 5 The implicit role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: logLevel: 9 - role: proxy coherence: logLevel: 5 The data role will have a Coherence log level of 9 The proxy role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: logLevel: 9 roles: - role: data - role: proxy - role: web coherence: logLevel: 5 The data and proxy roles will use the default Coherence log level of 9 The web role overrides the default Coherence log level setting it to 5 Logging Config File The default logging configuration for Coherence clusters started by the Coherence Operator is to set Coherence to used JDK logging; the JDK logger is then configured with a configuration file. The default configuration file is embedded into the Pod by the Coherence Operator but this default my be overridden; for example an application deployed into the cluster may require different logging configurations. The name of the file is provided in the logging.configFile field. The logging configuration file must be available to the JVM when it starts, either by providing it in application code or by mounting a volume containing the file, or by using a ConfigMap . To set the logging configuration file when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties The implicit role will use the app-logging.properties logging configuration file To set the logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data logging: configFile: data-logging.properties - role: proxy logging: configFile: proxy-logging.properties The data role will use the data-logging.properties logging configuration file The proxy role will use the proxy-logging.properties logging configuration file To set a default logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties roles: - role: data - role: proxy - role: web logging: configFile: web-logging.properties The app-logging.properties logging configuration file is set as the default ans will be used by the data and proxy roles. The web role has a specific configuration file set and will use the web-logging.properties file Logging ConfigMap The logging.ConfigMap field can be used to specify the name of a ConfigMap that contains the logging configuration file to use. The ConfigMap should exist in the same namespace as the Coherence cluster. TBD&#8230;&#8203; ",
            "title": "Logging Configuration"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The Coherence Operator allows Coherence cluster Pods to be configured with a Fluentd side-car container that will push Coherence logs to Elasticsearch. The configuration for Fluentd is in the logging.fluentd section of the spec. TBD&#8230;&#8203; ",
            "title": "Fluentd Logging Configuration"
        },
        {
            "location": "/developer/07_execution",
            "text": " NOTE: The Coherence Operator by default runs in and monitors a single namespace. This is different behaviour to v1.0 of the Coherence Operator. For more details see the Operator SDK document on Operator Scope . ",
            "title": "Namespaces"
        },
        {
            "location": "/developer/07_execution",
            "text": " To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >./hack/kill-local.sh ",
            "title": "Stopping the Local Operator"
        },
        {
            "location": "/developer/07_execution",
            "text": " During development running the Coherence Operator locally is by far the simplest option as it is faster and it also allows remote debugging if you are using a suitable IDE. To run a local copy of the operator that will connect to whatever you local kubernetes config is pointing to: <markup lang=\"bash\" >make run Stopping the Local Operator To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >./hack/kill-local.sh ",
            "title": "Running Locally"
        },
        {
            "location": "/developer/07_execution",
            "text": " After running the operator the CRDs can be removed from the k8s cluster by running the make target: <markup lang=\"bash\" >make uninstall-crds ",
            "title": "Clean-up"
        },
        {
            "location": "/developer/07_execution",
            "text": " The simplest and most reliable way to deploy the operator to K8s is to use the Helm chart. After building the operator the chart is created in the build/_output/helm-charts/coherence-operator directory. Using the Helm chart will ensure that all of the required RBAC rules are created when deploying to an environment where RBAC is enabled. The chart can be installed in the usual way with Helm <markup lang=\"bash\" >helm install --name operator \\ --namespace operator-test \\ build/_output/helm-charts/coherence-operator ",
            "title": "Deploying to Kubernetes"
        },
        {
            "location": "/developer/07_execution",
            "text": " There are two ways to run the Coherence Operator, either deployed into a k8s cluster or by using the Operator SDK to run it locally on your dev machine (assuming your dev machine has access to a k8s cluster such as Docker Desktop on MacOS). Namespaces NOTE: The Coherence Operator by default runs in and monitors a single namespace. This is different behaviour to v1.0 of the Coherence Operator. For more details see the Operator SDK document on Operator Scope . Running Locally During development running the Coherence Operator locally is by far the simplest option as it is faster and it also allows remote debugging if you are using a suitable IDE. To run a local copy of the operator that will connect to whatever you local kubernetes config is pointing to: <markup lang=\"bash\" >make run Stopping the Local Operator To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >./hack/kill-local.sh Clean-up After running the operator the CRDs can be removed from the k8s cluster by running the make target: <markup lang=\"bash\" >make uninstall-crds Deploying to Kubernetes The simplest and most reliable way to deploy the operator to K8s is to use the Helm chart. After building the operator the chart is created in the build/_output/helm-charts/coherence-operator directory. Using the Helm chart will ensure that all of the required RBAC rules are created when deploying to an environment where RBAC is enabled. The chart can be installed in the usual way with Helm <markup lang=\"bash\" >helm install --name operator \\ --namespace operator-test \\ build/_output/helm-charts/coherence-operator ",
            "title": "Running Coherence Operator Development"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " Coherence persistence is a set of tools and technologies that manage the persistence and recovery of Coherence distributed caches. Cached data is persisted so that it can be quickly recovered after a catastrophic failure or after a cluster restart due to planned maintenance. See the main Coherence documentation The Coherence Operator supports configuring Coherence Persistence in two parts, snapshots and continuous persistence. Snapshots is the process of saving the state of caches to a named snapshot as a set of files on disc. This cache state can later be restored by reloading a named snapshot from disc - like a backup/restore operation. Continuous persistence is where Coherence continually writes the sate of caches to disc. When a Coherence cluster is stopped and restarted, either on purpose or due to a failue, the data on disc is automatically reloaded and the cache state is restored. Ideally, the storage used for persistence and snapshots is fast local storage such as SSD. When using stand-alone Coherence it is a simple process to manage local storage but when using Coherence in containers, and especially inside Kubernetes, managing storage is a more complex task when that storage needs to be persisted longer than the lifetime of the containers and re-attached to the containers if they are restarted. The Coherence Operator aims to make using Coherence persistence in Kubernetes simpler by allowing the more common use-cases to be easily configured. Each role in a CoherenceCluster resource maps to a Kubernetes StatefulSet . One of the advantages of StatefulSets is that they allow easy management of PersistentVolumeClaims which are ideal for use as storage for Coherence persistence as they have a lifetime outside of the Pods are are reattached to the StatefulSet Pods when they are restarted. ",
            "title": "Coherence Persistence"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When configuring a CoherenceCluster with a single implicit role the coherence.snapshots configuration is added directly to the spec section of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: snapshots: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi The implicit storage role has snapshots enabled and configured to use a PVC with custom StorageClass . ",
            "title": "Snapshots using Persistent Volumes for a Single Implicit Role"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When configuring a CoherenceCluster with one or more explicit roles the coherence.snapshots configuration is added directly to the configuration of each role in the roles list of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: snapshots: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi - role: proxy coherence: snapshots: enabled: false The data role has snapshots enabled and configured to use a PVC with custom StorageClass . The proxy role has snapshots explicitly disabled. ",
            "title": "Snapshots using Persistent Volumes for Explicit Roles"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When configuring a explicit roles in the roles list of a CoherenceCluster default values for the coherence.snapshots configuration can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: snapshots: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi roles: - role: data - role: proxy coherence: snapshots: enabled: false The default snapshots configuration is to enable snapshots using a PVC with custom StorageClass . The data role does not specify an explict snapshots configuration so it will use the defaults. The proxy role has snapshots explicitly disabled. ",
            "title": "Snapshots using Persistent Volumes for Explicit Roles with Defaults"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When managing Coherence clusters using the Coherence Operator the simples configuration is to write snapshots to a volume mapped to a PersistentVolumeClaim and to let the StatefulSet manage the PVCs . Snapshots are configured in the coherence.snapshots section of the role specification in the CoherenceCluster CRD. <markup lang=\"yaml\" >coherence: snapshots: enabled: true persistentVolumeClaim: # PVC spec... Snapshots should be enabled by setting the coherence.snapshots.enabled field to true. The persistentVolumeClaim section allows the PVC used for snapshot files to be configured. The default value for coherence.snapshots.enabled is false so no snapshot location will be configured for Coherence caches to use. If snapshots.enabled is either undefined or false it is still possible to use Coherence snapshot functionality but in this case snapshot files will be written to storage inside the Coherence container and will be lost if the container is shutdown. For example, if a Kubernetes cluster has a custom StorageClass named fast defined below: <markup lang=\"yaml\" >kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast provisioner: k8s.io/minikube-hostpath parameters: type: pd-ssd Then a CoherenceCluster can be created with snapshots enabled and configured to use the fast storage class: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: snapshots: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi The content of the persistentVolumeClaim is any valid yaml for defining a PersistentVolumeClaimSpec that would be allowed when configuring the spec section of a PVC in the volumeClaimTemplates section of a StatefulSet as described in the Kubernetes API documentation . Snapshots using Persistent Volumes for a Single Implicit Role When configuring a CoherenceCluster with a single implicit role the coherence.snapshots configuration is added directly to the spec section of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: snapshots: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi The implicit storage role has snapshots enabled and configured to use a PVC with custom StorageClass . Snapshots using Persistent Volumes for Explicit Roles When configuring a CoherenceCluster with one or more explicit roles the coherence.snapshots configuration is added directly to the configuration of each role in the roles list of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: snapshots: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi - role: proxy coherence: snapshots: enabled: false The data role has snapshots enabled and configured to use a PVC with custom StorageClass . The proxy role has snapshots explicitly disabled. Snapshots using Persistent Volumes for Explicit Roles with Defaults When configuring a explicit roles in the roles list of a CoherenceCluster default values for the coherence.snapshots configuration can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: snapshots: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi roles: - role: data - role: proxy coherence: snapshots: enabled: false The default snapshots configuration is to enable snapshots using a PVC with custom StorageClass . The data role does not specify an explict snapshots configuration so it will use the defaults. The proxy role has snapshots explicitly disabled. ",
            "title": "Managing Coherence Snapshots using Persistent Volumes"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " Although PersistentVolumeClaims are the recommended way to manage storage for Coherence snapshots the Coherence Operator also supports using standard Kubernetes Volumes as a storage mechanism. When using standard Kubernetes Volumes for snapshot storage it is important to ensure that CoherenceClusters are configured and managed in such a way that the same Volumes are reattached to Pods if clusters are restarted or if individual Pods are restarted or rescheduled by Kubernetes. If this is not done then snapshot data can be lost. There are many ways to accomplish this using particular Volume types or controlling Pod scheduling but this configuration is beyond the scope of this document and the relevant Kubernetes or storage provider documentation should be consulted. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: snapshots: enabled: true volume: hostPath: path: /data Snapshots storage is configured to use a hostPath volume mapped to the /data directory on the host As with configuring snapshots to use PersistentVolumeClaims configuring them to use Volumes can be done at different levels in the CoherenceCluster spec depending on whether there is a single implicit role, multiple explicit roles and default values to apply to explicit roles. ",
            "title": "Managing Coherence Snapshots using Standard Volumes"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When configuring a CoherenceCluster with a single implicit role the coherence.persistence configuration is added directly to the spec section of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: persistence: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi The implicit storage role has persistence enabled and configured to use a PVC with custom StorageClass . ",
            "title": "Persistence using Persistent Volumes for a Single Implicit Role"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When configuring a CoherenceCluster with one or more explicit roles the coherence.persistence configuration is added directly to the configuration of each role in the roles list of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: persistence: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi - role: proxy coherence: persistence: enabled: false The data role has persistence enabled and configured to use a PVC with custom StorageClass . The proxy role has persistence explicitly disabled. ",
            "title": "Persistence using Persistent Volumes for Explicit Roles"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When configuring a explicit roles in the roles list of a CoherenceCluster default values for the coherence.persistence configuration can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: persistence: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi roles: - role: data - role: proxy coherence: persistence: enabled: false The default persistence configuration is to enable persistence using a PVC with custom StorageClass . The data role does not specify an explict persistence configuration so it will use the defaults. The proxy role has persistence explicitly disabled. ",
            "title": "Persistence using Persistent Volumes for Explicit Roles with Defaults"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " When managing Coherence clusters using the Coherence Operator the simples configuration is to write persistence files to a volume mapped to a PersistentVolumeClaim and to let the StatefulSet manage the PVCs . Persistence is configured in the coherence.persistence section of the role specification in the CoherenceCluster CRD. <markup lang=\"yaml\" >coherence: persistence: enabled: true persistentVolumeClaim: # PVC spec... Persistence should be enabled by setting the coherence.persistence.enabled field to true. The persistentVolumeClaim section allows the PVC used for snapshot files to be configured. The default value for coherence.persistence.enabled is false so no snapshot location will be configured for Coherence caches to use. For example, if a Kubernetes cluster has a custom StorageClass named fast defined below: <markup lang=\"yaml\" >kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast provisioner: k8s.io/minikube-hostpath parameters: type: pd-ssd Then a CoherenceCluster can be created with persistence enabled and configured to use the fast storage class: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: persistence: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi The content of the persistentVolumeClaim is any valid yaml for defining a PersistentVolumeClaimSpec that would be allowed when configuring the spec section of a PVC in the volumeClaimTemplates section of a StatefulSet as described in the Kubernetes API documentation . Persistence using Persistent Volumes for a Single Implicit Role When configuring a CoherenceCluster with a single implicit role the coherence.persistence configuration is added directly to the spec section of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: persistence: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi The implicit storage role has persistence enabled and configured to use a PVC with custom StorageClass . Persistence using Persistent Volumes for Explicit Roles When configuring a CoherenceCluster with one or more explicit roles the coherence.persistence configuration is added directly to the configuration of each role in the roles list of the CoherenceCluster . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: persistence: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi - role: proxy coherence: persistence: enabled: false The data role has persistence enabled and configured to use a PVC with custom StorageClass . The proxy role has persistence explicitly disabled. Persistence using Persistent Volumes for Explicit Roles with Defaults When configuring a explicit roles in the roles list of a CoherenceCluster default values for the coherence.persistence configuration can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: persistence: enabled: true persistentVolumeClaim: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi roles: - role: data - role: proxy coherence: persistence: enabled: false The default persistence configuration is to enable persistence using a PVC with custom StorageClass . The data role does not specify an explict persistence configuration so it will use the defaults. The proxy role has persistence explicitly disabled. ",
            "title": "Managing Coherence Persistence using Persistent Volumes"
        },
        {
            "location": "/clusters/062_coherence_persistence",
            "text": " Although PersistentVolumeClaims are the recommended way to manage storage for Coherence persistence the Coherence Operator also supports using standard Kubernetes Volumes as a storage mechanism. When using standard Kubernetes Volumes for snapshot storage it is important to ensure that CoherenceClusters are configured and managed in such a way that the same Volumes are reattached to Pods if clusters are restarted or if individual Pods are restarted or rescheduled by Kubernetes. If this is not done then snapshot data can be lost. There are many ways to accomplish this using particular Volume types or controlling Pod scheduling but this configuration is beyond the scope of this document and the relevant Kubernetes or storage provider documentation should be consulted. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: persistence: enabled: true volume: hostPath: path: /data Snapshots storage is configured to use a hostPath volume mapped to the /data directory on the host As with configuring persistence to use PersistentVolumeClaims configuring them to use Volumes can be done at different levels in the CoherenceCluster spec depending on whether there is a single implicit role, multiple explicit roles and default values to apply to explicit roles. ",
            "title": "Managing Coherence Persistence using Standard Volumes"
        },
        {
            "location": "/examples/010_overview",
            "text": " This comprehensive example shows how to build applications using the Coherence Operator. ",
            "title": "preambule"
        },
        {
            "location": "/examples/010_overview",
            "text": " TBC - We will have a description of the examples and then a link to the GitHub repository which will have the full example in details and README.md. ",
            "title": "Examples Overview"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " Since version 12.2.1.4 Coherence has had functionality to expose a management API over ReST. This API is disabled by default in Coherence clusters but can be enabled and configured by setting the relevant fields in the CoherenceCluster resource. ",
            "title": "Coherence Management over ReST"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " When configuring a single implicit role in a CoherenceCluster the management over ReST API can be enabled by setting the coherence.management.enabled to true in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true Management over ReST will be enabled and the http endpoint will bind to port 30000 in the container. The port is not exposed in a Service . ",
            "title": "Enabling Management Over ReST for the Implicit Role"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " When configuring a explicit roles in the roles list of a CoherenceCluster the management over ReST API can be enabled or disabled by setting the coherence.management.enabled for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: management: enabled: true - role: proxy coherence: management: enabled: false The data role has the management over ReST enabled. The proxy role has the management over ReST disabled. ",
            "title": "Enabling Management Over ReST for Explicit Roles"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " When configuring a explicit roles in the roles list of a CoherenceCluster a default value for the coherence.management.enabled field can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true roles: - role: data - role: proxy coherence: management: enabled: false The default value for enabling management over ReST is true which will apply to all roles in the roles list unless the field is specifically overridden. The data role does not specify a value for the coherence.management.enabled field so it will use the default value of true so management over ReST will be enabled. The proxy role overrides the default value for the coherence.management.enabled field and sets it to false so management over ReST will be disabled. ",
            "title": "Enabling Management Over ReST for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " Enabling management over ReST only enables the http server so that the endpoint is available in the container. If external access to the API is required via a service then the port needs to be exposed just like any other additional ports as described in Expose Ports and Services . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true ports: - name: management port: 30000 Management over ReST will be enabled and the default port value will be used so that the http endpoint will bind to port 30000 in the container. An additional port named management is added to the ports array which will cause the management port to be exposed on a service. The port specified is 30000 as that is the default port that the management API will bind to. ",
            "title": "Exposing the Management over ReST API via a Service"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " The default port in the container that the management API uses is 30000. It is possible to change ths port using the coherence.management.port field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true port: 9000 ports: - name: management port: 9000 Management over ReST is enabled and configured to bind to port 9000 in the container. The corresponding port value of 9000 must be used when exposing the port on a Service . ",
            "title": "Expose Management Over ReST on a Different Port"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " It is possible to configure the management API endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.management.ssl section of the spec. See Management over ReST for a more in depth guide to configuring SSL. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true ssl: enabled: true keyStore: management-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: management-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: management-secret The enabled field when set to true enables SSL for the management API or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. ",
            "title": "Configuring Management Over ReST With SSL"
        },
        {
            "location": "/clusters/058_coherence_management",
            "text": " Coherence management over ReST can be enabled or disabled by setting the coherence.management.enabled field. Note Enabling management over ReST will add a number of .jar files to the classpath of the Coherence JVM. In Coherence 12.2.1.4 those .jar file are: <markup >org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34 org.glassfish.hk2:hk2-api:jar:2.4.0-b34 org.glassfish.hk2:hk2-locator:jar:2.4.0-b34 org.glassfish.hk2:hk2-utils:jar:2.4.0-b34 org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34 com.fasterxml.jackson.core:jackson-annotations:jar:2.9.9 com.fasterxml.jackson.core:jackson-core:jar:2.9.9 com.fasterxml.jackson.core:jackson-databind:jar:2.9.9.2 com.fasterxml.jackson.jaxrs:jackson-jaxrs-base:jar:2.9.9 com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:jar:2.9.9 com.fasterxml.jackson.module:jackson-module-jaxb-annotations:jar:2.9.9 javax.annotation:javax.annotation-api:jar:1.2 javax.validation:validation-api:jar:1.1.0.Final javax.ws.rs:javax.ws.rs-api:jar:2.0.1 org.glassfish.jersey.core:jersey-client:jar:2.22.4 org.glassfish.jersey.core:jersey-common:jar:2.22.4 org.glassfish.jersey.ext:jersey-entity-filtering:jar:2.22.4 org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.22.4 org.glassfish.jersey.media:jersey-media-json-jackson:jar:2.22.4 org.glassfish.jersey.core:jersey-server:jar:2.22.4 org.glassfish.hk2:osgi-resource-locator:jar:1.0.1 If adding additional application .jar files care should be taken that there are no version conflicts. If conflicts are an issue there are alternative approaches available to exposing the management over ReST API. The list above is subject to change in later Coherence patches and version. Enabling Management Over ReST for the Implicit Role When configuring a single implicit role in a CoherenceCluster the management over ReST API can be enabled by setting the coherence.management.enabled to true in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true Management over ReST will be enabled and the http endpoint will bind to port 30000 in the container. The port is not exposed in a Service . Enabling Management Over ReST for Explicit Roles When configuring a explicit roles in the roles list of a CoherenceCluster the management over ReST API can be enabled or disabled by setting the coherence.management.enabled for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: management: enabled: true - role: proxy coherence: management: enabled: false The data role has the management over ReST enabled. The proxy role has the management over ReST disabled. Enabling Management Over ReST for Explicit Roles with a Default When configuring a explicit roles in the roles list of a CoherenceCluster a default value for the coherence.management.enabled field can be set in the CoherenceCluster spec section that will apply to all roles in the roles list unless overridden for a specific role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true roles: - role: data - role: proxy coherence: management: enabled: false The default value for enabling management over ReST is true which will apply to all roles in the roles list unless the field is specifically overridden. The data role does not specify a value for the coherence.management.enabled field so it will use the default value of true so management over ReST will be enabled. The proxy role overrides the default value for the coherence.management.enabled field and sets it to false so management over ReST will be disabled. Exposing the Management over ReST API via a Service Enabling management over ReST only enables the http server so that the endpoint is available in the container. If external access to the API is required via a service then the port needs to be exposed just like any other additional ports as described in Expose Ports and Services . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true ports: - name: management port: 30000 Management over ReST will be enabled and the default port value will be used so that the http endpoint will bind to port 30000 in the container. An additional port named management is added to the ports array which will cause the management port to be exposed on a service. The port specified is 30000 as that is the default port that the management API will bind to. Expose Management Over ReST on a Different Port The default port in the container that the management API uses is 30000. It is possible to change ths port using the coherence.management.port field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true port: 9000 ports: - name: management port: 9000 Management over ReST is enabled and configured to bind to port 9000 in the container. The corresponding port value of 9000 must be used when exposing the port on a Service . Configuring Management Over ReST With SSL It is possible to configure the management API endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.management.ssl section of the spec. See Management over ReST for a more in depth guide to configuring SSL. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: management: enabled: true ssl: enabled: true keyStore: management-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: management-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: management-secret The enabled field when set to true enables SSL for the management API or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. ",
            "title": "Enabling Management Over ReST"
        },
        {
            "location": "/clusters/110_volumes",
            "text": " Although a Coherence cluster member may not need access to specific volumes custom applications deployed into a cluster may require them. For this reason it is possible to configure roles in a CoherenceCluster with arbitrary VolumeMounts . ",
            "title": "preambule"
        },
        {
            "location": "/clusters/110_volumes",
            "text": " When defining a CoherenceCluster with a single implicit role the Volumes and VolumeMounts are added directly to the CoherenceCluster spec <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: test-volume hostPath: path: /data type: Directory volumeMounts: - name: test-volume mountPath: /test-data An additional Volume named test-volume will be added to the Pod . In this case the Volume is a hostPath volume type. A corresponding VolumeMount is added so that the test-volume will be mounted into the Coherence container with the path /test-data Multiple Volumes and VolumeMappings can be added: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data volumes: - name: test-volume hostPath: path: /data type: Directory volumeMounts: - name: test-volume mountPath: /test-data - role: proxy volumes: - name: proxy-volume hostPath: path: /proxy-data type: Directory volumeMounts: - name: test-volume mountPath: /data An additional Host Path Volume named test-volume will be added to the containers in the Pods for the data role. An additional VolumeMount to mount the test-volume to the /test-data path will be added to the containers in the Pods for the data role. An additional Host Path Volume named proxy-volume will be added to the containers in the Pods for the proxy role. An additional VolumeMount to mount the proxy-volume to the /proxy-data path will be added to the containers in the Pods for the proxy role. ",
            "title": "Adding Volumes to the Implicit Role"
        },
        {
            "location": "/clusters/110_volumes",
            "text": " When creating a CoherenceCluster with one or more explicit roles additional Volumes can be added to the configuration of each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: data-volume hostPath: path: /test-data type: Directory - name: config-volume hostPath: path: /test-config type: Directory volumeMounts: - name: test-volume mountPath: /data - name: config-volume mountPath: /config The volumes list has two additional Volumes , data-volume and config-volume The volumeMounts list has two corresponding VolumeMounts . ",
            "title": "Adding Volumes to Explicit Roles"
        },
        {
            "location": "/clusters/110_volumes",
            "text": " When creating a CoherenceCluster with one or more explicit roles additional Volumes and VolumeMounts can be added as defaults that will apply to all roles in the roles list. The additional Volumes and VolumeMounts in the defaults section will be merged with any additional Volumes and VolumeMounts specified for the role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: data-volume hostPath: path: /test-data type: Directory volumeMounts: - name: test-volume mountPath: /data roles: - role: data - role: proxy volumes: - name: config-volume hostPath: path: /proxy-config type: Directory volumeMounts: - name: config-volume mountPath: /config The default volumes list has one additional Volumes , data-volume The default volumeMounts list has one corresponding VolumeMounts The data role does not have any additional Volumes or VolumeMounts so it will just inherit the default Volume named data-volume and VolumeMount named test-volume The proxy role has an additional Volume named config-volume so when the Volume lists are merged it will have two additional Volumes config-volume and test-volume The proxy role has an additional VolumeMount named config-volume so when the VolumeMount lists are merged it will have two additional VolumeMounts config-volume and test-volume When configuring explicit roles with default Volumes and VolumeMounts if the role defines a Volume or VolumeMount with the same name as one defined in the defaults then the role&#8217;s definition overrides the default definition. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: data-volume hostPath: path: /test-data type: Directory volumeMounts: - name: test-volume mountPath: /data roles: - role: data - role: proxy volumes: - name: data-volume hostPath: path: /proxy-data type: Directory - name: config-volume hostPath: path: /proxy-config type: Directory volumeMounts: - name: config-volume mountPath: /config The proxy role overrides the default data-volume Volume with a different configuration. ",
            "title": "Adding Volumes to Explicit Roles with Defaults"
        },
        {
            "location": "/clusters/110_volumes",
            "text": " There are two parts to configuring a Volume that will be accessible to an application running in th Coherence Pods . First a Volume must be defined for the Pod itself and then a corresponding VolumeMount must be configured that will be added to the Coherence container in the Pods . Additional Volumes and VolumeMounts can be added to a CoherenceCluster by defining each additional Volume and VolumeMount using exactly the same yaml that would be used if adding Volumes to Kubernetes Pods as described in the Kubernetes Volumes documentation Adding Volumes to the Implicit Role When defining a CoherenceCluster with a single implicit role the Volumes and VolumeMounts are added directly to the CoherenceCluster spec <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: test-volume hostPath: path: /data type: Directory volumeMounts: - name: test-volume mountPath: /test-data An additional Volume named test-volume will be added to the Pod . In this case the Volume is a hostPath volume type. A corresponding VolumeMount is added so that the test-volume will be mounted into the Coherence container with the path /test-data Multiple Volumes and VolumeMappings can be added: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data volumes: - name: test-volume hostPath: path: /data type: Directory volumeMounts: - name: test-volume mountPath: /test-data - role: proxy volumes: - name: proxy-volume hostPath: path: /proxy-data type: Directory volumeMounts: - name: test-volume mountPath: /data An additional Host Path Volume named test-volume will be added to the containers in the Pods for the data role. An additional VolumeMount to mount the test-volume to the /test-data path will be added to the containers in the Pods for the data role. An additional Host Path Volume named proxy-volume will be added to the containers in the Pods for the proxy role. An additional VolumeMount to mount the proxy-volume to the /proxy-data path will be added to the containers in the Pods for the proxy role. Adding Volumes to Explicit Roles When creating a CoherenceCluster with one or more explicit roles additional Volumes can be added to the configuration of each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: data-volume hostPath: path: /test-data type: Directory - name: config-volume hostPath: path: /test-config type: Directory volumeMounts: - name: test-volume mountPath: /data - name: config-volume mountPath: /config The volumes list has two additional Volumes , data-volume and config-volume The volumeMounts list has two corresponding VolumeMounts . Adding Volumes to Explicit Roles with Defaults When creating a CoherenceCluster with one or more explicit roles additional Volumes and VolumeMounts can be added as defaults that will apply to all roles in the roles list. The additional Volumes and VolumeMounts in the defaults section will be merged with any additional Volumes and VolumeMounts specified for the role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: data-volume hostPath: path: /test-data type: Directory volumeMounts: - name: test-volume mountPath: /data roles: - role: data - role: proxy volumes: - name: config-volume hostPath: path: /proxy-config type: Directory volumeMounts: - name: config-volume mountPath: /config The default volumes list has one additional Volumes , data-volume The default volumeMounts list has one corresponding VolumeMounts The data role does not have any additional Volumes or VolumeMounts so it will just inherit the default Volume named data-volume and VolumeMount named test-volume The proxy role has an additional Volume named config-volume so when the Volume lists are merged it will have two additional Volumes config-volume and test-volume The proxy role has an additional VolumeMount named config-volume so when the VolumeMount lists are merged it will have two additional VolumeMounts config-volume and test-volume When configuring explicit roles with default Volumes and VolumeMounts if the role defines a Volume or VolumeMount with the same name as one defined in the defaults then the role&#8217;s definition overrides the default definition. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: volumes: - name: data-volume hostPath: path: /test-data type: Directory volumeMounts: - name: test-volume mountPath: /data roles: - role: data - role: proxy volumes: - name: data-volume hostPath: path: /proxy-data type: Directory - name: config-volume hostPath: path: /proxy-config type: Directory volumeMounts: - name: config-volume mountPath: /config The proxy role overrides the default data-volume Volume with a different configuration. ",
            "title": "Configure Additional Volumes"
        },
        {
            "location": "/clusters/150_readiness_liveness",
            "text": "",
            "title": "Readiness &amp; Liveness Probes"
        },
        {
            "location": "/install/02_pre_release_versions",
            "text": " Pre-release version of the Coherence Operator are made available from time to time. ",
            "title": "preambule"
        },
        {
            "location": "/install/02_pre_release_versions",
            "text": " Not all pre-release images are pushed to public repositories such as Docker Hub. Consequently when installing those versions of the Coherence Operator credentials and Kubernetes pull secrets will be required. For example to access an image in the iad.ocir.io/odx-stateservice repository you would need to have your own credentials for that repository so that a secret can be created. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; \\ create secret docker-registry coherence-operator-secret \\ --docker-server=$DOCKER_REPO \\ --docker-username=$DOCKER_USERNAME \\ --docker-password=$DOCKER_PASSWORD \\ --docker-email=$DOCKER_EMAIL Replace &lt;namespace&gt; with the Kubernetes namespace that the Coherence Operator will be installed into. In this example the name of the secret to be created is coherence-operator-secret Replace $DOCKER_REPO with the name of the Docker repository that the images are to be pulled from. Replace $DOCKER_USERNAME with your username for that repository. Replace $DOCKER_PASSWORD with your password for that repository. Replace $DOCKER_EMAIL with your email (or even a fake email). See the Kubernetes documentation on pull secrets for more details. Once a secret has been created in the namespace the Coherence Operator can be installed with an extra value parameter to specify the secret to use: <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 2.0.0-1909130555 \\ --namespace &lt;namespace&gt; \\ --set imagePullSecrets[0].name=coherence-operator-secret \\ --name coherence-operator Set the pull secret to use to the same name that was created above. ",
            "title": "Accessing Pre-Release Coherence Operator Docker Images"
        },
        {
            "location": "/install/02_pre_release_versions",
            "text": " Pre-release versions of the Coherence Operator are not guaranteed to be bug free and should not be used for production use. Pre-release versions of the Helm chart and Docker images may be removed and hence made unavailable without notice. APIs and CRD specifications are not guaranteed to remain stable or backwards compatible between pre-release versions. To access pre-release versions of the Helm chart add the unstable chart repository. <markup lang=\"bash\" >helm repo add coherence-unstable https://oracle.github.io/coherence-operator/charts-unstable helm repo update To list all of the available Coherence Operator chart versions: <markup lang=\"bash\" >helm search coherence-operator -l The -l parameter shows all versions as opposed to just the latest versions if it was omitted. A specific pre-release version of the Helm chart can be installed using the --version argument, for example to use version 2.0.0-alpha1 : <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 2.0.0-alpha1 \\ --namespace &lt;namespace&gt; \\ --name coherence-operator The --version argument is used to specify the exact version of the chart The optional --namespace parameter to specify which namespace to install the operator into, if omitted then Helm will install into whichever is currently the default namespace for your Kubernetes configuration. When using pre-release versions of the Helm chart it is always advisable to install a specific version otherwise Helm will try to work out the latest version in the pre-release repo and as pre-release version numbers are not strictly sem-ver compliant this may be unreliable. Accessing Pre-Release Coherence Operator Docker Images Not all pre-release images are pushed to public repositories such as Docker Hub. Consequently when installing those versions of the Coherence Operator credentials and Kubernetes pull secrets will be required. For example to access an image in the iad.ocir.io/odx-stateservice repository you would need to have your own credentials for that repository so that a secret can be created. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; \\ create secret docker-registry coherence-operator-secret \\ --docker-server=$DOCKER_REPO \\ --docker-username=$DOCKER_USERNAME \\ --docker-password=$DOCKER_PASSWORD \\ --docker-email=$DOCKER_EMAIL Replace &lt;namespace&gt; with the Kubernetes namespace that the Coherence Operator will be installed into. In this example the name of the secret to be created is coherence-operator-secret Replace $DOCKER_REPO with the name of the Docker repository that the images are to be pulled from. Replace $DOCKER_USERNAME with your username for that repository. Replace $DOCKER_PASSWORD with your password for that repository. Replace $DOCKER_EMAIL with your email (or even a fake email). See the Kubernetes documentation on pull secrets for more details. Once a secret has been created in the namespace the Coherence Operator can be installed with an extra value parameter to specify the secret to use: <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 2.0.0-1909130555 \\ --namespace &lt;namespace&gt; \\ --set imagePullSecrets[0].name=coherence-operator-secret \\ --name coherence-operator Set the pull secret to use to the same name that was created above. ",
            "title": "Accessing Pre-Release Versions"
        },
        {
            "location": "/management/060_cohql",
            "text": " You can use Coherence Query Language (CohQL) to interact with Coherence caches. ",
            "title": "preambule"
        },
        {
            "location": "/management/060_cohql",
            "text": " Deploy a simple CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"example-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: example-cluster spec: role: storage replicas: 3 Add an imagePullSecrets entry if required to pull images from a private repository. <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f example-cluster.yaml coherencecluster.coherence.oracle.com/example-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=example-cluster NAME READY STATUS RESTARTS AGE example-cluster-storage-0 1/1 Running 0 59s example-cluster-storage-1 1/1 Running 0 59s example-cluster-storage-2 1/1 Running 0 59s ",
            "title": "1. Install a Coherence Cluster"
        },
        {
            "location": "/management/060_cohql",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; example-cluster-storage-0 bash /scripts/startCoherence.sh queryplus Run the following CohQL commands to view and insert data into the cluster. <markup lang=\"sql\" >CohQL&gt; select count() from 'test'; Results 0 CohQL&gt; insert into 'test' key('key-1') value('value-1'); CohQL&gt; select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] You can issue the help command to get details help information in each command or commands command to get a brief view of all commands. Issue the command bye to exit CohQL. <markup lang=\"sql\" >CohQL&gt; commands java com.tangosol.coherence.dslquery.QueryPlus [-t] [-c] [-s] [-e] [-l &lt;cmd&gt;]* [-f &lt;file&gt;]* [-g &lt;garFile&gt;] [-a &lt;appName&gt;] [-dp &lt;parition-list&gt;] [-timeout &lt;value&gt;] Command Line Arguments: -a the application name to use. Used in combination with the -g argument. -c exit when command line processing is finished -e or -extend extended language mode. Allows object literals in update and insert statements. elements between '[' and']'denote an ArrayList. elements between '{' and'}'denote a HashSet. elements between '{' and'}'with key/value pairs separated by ':' denotes a HashMap. A literal HashMap preceded by a class name are processed by calling a zero argument constructor then followed by each pair key being turned into a setter and invoked with the value. -f &lt;value&gt; Each instance of -f followed by a filename load one file of statements. -g &lt;value&gt; An optional GAR file to load before running QueryPlus. If the -a argument is not used the application name will be the GAR file name without the parent directory name. -l &lt;value&gt; Each instance of -l followed by a statement will execute one statement. -s silent mode. Suppress prompts and result headings, read from stdin and write to stdout. Useful for use in pipes or filters -t or -trace turn on tracing. This shows information useful for debugging -dp &lt;list&gt; A comma delimited list of domain partition names to use. On start-up the first domain partition in the list will be the current partition. The -dp argument is only applicable in combination with the -g argument. -timeout &lt;value&gt; Specifies the timeout value for CohQL statements in milli-seconds. BYE | QUIT (ENSURE | CREATE) CACHE 'cache-name' (ENSURE | CREATE) INDEX [ON] 'cache-name' value-extractor-list DROP CACHE 'cache-name' TRUNCATE CACHE 'cache-name' DROP INDEX [ON] 'cache-name' value-extractor-list BACKUP CACHE 'cache-name' [TO] [FILE] 'filename' RESTORE CACHE 'cache-name' [FROM] [FILE] 'filename' INSERT INTO 'cache-name' [KEY (literal | new java-constructor | static method)] VALUE (literal | new java-constructor | static method) DELETE FROM 'cache-name'[[AS] alias] [WHERE conditional-expression] UPDATE 'cache-name' [[AS] alias] SET update-statement {, update-statement}* [WHERE conditional-expression] SELECT (properties* aggregators* | * | alias) FROM 'cache-name' [[AS] alias] [WHERE conditional-expression] [GROUP [BY] properties+] SOURCE FROM [FILE] 'filename' @ 'filename' . filename SHOW PLAN 'CohQL command' | EXPLAIN PLAN for 'CohQL command' TRACE 'CohQL command' LIST SERVICES [ENVIRONMENT] LIST [ARCHIVED] SNAPSHOTS ['service'] LIST ARCHIVER 'service' CREATE SNAPSHOT 'snapshot-name' 'service' RECOVER SNAPSHOT 'snapshot-name' 'service' REMOVE [ARCHIVED] SNAPSHOT 'snapshot-name' 'service' VALIDATE SNAPSHOT 'snapshot-directory' [VERBOSE] VALIDATE SNAPSHOT 'snapshot-name' 'service-name' [VERBOSE] VALIDATE ARCHIVED SNAPSHOT 'snapshot-name' 'service-name' [VERBOSE] ARCHIVE SNAPSHOT 'snapshot-name' 'service' RETRIEVE ARCHIVED SNAPSHOT 'snapshot-name' 'service' [OVERWIRTE] RESUME SERVICE 'service' SUSPEND SERVICE 'service' FORCE RECOVERY 'service' COMMANDS EXTENDED LANGUAGE (ON | OFF) HELP SANITY [CHECK] (ON | OFF) SERVICES INFO TRACE (ON | OFF) WHENEVER COHQLERROR THEN (CONTINUE | EXIT) ALTER SESSION SET DOMAIN PARTITION &lt;partition-name&gt; ALTER SESSION SET TIMEOUT &lt;milli-seconds&gt; ",
            "title": "2. Connect to CohQL client to add data"
        },
        {
            "location": "/management/060_cohql",
            "text": " After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f example-cluster.yaml ",
            "title": "3. Clean Up"
        },
        {
            "location": "/management/060_cohql",
            "text": " CohQL is a light-weight syntax (in the tradition of SQL) that is used to perform cache operations on a Coherence cluster. The language can be used either programmatically or from a command-line tool. The example shows how to access the Coherence CohQL client in a running cluster. See the Coherence CohQL documentation for more information. 1. Install a Coherence Cluster Deploy a simple CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"example-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: example-cluster spec: role: storage replicas: 3 Add an imagePullSecrets entry if required to pull images from a private repository. <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f example-cluster.yaml coherencecluster.coherence.oracle.com/example-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=example-cluster NAME READY STATUS RESTARTS AGE example-cluster-storage-0 1/1 Running 0 59s example-cluster-storage-1 1/1 Running 0 59s example-cluster-storage-2 1/1 Running 0 59s 2. Connect to CohQL client to add data <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; example-cluster-storage-0 bash /scripts/startCoherence.sh queryplus Run the following CohQL commands to view and insert data into the cluster. <markup lang=\"sql\" >CohQL&gt; select count() from 'test'; Results 0 CohQL&gt; insert into 'test' key('key-1') value('value-1'); CohQL&gt; select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] You can issue the help command to get details help information in each command or commands command to get a brief view of all commands. Issue the command bye to exit CohQL. <markup lang=\"sql\" >CohQL&gt; commands java com.tangosol.coherence.dslquery.QueryPlus [-t] [-c] [-s] [-e] [-l &lt;cmd&gt;]* [-f &lt;file&gt;]* [-g &lt;garFile&gt;] [-a &lt;appName&gt;] [-dp &lt;parition-list&gt;] [-timeout &lt;value&gt;] Command Line Arguments: -a the application name to use. Used in combination with the -g argument. -c exit when command line processing is finished -e or -extend extended language mode. Allows object literals in update and insert statements. elements between '[' and']'denote an ArrayList. elements between '{' and'}'denote a HashSet. elements between '{' and'}'with key/value pairs separated by ':' denotes a HashMap. A literal HashMap preceded by a class name are processed by calling a zero argument constructor then followed by each pair key being turned into a setter and invoked with the value. -f &lt;value&gt; Each instance of -f followed by a filename load one file of statements. -g &lt;value&gt; An optional GAR file to load before running QueryPlus. If the -a argument is not used the application name will be the GAR file name without the parent directory name. -l &lt;value&gt; Each instance of -l followed by a statement will execute one statement. -s silent mode. Suppress prompts and result headings, read from stdin and write to stdout. Useful for use in pipes or filters -t or -trace turn on tracing. This shows information useful for debugging -dp &lt;list&gt; A comma delimited list of domain partition names to use. On start-up the first domain partition in the list will be the current partition. The -dp argument is only applicable in combination with the -g argument. -timeout &lt;value&gt; Specifies the timeout value for CohQL statements in milli-seconds. BYE | QUIT (ENSURE | CREATE) CACHE 'cache-name' (ENSURE | CREATE) INDEX [ON] 'cache-name' value-extractor-list DROP CACHE 'cache-name' TRUNCATE CACHE 'cache-name' DROP INDEX [ON] 'cache-name' value-extractor-list BACKUP CACHE 'cache-name' [TO] [FILE] 'filename' RESTORE CACHE 'cache-name' [FROM] [FILE] 'filename' INSERT INTO 'cache-name' [KEY (literal | new java-constructor | static method)] VALUE (literal | new java-constructor | static method) DELETE FROM 'cache-name'[[AS] alias] [WHERE conditional-expression] UPDATE 'cache-name' [[AS] alias] SET update-statement {, update-statement}* [WHERE conditional-expression] SELECT (properties* aggregators* | * | alias) FROM 'cache-name' [[AS] alias] [WHERE conditional-expression] [GROUP [BY] properties+] SOURCE FROM [FILE] 'filename' @ 'filename' . filename SHOW PLAN 'CohQL command' | EXPLAIN PLAN for 'CohQL command' TRACE 'CohQL command' LIST SERVICES [ENVIRONMENT] LIST [ARCHIVED] SNAPSHOTS ['service'] LIST ARCHIVER 'service' CREATE SNAPSHOT 'snapshot-name' 'service' RECOVER SNAPSHOT 'snapshot-name' 'service' REMOVE [ARCHIVED] SNAPSHOT 'snapshot-name' 'service' VALIDATE SNAPSHOT 'snapshot-directory' [VERBOSE] VALIDATE SNAPSHOT 'snapshot-name' 'service-name' [VERBOSE] VALIDATE ARCHIVED SNAPSHOT 'snapshot-name' 'service-name' [VERBOSE] ARCHIVE SNAPSHOT 'snapshot-name' 'service' RETRIEVE ARCHIVED SNAPSHOT 'snapshot-name' 'service' [OVERWIRTE] RESUME SERVICE 'service' SUSPEND SERVICE 'service' FORCE RECOVERY 'service' COMMANDS EXTENDED LANGUAGE (ON | OFF) HELP SANITY [CHECK] (ON | OFF) SERVICES INFO TRACE (ON | OFF) WHENEVER COHQLERROR THEN (CONTINUE | EXIT) ALTER SESSION SET DOMAIN PARTITION &lt;partition-name&gt; ALTER SESSION SET TIMEOUT &lt;milli-seconds&gt; 3. Clean Up After running the above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f example-cluster.yaml ",
            "title": "Accessing the CohQL client"
        },
        {
            "location": "/about/04_obtain_coherence_images",
            "text": " Coherence images are not available from public registries such as Docker Hub and must be pulled from one of two private registries. ",
            "title": "preambule"
        },
        {
            "location": "/about/04_obtain_coherence_images",
            "text": " Get the Coherence Docker image from the Oracle Container Registry: In a web browser, navigate to Oracle Container Registry and click Sign In. Enter your Oracle credentials or create an account if you don&#8217;t have one. Search for coherence in the Search Oracle Container Registry field. Click coherence in the search result list. On the Oracle Coherence page, select the language from the drop-down list and click Continue. Click Accept on the Oracle Standard Terms and Conditions page. Once this is done the Oracle Container Registry credentials can be used to create Kubernetes secret to pull the Coherence image. See Using Private Image Registries ",
            "title": "Coherence Images from Oracle Container Registry"
        },
        {
            "location": "/about/04_obtain_coherence_images",
            "text": " In a https://hub.docker.com/_/oracle-coherence-12c In a web browser, navigate to Docker Hub and click Sign In. Search for the official Oracle Coherence images Click on the Proceed to Checkout button Accept the license agreements by clicking the check boxes. Click the Get Content button Once this is done the Docker Hub credentials can be used to create Kubernetes secret to pull the Coherence image. See Using Private Image Registries ",
            "title": "Coherence Images from Docker Store"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " When using the implicit role configuration the Coherence image to use is set directly in the CoherenceCluster spec coherence.image` section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4 The coherence container in the implicit role&#8217;s Pod will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4 ",
            "title": "Setting the Coherence Image for the Implicit Role"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " When using the explicit roles in a CoherenceCluster roles list the Coherence image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 The coherence container in the data role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 The coherence container in the proxy role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 ",
            "title": "Setting the Coherence Image for Explicit Roles"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " When using the explicit roles in a CoherenceCluster roles list the Coherence image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data - role: proxy The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the data and the proxy roles. The coherence container in all of the Pods will use this image. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy - role: web The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the proxy and the web roles. The coherence container in all of the Pods will use this image. The container-registry.oracle.com/middleware/coherence/12.2.1.4.1 image is specifically set for the data role so the coherence container in the Pods for the data role will use this image. ",
            "title": "Setting the Coherence Image for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " The Coherence Operator has a default setting for the Coherence image that will be used when by Pods in a CoherenceCluster . This default value can be overridden to enable roles in the cluster to use a different image. If the image being configured is from a registry requiring authentication see the section on pulling from private registries . As well as setting the image name it is also sometimes useful to set the Coherence image&#8217;s image pull policy . Setting the Coherence Image for the Implicit Role When using the implicit role configuration the Coherence image to use is set directly in the CoherenceCluster spec coherence.image` section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4 The coherence container in the implicit role&#8217;s Pod will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4 Setting the Coherence Image for Explicit Roles When using the explicit roles in a CoherenceCluster roles list the Coherence image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 The coherence container in the data role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 The coherence container in the proxy role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 Setting the Coherence Image for Explicit Roles with a Default When using the explicit roles in a CoherenceCluster roles list the Coherence image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data - role: proxy The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the data and the proxy roles. The coherence container in all of the Pods will use this image. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy - role: web The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the proxy and the web roles. The coherence container in all of the Pods will use this image. The container-registry.oracle.com/middleware/coherence/12.2.1.4.1 image is specifically set for the data role so the coherence container in the Pods for the data role will use this image. ",
            "title": "Setting the Coherence Image"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always ",
            "title": "Setting the Image Pull Policy for the Implicit Role"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 imagePullPolicy: Always - role: proxy coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: imagePullPolicy: Always roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: web coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles with Default"
        },
        {
            "location": "/clusters/056_coherence_image",
            "text": " The image pull policy controls when (and if) Kubernetes will pull the Coherence image onto the node where the Coherence Pods are being schedules. See Kubernetes imagePullPolicy for more information. The Kubernetes default pull policy is IfNotPresent unless the image tag is :latest in which case the default policy is Always . The IfNotPresent policy causes the Kubelet to skip pulling an image if it already exists. Note that you should avoid using the :latest tag, see Kubernetes Best Practices for Configuration for more information. The Coherence image&#8217;s pull policy is set using the imagePullPolicy field in the spec.images.coherence section. Setting the Image Pull Policy for the Implicit Role To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always Setting the Image Pull Policy for Explicit Roles To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 imagePullPolicy: Always - role: proxy coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent Setting the Image Pull Policy for Explicit Roles with Default To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: coherence: imagePullPolicy: Always roles: - role: data coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: web coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Coherence Image Pull Policy"
        },
        {
            "location": "/about/06_cluster_discovery",
            "text": " A Coherence cluster is made up of one or more JVMs. In order for these JVMs to form a cluster they need to be able to discover other cluster members. The default mechanism for discovery is multicast broadcast but this does not work in most container environments. Coherence provides an alternative mechanism where the addresses of the hosts where the members of the cluster will run is provided in the form of a \"well known address\" (or WKA) list. This address list is then used by Coherence when it starts in a JVM to discover other cluster members running on the hosts in the WKA list. When running in containers each container is effectively a host and has its own host name and IP address (or addresses) and in Kubernetes it is the Pod that is effectively a host. When starting a container it is usually not possible to know in advance what the host names of the containers or Pods will be so there needs to be another solution to providing the WKA list. When Coherence processes a WKA list it will perform a DNS lookup for each host name in the list. If a host name resolves to more than one IP address then all of those IP addresses are used in cluster discovery. This feature of Coherence when combined with Kubernetes Services allows discovery of cluster members without resorting to a custom discovery mechanism. A Kubernetes Service has a DNS name and that name will resolve to all of the IP addresses for the Pods that match that Service selector. This means that a Coherence JVM only needs to be given the DNS name of a Service as the single host name in its WKA list and it will form a cluster with any other JVM using the same host name for WKA and the same cluster name. When the Coherence Operator creates resolves a CoherenceCluster configuration into a running set of Pods if creates a headless service specifically for the purposes of WKA for that cluster. For example, if a CoherenceCluster is created with the following yaml: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster A Coherence cluster will br created with a cluster name test-cluster The yaml for the WKA Service would look like the following: <markup lang=\"yaml\" title=\"wka-service.yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-wka annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\" labels: coherenceCluster: test-cluster component: coherenceWkaService spec: clusterIP: None ports: - name: coherence protocol: TCP port: 7 targetPort: 7 selector: coherenceCluster: test-cluster component: coherencePod The Service name is made up of the cluster name with the suffix -wka so in this case test-cluster-wka The Service is configured to allow unready Pods so that all Pods matching the selector will be resolved as members of this service regardless of their ready state. This is important so that Coherence JVMs can discover other members before they are fully ready. The service has a clusterIP of Nonde so it is headless A single port is exposed, in this case the echo port (7) even though nothing in the Coherence Pods binds to this port. Ideally no port would be included but the service has to have at least one port defined. The selector will match all Pods with the labels coherenceCluster=test-cluster and component=coherencePod which are labels that the Coherence Operator will assign to all Pods in this cluster Because this Service is created in the same Namespace as the rest of the Coherence cluster Pods the JVMs can use the raw Service name as the WKA list, in the example above the WKA list would just be test-cluster-wka . ",
            "title": "Coherence Cluster Discovery"
        },
        {
            "location": "/clusters/070_applications",
            "text": " Whilst the Coherence Operator can manage plain Coherence clusters typically custom application code and configuration files would be added to a Coherence JVM&#8217;s classpath. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/070_applications",
            "text": " Different application code and configuration can be added to different roles in a CoherenceCluster by specifying the application&#8217;s configuration in the application section of a role spec. There are a number of different fields that can be configured for an application described below. All of the configuration described below is optional. See the in-depth guide on Coherence application deployments for more details on creating and deploying CoherenceClusters with custom application code. <markup lang=\"yaml\" title=\"Application Spec\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 imagePullPolicy: Always appDir: \"/app\" libDir: \"/app/lib\" configDir: \"/app/conf\" main: io.acme.Server args: - \"foo\" - \"bar\" Setting the Application Image - The application&#8217;s image that is used to provide application .jar files and configuration files to add to the JVM classpath. Setting the Application Image Pull Policy - The pull policy that Kubernetes will use to pull the application&#8217;s image Setting the Application Directory - The name of the folder in the application&#8217;s image containing application artifacts. This will be the working directory for the Coherence container. Setting the Application Lib Directory - The name of the folder in the application&#8217;s image containing .jar files to add to the JVM class path Setting the Application Config Directory - The name of the folder in the application&#8217;s image containing configuration files that will be add to the JVM classpath Setting the Application Main Class - The application&#8217;s custom main main Class to use if running a class other than Coherence DefaultCacheServer Setting the Application Main Class Arguments - The arguments to pass to the application&#8217;s main Class ",
            "title": "Configure Applications"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When using the implicit role configuration the application image to use is set directly in the CoherenceCluster spec application.image field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 The acme/orders-data:1.0.0 will be used to add additional .jar files and configuration files to the classpath of the Coherence container in the implicit storage role&#8217;s Pods ",
            "title": "Setting the Application Image for the Implicit Role"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When using the explicit roles in a CoherenceCluster roles list the application image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: image: acme/orders-data:1.0.0 - role: proxy application: image: acme/orders-proxy/1.0.0 The data role Pods will use the application image acme/orders-data:1.0.0 The proxy role Pods will use the application image acme/orders-proxy/1.0.0 ",
            "title": "Setting the Application Image for Explicit Roles"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When using the explicit roles in a CoherenceCluster roles list the application image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 roles: - role: data - role: proxy The data and the proxy roles will both use the application image acme/orders-data:1.0.0 <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 roles: - role: data - role: proxy - role: web application: image: acme/orders-front-end/1.0.0 The data and the proxy roles will both use the application image acme/orders-data:1.0.0 The web role will use the application image acme/orders-web/1.0.0 ",
            "title": "Setting the Application Image for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/070_applications",
            "text": " The application image is the Docker image containing the .jar files and configuration files of the Coherence application that should be deployed in the Coherence cluster. For more information see the Coherence application deployments guide. Whilst the Coherence Operator makes it simple to deploy and manage a Coherence cluster in Kubernetes in the majority of use cases there will be a requirement for application code to be deployed and run in the Coherence JVMs. This application code and any application configuration files are supplied as a separate image. This image is loaded as an init-container by the Coherence Pods and the relevant .jar files and configuration files from this image are added to the Coherence JVM&#8217;s classpath. As well as setting the image name it is also sometimes useful to set the application image&#8217;s image pull policy . If the image being configured is from a registry requiring authentication see the section on pulling from private registries . Setting the Application Image for the Implicit Role When using the implicit role configuration the application image to use is set directly in the CoherenceCluster spec application.image field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 The acme/orders-data:1.0.0 will be used to add additional .jar files and configuration files to the classpath of the Coherence container in the implicit storage role&#8217;s Pods Setting the Application Image for Explicit Roles When using the explicit roles in a CoherenceCluster roles list the application image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: image: acme/orders-data:1.0.0 - role: proxy application: image: acme/orders-proxy/1.0.0 The data role Pods will use the application image acme/orders-data:1.0.0 The proxy role Pods will use the application image acme/orders-proxy/1.0.0 Setting the Application Image for Explicit Roles with a Default When using the explicit roles in a CoherenceCluster roles list the application image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 roles: - role: data - role: proxy The data and the proxy roles will both use the application image acme/orders-data:1.0.0 <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 roles: - role: data - role: proxy - role: web application: image: acme/orders-front-end/1.0.0 The data and the proxy roles will both use the application image acme/orders-data:1.0.0 The web role will use the application image acme/orders-web/1.0.0 ",
            "title": "Setting the Application Image"
        },
        {
            "location": "/clusters/070_applications",
            "text": " To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always ",
            "title": "Setting the Image Pull Policy for the Implicit Role"
        },
        {
            "location": "/clusters/070_applications",
            "text": " To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: image: acme/orders-data:1.0.0 imagePullPolicy: Always - role: proxy application: image: acme/orders-proxy/1.0.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles"
        },
        {
            "location": "/clusters/070_applications",
            "text": " To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: imagePullPolicy: Always roles: - role: data application: image: acme/orders-data:1.0.0 - role: proxy application: image: acme/orders-proxy/1.0.1 - role: web application: image: acme/orders-front-end/1.0.1 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles with Default"
        },
        {
            "location": "/clusters/070_applications",
            "text": " The image pull policy controls when (and if) Kubernetes will pull the application image onto the node where the Coherence Pods are being schedules. See Kubernetes imagePullPolicy for more information. The Kubernetes default pull policy is IfNotPresent unless the image tag is :latest in which case the default policy is Always . The IfNotPresent policy causes the Kubelet to skip pulling an image if it already exists. Note that you should avoid using the :latest tag, see Kubernetes Best Practices for Configuration for more information. The application image&#8217;s pull policy is set using the imagePullPolicy field in the spec.application section. Setting the Image Pull Policy for the Implicit Role To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: image: acme/orders-data:1.0.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always Setting the Image Pull Policy for Explicit Roles To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: image: acme/orders-data:1.0.0 imagePullPolicy: Always - role: proxy application: image: acme/orders-proxy/1.0.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent Setting the Image Pull Policy for Explicit Roles with Default To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: imagePullPolicy: Always roles: - role: data application: image: acme/orders-data:1.0.0 - role: proxy application: image: acme/orders-proxy/1.0.1 - role: web application: image: acme/orders-front-end/1.0.1 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Application Image Pull Policy"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When configuring a CoherenceCluster with a single implicit role the application&#8217;s lib directory is specified in the application.libDir field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: /lib The application image contains a directory named /app-lib that contains the .jar files to add to the JVM classpath. ",
            "title": "Setting the Application Lib Directory for the Implicit Role"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.libDir field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: libDir: app-lib - role: proxy application: libDir: proxy-lib The application image contains a directory named /app-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the data role. The application image contains a directory named /proxy-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the proxy role. ",
            "title": "Setting the Application Lib Directory for Explicit Roles"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.libDir field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: app-lib roles: - role: data - role: proxy application: libDir: proxy-lib The default value for the libDir for all roles will be /app-lib unless overridden for a specific role. The data role does not specify a value for libDir so it will use the default app-lib . The application image should contain a directory named /app-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the data role. The proxy role has an explicit value set for the libDir field. The application image should a directory named /proxy-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the proxy role. ",
            "title": "Setting the Application Lib Directory for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/070_applications",
            "text": " A typical Coherence application may also require additional dependencies (usually .jar files) that need to be added to the classpath. The applications&#8217;s lib directory is a directory in the application&#8217;s image that contains these additional .jar files. The Coherence Operator will add the files to the classpath with the wildcard setting (e.g. -cp /lib/* ) it does not add each file in the lib directory individually to the classpath. This means that the contents of the lib directory are added to the classpath using the rules that the JVM uses to process wild card classpath entries. The lib directory is set in the application.libDir field. This field is optional and if not specified the default directory name used will be /app/lib . Setting the Application Lib Directory for the Implicit Role When configuring a CoherenceCluster with a single implicit role the application&#8217;s lib directory is specified in the application.libDir field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: /lib The application image contains a directory named /app-lib that contains the .jar files to add to the JVM classpath. Setting the Application Lib Directory for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list the application.libDir field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: libDir: app-lib - role: proxy application: libDir: proxy-lib The application image contains a directory named /app-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the data role. The application image contains a directory named /proxy-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the proxy role. Setting the Application Lib Directory for Explicit Roles with a Default When creating a CoherenceCluster with explicit roles in the roles list the application.libDir field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: app-lib roles: - role: data - role: proxy application: libDir: proxy-lib The default value for the libDir for all roles will be /app-lib unless overridden for a specific role. The data role does not specify a value for libDir so it will use the default app-lib . The application image should contain a directory named /app-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the data role. The proxy role has an explicit value set for the libDir field. The application image should a directory named /proxy-lib that contains the .jar files to add to the JVM classpath in all of the Pods for the proxy role. ",
            "title": "Setting the Application Lib Directory"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When configuring a CoherenceCluster with a single implicit role the application&#8217;s configuration directory is specified in the application.configDir field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: configDir: app-conf The application image contains a directory named /app-conf that contains any configuration files to add to the JVM classpath. ",
            "title": "Setting the Application Config Directory for the Implicit Role"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.configDir field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: configDir: app-conf - role: proxy application: configDir: proxy-conf The application image contains a directory named /app-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the data role. The application image contains a directory named /proxy-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the proxy role. ",
            "title": "Setting the Application Config Directory for Explicit Roles"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.configDir field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: configDir: app-conf roles: - role: data - role: proxy application: configDir: proxy-conf The default value for the configDir field is app-conf/ which will be used for all roles unless specifically overridden for a role. The data role does not specify a value for configDir so it will use the default. The application image should contain a directory named /app-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the data role. The proxy role has an explicit value set for the configDir field. The application image should a directory named /proxy-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the proxy role. ",
            "title": "Setting the Application Config Directory for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/070_applications",
            "text": " A Coherence application may require additional files added to the classpath such as configuration files and other resources. These additional files can be placed into the config directory of the application&#8217;s image and this directory added to the classpath of the Coherence JVM. Just the directory is added to the classpath (e.g. -cp /conf ) the contents themselves are not added. The configuration directory is set in the application.configDir field. This field is optional and if not specified the default directory name used will be /app/conf . Setting the Application Config Directory for the Implicit Role When configuring a CoherenceCluster with a single implicit role the application&#8217;s configuration directory is specified in the application.configDir field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: configDir: app-conf The application image contains a directory named /app-conf that contains any configuration files to add to the JVM classpath. Setting the Application Config Directory for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list the application.configDir field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: configDir: app-conf - role: proxy application: configDir: proxy-conf The application image contains a directory named /app-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the data role. The application image contains a directory named /proxy-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the proxy role. Setting the Application Config Directory for Explicit Roles with a Default When creating a CoherenceCluster with explicit roles in the roles list the application.configDir field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: configDir: app-conf roles: - role: data - role: proxy application: configDir: proxy-conf The default value for the configDir field is app-conf/ which will be used for all roles unless specifically overridden for a role. The data role does not specify a value for configDir so it will use the default. The application image should contain a directory named /app-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the data role. The proxy role has an explicit value set for the configDir field. The application image should a directory named /proxy-conf that contains the configuration files to add to the JVM classpath in all of the Pods for the proxy role. ",
            "title": "Setting the Application Config Directory"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When configuring a CoherenceCluster with a single implicit role the application&#8217;s working directory is specified in the spec.application.appDir field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: appDir: app The application image contains a directory named /app that will effectively become the working directory for the Coherence JVM in the Pods for the role. ",
            "title": "Setting the Application Directory for the Implicit Role"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.appDir field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: appDir: data-app - role: proxy application: appDir: proxy-app The application image contains a directory named /data-app that will effectively become the working directory for the Coherence JVM in the Pods for the data role. The application image contains a directory named /proxy-app that will effectively become the working directory for the Coherence JVM in the Pods for the proxy role. ",
            "title": "Setting the Application Directory for Explicit Roles"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.appDir field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: appDir: app roles: - role: data - role: proxy application: appDir: proxy-app The default value for the appDir field is /app which will be used for all roles unless specifically overridden for a role. The data role does not specify a value for appDir so it will use the default. The application image should contain a directory named /app will effectively become the working directory for the Coherence JVM in the Pods for the data role. The proxy role has an explicit value set for the appDir field. The application image should a directory named /proxy-app will effectively become the working directory for the Coherence JVM in the Pods for the proxy role ",
            "title": "Setting the Application Directory for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/070_applications",
            "text": " Sometimes an application may have more than just .jar files or configuration files in the conf folder. An application may have a number of artifacts that it needs to access from a working directory so for this use case an application directory can be specified that will effectively become the working directory for the Coherence JVM in the Pods . The application directory may be a parent directory of the lib or configuration directory or they may be separate directory trees. The application directory is set in the application.appDir field. This field is optional and if not specified the default directory name used will be /app . Setting the Application Directory for the Implicit Role When configuring a CoherenceCluster with a single implicit role the application&#8217;s working directory is specified in the spec.application.appDir field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: appDir: app The application image contains a directory named /app that will effectively become the working directory for the Coherence JVM in the Pods for the role. Setting the Application Directory for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list the application.appDir field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: appDir: data-app - role: proxy application: appDir: proxy-app The application image contains a directory named /data-app that will effectively become the working directory for the Coherence JVM in the Pods for the data role. The application image contains a directory named /proxy-app that will effectively become the working directory for the Coherence JVM in the Pods for the proxy role. Setting the Application Directory for Explicit Roles with a Default When creating a CoherenceCluster with explicit roles in the roles list the application.appDir field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: appDir: app roles: - role: data - role: proxy application: appDir: proxy-app The default value for the appDir field is /app which will be used for all roles unless specifically overridden for a role. The data role does not specify a value for appDir so it will use the default. The application image should contain a directory named /app will effectively become the working directory for the Coherence JVM in the Pods for the data role. The proxy role has an explicit value set for the appDir field. The application image should a directory named /proxy-app will effectively become the working directory for the Coherence JVM in the Pods for the proxy role ",
            "title": "Setting the Application Directory"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When configuring a CoherenceCluster with a single implicit role the application&#8217;s working directory is specified in the application.main field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. ",
            "title": "Setting the Application Main Class for the Implicit Role"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.main field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: libDir: lib main: com.acme.Main - role: proxy application: libDir: lib main: com.acme.Proxy The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. The proxy role will use the com.acme.Proxy class as the main class ",
            "title": "Setting the Application Main Class for Explicit Roles"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.main field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main roles: - role: data - role: proxy application: main: com.acme.Proxy The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class for all roles that do not specifically specify a main . The data role does not specify a main field so the Coherence JVM in the Pods for the data role will all use the com.acme.Main class as the main class. The proxy role will specifies a main class to use so all Coherence JVMs in the Pods for the proxy role will use the com.acme.Proxy class as the main class. ",
            "title": "Setting the Application Main Class for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/070_applications",
            "text": " By default Coherence containers will run the main method in the com.tangosol.coherence.DefaultCacheServer class. Sometimes an application requires a different class as the main class (or even a main that is not a class at all, for example when running a Node JS application on top of the Graal VM the main could be a Javascript file). The main to be used can be configured for each role in a CoherenceCluster . Setting the Application Main Class for the Implicit Role When configuring a CoherenceCluster with a single implicit role the application&#8217;s working directory is specified in the application.main field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. Setting the Application Main Class for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list the application.main field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: libDir: lib main: com.acme.Main - role: proxy application: libDir: lib main: com.acme.Proxy The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. The proxy role will use the com.acme.Proxy class as the main class Setting the Application Main Class for Explicit Roles with a Default When creating a CoherenceCluster with explicit roles in the roles list the application.main field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main roles: - role: data - role: proxy application: main: com.acme.Proxy The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class for all roles that do not specifically specify a main . The data role does not specify a main field so the Coherence JVM in the Pods for the data role will all use the com.acme.Main class as the main class. The proxy role will specifies a main class to use so all Coherence JVMs in the Pods for the proxy role will use the com.acme.Proxy class as the main class. ",
            "title": "Setting the Application Main"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When configuring a CoherenceCluster with a single implicit role the application&#8217;s working directory is specified in the application.main field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main args: - \"argOne\" - \"argTwo\" The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. The arguments \"argOne\" and \"argTwo\" will be passed to the com.acme.Main class main() method. ",
            "title": "Setting the Application Main Arguments for the Implicit Role"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.args field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: libDir: lib main: com.acme.Main args: - \"argOne\" - \"argTwo\" - role: proxy application: libDir: lib main: com.acme.Main args: - \"argThree\" - \"argFour\" The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. The arguments \"argOne\" and \"argTwo\" will be passed to the com.acme.Main class main() method in Pods for the data role. The proxy role specifies different arguments. The arguments \"argThree\" and \"argFour\" will be passed to the com.acme.Main class main() method in Pods for the proxy role. ",
            "title": "Setting the Application Main Arguments for Explicit Roles"
        },
        {
            "location": "/clusters/070_applications",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list the application.main field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main args: - \"argOne\" - \"argTwo\" roles: - role: data - role: proxy application: args: - \"argThree\" - \"argFour\" - role: web application: args: [] The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class for all roles that do not specifically specify a main . The default args are \"argOne\" and \"argTwo\" The data role does not specify an args field so the Coherence JVM in the Pods for the data role will all use the default arguments of \"argOne\" and \"argTwo\" The proxy role specifies different arguments. The arguments \"argThree\" and \"argFour\" will be passed to the com.acme.Main class main() method in Pods for the proxy role. The web role specifies an empty array for the args field so no arguments will be passed to its main class. ",
            "title": "Setting the Application Main Arguments for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/070_applications",
            "text": " Some applications that specify a custom main may also require command line arguments to be passed to the main , These additional arguments can also be configured for the roles in a CoherenceCluster . Application arguments are specified as a string array. Setting the Application Main Arguments for the Implicit Role When configuring a CoherenceCluster with a single implicit role the application&#8217;s working directory is specified in the application.main field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main args: - \"argOne\" - \"argTwo\" The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. The arguments \"argOne\" and \"argTwo\" will be passed to the com.acme.Main class main() method. Setting the Application Main Arguments for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list the application.args field can be set specifically for each role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data application: libDir: lib main: com.acme.Main args: - \"argOne\" - \"argTwo\" - role: proxy application: libDir: lib main: com.acme.Main args: - \"argThree\" - \"argFour\" The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class when starting the JVMs for the data role. The arguments \"argOne\" and \"argTwo\" will be passed to the com.acme.Main class main() method in Pods for the data role. The proxy role specifies different arguments. The arguments \"argThree\" and \"argFour\" will be passed to the com.acme.Main class main() method in Pods for the proxy role. Setting the Application Main Arguments for Explicit Roles with a Default When creating a CoherenceCluster with explicit roles in the roles list the application.main field can be set at the spec level as a default that applies to all of the roles in the roles list unless specifically overridden for an individual role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: application: libDir: lib main: com.acme.Main args: - \"argOne\" - \"argTwo\" roles: - role: data - role: proxy application: args: - \"argThree\" - \"argFour\" - role: web application: args: [] The application image should contain a directory named /lib that will contain the .jar files containing the application classes and dependencies. One of those classes will be com.acme.Main which will be executed as the main class for all roles that do not specifically specify a main . The default args are \"argOne\" and \"argTwo\" The data role does not specify an args field so the Coherence JVM in the Pods for the data role will all use the default arguments of \"argOne\" and \"argTwo\" The proxy role specifies different arguments. The arguments \"argThree\" and \"argFour\" will be passed to the com.acme.Main class main() method in Pods for the proxy role. The web role specifies an empty array for the args field so no arguments will be passed to its main class. ",
            "title": "Setting the Application Main Arguments"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " Any ports that are used by Coherence or by application code that need to be exposed outside of the Pods for a role need to be declared in the CoherenceCluster spec for the role. ",
            "title": "Expose Ports and Services"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " The Coherence container in Pods in a role in a CoherenceCluster has two ports declared by default, none of the ports are exposed on services. Port Name Description 7 coherence This is the standard echo port. Nothing in the container binds to this port it is only declared on the container so that the headless Service used for Coherence WKA can declare a port. 6676 health This is the port used to expose the default readiness, liveness and StatusHA ReST endpoints on. When exposing additional ports as described in the sections below the names for the additional ports cannot be either coherence or health that are the names used for the default ports above or the Pods may fail to start. ",
            "title": "Default Ports"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " When configuring a CoherenceCluster with a single implicit role the additional ports are added to the ports array in the CoherenceCluster spec section. For Example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: extend port: 20000 - name: rest port: 8080 The ports array for the single implicit role contains two additional ports. The first named extend on port 20000 and the second named rest on port 8080 . Both of the ports in the above example will be exposed on separate Services using the default service configuration. ",
            "title": "Configure Additional Ports for the Implicit Role"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " When configuring a CoherenceCluster with explicit roles in the roles list the additional ports are added to the ports array for each role. For Example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data ports: - name: management port: 30000 - role: proxy ports: - name: extend port: 20000 The data role adds an additional port named management with a port value of 30000 that will be exposed on a service named test-cluster-data-management . The proxy role adds an additional port named extend with a port value of 20000 that will be exposed on a service named test-cluster-data-extend . ",
            "title": "Configure Additional Ports for Explicit Roles"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " When configuring a CoherenceCluster with explicit roles default additional ports can be added to the CoherenceCluster spec.ports array that will apply to all roles in the roles list. Additional ports can then also be specified for individual roles in the roles list. The ports array for an individual role will then be a merge of the default ports and the role&#8217;s ports. If a port in a role has the same name as a default port then the role&#8217;s port will override the default port. For Example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: management port: 30000 roles: - role: data - role: proxy ports: - name: extend port: 20000 - role: web ports: - name: http port: 8080 - name: management port: 9000 The default additional ports section specifies a single additional port named management on port 30000 . The data role does not specify any additional ports so will just have the default additional management port that will be exposed on a service named test-cluster-data-management . The proxy role adds an additional port named extend with a port value of 20000 that will be exposed on a service named test-cluster-data-extend . The proxy role will also have the default additional management port exposed on a service named test-cluster-proxy-management . The web role specified an additional port named http on port 8080 that will be exposed on a service named test-cluster-web-http . The web role also overrides the default management port changing the port value from 30000 to 9000 that will be exposed on a service named test-cluster-web-management . ",
            "title": "Configure Additional Ports for Explicit Roles with Defaults"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " Additional ports can be declared for a role by adding them to the ports array that is part of the role spec. A port has the following fields: <markup lang=\"yaml\" >ports: - name: extend port: 20000 protocol: TCP service: {} The port must have a name that is unique within the role The port value must be specified The protocol is optional and defaults to TCP . The valid values are the same as when declaring ports for Kubernetes services and Pods . The service section is optional and is used to configure the Service that will be used to expose the port. see Configure Services for Additional Ports By default a Kubernetes Service of type ClusterIP will be created for each additional port. The Service port and targetPort will both default to the specified port value. The port value for the Service can be overridden in the service spec. The name of the Service created will default to a name made up from the cluster name, the role name and the port name in the format &lt;cluster-name&gt;-&lt;role-name&gt;-&lt;port-name&gt; . This can be overriden by specifying a different name in the service section of the additional port configuration. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data ports: - name: extend port: 20000 The cluster name is test-cluster The role name is data The port name is extend The Service created for the extend port would be: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-data-extend the Service name is test-cluster-data-extend made up of the cluster name test-cluster the role name data and the port name extend . Configure Additional Ports for the Implicit Role When configuring a CoherenceCluster with a single implicit role the additional ports are added to the ports array in the CoherenceCluster spec section. For Example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: extend port: 20000 - name: rest port: 8080 The ports array for the single implicit role contains two additional ports. The first named extend on port 20000 and the second named rest on port 8080 . Both of the ports in the above example will be exposed on separate Services using the default service configuration. Configure Additional Ports for Explicit Roles When configuring a CoherenceCluster with explicit roles in the roles list the additional ports are added to the ports array for each role. For Example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data ports: - name: management port: 30000 - role: proxy ports: - name: extend port: 20000 The data role adds an additional port named management with a port value of 30000 that will be exposed on a service named test-cluster-data-management . The proxy role adds an additional port named extend with a port value of 20000 that will be exposed on a service named test-cluster-data-extend . Configure Additional Ports for Explicit Roles with Defaults When configuring a CoherenceCluster with explicit roles default additional ports can be added to the CoherenceCluster spec.ports array that will apply to all roles in the roles list. Additional ports can then also be specified for individual roles in the roles list. The ports array for an individual role will then be a merge of the default ports and the role&#8217;s ports. If a port in a role has the same name as a default port then the role&#8217;s port will override the default port. For Example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: management port: 30000 roles: - role: data - role: proxy ports: - name: extend port: 20000 - role: web ports: - name: http port: 8080 - name: management port: 9000 The default additional ports section specifies a single additional port named management on port 30000 . The data role does not specify any additional ports so will just have the default additional management port that will be exposed on a service named test-cluster-data-management . The proxy role adds an additional port named extend with a port value of 20000 that will be exposed on a service named test-cluster-data-extend . The proxy role will also have the default additional management port exposed on a service named test-cluster-proxy-management . The web role specified an additional port named http on port 8080 that will be exposed on a service named test-cluster-web-http . The web role also overrides the default management port changing the port value from 30000 to 9000 that will be exposed on a service named test-cluster-web-management . ",
            "title": "Configure Additional Ports"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " By default a Service will be created for all additional ports in the ports array. If for some reason this is not required Service creation can be disabled by setting the service.enabled field to false . The additional port will still be added as a named port to the Coherence Container spec in the Pod . An example of when service creation needs to be disabled is when the Service will be created externally and not managed by the operator. For example: <markup lang=\"yaml\" > ports: - name: extend port: 20000 protocol: TCP service: enabled: false ",
            "title": "Enabling or Disabling Service Creation"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " As already described above the name of a Service created for an additional port is a combination of cluster name, role name and port name. This can be overridden by setting the service.name field to the required name of the Service . Bear in mind when overriding Service names that they must be unique within the Kubernetes namespace that the CoherenceCluster is being installed into. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: http port: 8080 service: name: front-end port: 80 In the example above the service name has been overridden to be front-end and the service port overridden to 80 , which will generate a Service like the following: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: front-end spec: ports: - name: http port: 80 targetPort: 8080 The Service name has been overridden to front-end The port name is http the same as the name of the additional port in the role spec. The port is 80 which is the value from the additional port&#8217;s service.port field. The targetPort is 8080 which is the port that the container will use from the port value of the additional port in the role spec. ",
            "title": "Changing a Service Name"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " It is possible to add extra annotations to the Service created for a port by adding additional annotations to the service.annotations field. The format of the annotations is exactly the same as when creating a Kubernetes Service as also documented in the Kubernetes annotations documentation. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: http port: 8080 service: annotations: key1 : value1 key2 : value2 The http port created from the yaml above will be exposed on a service that looks like the following: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-storage annotations: key1 : value1 key2 : value2 spec: ports: - name: http port: 8080 targetPort: 8080 The additional annotations from the http port&#8217;s service configuration have been added to the Service ",
            "title": "Adding Service Annotations"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " It is possible to add extra labels to the Service created for a port by adding additional labels to the service.labels field. The format of the labels is exactly the same as when creating a Kubernetes Service as also documented in the Kubernetes labels documentation. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: http port: 8080 service: labels: key1 : value1 key2 : value2 The http port created from the yaml above will be exposed on a service that looks like the following: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-storage labels: key1 : value1 key2 : value2 spec: ports: - name: http port: 8080 targetPort: 8080 The additional labels from the http port&#8217;s service configuration have been added to the Service ",
            "title": "Adding Service Labels"
        },
        {
            "location": "/clusters/090_ports_and_services",
            "text": " A number of fields may be specified to configure the Service that will be created to expose the port. <markup lang=\"yaml\" > ports: - name: extend port: 20000 protocol: TCP service: enabled: true name: test-cluster-data-extend port: 20000 type: annotations: {} labels: {} externalName: sessionAffinity: publishNotReadyAddresses: externalTrafficPolicy: loadBalancerIP: healthCheckNodePort: loadBalancerSourceRanges: [] sessionAffinityConfig: {} Optionally enable or disable creation of a Service for the port, the defautl value is true . Optionally override the default generated Service name. Optionally use a different port in the Service to that used by the Container . If the service.port is not specified the same value will be used for both the container port and service port. Apart from the enabled and name fields, all of the fields shown above have exactly the same meaning and default behaviour that they do for a normal Kubernetes Service Enabling or Disabling Service Creation By default a Service will be created for all additional ports in the ports array. If for some reason this is not required Service creation can be disabled by setting the service.enabled field to false . The additional port will still be added as a named port to the Coherence Container spec in the Pod . An example of when service creation needs to be disabled is when the Service will be created externally and not managed by the operator. For example: <markup lang=\"yaml\" > ports: - name: extend port: 20000 protocol: TCP service: enabled: false Changing a Service Name As already described above the name of a Service created for an additional port is a combination of cluster name, role name and port name. This can be overridden by setting the service.name field to the required name of the Service . Bear in mind when overriding Service names that they must be unique within the Kubernetes namespace that the CoherenceCluster is being installed into. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: http port: 8080 service: name: front-end port: 80 In the example above the service name has been overridden to be front-end and the service port overridden to 80 , which will generate a Service like the following: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: front-end spec: ports: - name: http port: 80 targetPort: 8080 The Service name has been overridden to front-end The port name is http the same as the name of the additional port in the role spec. The port is 80 which is the value from the additional port&#8217;s service.port field. The targetPort is 8080 which is the port that the container will use from the port value of the additional port in the role spec. Adding Service Annotations It is possible to add extra annotations to the Service created for a port by adding additional annotations to the service.annotations field. The format of the annotations is exactly the same as when creating a Kubernetes Service as also documented in the Kubernetes annotations documentation. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: http port: 8080 service: annotations: key1 : value1 key2 : value2 The http port created from the yaml above will be exposed on a service that looks like the following: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-storage annotations: key1 : value1 key2 : value2 spec: ports: - name: http port: 8080 targetPort: 8080 The additional annotations from the http port&#8217;s service configuration have been added to the Service Adding Service Labels It is possible to add extra labels to the Service created for a port by adding additional labels to the service.labels field. The format of the labels is exactly the same as when creating a Kubernetes Service as also documented in the Kubernetes labels documentation. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: ports: - name: http port: 8080 service: labels: key1 : value1 key2 : value2 The http port created from the yaml above will be exposed on a service that looks like the following: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-storage labels: key1 : value1 key2 : value2 spec: ports: - name: http port: 8080 targetPort: 8080 The additional labels from the http port&#8217;s service configuration have been added to the Service ",
            "title": "Configure Services for Additional Ports"
        },
        {
            "location": "/clusters/030_roles",
            "text": " A CoherenceCluster is made up of one or more roles defined in its spec . ",
            "title": "preambule"
        },
        {
            "location": "/clusters/030_roles",
            "text": " A role is what is actually configured in the CoherenceCluster spec. In a traditional Coherence application that may have had a number of storage enabled members and a number of storage disable Coherence*Extend proxy members this cluster would have effectively had two roles, \"storage\" and \"proxy\". Some clusters may simply have just a storage role and some complex Coherence applications and clusters may have many roles and even different roles storage enabled for different caches/services within the same cluster. The Coherence Operator uses an internal crd named CoherenceRole to represent a role in a Coherence Cluster. A CoherenceRole would not typically be modified directly outside of a handful of specialized operations, such as scaling. Any modification to a role would normally be done by modifying that role in the corresponding CoherenceCluster and leaving the COherence Operator to update the CoherenceRole . ",
            "title": "Define Coherence Roles"
        },
        {
            "location": "/clusters/030_roles",
            "text": " As mentioned previously, all of the fields in a CoherenceCluster spec are optional meaning that the yaml below is perfectly valid. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster This yaml will create an implicit single role with a default role name of storage and a default replica count of three. The implicit role can be modified by specifying role related fields in the CoherenceCluster spec . The role name and replica count of the implicit role can be overridden using the corresponding fields <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 The role name is set with the role field, in this case the role name of the implicit role is now data The replica count is set using the replicas field, in this case the implicit role will now have six replicas. Other role fields can also be used, for example, to set the cache configuration file use by the implicit roles: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 cacheConfig: test-config.xml The cacheConfig field is used to set the cache configuration to test-config.xml . ",
            "title": "Implicit Default Role"
        },
        {
            "location": "/clusters/030_roles",
            "text": " It is possible to also create roles explicitly in the roles list of the CoherenceCluster spec . If creating a Coherence cluster with more than one role then all roles must be defined in the roles list. If creating a Coherence cluster with a single role it is optional whether the specification of that role is put into the CoherenceCluster``spec directly as shown above or whether the single role is added to the roles list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 The yaml above defines a single explicit role in the roles list When defining explict roles the role name is mandatory. The role name is set with the role field, in this case the role name of the role is data The replica count is set using the replicas field, in this case the role will have six replicas. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The yaml above defines a two explicit roles in the roles list The first role has a role name of data and a replica count of six. The second role has a role name of proxy and a replica count of three. ",
            "title": "Explicit Roles"
        },
        {
            "location": "/clusters/030_roles",
            "text": " When defining explicit roles in the roles list any values added to the CoherenceCluster spec directly (where an implicit role would normally be configured) become default values shared by all roles in the roles list unless specifically overridden by a role . This makes it easier to maintain configuration common to all roles in a single location in the spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy The roles list contains two explicit roles, data and proxy The replicas value is set at the spec level and so will be shared by both of the explicit roles In the above example the cluster will have a total of 12 members, six for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 2 Now a new web role has been introduced The replicas count in the spec section will still apply to the data and proxy roles, whch will each have a replica count of 6 The web role has a specific replicas value which will override the spec.replicas value so the web role will have two replicas ",
            "title": "Explicit Roles - Shared Values"
        },
        {
            "location": "/clusters/030_roles",
            "text": " All CoherenceCluster resources will have at lest one role defined. This could be the implicit default role or it could be one more explicit roles. Implicit Default Role As mentioned previously, all of the fields in a CoherenceCluster spec are optional meaning that the yaml below is perfectly valid. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster This yaml will create an implicit single role with a default role name of storage and a default replica count of three. The implicit role can be modified by specifying role related fields in the CoherenceCluster spec . The role name and replica count of the implicit role can be overridden using the corresponding fields <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 The role name is set with the role field, in this case the role name of the implicit role is now data The replica count is set using the replicas field, in this case the implicit role will now have six replicas. Other role fields can also be used, for example, to set the cache configuration file use by the implicit roles: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 cacheConfig: test-config.xml The cacheConfig field is used to set the cache configuration to test-config.xml . Explicit Roles It is possible to also create roles explicitly in the roles list of the CoherenceCluster spec . If creating a Coherence cluster with more than one role then all roles must be defined in the roles list. If creating a Coherence cluster with a single role it is optional whether the specification of that role is put into the CoherenceCluster``spec directly as shown above or whether the single role is added to the roles list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 The yaml above defines a single explicit role in the roles list When defining explict roles the role name is mandatory. The role name is set with the role field, in this case the role name of the role is data The replica count is set using the replicas field, in this case the role will have six replicas. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The yaml above defines a two explicit roles in the roles list The first role has a role name of data and a replica count of six. The second role has a role name of proxy and a replica count of three. Explicit Roles - Shared Values When defining explicit roles in the roles list any values added to the CoherenceCluster spec directly (where an implicit role would normally be configured) become default values shared by all roles in the roles list unless specifically overridden by a role . This makes it easier to maintain configuration common to all roles in a single location in the spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy The roles list contains two explicit roles, data and proxy The replicas value is set at the spec level and so will be shared by both of the explicit roles In the above example the cluster will have a total of 12 members, six for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 2 Now a new web role has been introduced The replicas count in the spec section will still apply to the data and proxy roles, whch will each have a replica count of 6 The web role has a specific replicas value which will override the spec.replicas value so the web role will have two replicas ",
            "title": "Defining a Coherence Role"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " The Coherence Operator facilitates safe rolling upgrade of either a application image or Coherence image. ",
            "title": "preambule"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " Install the Coherence Operator Create any secrets required to pull Docker images Create a new working directory and change to that directory ",
            "title": "1. Prerequisites"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": "<markup lang=\"bash\" >mkdir -p files-v1/lib files-v1/conf files-v2/lib files-v2/conf ",
            "title": "2. Create your directory structure"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " In your working directory directory create a file called Dockerfile-v1 with the following contents: <markup lang=\"dockerfile\" >FROM scratch COPY files-v1/lib/ /app/lib/ COPY files-v1/conf/ /app/conf/ In your working directory directory create a file called Dockerfile-v2 with the following contents: <markup lang=\"dockerfile\" >FROM scratch COPY files-v2/lib/ /app/lib/ COPY files-v2/conf/ /app/conf/ ",
            "title": "3. Create the Dockerfiles"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " Add the following content to a file in files-v1/conf called storage-cache-config.xml . This is the VERSION 1 cache config which has a single service called HRPartitionedCache . <markup lang=\"xml\" >&lt;?xml version='1.0'?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;!-- v1 Cache Config --&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;hr-*&lt;/cache-name&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;service-name&gt;HRPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Add the following content to a file in files-v2/conf called storage-cache-config.xml . This is the VERSION 2 cache config which adds an additional cahe mapping and cache service called FINPartitionedCache . <markup lang=\"xml\" >&lt;?xml version='1.0'?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;!-- v2 Cache Config --&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;hr-*&lt;/cache-name&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;fin-*&lt;/cache-name&gt; &lt;scheme-name&gt;fin-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;service-name&gt;HRPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;fin-scheme&lt;/scheme-name&gt; &lt;service-name&gt;FINPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "4. Add the required config files"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " In your working directory , issue the following: <markup lang=\"bash\" >docker build -t rolling-example:1.0.0 -f Dockerfile-v1 . docker build -t rolling-example:2.0.0 -f Dockerfile-v2 . docker images | grep rolling-example REPOSITORY TAG IMAGE ID CREATED SIZE rolling-example 2.0.0 3e195af6d5e1 8 seconds ago 1.36kB rolling-example 1.0.0 5ce9152dd12c 26 seconds ago 890B ",
            "title": "5. Build the Docker images"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " Create the file rolling-cluster.yaml with the following contents. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: rolling-cluster spec: replicas: 3 coherence: cacheConfig: storage-cache-config.xml application: image: rolling-example:1.0.0 Add an imagePullSecrets entry if required to pull images from a private repository. ",
            "title": "6. Create the Coherence cluster yaml"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " Issue the following to install the cluster: <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f rolling-cluster.yaml coherencecluster.coherence.oracle.com/rolling-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=rolling-cluster NAME READY STATUS RESTARTS AGE rolling-cluster-storage-0 1/1 Running 0 58s rolling-cluster-storage-1 1/1 Running 0 58s rolling-cluster-storage-2 1/1 Running 0 58s Ensure all pods are running and ready before you continue. ",
            "title": "7. Install the Coherence Cluster"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; rolling-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt, type cache hr-test and you will notice the following indicating your cache configuration file with the service name of HRPartitionedCache is being loaded. <markup lang=\"bash\" >... Cache Configuration: hr-test SchemeName: server AutoStart: true ServiceName: HRPartitionedCache .. Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Issue the following to confirm there is no cache mapping and service for fin-* as yet. <markup lang=\"bash\" >cache fin-test java.lang.IllegalArgumentException: ensureCache cannot find a mapping for cache fin-test Type bye to exit the console. ",
            "title": "8. Add Data to a cache in the HR service"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " Edit the rolling-cluster.yaml file and change the image: version from 1.0.0 to 2.0.0 . <markup lang=\"yaml\" >image: rolling-example:2.0.0 Issue the following to apply the new yaml: <markup lang=\"bash\" >kubectl apply -n &lt;namespace&gt; -f rolling-cluster.yaml coherencecluster.coherence.oracle.com/rolling-cluster configured Use the following command to check the status of the rolling upgrade of all pods. The command below will not return until upgrade of all pods is complete. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; rollout status sts/rolling-cluster-storage Waiting for 1 pods to be ready... statefulset rolling update complete 3 pods at revision rolling-cluster-storage-67f5cfdcb... ",
            "title": "9. Update the application image version to 2.0.0"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; rolling-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt, type cache hr-test and then size and you will see the 10,000 entries are still present because the upgrade was done is a safe manner. ",
            "title": "10. Validate the HR cache data still exists"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " At the prompt, type cache find-test and you will notice the following indicating your cache configuration file with the service name of FINPartitionedCache is now being loaded. <markup lang=\"bash\" >... Cache Configuration: fin-test SchemeName: server AutoStart: true ServiceName: FINPartitionedCache .. Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. ",
            "title": "11. Add Data to a cache in the new HR service"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": "<markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f rolling-cluster.yaml coherencecluster.coherence.oracle.com \"rolling-cluster\" deleted ",
            "title": "12. Uninstall the Coherence Cluster"
        },
        {
            "location": "/app-deployment/090_rolling",
            "text": " As described in Packaging Applications it is usual to create sidecar Docker image which provides the application classes to Kubernetes. The docker image is tagged with a version number and this version number is used by Kubernetes to enable safe rolling upgrades. The safe rolling upgrade feature allows you to instruct Kubernetes, through the operator, to replace the currently installed version of your application classes with a different one. Kubernetes does not verify whether the classes are new or old. The operator also ensures that the replacement is done without data loss or interruption of service. This is achieved simply with the Coherence Operator by using the kubectl apply command to against an existing cluster to change the attached docker image. This example shows how to issue a rolling upgrade to upgrade a cluster application image from v1.0.0 to v2.0.0 which introduces a second cache service while preserving the data in the first. Version 1 - hr-* cache mapping maps to HRPartitionedCache service Version 2 - additional fin-* cache mapping maps to FINPartitionedCache service. 1. Prerequisites Install the Coherence Operator Create any secrets required to pull Docker images Create a new working directory and change to that directory 2. Create your directory structure <markup lang=\"bash\" >mkdir -p files-v1/lib files-v1/conf files-v2/lib files-v2/conf 3. Create the Dockerfiles In your working directory directory create a file called Dockerfile-v1 with the following contents: <markup lang=\"dockerfile\" >FROM scratch COPY files-v1/lib/ /app/lib/ COPY files-v1/conf/ /app/conf/ In your working directory directory create a file called Dockerfile-v2 with the following contents: <markup lang=\"dockerfile\" >FROM scratch COPY files-v2/lib/ /app/lib/ COPY files-v2/conf/ /app/conf/ 4. Add the required config files Add the following content to a file in files-v1/conf called storage-cache-config.xml . This is the VERSION 1 cache config which has a single service called HRPartitionedCache . <markup lang=\"xml\" >&lt;?xml version='1.0'?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;!-- v1 Cache Config --&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;hr-*&lt;/cache-name&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;service-name&gt;HRPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Add the following content to a file in files-v2/conf called storage-cache-config.xml . This is the VERSION 2 cache config which adds an additional cahe mapping and cache service called FINPartitionedCache . <markup lang=\"xml\" >&lt;?xml version='1.0'?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;!-- v2 Cache Config --&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;hr-*&lt;/cache-name&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;fin-*&lt;/cache-name&gt; &lt;scheme-name&gt;fin-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hr-scheme&lt;/scheme-name&gt; &lt;service-name&gt;HRPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;fin-scheme&lt;/scheme-name&gt; &lt;service-name&gt;FINPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;{back-limit-bytes 0B}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; 5. Build the Docker images In your working directory , issue the following: <markup lang=\"bash\" >docker build -t rolling-example:1.0.0 -f Dockerfile-v1 . docker build -t rolling-example:2.0.0 -f Dockerfile-v2 . docker images | grep rolling-example REPOSITORY TAG IMAGE ID CREATED SIZE rolling-example 2.0.0 3e195af6d5e1 8 seconds ago 1.36kB rolling-example 1.0.0 5ce9152dd12c 26 seconds ago 890B 6. Create the Coherence cluster yaml Create the file rolling-cluster.yaml with the following contents. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: rolling-cluster spec: replicas: 3 coherence: cacheConfig: storage-cache-config.xml application: image: rolling-example:1.0.0 Add an imagePullSecrets entry if required to pull images from a private repository. 7. Install the Coherence Cluster Issue the following to install the cluster: <markup lang=\"bash\" >kubectl create -n &lt;namespace&gt; -f rolling-cluster.yaml coherencecluster.coherence.oracle.com/rolling-cluster created kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=rolling-cluster NAME READY STATUS RESTARTS AGE rolling-cluster-storage-0 1/1 Running 0 58s rolling-cluster-storage-1 1/1 Running 0 58s rolling-cluster-storage-2 1/1 Running 0 58s Ensure all pods are running and ready before you continue. 8. Add Data to a cache in the HR service <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; rolling-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt, type cache hr-test and you will notice the following indicating your cache configuration file with the service name of HRPartitionedCache is being loaded. <markup lang=\"bash\" >... Cache Configuration: hr-test SchemeName: server AutoStart: true ServiceName: HRPartitionedCache .. Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Issue the following to confirm there is no cache mapping and service for fin-* as yet. <markup lang=\"bash\" >cache fin-test java.lang.IllegalArgumentException: ensureCache cannot find a mapping for cache fin-test Type bye to exit the console. 9. Update the application image version to 2.0.0 Edit the rolling-cluster.yaml file and change the image: version from 1.0.0 to 2.0.0 . <markup lang=\"yaml\" >image: rolling-example:2.0.0 Issue the following to apply the new yaml: <markup lang=\"bash\" >kubectl apply -n &lt;namespace&gt; -f rolling-cluster.yaml coherencecluster.coherence.oracle.com/rolling-cluster configured Use the following command to check the status of the rolling upgrade of all pods. The command below will not return until upgrade of all pods is complete. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; rollout status sts/rolling-cluster-storage Waiting for 1 pods to be ready... statefulset rolling update complete 3 pods at revision rolling-cluster-storage-67f5cfdcb... 10. Validate the HR cache data still exists <markup lang=\"bash\" >kubectl exec -it -n &lt;namespace&gt; rolling-cluster-storage-0 bash /scripts/startCoherence.sh console At the prompt, type cache hr-test and then size and you will see the 10,000 entries are still present because the upgrade was done is a safe manner. 11. Add Data to a cache in the new HR service At the prompt, type cache find-test and you will notice the following indicating your cache configuration file with the service name of FINPartitionedCache is now being loaded. <markup lang=\"bash\" >... Cache Configuration: fin-test SchemeName: server AutoStart: true ServiceName: FINPartitionedCache .. Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. 12. Uninstall the Coherence Cluster <markup lang=\"bash\" >kubectl delete -n &lt;namespace&gt; -f rolling-cluster.yaml coherencecluster.coherence.oracle.com \"rolling-cluster\" deleted ",
            "title": "Rolling Upgrades"
        },
        {
            "location": "/developer/08_docs",
            "text": " The Coherence Operator documentation can be built directly from make commands. ",
            "title": "preambule"
        },
        {
            "location": "/developer/08_docs",
            "text": " To build the documentation run <markup lang=\"bash\" >make docs This will build the documentation into the directory build/_output/docs ",
            "title": "Build"
        },
        {
            "location": "/developer/08_docs",
            "text": " To see the results of local changes to the documentation it is possible to run a local web-server that will allow the docs to be viewed in a browser. <markup lang=\"bash\" >make server-docs This will start a local web-server on http://localhost:8080 This is useful to see changes in real time as documentation is edited and re-built. The server does no need to be restarted between documentation builds. The local web-server requires Python ",
            "title": "View"
        },
        {
            "location": "/developer/08_docs",
            "text": " The Coherence Operator documentation is written in Ascii Doc format and is built with tools provided by our friends over in the Helidon team. The documentation source is under the docs/ directory. Build To build the documentation run <markup lang=\"bash\" >make docs This will build the documentation into the directory build/_output/docs View To see the results of local changes to the documentation it is possible to run a local web-server that will allow the docs to be viewed in a browser. <markup lang=\"bash\" >make server-docs This will start a local web-server on http://localhost:8080 This is useful to see changes in real time as documentation is edited and re-built. The server does no need to be restarted between documentation builds. The local web-server requires Python ",
            "title": "Building the Coherence Operator Documentation"
        }
 ]
}