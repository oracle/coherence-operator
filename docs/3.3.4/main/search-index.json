{
    "docs": [
        {
            "location": "/docs/coherence/040_override_file",
            "text": " The name of the Coherence operations configuration file (commonly called the overrides file) that the Coherence processes in a Coherence resource will use can be set with the spec.coherence.overrideConfig field. By setting this field the coherence.override system property will be set in the Coherence JVM. When the spec.coherence.overrideConfig is blank or not specified, Coherence use its default behaviour to find the operational configuration file to use. Typically, this is to use the first occurrence of tangosol-coherence-override.xml that is found on the classpath (consult the Coherence documentation for an explanation of the default behaviour). To set a specific operational configuration file to use set the spec.coherence.overrideConfig field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: overrideConfig: test-override.xml The spec.coherence.overrideConfig field has been set to test-override.xml which will effectively pass -Dcoherence.override=test-override.xml to the JVM command line. ",
            "title": "Set the Operational Configuration File Name"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The Coherence Operator is available as an image from the GitHub container registry ghcr.io/oracle/coherence-operator:3.3.4 that can easily be installed into a Kubernetes cluster. ",
            "title": "preambule"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The prerequisites apply to all installation methods. Access to Oracle Coherence Operator images. Access to a Kubernetes v1.19.0+ cluster. The Operator test pipeline is run using Kubernetes versions v1.19 upto v1.26 A Coherence application image using Coherence version 12.2.1.3 or later. Note that some functionality (e.g. metrics) is only available in Coherence 12.2.1.4 and later. Note ARM Support: As of version 3.2.0, the Coherence Operator is build as a multi-architecture image that supports running in Kubernetes on both Linux/amd64 and Linux/arm64. The prerequisite is that the Coherence application image used has been built to support ARM. Note Istio (or similar service meshes) When installing the Operator and Coherence into Kubernetes cluster that use Istio or similar meshes there are a number of pre-requisites that must be understood. See the Istio example for more details. There are a number of ways to install the Coherence Operator documented below: Simple installation using Kubectl Install the Helm chart Kubectl with Kustomize VMWare Tanzu Package (kapp-controller) ",
            "title": "Prerequisites"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The Coherence Operator runs in HA mode by default. The Deployment created by the installation will have a replica count of 3. In reduced capacity Kubernetes clusters, for example, local laptop development and test, the replica count can be reduced. It is recommended to leave the default of 3 for production environments. Instructions on how to change the replica count for the different install methods are included below. The Coherence Operator runs a REST server that the Coherence cluster members will query to discover the site and rack names that should be used by Coherence. If the Coherence Operator is not running when a Coherence Pod starts, then the Coherence member in that Pod will be unable to properly configure its site and rack names, possibly leading to data distribution that is not safely distributed over sites. In production, and in Kubernetes clusters that are spread over multiple availability zones and failure domains, it is important to run the Operator in HA mode. The Operator yaml files and Helm chart include a default Pod scheduling configuration that uses anti-affinity to distribute the three replicas onto nodes that have different topology.kubernetes.io/zone labels. This label is a standard Kubernetes label used to describe the zone the node is running in, and is typically applied by Kubernetes cloud vendors. ",
            "title": "High Availability"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Installing the Coherence Operator using the methods below will create a number of ClusterRole RBAC resources. Some corporate security policies do not like to give cluster wide roles to third-party products. To help in this situation the operator can be installed without cluster roles, but with caveats (see the RBAC documentation) for more details. OpenShift - the Coherence Operator works without modification on OpenShift, but some versions of the Coherence images will not work out of the box. See the OpensShift section of the documentation that explains how to run Coherence clusters with the Operator on OpenShift. Whilst Coherence works out of the box on many Kubernetes installations, some Kubernetes installations may configure iptables in a way that causes Coherence to fail to create clusters. See the O/S Network Configuration section of the documentation for more details if you have well-known-address issues when Pods attempt to form a cluster. ",
            "title": "Notes"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Contents Prerequisites before installation Operator High Availability Coherence Operator Images Operator Scope - monitoring all or a fixed set of namespaces Installation Options Simple installation using Kubectl Install the Helm chart Set the Operator Image Image Pull Secrets Set the Watch Namespaces Install the Operator with a Security Context Set Additional Labels Set Additional Annotations Uninstall the Coherence Operator Helm chart Kubectl with Kustomize VMWare Tanzu Package (kapp-controller) Prerequisites The prerequisites apply to all installation methods. Access to Oracle Coherence Operator images. Access to a Kubernetes v1.19.0+ cluster. The Operator test pipeline is run using Kubernetes versions v1.19 upto v1.26 A Coherence application image using Coherence version 12.2.1.3 or later. Note that some functionality (e.g. metrics) is only available in Coherence 12.2.1.4 and later. Note ARM Support: As of version 3.2.0, the Coherence Operator is build as a multi-architecture image that supports running in Kubernetes on both Linux/amd64 and Linux/arm64. The prerequisite is that the Coherence application image used has been built to support ARM. Note Istio (or similar service meshes) When installing the Operator and Coherence into Kubernetes cluster that use Istio or similar meshes there are a number of pre-requisites that must be understood. See the Istio example for more details. There are a number of ways to install the Coherence Operator documented below: Simple installation using Kubectl Install the Helm chart Kubectl with Kustomize VMWare Tanzu Package (kapp-controller) High Availability The Coherence Operator runs in HA mode by default. The Deployment created by the installation will have a replica count of 3. In reduced capacity Kubernetes clusters, for example, local laptop development and test, the replica count can be reduced. It is recommended to leave the default of 3 for production environments. Instructions on how to change the replica count for the different install methods are included below. The Coherence Operator runs a REST server that the Coherence cluster members will query to discover the site and rack names that should be used by Coherence. If the Coherence Operator is not running when a Coherence Pod starts, then the Coherence member in that Pod will be unable to properly configure its site and rack names, possibly leading to data distribution that is not safely distributed over sites. In production, and in Kubernetes clusters that are spread over multiple availability zones and failure domains, it is important to run the Operator in HA mode. The Operator yaml files and Helm chart include a default Pod scheduling configuration that uses anti-affinity to distribute the three replicas onto nodes that have different topology.kubernetes.io/zone labels. This label is a standard Kubernetes label used to describe the zone the node is running in, and is typically applied by Kubernetes cloud vendors. Notes Installing the Coherence Operator using the methods below will create a number of ClusterRole RBAC resources. Some corporate security policies do not like to give cluster wide roles to third-party products. To help in this situation the operator can be installed without cluster roles, but with caveats (see the RBAC documentation) for more details. OpenShift - the Coherence Operator works without modification on OpenShift, but some versions of the Coherence images will not work out of the box. See the OpensShift section of the documentation that explains how to run Coherence clusters with the Operator on OpenShift. Whilst Coherence works out of the box on many Kubernetes installations, some Kubernetes installations may configure iptables in a way that causes Coherence to fail to create clusters. See the O/S Network Configuration section of the documentation for more details if you have well-known-address issues when Pods attempt to form a cluster. ",
            "title": "Coherence Operator Installation"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The Coherence Operator uses a single image, the Operator also runs as an init-container in the Coherence cluster Pods. ghcr.io/oracle/coherence-operator:3.3.4 - The Operator image. If no image is specified in the Coherence yaml, then the default Coherence image will also be used, ghcr.io/oracle/coherence-ce:22.06.7 - The default Coherence image. If using a private image registry then these images will all need to be pushed to that registry for the Operator to work. The default Coherence image may be omitted if all Coherence applications will use custom Coherence images. ",
            "title": "Coherence Operator Images"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The recommended way to install the Coherence Operator is to install a single instance of the operator into a namespace and where it will then control Coherence resources in all namespaces across the Kubernetes cluster. Alternatively it may be configured to watch a sub-set of namespaces by setting the WATCH_NAMESPACE environment variable. The watch namespace(s) does not have to include the installation namespace. Caution In theory, it is possible to install multiple instances of the Coherence Operator into different namespaces, where each instance monitors a different set of namespaces. There are a number of potential issues with this approach, so it is not recommended. Only one version of a CRD can be installed - There is currently only a single version of the CRD, but different releases of the Operator may use slightly different specs of this CRD version, for example a new Operator release may introduce extra fields not in the previous releases. As the CRD version is fixed at v1 there is no guarantee which CRD version has actually installed, which could lead to subtle issues. The operator creates and installs defaulting and validating web-hooks. A web-hook is associated to a CRD resource so installing multiple web-hooks for the same resource may lead to issues. If an operator is uninstalled, but the web-hook configuration remains, then Kubernetes will not accept modifications to resources of that type as it will be unable to contact the web-hook. It is possible to run the Operator without web-hooks, but this has its own caveats see the Web Hooks documentation for how to do this. Important If multiple instance of the Operator are installed, where they are monitoring the same namespaces, this can cause issues. For example, when a Coherence resource is then changed, all the Operator deployments will receive the same events from Etcd and try to apply the same changes. Sometimes this may work, sometimes there may be errors, for example multiple Operators trying to remove finalizers and delete a Coherence cluster. ",
            "title": "Operator Scope"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " When installing with single manifest yaml file, the replica count can be changed by editing the yaml file itself to change the occurrence of replicas: 3 in the manifest yaml to replicas: 1 For example, this could be done using sed <markup lang=\"bash\" >sed -i -e 's/replicas: 3/replicas: 1/g' coherence-operator.yaml Or on MacOS, where sed is slightly different: <markup lang=\"bash\" >sed -i '' -e 's/replicas: 3/replicas: 1/g' coherence-operator.yaml ",
            "title": "Change the Operator Replica Count"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " If you want the default Coherence Operator installation then the simplest solution is use kubectl to apply the manifests from the Operator release. <markup lang=\"bash\" >kubectl apply -f https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml This will create a namespace called coherence and install the Operator into it along with all the required ClusterRole and RoleBinding resources. The coherence namespace can be changed by downloading and editing the yaml file. Because the coherence-operator.yaml manifest also creates the namespace, the corresponding kubectl delete command will remove the namespace and everything deployed to it ! If you do not want this behaviour you should edit the coherence-operator.yaml to remove the namespace section from the start of the file. Instead of using a hard coded version in the command above you can find the latest Operator version using curl : <markup lang=\"bash\" >export VERSION=$(curl -s \\ https://api.github.com/repos/oracle/coherence-operator/releases/latest \\ | grep '\"name\": \"v' \\ | cut -d '\"' -f 4 \\ | cut -b 2-10) Then download with: <markup lang=\"bash\" >kubectl apply -f https://github.com/oracle/coherence-operator/releases/download/${VERSION}/coherence-operator.yaml Change the Operator Replica Count When installing with single manifest yaml file, the replica count can be changed by editing the yaml file itself to change the occurrence of replicas: 3 in the manifest yaml to replicas: 1 For example, this could be done using sed <markup lang=\"bash\" >sed -i -e 's/replicas: 3/replicas: 1/g' coherence-operator.yaml Or on MacOS, where sed is slightly different: <markup lang=\"bash\" >sed -i '' -e 's/replicas: 3/replicas: 1/g' coherence-operator.yaml ",
            "title": "Default Install with Kubectl"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Add the coherence helm repository using the following commands: <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update To avoid confusion, the URL https://oracle.github.io/coherence-operator/charts is a Helm repo, it is not a website you open in a browser. You may think we shouldn&#8217;t have to say this, but you&#8217;d be surprised. ",
            "title": "Add the Coherence Helm Repository"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Once the Coherence Helm repo has been configured the Coherence Operator can be installed using a normal Helm 3 install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ coherence \\ coherence/coherence-operator where &lt;namespace&gt; is the namespace that the Coherence Operator will be installed into. coherence is the name of this Helm installation. ",
            "title": "Install the Coherence Operator Helm chart"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The Helm chart uses a default Operator image from ghcr.io/oracle/coherence-operator:3.3.4 . If the image needs to be pulled from a different location (for example an internal registry) then there are two ways to override the default. Either set the individual image.registry , image.name and image.tag values, or set the whole image name by setting the image value. For example, if the Operator image has been deployed into a private registry named foo.com but with the same image name coherence-operator and tag 3.3.4 as the default image, then just the image.registry needs to be specified. In the example below, the image used to run the Operator will be foo.com/coherence-operator:3.3.4 . <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set image.registry=foo.com \\ coherence-operator \\ coherence/coherence-operator All three of the image parts can be specified individually using --set options. In the example below, the image used to run the Operator will be foo.com/operator:1.2.3 . <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set image.registry=foo.com \\ --set image.name=operator \\ --set image.tag=1.2.3 coherence-operator \\ coherence/coherence-operator Alternatively, the image can be set using a single image value. For example, the command below will set the Operator image to images.com/coherence-operator:0.1.2 . <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set image=images.com/coherence-operator:0.1.2 \\ coherence-operator \\ coherence/coherence-operator ",
            "title": "Set the Operator Image"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Create a values file that specifies the secrets, for example the private-repo-values.yaml file below: <markup lang=\"yaml\" title=\"private-repo-values.yaml\" >imagePullSecrets: - name: registry-secrets Now use that file in the Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ -f private-repo-values.yaml coherence-operator \\ coherence/coherence-operator the private-repo-values.yaml values fle will be used by Helm to inject the settings into the Operator deployment ",
            "title": "Add Pull Secrets Using a Values File"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Although the imagePullSecrets field in the values file is an array of name to value pairs it is possible to set these values with the normal Helm --set parameter. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set imagePullSecrets[0].name=registry-secrets coherence-operator \\ coherence/coherence-operator this creates the same imagePullSecrets as the values file above. ",
            "title": "Add Pull Secrets Using --set"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " If the image is to be pulled from a secure repository that requires credentials then the image pull secrets can be specified. See the Kubernetes documentation on Pulling from a Private Registry . Add Pull Secrets Using a Values File Create a values file that specifies the secrets, for example the private-repo-values.yaml file below: <markup lang=\"yaml\" title=\"private-repo-values.yaml\" >imagePullSecrets: - name: registry-secrets Now use that file in the Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ -f private-repo-values.yaml coherence-operator \\ coherence/coherence-operator the private-repo-values.yaml values fle will be used by Helm to inject the settings into the Operator deployment Add Pull Secrets Using --set Although the imagePullSecrets field in the values file is an array of name to value pairs it is possible to set these values with the normal Helm --set parameter. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set imagePullSecrets[0].name=registry-secrets coherence-operator \\ coherence/coherence-operator this creates the same imagePullSecrets as the values file above. ",
            "title": "Image Pull Secrets"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To change the replica count when installing the Operator using Helm, the replicas value can be set. For example, to change the replica count from 3 to 1, the --set replicas=1 option can be used. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set replicas=1 coherence \\ coherence/coherence-operator ",
            "title": "Change the Operator Replica Count"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " When installing the Operator using the Helm chart, there is a convenience value that can be set if the Operator should only monitor the same namespace that it is installed into. By setting the onlySameNamespace value to true the watch namespace will be set to the installation namespace. If the onlySameNamespace value is set to true then any value set for the watchNamespaces value will be ignored. For example, the command below will set onlySameNamespace to true, and the Operator will be installed into, and only monitor the coh-testing namespace. <markup lang=\"bash\" >helm install \\ --namespace coh-testing \\ --set onlySameNamespace=true \\ coherence-operator \\ coherence/coherence-operator In the example below, the onlySameNamespace is set to true, so the Operator will be installed into, and only monitor the coh-testing namespace. Even though the watchNamespaces value is set, it will be ignored. <markup lang=\"bash\" >helm install \\ --namespace coh-testing \\ --set watchNamespaces=payments,catalog,customers \\ --set onlySameNamespace=true \\ coherence-operator \\ coherence/coherence-operator ",
            "title": "Set the Watch Namespace to the Operator&#8217;s Install Namespace"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To set the watch namespaces when installing with helm set the watchNamespaces value, for example: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set watchNamespaces=payments,catalog,customers \\ coherence-operator \\ coherence/coherence-operator The payments , catalog and customers namespaces will be watched by the Operator. Set the Watch Namespace to the Operator&#8217;s Install Namespace When installing the Operator using the Helm chart, there is a convenience value that can be set if the Operator should only monitor the same namespace that it is installed into. By setting the onlySameNamespace value to true the watch namespace will be set to the installation namespace. If the onlySameNamespace value is set to true then any value set for the watchNamespaces value will be ignored. For example, the command below will set onlySameNamespace to true, and the Operator will be installed into, and only monitor the coh-testing namespace. <markup lang=\"bash\" >helm install \\ --namespace coh-testing \\ --set onlySameNamespace=true \\ coherence-operator \\ coherence/coherence-operator In the example below, the onlySameNamespace is set to true, so the Operator will be installed into, and only monitor the coh-testing namespace. Even though the watchNamespaces value is set, it will be ignored. <markup lang=\"bash\" >helm install \\ --namespace coh-testing \\ --set watchNamespaces=payments,catalog,customers \\ --set onlySameNamespace=true \\ coherence-operator \\ coherence/coherence-operator ",
            "title": "Set the Watch Namespaces"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The Operator container can be configured with a Pod securityContext or a container securityContext , so that it runs as a non-root user. This can be done using a values file: Set the Pod securityContext <markup lang=\"yaml\" title=\"security-values.yaml\" >podSecurityContext: runAsNonRoot: true runAsUser: 1000 Set the Container securityContext <markup lang=\"yaml\" title=\"security-values.yaml\" >securityContext: runAsNonRoot: true runAsUser: 1000 Then the security-values.yaml values file above can be used in the Helm install command. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --values security-values.yaml \\ coherence \\ coherence/coherence-operator Alternatively, the Pod or container securityContext values can be set on the command line as --set parameters: Set the Pod securityContext <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set podSecurityContext.runAsNonRoot=true \\ --set podSecurityContext.runAsUser=1000 \\ coherence \\ coherence/coherence-operator Set the Container securityContext <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set securityContext.runAsNonRoot=true \\ --set securityContext.runAsUser=1000 \\ coherence \\ coherence/coherence-operator ",
            "title": "Install the Operator with a Security Context"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To add labels to the Operator Pods set the labels value, either on the command line using --set or in the values file. Note Setting labels will only apply the additional labels to the Operator Pods, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set labels.one=value-one \\ --set labels.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional labels one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: coherence-operator labels: one: value-one two: value-two The same labels could also be specified in a values file: <markup title=\"add-labels-values.yaml\" >labels: one: value-one two: value-two ",
            "title": "Adding Pod Labels"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To add labels to the Operator Deployment set the deploymentLabels value, either on the command line using --set or in the values file. Note Setting deploymentLabels will only apply the additional labels to the Deployment, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set deploymentLabels.one=value-one \\ --set deploymentLabels.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional labels one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: Deployment metadata: name: coherence-operator labels: one: value-one two: value-two The same labels could also be specified in a values file: <markup title=\"add-labels-values.yaml\" >deploymentLabels: one: value-one two: value-two ",
            "title": "Adding Deployment Labels"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " When installing the Operator with Helm, it is possible to set additional labels to be applied to the Operator Pods and to the Operator Deployment. Adding Pod Labels To add labels to the Operator Pods set the labels value, either on the command line using --set or in the values file. Note Setting labels will only apply the additional labels to the Operator Pods, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set labels.one=value-one \\ --set labels.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional labels one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: coherence-operator labels: one: value-one two: value-two The same labels could also be specified in a values file: <markup title=\"add-labels-values.yaml\" >labels: one: value-one two: value-two Adding Deployment Labels To add labels to the Operator Deployment set the deploymentLabels value, either on the command line using --set or in the values file. Note Setting deploymentLabels will only apply the additional labels to the Deployment, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set deploymentLabels.one=value-one \\ --set deploymentLabels.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional labels one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: Deployment metadata: name: coherence-operator labels: one: value-one two: value-two The same labels could also be specified in a values file: <markup title=\"add-labels-values.yaml\" >deploymentLabels: one: value-one two: value-two ",
            "title": "Set Additional Labels"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To add annotations to the Operator Pods set the annotations value, either on the command line using --set or in the values file. Note Setting annotations will only apply the additional annotations to the Operator Pods, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set annotations.one=value-one \\ --set annotations.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional annotations one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: coherence-operator annotations: one: value-one two: value-two The same annotations could also be specified in a values file: <markup title=\"add-annotations-values.yaml\" >annotations: one: value-one two: value-two ",
            "title": "Adding Pod Annotations"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To add annotations to the Operator Deployment set the deploymentAnnotations value, either on the command line using --set or in the values file. Note Setting deploymentAnnotations will only apply the additional annotations to the Deployment, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set deploymentAnnotations.one=value-one \\ --set deploymentAnnotations.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional annotations one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: Deployment metadata: name: coherence-operator annotations: one: value-one two: value-two The same annotations could also be specified in a values file: <markup title=\"add-annotations-values.yaml\" >deploymentAnnotations: one: value-one two: value-two ",
            "title": "Adding Deployment Annotations"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " When installing the Operator with Helm, it is possible to set additional annotations to be applied to the Operator Pods and to the Operator Deployment. Adding Pod Annotations To add annotations to the Operator Pods set the annotations value, either on the command line using --set or in the values file. Note Setting annotations will only apply the additional annotations to the Operator Pods, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set annotations.one=value-one \\ --set annotations.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional annotations one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: coherence-operator annotations: one: value-one two: value-two The same annotations could also be specified in a values file: <markup title=\"add-annotations-values.yaml\" >annotations: one: value-one two: value-two Adding Deployment Annotations To add annotations to the Operator Deployment set the deploymentAnnotations value, either on the command line using --set or in the values file. Note Setting deploymentAnnotations will only apply the additional annotations to the Deployment, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set deploymentAnnotations.one=value-one \\ --set deploymentAnnotations.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional annotations one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: Deployment metadata: name: coherence-operator annotations: one: value-one two: value-two The same annotations could also be specified in a values file: <markup title=\"add-annotations-values.yaml\" >deploymentAnnotations: one: value-one two: value-two ",
            "title": "Set Additional Annotations"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To uninstall the operator: <markup lang=\"bash\" >helm delete coherence-operator --namespace &lt;namespace&gt; ",
            "title": "Uninstall the Coherence Operator Helm chart"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " For more flexibility but the simplest way to install the Coherence Operator is to use the Helm chart. This ensures that all the correct resources will be created in Kubernetes. Add the Coherence Helm Repository Add the coherence helm repository using the following commands: <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update To avoid confusion, the URL https://oracle.github.io/coherence-operator/charts is a Helm repo, it is not a website you open in a browser. You may think we shouldn&#8217;t have to say this, but you&#8217;d be surprised. Install the Coherence Operator Helm chart Once the Coherence Helm repo has been configured the Coherence Operator can be installed using a normal Helm 3 install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ coherence \\ coherence/coherence-operator where &lt;namespace&gt; is the namespace that the Coherence Operator will be installed into. coherence is the name of this Helm installation. Set the Operator Image The Helm chart uses a default Operator image from ghcr.io/oracle/coherence-operator:3.3.4 . If the image needs to be pulled from a different location (for example an internal registry) then there are two ways to override the default. Either set the individual image.registry , image.name and image.tag values, or set the whole image name by setting the image value. For example, if the Operator image has been deployed into a private registry named foo.com but with the same image name coherence-operator and tag 3.3.4 as the default image, then just the image.registry needs to be specified. In the example below, the image used to run the Operator will be foo.com/coherence-operator:3.3.4 . <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set image.registry=foo.com \\ coherence-operator \\ coherence/coherence-operator All three of the image parts can be specified individually using --set options. In the example below, the image used to run the Operator will be foo.com/operator:1.2.3 . <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set image.registry=foo.com \\ --set image.name=operator \\ --set image.tag=1.2.3 coherence-operator \\ coherence/coherence-operator Alternatively, the image can be set using a single image value. For example, the command below will set the Operator image to images.com/coherence-operator:0.1.2 . <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set image=images.com/coherence-operator:0.1.2 \\ coherence-operator \\ coherence/coherence-operator Image Pull Secrets If the image is to be pulled from a secure repository that requires credentials then the image pull secrets can be specified. See the Kubernetes documentation on Pulling from a Private Registry . Add Pull Secrets Using a Values File Create a values file that specifies the secrets, for example the private-repo-values.yaml file below: <markup lang=\"yaml\" title=\"private-repo-values.yaml\" >imagePullSecrets: - name: registry-secrets Now use that file in the Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ -f private-repo-values.yaml coherence-operator \\ coherence/coherence-operator the private-repo-values.yaml values fle will be used by Helm to inject the settings into the Operator deployment Add Pull Secrets Using --set Although the imagePullSecrets field in the values file is an array of name to value pairs it is possible to set these values with the normal Helm --set parameter. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set imagePullSecrets[0].name=registry-secrets coherence-operator \\ coherence/coherence-operator this creates the same imagePullSecrets as the values file above. Change the Operator Replica Count To change the replica count when installing the Operator using Helm, the replicas value can be set. For example, to change the replica count from 3 to 1, the --set replicas=1 option can be used. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set replicas=1 coherence \\ coherence/coherence-operator Set the Watch Namespaces To set the watch namespaces when installing with helm set the watchNamespaces value, for example: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set watchNamespaces=payments,catalog,customers \\ coherence-operator \\ coherence/coherence-operator The payments , catalog and customers namespaces will be watched by the Operator. Set the Watch Namespace to the Operator&#8217;s Install Namespace When installing the Operator using the Helm chart, there is a convenience value that can be set if the Operator should only monitor the same namespace that it is installed into. By setting the onlySameNamespace value to true the watch namespace will be set to the installation namespace. If the onlySameNamespace value is set to true then any value set for the watchNamespaces value will be ignored. For example, the command below will set onlySameNamespace to true, and the Operator will be installed into, and only monitor the coh-testing namespace. <markup lang=\"bash\" >helm install \\ --namespace coh-testing \\ --set onlySameNamespace=true \\ coherence-operator \\ coherence/coherence-operator In the example below, the onlySameNamespace is set to true, so the Operator will be installed into, and only monitor the coh-testing namespace. Even though the watchNamespaces value is set, it will be ignored. <markup lang=\"bash\" >helm install \\ --namespace coh-testing \\ --set watchNamespaces=payments,catalog,customers \\ --set onlySameNamespace=true \\ coherence-operator \\ coherence/coherence-operator Install the Operator with a Security Context The Operator container can be configured with a Pod securityContext or a container securityContext , so that it runs as a non-root user. This can be done using a values file: Set the Pod securityContext <markup lang=\"yaml\" title=\"security-values.yaml\" >podSecurityContext: runAsNonRoot: true runAsUser: 1000 Set the Container securityContext <markup lang=\"yaml\" title=\"security-values.yaml\" >securityContext: runAsNonRoot: true runAsUser: 1000 Then the security-values.yaml values file above can be used in the Helm install command. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --values security-values.yaml \\ coherence \\ coherence/coherence-operator Alternatively, the Pod or container securityContext values can be set on the command line as --set parameters: Set the Pod securityContext <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set podSecurityContext.runAsNonRoot=true \\ --set podSecurityContext.runAsUser=1000 \\ coherence \\ coherence/coherence-operator Set the Container securityContext <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set securityContext.runAsNonRoot=true \\ --set securityContext.runAsUser=1000 \\ coherence \\ coherence/coherence-operator Set Additional Labels When installing the Operator with Helm, it is possible to set additional labels to be applied to the Operator Pods and to the Operator Deployment. Adding Pod Labels To add labels to the Operator Pods set the labels value, either on the command line using --set or in the values file. Note Setting labels will only apply the additional labels to the Operator Pods, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set labels.one=value-one \\ --set labels.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional labels one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: coherence-operator labels: one: value-one two: value-two The same labels could also be specified in a values file: <markup title=\"add-labels-values.yaml\" >labels: one: value-one two: value-two Adding Deployment Labels To add labels to the Operator Deployment set the deploymentLabels value, either on the command line using --set or in the values file. Note Setting deploymentLabels will only apply the additional labels to the Deployment, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set deploymentLabels.one=value-one \\ --set deploymentLabels.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional labels one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: Deployment metadata: name: coherence-operator labels: one: value-one two: value-two The same labels could also be specified in a values file: <markup title=\"add-labels-values.yaml\" >deploymentLabels: one: value-one two: value-two Set Additional Annotations When installing the Operator with Helm, it is possible to set additional annotations to be applied to the Operator Pods and to the Operator Deployment. Adding Pod Annotations To add annotations to the Operator Pods set the annotations value, either on the command line using --set or in the values file. Note Setting annotations will only apply the additional annotations to the Operator Pods, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set annotations.one=value-one \\ --set annotations.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional annotations one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: coherence-operator annotations: one: value-one two: value-two The same annotations could also be specified in a values file: <markup title=\"add-annotations-values.yaml\" >annotations: one: value-one two: value-two Adding Deployment Annotations To add annotations to the Operator Deployment set the deploymentAnnotations value, either on the command line using --set or in the values file. Note Setting deploymentAnnotations will only apply the additional annotations to the Deployment, they will not be applied to any other resource created by the Helm chart. For example, using the command line: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set deploymentAnnotations.one=value-one \\ --set deploymentAnnotations.two=value-two \\ coherence \\ coherence/coherence-operator The command above would add the following additional annotations one and two to the Operator Pod as shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: Deployment metadata: name: coherence-operator annotations: one: value-one two: value-two The same annotations could also be specified in a values file: <markup title=\"add-annotations-values.yaml\" >deploymentAnnotations: one: value-one two: value-two Uninstall the Coherence Operator Helm chart To uninstall the operator: <markup lang=\"bash\" >helm delete coherence-operator --namespace &lt;namespace&gt; ",
            "title": "Installing With Helm"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " To change the replica count using Kustomize a patch file needs to be applied. The Operator manifests include a patch file, named manager/single-replica-patch.yaml , that changes the replica count from 3 to 1. This patch can be applied with the following Kustomize command. <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit add patch \\ --kind Deployment --name controller-manager \\ --path single-replica-patch.yaml ",
            "title": "Change the Operator Replica Count"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " If you need to use different iamge names from the defaults kustomize can be used to specify different names: Change the name of the Operator image by running the command below, changing the image name to the registry and image name that you are using for the Operator, for example if you have the images in a custom registry <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit set image controller=myregistry/coherence-operator:3.3.4 Change the name of the Operator image by running the command below, changing the image name to the registry and image name that you are using for the Operator utilities image <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit add configmap env-vars --from-literal OPERATOR_IMAGE=myregistry/coherence-operator:3.3.4 Change the name of the default Coherence image. If you are always going to be deploying your own application images then this does not need to change. <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal COHERENCE_IMAGE=$(COHERENCE_IMAGE) Set the namespace to install into, the example below sets the namespace to coherence-test : <markup lang=\"bash\" >cd ./default &amp;&amp; /kustomize edit set namespace coherence-test ",
            "title": "Set Image Names"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The Operator requires a Secret for its web-hook certificates. This Secret needs to exist but can be empty. The Secret must be in the same namespace that the Operator will be deployed to. For example, if the Operator namespace is coherence-test , then the Secret can be created with this command: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic coherence-webhook-server-cert The Operator can now be installed by running the following command from the manifests directory: <markup lang=\"bash\" >kustomize build ./default | kubectl apply -f - ",
            "title": "Install"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " If you have Kustomize installed (or can install it from https://github.com/kubernetes-sigs/kustomize ) you can use Kustomize to configure the yaml and install. Change the Operator Replica Count To change the replica count using Kustomize a patch file needs to be applied. The Operator manifests include a patch file, named manager/single-replica-patch.yaml , that changes the replica count from 3 to 1. This patch can be applied with the following Kustomize command. <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit add patch \\ --kind Deployment --name controller-manager \\ --path single-replica-patch.yaml Set Image Names If you need to use different iamge names from the defaults kustomize can be used to specify different names: Change the name of the Operator image by running the command below, changing the image name to the registry and image name that you are using for the Operator, for example if you have the images in a custom registry <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit set image controller=myregistry/coherence-operator:3.3.4 Change the name of the Operator image by running the command below, changing the image name to the registry and image name that you are using for the Operator utilities image <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit add configmap env-vars --from-literal OPERATOR_IMAGE=myregistry/coherence-operator:3.3.4 Change the name of the default Coherence image. If you are always going to be deploying your own application images then this does not need to change. <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal COHERENCE_IMAGE=$(COHERENCE_IMAGE) Set the namespace to install into, the example below sets the namespace to coherence-test : <markup lang=\"bash\" >cd ./default &amp;&amp; /kustomize edit set namespace coherence-test Install The Operator requires a Secret for its web-hook certificates. This Secret needs to exist but can be empty. The Secret must be in the same namespace that the Operator will be deployed to. For example, if the Operator namespace is coherence-test , then the Secret can be created with this command: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic coherence-webhook-server-cert The Operator can now be installed by running the following command from the manifests directory: <markup lang=\"bash\" >kustomize build ./default | kubectl apply -f - ",
            "title": "Install with Kustomize"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Instead of using Kustomize to modify and install the Operator we can use kubectl to generate the yaml from the manifests. You can then edit this yaml and manually deploy it with kubectl . Run the following command from the manifests directory: <markup lang=\"bash\" >kubectl create --dry-run -k default/ -o yaml &gt; operator.yaml This will create a file in the manifests directory called operator.yaml that contains all the yaml required to install the Operator. You can then edit this yaml to change image names or add other settings. The Operator can be installed using the generated yaml. For example if the Operator is to be deployed to the coherence-test namespace: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic coherence-webhook-server-cert kubectl -n coherence-test create -f operator.yaml ",
            "title": "Generate Yaml - Install with Kubectl"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " If you want to use yaml directly to install the operator, with something like kubectl , you can use the manifest files published with the GitHub release at this link: 3.3.4 Manifests These manifest files are for use with a tool called Kustomize, which is built into kubectl see the documentation here: https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/ Download the 3.3.4 Manifests from the release page and unpack the file, which should produce a directory called manifests with a structure like this: <markup >manifests default config.yaml kustomization.yaml manager kustomization.yaml manager.yaml service.yaml rbac coherence_editor_role.yaml coherence_viewer_role.yaml kustomization.yaml leader_election_role.yaml leader_election_role_binding.yaml role.yaml role_binding.yaml There are two ways to use these manifest files, either install using kustomize or generate the yaml and manually install with kubectl . All the commands below are run from a console in the manifests/ directory from the extracted file above. Install with Kustomize If you have Kustomize installed (or can install it from https://github.com/kubernetes-sigs/kustomize ) you can use Kustomize to configure the yaml and install. Change the Operator Replica Count To change the replica count using Kustomize a patch file needs to be applied. The Operator manifests include a patch file, named manager/single-replica-patch.yaml , that changes the replica count from 3 to 1. This patch can be applied with the following Kustomize command. <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit add patch \\ --kind Deployment --name controller-manager \\ --path single-replica-patch.yaml Set Image Names If you need to use different iamge names from the defaults kustomize can be used to specify different names: Change the name of the Operator image by running the command below, changing the image name to the registry and image name that you are using for the Operator, for example if you have the images in a custom registry <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit set image controller=myregistry/coherence-operator:3.3.4 Change the name of the Operator image by running the command below, changing the image name to the registry and image name that you are using for the Operator utilities image <markup lang=\"bash\" >cd ./manager &amp;&amp; kustomize edit add configmap env-vars --from-literal OPERATOR_IMAGE=myregistry/coherence-operator:3.3.4 Change the name of the default Coherence image. If you are always going to be deploying your own application images then this does not need to change. <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal COHERENCE_IMAGE=$(COHERENCE_IMAGE) Set the namespace to install into, the example below sets the namespace to coherence-test : <markup lang=\"bash\" >cd ./default &amp;&amp; /kustomize edit set namespace coherence-test Install The Operator requires a Secret for its web-hook certificates. This Secret needs to exist but can be empty. The Secret must be in the same namespace that the Operator will be deployed to. For example, if the Operator namespace is coherence-test , then the Secret can be created with this command: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic coherence-webhook-server-cert The Operator can now be installed by running the following command from the manifests directory: <markup lang=\"bash\" >kustomize build ./default | kubectl apply -f - Generate Yaml - Install with Kubectl Instead of using Kustomize to modify and install the Operator we can use kubectl to generate the yaml from the manifests. You can then edit this yaml and manually deploy it with kubectl . Run the following command from the manifests directory: <markup lang=\"bash\" >kubectl create --dry-run -k default/ -o yaml &gt; operator.yaml This will create a file in the manifests directory called operator.yaml that contains all the yaml required to install the Operator. You can then edit this yaml to change image names or add other settings. The Operator can be installed using the generated yaml. For example if the Operator is to be deployed to the coherence-test namespace: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic coherence-webhook-server-cert kubectl -n coherence-test create -f operator.yaml ",
            "title": "Install with Kubectl and Kustomize"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " The first step to deploy the Coherence Operator package in Tanzu is to add the repository. This can be done using the Tanzu CLI. <markup lang=\"bash\" >tanzu package repository add coherence-repo \\ --url ghcr.io/oracle/coherence-operator-repo:3.3.3 \\ --namespace coherence \\ --create-namespace The installed repositories can be listed using the CLI: <markup lang=\"bash\" >tanzu package repository list --namespace coherence which should display something like the following <markup lang=\"bash\" >NAME REPOSITORY TAG STATUS DETAILS coherence-repo ghcr.io/oracle/coherence-operator-repo 1h Reconcile succeeded The available packages in the Coherence repository can also be displayed using the CLI <markup lang=\"bash\" >tanzu package available list --namespace coherence which should include the Operator package, coherence-operator.oracle.github.com something like the following <markup lang=\"bash\" >NAME DISPLAY-NAME SHORT-DESCRIPTION LATEST-VERSION coherence-operator.oracle.github.com Oracle Coherence Operator A Kubernetes operator for managing Oracle Coherence clusters 3.3.3 ",
            "title": "Install the Coherence Repository"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " Once the Coherence Operator repository has been installed, the coherence-operator.oracle.github.com package can be installed, which will install the Coherence Operator itself. <markup lang=\"bash\" >tanzu package install coherence \\ --package-name coherence-operator.oracle.github.com \\ --version 3.3.3 \\ --namespace coherence The Tanzu CLI will display the various steps it is going through to install the package and if all goes well, finally display Added installed package 'coherence' The packages installed in the coherence namespace can be displayed using the CLI. <markup lang=\"bash\" >tanzu package installed list --namespace coherence which should display the Coherence Operator package. <markup lang=\"bash\" >NAME PACKAGE-NAME PACKAGE-VERSION STATUS coherence coherence-operator.oracle.github.com 3.3.3 Reconcile succeeded The Operator is now installed and ready to mage Coherence clusters. ",
            "title": "Install the Coherence Operator Package"
        },
        {
            "location": "/docs/installation/01_installation",
            "text": " If using VMWare Tanzu the Coherence Operator can be installed as a package. Under the covers, Tanzu uses the Carvel tool set to deploy packages. The Carvel tools can be used outside Tanzu, so the Coherence Operator repo and package images could also be deployed using a standalone Carvel kapp-controller . The Coherence Operator release published two images required to deploy the Operator as a Tanzu package. ghcr.io/oracle/coherence-operator-package:3.3.4 - the Coherence Operator package ghcr.io/oracle/coherence-operator-repo:3.3.4 - the Coherence Operator repository Install the Coherence Repository The first step to deploy the Coherence Operator package in Tanzu is to add the repository. This can be done using the Tanzu CLI. <markup lang=\"bash\" >tanzu package repository add coherence-repo \\ --url ghcr.io/oracle/coherence-operator-repo:3.3.3 \\ --namespace coherence \\ --create-namespace The installed repositories can be listed using the CLI: <markup lang=\"bash\" >tanzu package repository list --namespace coherence which should display something like the following <markup lang=\"bash\" >NAME REPOSITORY TAG STATUS DETAILS coherence-repo ghcr.io/oracle/coherence-operator-repo 1h Reconcile succeeded The available packages in the Coherence repository can also be displayed using the CLI <markup lang=\"bash\" >tanzu package available list --namespace coherence which should include the Operator package, coherence-operator.oracle.github.com something like the following <markup lang=\"bash\" >NAME DISPLAY-NAME SHORT-DESCRIPTION LATEST-VERSION coherence-operator.oracle.github.com Oracle Coherence Operator A Kubernetes operator for managing Oracle Coherence clusters 3.3.3 Install the Coherence Operator Package Once the Coherence Operator repository has been installed, the coherence-operator.oracle.github.com package can be installed, which will install the Coherence Operator itself. <markup lang=\"bash\" >tanzu package install coherence \\ --package-name coherence-operator.oracle.github.com \\ --version 3.3.3 \\ --namespace coherence The Tanzu CLI will display the various steps it is going through to install the package and if all goes well, finally display Added installed package 'coherence' The packages installed in the coherence namespace can be displayed using the CLI. <markup lang=\"bash\" >tanzu package installed list --namespace coherence which should display the Coherence Operator package. <markup lang=\"bash\" >NAME PACKAGE-NAME PACKAGE-VERSION STATUS coherence coherence-operator.oracle.github.com 3.3.3 Reconcile succeeded The Operator is now installed and ready to mage Coherence clusters. ",
            "title": "Install as a VMWare Tanzu Package (Carvel kapp-controller)"
        },
        {
            "location": "/docs/ports/020_container_ports",
            "text": " Exposing the Coherence metrics port or Coherence Management over REST port are treated as a special case in the configuration. Normally both the port&#8217;s name and port value are required fields. If the port name is metrics or management the Operator already knows the port values (either from the defaults or from the metrics or management configuration) so these do not need to be specified again. For example, if the Coherence resource above also exposed Coherence metrics and management it might look like this: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: metrics: enabled: true port: 9876 management: enabled: true port: 1234 ports: - name: rest port: 8080 - name: metrics - name: management The rest port is not a special case and must have a port defined, in this case 8080 . The metrics port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.metrics section to be 9876. The management port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.management section to be 1234. If the port value is not set in coherence.metrics.port or in coherence.management.port then the Operator will use the defaults for these values, 9612 for metrics and 30000 for management. ",
            "title": "Metrics &amp; Management Ports"
        },
        {
            "location": "/docs/ports/020_container_ports",
            "text": " Except for rare cases most applications deployed into a Kubernetes cluster will need to expose ports that they provide services on to other applications. This is covered in the Kubernetes documentation, Connect Applications with Services The Coherence CRD makes it simple to expose ports and configure their services. The CRD contains a field named ports , which is an array of named ports. In the most basic configuration the only required values are the name and port to expose, for example: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 This example exposes a single port named rest on port 8080 . When the example above is deployed the Coherence Operator will add configure the ports for the Coherence container in the Pods to expose that port and will also create a Service for the port. For example, the relevant snippet of the StatefulSet configuration would be: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster spec: template: spec: containers: - name: coherence ports: - name: rest containerPort: 8080 The Operator has added the rest port to the coherence containers port list. The name field in the Coherence CRD&#8217;s port spec maps to the name field in the Container port spec. The port field in the Coherence CRD&#8217;s port spec maps to the containerPort in the Container port spec. For each additional port the Operator will create a Service of type ClusterIP with a default configuration. The name of the service will be the Coherence resource&#8217;s name with the port name appended to it, so in this case it will be test-cluster-rest . The Service might look like this: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-rest spec: ports: - name: rest port: 8080 targetPort: rest type: ClusterIP selector: coherenceDeployment: test-cluster coherenceCluster: test-cluster coherenceRole: storage coherenceComponent: coherencePod The Service name will be automatically generated (this can be overridden). The ports section will have just the single port being exposed by this service with the same name as the port. The port exposed by the Service will be the same as the container port value (this can be overridden). The target port will be set to the port being exposed from the container. The default Service type is ClusterIP (this can be overridden). A selector will be created to match the Pods in the Coherence resource. The Coherence CRD spec allows port and service to be further configured and allows a Prometheus ServiceMonitor to be created for the port if that port is to expose metrics. See also: Configure Services for Ports Prometheus ServiceMonitors Metrics &amp; Management Ports Exposing the Coherence metrics port or Coherence Management over REST port are treated as a special case in the configuration. Normally both the port&#8217;s name and port value are required fields. If the port name is metrics or management the Operator already knows the port values (either from the defaults or from the metrics or management configuration) so these do not need to be specified again. For example, if the Coherence resource above also exposed Coherence metrics and management it might look like this: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: metrics: enabled: true port: 9876 management: enabled: true port: 1234 ports: - name: rest port: 8080 - name: metrics - name: management The rest port is not a special case and must have a port defined, in this case 8080 . The metrics port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.metrics section to be 9876. The management port is exposed, but the port is not required as the Operator already knows the port value, which is configured in the coherence.management section to be 1234. If the port value is not set in coherence.metrics.port or in coherence.management.port then the Operator will use the defaults for these values, 9612 for metrics and 30000 for management. ",
            "title": "Additional Container Ports"
        },
        {
            "location": "/docs/ports/020_container_ports",
            "text": " The only mandatory fields when adding a port to a Coherence resource are the name and port number. There are a number of optional fields, which when not specified use the Kubernetes default values. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 protocol: TCP hostIP: 10.10.1.19 hostPort: 1000 nodePort: 5000 The additional fields, protocol , hostIP , hostPort have the same meaning and same defaults in the Coherence CRD port spec as they have in a Kubernetes container port (see the Kubernetes ContainerPort API reference). These fields map directly from the Coherence CRD port spec to the container port spec. The example above would create a container port shown below: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster spec: template: spec: containers: - name: coherence ports: - name: rest containerPort: 8080 protocol: TCP hostIP: 10.10.1.19 hostPort: 1000 The nodePort field in the Coherence CRD port spec maps to the nodePort field in the Service port spec. The nodePort is described in the Kubernetes ServicePort API reference. The Coherence CRD example above with nodePort set would create a Service with the same nodePort value: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: test-cluster-rest spec: ports: - name: rest port: 8080 targetPort: rest nodePort: 5000 type: ClusterIP selector: coherenceDeployment: test-cluster coherenceCluster: test-cluster coherenceRole: storage coherenceComponent: coherencePod ",
            "title": "Configuring the Port"
        },
        {
            "location": "/examples/no-operator/000_overview",
            "text": " There are some common prerequisites used by all the examples. The Server Image These examples use the image built in the Build a Coherence Server Image example. The image is nothing more than a cache configuration file that has an Extend proxy along with Coherence metrics and management over REST. We will use this image in the various examples we cover here. When we run the image it will start a simple storage enabled Coherence server. The Test Client In the test-client/ directory is a simple Maven project that we will use to run a simple Extend client. Network Policies When running in Kubernetes cluster where NetworkPolicy rules are applied there are certain ingress and egress policies required to allow Coherence to work. These are covered in the Network Policies Example ",
            "title": "Prerequisites"
        },
        {
            "location": "/examples/no-operator/000_overview",
            "text": " Although this project is all about the Coherence Kubernetes Operator, there are occasions where using an Operator is not possible. For example, some corporate or cloud security policies ban the use of CRDs, or have very restrictive RBAC policies that ultimately make it impossible to run Operators that uses their own CRDs or require cluster roles (or even just namespace roles). These example shows how to run a Coherence clusters in Kubernetes manually. Obviously the features of the Operator such as safe scaling, safe rolling upgrades, etc. will not be available. Note We really recommend that you try and use the Coherence Operator for managing Coherence clusters in Kubernetes. It is possible to run the Operator with fewer RBAC permissions, for example without ClusterRoles and only using Roles restricted to a single namespace. The Operator can also run without installing its web-hooks. Ultimately though it requires the CRD to be installed, which could be done manually instead of allowing the Operator to install it. If you really cannot change the minds of those dictating policies that mean you cannot use the Operator then these examples may be useful. Tip The complete source code for the examples is in the Coherence Operator GitHub repository. Prerequisites There are some common prerequisites used by all the examples. The Server Image These examples use the image built in the Build a Coherence Server Image example. The image is nothing more than a cache configuration file that has an Extend proxy along with Coherence metrics and management over REST. We will use this image in the various examples we cover here. When we run the image it will start a simple storage enabled Coherence server. The Test Client In the test-client/ directory is a simple Maven project that we will use to run a simple Extend client. Network Policies When running in Kubernetes cluster where NetworkPolicy rules are applied there are certain ingress and egress policies required to allow Coherence to work. These are covered in the Network Policies Example ",
            "title": "Coherence in Kubernetes Without the Operator"
        },
        {
            "location": "/examples/no-operator/000_overview",
            "text": " Simple Server Run a simple Coherence storage enabled cluster as a StatefulSet and connect an Extend client to it. Simple Server with Metrics Expands the simple storage enabled server to expose metrics that can be scraped by Prometheus. Securing Extend with TLS Expands the simple storage enabled server to secure Extend using TLS. Running Coherence with Istio Expands the simple storage enabled server to secure Extend using TLS. ",
            "title": "The Examples"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " This example shows how to deploy a simple Coherence cluster in Kubernetes manually, without using the Coherence Operator. Tip The complete source code for this example is in the Coherence Operator GitHub repository. Prerequisites This example assumes that you have already built the example server image. ",
            "title": "A Simple Coherence Cluster in Kubernetes"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": "<markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-sts labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: statefulset-service spec: type: ClusterIP clusterIP: None ports: - name: tcp-coherence port: 7 protocol: TCP targetPort: 7 publishNotReadyAddresses: true selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage The Service above named storage-sts has a selector that must match labels on the Pods in the StatefulSet . We use port 7 in this Service because all services must define at least one port, but we never use this port and nothing in the Coherence Pods will bind to port 7. ",
            "title": "StatefulSet Headless Service"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " When running Coherence clusters in Kubernetes we need to use well-known-addressing for Coherence cluster discovery. For this to work we create a Service that we can use for discovery of Pods that are in the cluster. In this example we only have a single StatefulSet , so we could just use the headless service above for WKA too. But in Coherence clusters where there are multiple StatefulSets in the cluster we would have to use a separate Service . <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-wka labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: wka-service spec: type: ClusterIP clusterIP: None ports: - name: tcp-coherence port: 7 protocol: TCP targetPort: 7 publishNotReadyAddresses: true selector: coherence.oracle.com/cluster: test-cluster The Service above named storage-wka is almost identical to the StatefulSet service. It only has a single selector label, so will match all Pods with the label coherence.oracle.com/cluster: test-cluster regardless of which StatefulSet they belong to. The other important property of the WKA Service is that it must have the field publishNotReadyAddresses: true so that Pods with matching labels are assigned to the Service even when those Pods are not ready. ",
            "title": "Coherence Well Known Address Headless Service"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " We can now create the StatefulSet yaml. <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage spec: selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage serviceName: storage-sts replicas: 3 template: metadata: labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage spec: containers: - name: coherence image: simple-coherence:1.0.0 command: - java args: - -cp - \"@/app/jib-classpath-file\" - -Xms1800m - -Xmx1800m - \"@/app/jib-main-class-file\" env: - name: COHERENCE_CLUSTER value: storage - name: COHERENCE_WKA value: storage-wka.svc - name: COHERENCE_CACHECONFIG value: \"test-cache-config.xml\" ports: - name: extend containerPort: 20000 The StatefulSet above will create a Coherence cluster with three replicas (or Pods ). There is a single container in the Pod named coherence that will run the image simple-coherence:1.0.0 we created above. The command line used to run the container will be java -cp @/app/jib-classpath-file -Xms1800m -Xmx1800m @/app/jib-main-class-file Because we used JIB to create the image, there will be a file named /app/jib-classpath-file that contains the classpath for the application. We can use this to set the classpath on the JVM command line using -cp @/app/jib-classpath-file so in our yaml we know we will have the correct classpath for the image we built. If we change the classpath by changing project dependencies in the pom.xml file for our project and rebuild the image the container in Kubernetes will automatically use the changed classpath. JIB also creates a file in the image named /app/jib-main-class-file which contains the name of the main class we specified in the JIB Maven plugin. We can use @/app/jib-main-class-file in place of the main class in our command line so that we run the correct main class in our container. If we change the main class in the JIB settings when we build the image our container in Kubernetes will automatically run the correct main class. We set both the min and max heap to 1.8 GB (it is a Coherence recommendation to set both min and max heap to the same value rather than set a smaller -Xms). The main class that will run will be com.tangosol.net.Coherence . The cache configuration file configures a Coherence Extend proxy service, which will listen on port 20000 . We need to expose this port in the container&#8217;s ports section. We set a number of environment variables for the container: Name Value Description COHERENCE_CLUSTER storage This sets the cluster name in Coherence (the same as setting -Dcoherence.cluster=storage ) COHERENCE_WKA storage-wka This sets the DNS name Coherence will use for discovery of other Pods in cluster. It is set to the name of the WKA Service created above. COHERENCE_CACHECONFIG \"test-cache-config.xml\" This tells Coherence the name of the cache configuration file to use (the same as setting -Dcoherence.cacheconfig=test-cache-config.xml ); ",
            "title": "The StatefulSet"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " In the cache configuration used in the image Coherence will run a Coherence Extend proxy service, listening on port 20000. This port has been exposed in the Coherence container in the StatefulSet and we can also expose it via a Service . <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-extend labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: wka-service spec: type: ClusterIP ports: - name: extend port: 20000 protocol: TCP targetPort: extend selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage The type of the Service above is ClusterIP , but we could just as easily use a different type depending on how the service will be used. For example, we might use ingress, or Istio, or a load balancer if the Extend clients were connecting from outside the Kubernetes cluster. In local development we can just port forward to the service above. ",
            "title": "Coherence Extend Service"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " We will run Coherence using a StatefulSet and in Kubernetes all StatefulSet resources also require a headless Service . StatefulSet Headless Service <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-sts labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: statefulset-service spec: type: ClusterIP clusterIP: None ports: - name: tcp-coherence port: 7 protocol: TCP targetPort: 7 publishNotReadyAddresses: true selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage The Service above named storage-sts has a selector that must match labels on the Pods in the StatefulSet . We use port 7 in this Service because all services must define at least one port, but we never use this port and nothing in the Coherence Pods will bind to port 7. Coherence Well Known Address Headless Service When running Coherence clusters in Kubernetes we need to use well-known-addressing for Coherence cluster discovery. For this to work we create a Service that we can use for discovery of Pods that are in the cluster. In this example we only have a single StatefulSet , so we could just use the headless service above for WKA too. But in Coherence clusters where there are multiple StatefulSets in the cluster we would have to use a separate Service . <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-wka labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: wka-service spec: type: ClusterIP clusterIP: None ports: - name: tcp-coherence port: 7 protocol: TCP targetPort: 7 publishNotReadyAddresses: true selector: coherence.oracle.com/cluster: test-cluster The Service above named storage-wka is almost identical to the StatefulSet service. It only has a single selector label, so will match all Pods with the label coherence.oracle.com/cluster: test-cluster regardless of which StatefulSet they belong to. The other important property of the WKA Service is that it must have the field publishNotReadyAddresses: true so that Pods with matching labels are assigned to the Service even when those Pods are not ready. The StatefulSet We can now create the StatefulSet yaml. <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage spec: selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage serviceName: storage-sts replicas: 3 template: metadata: labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage spec: containers: - name: coherence image: simple-coherence:1.0.0 command: - java args: - -cp - \"@/app/jib-classpath-file\" - -Xms1800m - -Xmx1800m - \"@/app/jib-main-class-file\" env: - name: COHERENCE_CLUSTER value: storage - name: COHERENCE_WKA value: storage-wka.svc - name: COHERENCE_CACHECONFIG value: \"test-cache-config.xml\" ports: - name: extend containerPort: 20000 The StatefulSet above will create a Coherence cluster with three replicas (or Pods ). There is a single container in the Pod named coherence that will run the image simple-coherence:1.0.0 we created above. The command line used to run the container will be java -cp @/app/jib-classpath-file -Xms1800m -Xmx1800m @/app/jib-main-class-file Because we used JIB to create the image, there will be a file named /app/jib-classpath-file that contains the classpath for the application. We can use this to set the classpath on the JVM command line using -cp @/app/jib-classpath-file so in our yaml we know we will have the correct classpath for the image we built. If we change the classpath by changing project dependencies in the pom.xml file for our project and rebuild the image the container in Kubernetes will automatically use the changed classpath. JIB also creates a file in the image named /app/jib-main-class-file which contains the name of the main class we specified in the JIB Maven plugin. We can use @/app/jib-main-class-file in place of the main class in our command line so that we run the correct main class in our container. If we change the main class in the JIB settings when we build the image our container in Kubernetes will automatically run the correct main class. We set both the min and max heap to 1.8 GB (it is a Coherence recommendation to set both min and max heap to the same value rather than set a smaller -Xms). The main class that will run will be com.tangosol.net.Coherence . The cache configuration file configures a Coherence Extend proxy service, which will listen on port 20000 . We need to expose this port in the container&#8217;s ports section. We set a number of environment variables for the container: Name Value Description COHERENCE_CLUSTER storage This sets the cluster name in Coherence (the same as setting -Dcoherence.cluster=storage ) COHERENCE_WKA storage-wka This sets the DNS name Coherence will use for discovery of other Pods in cluster. It is set to the name of the WKA Service created above. COHERENCE_CACHECONFIG \"test-cache-config.xml\" This tells Coherence the name of the cache configuration file to use (the same as setting -Dcoherence.cacheconfig=test-cache-config.xml ); Coherence Extend Service In the cache configuration used in the image Coherence will run a Coherence Extend proxy service, listening on port 20000. This port has been exposed in the Coherence container in the StatefulSet and we can also expose it via a Service . <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-extend labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: wka-service spec: type: ClusterIP ports: - name: extend port: 20000 protocol: TCP targetPort: extend selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage The type of the Service above is ClusterIP , but we could just as easily use a different type depending on how the service will be used. For example, we might use ingress, or Istio, or a load balancer if the Extend clients were connecting from outside the Kubernetes cluster. In local development we can just port forward to the service above. ",
            "title": "StatefulSet and Services"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " Now we have an image we can create the yaml files required to run the Coherence cluster in Kubernetes. StatefulSet and Services We will run Coherence using a StatefulSet and in Kubernetes all StatefulSet resources also require a headless Service . StatefulSet Headless Service <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-sts labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: statefulset-service spec: type: ClusterIP clusterIP: None ports: - name: tcp-coherence port: 7 protocol: TCP targetPort: 7 publishNotReadyAddresses: true selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage The Service above named storage-sts has a selector that must match labels on the Pods in the StatefulSet . We use port 7 in this Service because all services must define at least one port, but we never use this port and nothing in the Coherence Pods will bind to port 7. Coherence Well Known Address Headless Service When running Coherence clusters in Kubernetes we need to use well-known-addressing for Coherence cluster discovery. For this to work we create a Service that we can use for discovery of Pods that are in the cluster. In this example we only have a single StatefulSet , so we could just use the headless service above for WKA too. But in Coherence clusters where there are multiple StatefulSets in the cluster we would have to use a separate Service . <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-wka labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: wka-service spec: type: ClusterIP clusterIP: None ports: - name: tcp-coherence port: 7 protocol: TCP targetPort: 7 publishNotReadyAddresses: true selector: coherence.oracle.com/cluster: test-cluster The Service above named storage-wka is almost identical to the StatefulSet service. It only has a single selector label, so will match all Pods with the label coherence.oracle.com/cluster: test-cluster regardless of which StatefulSet they belong to. The other important property of the WKA Service is that it must have the field publishNotReadyAddresses: true so that Pods with matching labels are assigned to the Service even when those Pods are not ready. The StatefulSet We can now create the StatefulSet yaml. <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage spec: selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage serviceName: storage-sts replicas: 3 template: metadata: labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage spec: containers: - name: coherence image: simple-coherence:1.0.0 command: - java args: - -cp - \"@/app/jib-classpath-file\" - -Xms1800m - -Xmx1800m - \"@/app/jib-main-class-file\" env: - name: COHERENCE_CLUSTER value: storage - name: COHERENCE_WKA value: storage-wka.svc - name: COHERENCE_CACHECONFIG value: \"test-cache-config.xml\" ports: - name: extend containerPort: 20000 The StatefulSet above will create a Coherence cluster with three replicas (or Pods ). There is a single container in the Pod named coherence that will run the image simple-coherence:1.0.0 we created above. The command line used to run the container will be java -cp @/app/jib-classpath-file -Xms1800m -Xmx1800m @/app/jib-main-class-file Because we used JIB to create the image, there will be a file named /app/jib-classpath-file that contains the classpath for the application. We can use this to set the classpath on the JVM command line using -cp @/app/jib-classpath-file so in our yaml we know we will have the correct classpath for the image we built. If we change the classpath by changing project dependencies in the pom.xml file for our project and rebuild the image the container in Kubernetes will automatically use the changed classpath. JIB also creates a file in the image named /app/jib-main-class-file which contains the name of the main class we specified in the JIB Maven plugin. We can use @/app/jib-main-class-file in place of the main class in our command line so that we run the correct main class in our container. If we change the main class in the JIB settings when we build the image our container in Kubernetes will automatically run the correct main class. We set both the min and max heap to 1.8 GB (it is a Coherence recommendation to set both min and max heap to the same value rather than set a smaller -Xms). The main class that will run will be com.tangosol.net.Coherence . The cache configuration file configures a Coherence Extend proxy service, which will listen on port 20000 . We need to expose this port in the container&#8217;s ports section. We set a number of environment variables for the container: Name Value Description COHERENCE_CLUSTER storage This sets the cluster name in Coherence (the same as setting -Dcoherence.cluster=storage ) COHERENCE_WKA storage-wka This sets the DNS name Coherence will use for discovery of other Pods in cluster. It is set to the name of the WKA Service created above. COHERENCE_CACHECONFIG \"test-cache-config.xml\" This tells Coherence the name of the cache configuration file to use (the same as setting -Dcoherence.cacheconfig=test-cache-config.xml ); Coherence Extend Service In the cache configuration used in the image Coherence will run a Coherence Extend proxy service, listening on port 20000. This port has been exposed in the Coherence container in the StatefulSet and we can also expose it via a Service . <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-extend labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: wka-service spec: type: ClusterIP ports: - name: extend port: 20000 protocol: TCP targetPort: extend selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage The type of the Service above is ClusterIP , but we could just as easily use a different type depending on how the service will be used. For example, we might use ingress, or Istio, or a load balancer if the Extend clients were connecting from outside the Kubernetes cluster. In local development we can just port forward to the service above. ",
            "title": "Create the Kubernetes Resources"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " We can combine all the snippets of yaml above into a single file and deploy it to Kubernetes. The source code for this example contains a file named coherence.yaml containing all the configuration above. We can deploy it with the following command: <markup lang=\"bash\" >kubectl apply -f coherence.yaml We can see all the resources created in Kubernetes by running the following command: <markup lang=\"bash\" >kubectl get all Which will display something like the following: <markup >NAME READY STATUS RESTARTS AGE pod/storage-0 1/1 Running 0 19s pod/storage-1 1/1 Running 0 17s pod/storage-2 1/1 Running 0 16s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/storage-extend ClusterIP 10.105.78.34 &lt;none&gt; 20000/TCP 19s service/storage-sts ClusterIP None &lt;none&gt; 7/TCP 19s service/storage-wka ClusterIP None &lt;none&gt; 7/TCP 19s NAME READY AGE statefulset.apps/storage 3/3 19s We can see there are three Pods as we specified three replicas. The three Services we specified have been created. Finally, the StatefulSet exists and has three ready replicas. ",
            "title": "Deploy to Kubernetes"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " Now we have a Coherence cluster running in Kubernetes we can try connecting a simple Extend client. For this example we will use the test client Maven project to run the client. To connect from our local dev machine into the server we will use port-forward in this example. We could have configured ingress and load balancing, etc. but for local dev and test port-forward is simple and easy. The client is configured to connect to an Extend proxy listening on 127.0.0.1:20000 . The server we have deployed into Kubernetes is listening also listening on port 20000 via the storage-extend service. If we run a port-forward process that forwards port 20000 on our local machine to port 20000 of the service we can connect the client without needing any other configuration. <markup lang=\"bash\" >kubectl port-forward service/storage-extend 20000:20000 Now in another terminal window, we can run the test client from the test-client/ directory execute the following command: <markup lang=\"bash\" >mvn exec:java This will start a Coherence interactive console which will eventually print the Map (?): prompt. The console is now waiting for commands, so we can go ahead and create a cache. At the Map (?): prompt type the command cache test and press enter. This will create a cache named test <markup >Map (?): cache test We should see output something like this: <markup >2021-09-17 12:25:12.143/14.600 Oracle Coherence CE 21.12.1 &lt;Info&gt; (thread=com.tangosol.net.CacheFactory.main(), member=1): Loaded cache configuration from \"file:/Users/jonathanknight/dev/Projects/GitOracle/coherence-operator-3.0/examples/no-operator/test-client/target/classes/client-cache-config.xml\" 2021-09-17 12:25:12.207/14.664 Oracle Coherence CE 21.12.1 &lt;D5&gt; (thread=com.tangosol.net.CacheFactory.main(), member=1): Created cache factory com.tangosol.net.ExtensibleConfigurableCacheFactory Cache Configuration: test SchemeName: remote ServiceName: RemoteCache ServiceDependencies: DefaultRemoteCacheServiceDependencies{RemoteCluster=null, RemoteService=Proxy, InitiatorDependencies=DefaultTcpInitiatorDependencies{EventDispatcherThreadPriority=10, RequestTimeoutMillis=30000, SerializerFactory=null, TaskHungThresholdMillis=0, TaskTimeoutMillis=0, ThreadPriority=10, WorkerThreadCount=0, WorkerThreadCountMax=2147483647, WorkerThreadCountMin=0, WorkerThreadPriority=5}{Codec=null, FilterList=[], PingIntervalMillis=0, PingTimeoutMillis=30000, MaxIncomingMessageSize=0, MaxOutgoingMessageSize=0}{ConnectTimeoutMillis=30000, RequestSendTimeoutMillis=30000}{LocalAddress=null, RemoteAddressProviderBldr=com.tangosol.coherence.config.builder.WrapperSocketAddressProviderBuilder@35f8cdc1, SocketOptions=SocketOptions{LingerTimeout=0, KeepAlive=true, TcpNoDelay=true}, SocketProvideBuilderr=com.tangosol.coherence.config.builder.SocketProviderBuilder@1e4cf40, isNameServiceAddressProvider=false}}{DeferKeyAssociationCheck=false} Map (test): The cache named test has been created and prompt has changed to Map (test): , so this confirms that we have connected to the Extend proxy in the server running in Kubernetes. We can not put data into the cache using the put command <markup >Map (test): put key-1 value-1 The command above puts an entry into the test cache with a key of \"key-1\" and a value of \"value-1\" and will print the previous value mapped to the \"key-1\" key, which in this case is null . <markup >Map (test): put key-1 value-1 null Map (test): We can now do a get command to fetch the entry we just put, which should print value-1 and re-display the command prompt. <markup >Map (test): get key-1 value-1 Map (test): To confirm we really have connected to the server we can kill the console wil ctrl-C, restart it and execute the cache and get commands again. <markup >Map (?): cache test ... output removed for brevity ... Map (test): get key-1 value-1 Map (test): We can see above that the get command returned value-1 which we previously inserted. ",
            "title": "Connect an Extend Client"
        },
        {
            "location": "/examples/no-operator/01_simple_server/README",
            "text": " We can now exit the test client by pressing ctrl-C, stop the port-forward process with crtl-C and undeploy the server: <markup lang=\"bash\" >kubectl delete -f coherence.yaml ",
            "title": "Clean-UP"
        },
        {
            "location": "/docs/management/020_management_over_rest",
            "text": " To deploy a Coherence resource with management over REST enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true ports: - name: management Setting the coherence.management.enabled field to true will enable Management over REST To expose Management over REST via a Service it is added to the ports list. The management port is a special case where the port number is optional so in this case Management over REST will bind to the default port 30000 . (see Exposing Ports for details) To expose Management over REST on a different port the alternative port value can be set in the coherence.management section, for example: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true port: 8080 ports: - name: management Management over REST will now be exposed on port 8080 ",
            "title": "Deploy Coherence with Management over REST Enabled"
        },
        {
            "location": "/docs/management/020_management_over_rest",
            "text": " After installing the basic management-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f management-cluster.yaml coherence.coherence.oracle.com/management-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=management-cluster NAME READY STATUS RESTARTS AGE management-cluster-0 1/1 Running 0 36s management-cluster-1 1/1 Running 0 36s management-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward management-cluster-0 30000:30000 Forwarding from [::1]:30000 -&gt; 30000 Forwarding from 127.0.0.1:30000 -&gt; 30000 ",
            "title": "Port-forward the Management over REST Port"
        },
        {
            "location": "/docs/management/020_management_over_rest",
            "text": " Now that a port is being forwarded from localhost to a Pod in the cluster the Management over REST endpoints can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/ Which should result in a response similar to the following: <markup lang=\"json\" >{ \"links\": [ { \"rel\": \"parent\", \"href\": \"http://127.0.0.1:30000/management/coherence\" }, { \"rel\": \"self\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"canonical\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"services\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/services\" }, { \"rel\": \"caches\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/caches\" }, { \"rel\": \"members\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/members\" }, { \"rel\": \"management\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/management\" }, { \"rel\": \"journal\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/journal\" }, { \"rel\": \"hotcache\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/hotcache\" }, { \"rel\": \"reporters\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\": \"webApplications\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/webApplications\" } ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2019-10-15T03:55:46.461Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"14.1.1.0.0\", \"running\": true, \"clusterName\": \"management-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2019-10-15 03:46:15.848, Address=10.1.2.184:36531, MachineId=49519, Location=site:coherence.coherence-test.svc,machine:docker-desktop,process:1,member:management-cluster-1, Role=storage)\", \"Member(Id=2, Timestamp=2019-10-15 03:46:19.405, Address=10.1.2.183:40341, MachineId=49519, Location=site:coherence.coherence-test.svc,machine:docker-desktop,process:1,member:management-cluster-2, Role=storage)\", \"Member(Id=3, Timestamp=2019-10-15 03:46:19.455, Address=10.1.2.185:38719, MachineId=49519, Location=site:coherence.coherence-test.svc,machine:docker-desktop,process:1,member:management-cluster-0, Role=storage)\" ], \"type\": \"Cluster\" } ",
            "title": "Access the REST Endpoint"
        },
        {
            "location": "/docs/management/020_management_over_rest",
            "text": " Issue the following curl command to access the Swagger endpoint, which documents all the REST APIs available. <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog Which should result in a response like the following: <markup lang=\"json\" >{ \"swagger\": \"2.0\", \"info\": { \"title\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"description\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"version\": \"14.1.1.0.0\" }, \"schemes\": [ \"http\", \"https\" ], ... The above output has been truncated due to the large size. ",
            "title": "Access the Swagger Endpoint"
        },
        {
            "location": "/docs/management/020_management_over_rest",
            "text": " Management over REST can be used for all Coherence management functions, the same as would be available when using standard MBean access over JMX. Please see the Coherence REST API for more information on these features. Connecting JVisualVM to Management over REST Enabling SSL Produce and extract a Java Flight Recorder (JFR) file Access the Reporter ",
            "title": "Other REST Resources"
        },
        {
            "location": "/docs/management/020_management_over_rest",
            "text": " Since version 12.2.1.4 Coherence has had functionality to expose a management API over REST. The Management over REST API is disabled by default in Coherence clusters but can be enabled and configured by setting the relevant fields in the Coherence CRD. The example below shows how to enable and access Coherence MBeans using Management over REST. Once the Management port has been exposed, for example via a load balancer or port-forward command, the REST endpoint is available at http://host:port/management/coherence/cluster . The Swagger JSON document for the API is available at http://host:port/management/coherence/cluster/metadata-catalog . See the REST API for Managing Oracle Coherence documentation for full details on each of the endpoints. Note: Use of Management over REST is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. Deploy Coherence with Management over REST Enabled To deploy a Coherence resource with management over REST enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true ports: - name: management Setting the coherence.management.enabled field to true will enable Management over REST To expose Management over REST via a Service it is added to the ports list. The management port is a special case where the port number is optional so in this case Management over REST will bind to the default port 30000 . (see Exposing Ports for details) To expose Management over REST on a different port the alternative port value can be set in the coherence.management section, for example: <markup lang=\"yaml\" title=\"management-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: management-cluster spec: coherence: management: enabled: true port: 8080 ports: - name: management Management over REST will now be exposed on port 8080 Port-forward the Management over REST Port After installing the basic management-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f management-cluster.yaml coherence.coherence.oracle.com/management-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=management-cluster NAME READY STATUS RESTARTS AGE management-cluster-0 1/1 Running 0 36s management-cluster-1 1/1 Running 0 36s management-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward management-cluster-0 30000:30000 Forwarding from [::1]:30000 -&gt; 30000 Forwarding from 127.0.0.1:30000 -&gt; 30000 Access the REST Endpoint Now that a port is being forwarded from localhost to a Pod in the cluster the Management over REST endpoints can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/ Which should result in a response similar to the following: <markup lang=\"json\" >{ \"links\": [ { \"rel\": \"parent\", \"href\": \"http://127.0.0.1:30000/management/coherence\" }, { \"rel\": \"self\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"canonical\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/\" }, { \"rel\": \"services\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/services\" }, { \"rel\": \"caches\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/caches\" }, { \"rel\": \"members\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/members\" }, { \"rel\": \"management\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/management\" }, { \"rel\": \"journal\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/journal\" }, { \"rel\": \"hotcache\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/hotcache\" }, { \"rel\": \"reporters\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\": \"webApplications\", \"href\": \"http://127.0.0.1:30000/management/coherence/cluster/webApplications\" } ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2019-10-15T03:55:46.461Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"14.1.1.0.0\", \"running\": true, \"clusterName\": \"management-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2019-10-15 03:46:15.848, Address=10.1.2.184:36531, MachineId=49519, Location=site:coherence.coherence-test.svc,machine:docker-desktop,process:1,member:management-cluster-1, Role=storage)\", \"Member(Id=2, Timestamp=2019-10-15 03:46:19.405, Address=10.1.2.183:40341, MachineId=49519, Location=site:coherence.coherence-test.svc,machine:docker-desktop,process:1,member:management-cluster-2, Role=storage)\", \"Member(Id=3, Timestamp=2019-10-15 03:46:19.455, Address=10.1.2.185:38719, MachineId=49519, Location=site:coherence.coherence-test.svc,machine:docker-desktop,process:1,member:management-cluster-0, Role=storage)\" ], \"type\": \"Cluster\" } Access the Swagger Endpoint Issue the following curl command to access the Swagger endpoint, which documents all the REST APIs available. <markup lang=\"bash\" >curl http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog Which should result in a response like the following: <markup lang=\"json\" >{ \"swagger\": \"2.0\", \"info\": { \"title\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"description\": \"RESTful Management Interface for Oracle Coherence MBeans\", \"version\": \"14.1.1.0.0\" }, \"schemes\": [ \"http\", \"https\" ], ... The above output has been truncated due to the large size. Other REST Resources Management over REST can be used for all Coherence management functions, the same as would be available when using standard MBean access over JMX. Please see the Coherence REST API for more information on these features. Connecting JVisualVM to Management over REST Enabling SSL Produce and extract a Java Flight Recorder (JFR) file Access the Reporter ",
            "title": "Management over REST"
        },
        {
            "location": "/docs/other/041_global_labels",
            "text": " It is possible to specify a global set of labels and annotations that will be applied to all resources. Global labels and annotations can be specified in two ways: For an individual Coherence deployment, in which case they will be applied to all the Kubernetes resources created for that deployment As part of the Operator install, in which case they will be applied to all Kubernetes resources managed by the Operator, including all Coherence clusters and related resources ",
            "title": "Global Labels and Annotations"
        },
        {
            "location": "/docs/other/041_global_labels",
            "text": " The Coherence CRD contains a global field that allows global labels and annotations to be specified. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 global: labels: one: \"label-one\" two: \"label-two\" If the yaml above is applied to Kubernetes, then every resource the Operator creates for the storage Coherence deployment, it will add the two labels, one=label-one and two=label-two . This includes the StatefulSet , the Pods , any Service such as the stateful set service, the WKA service, etc. If any of the labels in the global section are also in the Pod labels section or for the Services for exposed ports, those labels will take precedence. For example <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 labels: one: \"pod-label-one\" global: labels: one: \"label-one\" two: \"label-one\" In the yaml above, the global label one=label-one and two=labl-two will be applied to every resource created for the Coherence deployment except for the Pods. The Operator uses the spec.labels field to define Pods specific labels, so in this case the Pod labels will be one=pod-label-one from the spec.labels field and two=labl-two from the global labels. ",
            "title": "Specify Global Labels for a Coherence Resource"
        },
        {
            "location": "/docs/other/041_global_labels",
            "text": " When installing the Operator using the manifest yaml files, additional command line flags can be configured by manually editing the yaml file before installing. Download the yaml manifest file from the GitHub repo https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml Find the section of the yaml file the defines the Operator container args, the default looks like this <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election Then edit the argument list to add the required --global-label and --global-annotation flags. For example, to add the same --global-label one=label-one --global-annotation foo=bar --global-label two=label-two flags, the file would look like this: <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election - --global-label - one=label-one - --global-annotation - foo=bar - --global-label - two=label-two` Important Container arguments must each be a separate entry in the arg list. This is valid <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election - --global-label - one=label-one This is not valid <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election - --global-label one=label-one ",
            "title": "Installing Using the Manifest Files"
        },
        {
            "location": "/docs/other/041_global_labels",
            "text": " If installing the Operator using the Helm chart, the global labels and annotations can be specified as values as part of the Helm command or in a values file. For example, to add the same --global-label one=label-one --global-annotation foo=bar --global-label two=label-two flags, create a simple values file: <markup title=\"global-values.yaml\" >globalLabels: one: \"label-one\" two: \"label-two\" globalAnnotations: foo: \"bar\" Use the values file when installing the Helm chart <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --values global-values.yaml coherence \\ coherence/coherence-operator ",
            "title": "Installing Using the Helm Chart"
        },
        {
            "location": "/docs/other/041_global_labels",
            "text": " The Operator runner binary has various command line flags that can be specified on its command line. Two of these flags when starting the Operator are: --global-label to specify a global label key and value --global-annotation to specify a global annotation key and value Both of these command line flags can be specified multiple times if required. For example: <markup lang=\"bash\" >runner operator --global-label one=label-one --global-annoataion foo=bar --global-label two=label-two The command above will start the Operator with two global labels, one=label-one and two=labl-two and with one global annotation foo=bar . The Operator will then apply these labels and annotations to every Kubernetes resource that it creates. Installing Using the Manifest Files When installing the Operator using the manifest yaml files, additional command line flags can be configured by manually editing the yaml file before installing. Download the yaml manifest file from the GitHub repo https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml Find the section of the yaml file the defines the Operator container args, the default looks like this <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election Then edit the argument list to add the required --global-label and --global-annotation flags. For example, to add the same --global-label one=label-one --global-annotation foo=bar --global-label two=label-two flags, the file would look like this: <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election - --global-label - one=label-one - --global-annotation - foo=bar - --global-label - two=label-two` Important Container arguments must each be a separate entry in the arg list. This is valid <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election - --global-label - one=label-one This is not valid <markup lang=\"yaml\" title=\"coherence-operator.yaml\" > - args: - operator - --enable-leader-election - --global-label one=label-one Installing Using the Helm Chart If installing the Operator using the Helm chart, the global labels and annotations can be specified as values as part of the Helm command or in a values file. For example, to add the same --global-label one=label-one --global-annotation foo=bar --global-label two=label-two flags, create a simple values file: <markup title=\"global-values.yaml\" >globalLabels: one: \"label-one\" two: \"label-two\" globalAnnotations: foo: \"bar\" Use the values file when installing the Helm chart <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --values global-values.yaml coherence \\ coherence/coherence-operator ",
            "title": "Specify Global Labels when Installing the Operator"
        },
        {
            "location": "/docs/coherence/090_ipmonitor",
            "text": " The Coherence IPMonitor is a failure detection mechanism used by Coherence to detect machine failures. It does this by pinging the echo port, (port 7) on remote hosts that other cluster members are running on. When running in Kubernetes, every Pod has its own IP address, so it looks to Coherence like every member is on a different host. Failure detection using IPMonitor is less useful in Kubernetes than it is on physical machines or VMs, so the Operator disables the IPMonitor by default. This is configurable though and if it is felt that using IPMonitor is useful to an application, it can be re-enabled. To re-enable IPMonitor set the boolean flag enableIpMonitor in the coherence section of the Coherence resource yaml: <markup lang=\"yaml\" title=\"coherence-storage.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: enableIpMonitor: true Setting enableIpMonitor will disable the IPMonitor, which is the default behaviour when enableIpMonitor is not specified in the yaml. ",
            "title": "Coherence IPMonitor"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " This example shows how to deploy a simple Coherence cluster in Kubernetes manually, and enabling the Pods in that cluster to expose a http endpoint to allow access to Coherence metrics. This example expands on the StatefulSet used in the first simple deployment example. Tip The complete source code for this example is in the Coherence Operator GitHub repository. Prerequisites This example assumes that you have already built the example server image. ",
            "title": "Enabling Coherence Metrics"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " To expose Coherence metrics we just need to change the StatefulSet to set either the system properties or environment variables to enable metrics. We will also add a container port to the expose metrics endpoint. <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage spec: selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage serviceName: storage-sts replicas: 3 template: metadata: labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage spec: containers: - name: coherence image: simple-coherence:1.0.0 command: - java args: - -cp - \"@/app/jib-classpath-file\" - -Xms1800m - -Xmx1800m - \"@/app/jib-main-class-file\" env: - name: COHERENCE_CLUSTER value: storage - name: COHERENCE_WKA value: storage-wka - name: COHERENCE_CACHECONFIG value: \"test-cache-config.xml\" - name: COHERENCE_METRICS_HTTP_ENABLED value: \"true\" ports: - name: extend containerPort: 20000 - name: metrics containerPort: 9612 The yaml above is identical to that used in the simple server example apart from: * We added the COHERENCE_METRICS_HTTP_ENABLED environment variable with a value of \"true\" . Instead of this we could have added -Dcoherence.metrics.http.enabled=true to the args: list to set the coherence.metrics.http.enabled system property to true. Recent versions of Coherence work with both system properties or environment variables, and we just chose to use environment variables in this example. * We added a port named metrics with a port value of 9612 , which is the default port that the Coherence metrics endpoint binds to. ",
            "title": "The StatefulSet"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " In the simple server example we created some Services and a StatefulSet that ran a Coherence cluster in Kubernetes. In this example we will just cover the additional configurations we need to make to expose Coherence metrics. We will not bother repeating the configuration for the Services for the StatefulSet and well known addressing or the Service for exposing Extend. We will assume they are already part of our yaml file. The coherence-metrics.yaml file that is part of the source for this example contains all those resources. The StatefulSet To expose Coherence metrics we just need to change the StatefulSet to set either the system properties or environment variables to enable metrics. We will also add a container port to the expose metrics endpoint. <markup lang=\"yaml\" title=\"coherence.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage spec: selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage serviceName: storage-sts replicas: 3 template: metadata: labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage spec: containers: - name: coherence image: simple-coherence:1.0.0 command: - java args: - -cp - \"@/app/jib-classpath-file\" - -Xms1800m - -Xmx1800m - \"@/app/jib-main-class-file\" env: - name: COHERENCE_CLUSTER value: storage - name: COHERENCE_WKA value: storage-wka - name: COHERENCE_CACHECONFIG value: \"test-cache-config.xml\" - name: COHERENCE_METRICS_HTTP_ENABLED value: \"true\" ports: - name: extend containerPort: 20000 - name: metrics containerPort: 9612 The yaml above is identical to that used in the simple server example apart from: * We added the COHERENCE_METRICS_HTTP_ENABLED environment variable with a value of \"true\" . Instead of this we could have added -Dcoherence.metrics.http.enabled=true to the args: list to set the coherence.metrics.http.enabled system property to true. Recent versions of Coherence work with both system properties or environment variables, and we just chose to use environment variables in this example. * We added a port named metrics with a port value of 9612 , which is the default port that the Coherence metrics endpoint binds to. ",
            "title": "Create the Kubernetes Resources"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " We can combine all the snippets of yaml above into a single file and deploy it to Kubernetes. The source code for this example contains a file named coherence-metrics.yaml containing all the configuration above. We can deploy it with the following command: <markup lang=\"bash\" >kubectl apply -f coherence-metrics.yaml We can see all the resources created in Kubernetes are the same as the simple server example: <markup lang=\"bash\" >kubectl get all Which will display something like the following: <markup >pod/storage-0 1/1 Running 0 10s pod/storage-1 1/1 Running 0 7s pod/storage-2 1/1 Running 0 6s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 158d service/storage-extend ClusterIP 10.102.198.218 &lt;none&gt; 20000/TCP 10s service/storage-sts ClusterIP None &lt;none&gt; 7/TCP 10s service/storage-wka ClusterIP None &lt;none&gt; 7/TCP 10s NAME READY AGE statefulset.apps/storage 3/3 10s ",
            "title": "Deploy the Cluster"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " To test that we can access metrics we will port-forward to one of the Pods and use curl to get the metrics. We can choose any of the three Pods to test, or repeat the test for each Pod . In this example, we&#8217;ll just port-forward local port 9612 to port 9612 in pod/storage-0 . <markup lang=\"bash\" >kubectl port-forward pod/storage-0 9612:9612 Now in another terminal we can run the curl command to get metrics. As we are using port forwarding the host will be 127.0.0.1 and the port will be 9612 . <markup lang=\"bash\" >curl -X GET http://127.0.0.1:9612/metrics This should then bring back all the Coherence metrics for pod/storage-0 . The default format of the response is Prometheus text format. We can also retrieve individual metrics by name. For example, we can get the Coherence.Cluster.Size metric: <markup lang=\"bash\" >curl -X GET http://127.0.0.1:9612/metrics/Coherence.Cluster.Size which will display something like this: <markup lang=\"bash\" >vendor:coherence_cluster_size{cluster=\"storage\", version=\"21.12.1\"} 3 This displays the metric name in Prometheus format vendor:coherence_cluster_size , the metric labels cluster=\"storage\", version=\"21.12.1\" and the metric value, in this case 3 as there are three cluster members because we specified a replicas value of 3 in the StatefulSet . We can also receive the same response as json by using either the accepted media type header \"Accept: application/json\" : <markup lang=\"bash\" >curl -X GET -H \"Accept: application/json\" http://127.0.0.1:9612/metrics/Coherence.Cluster.Size Or by using the .json suffix on the URL <markup lang=\"bash\" >curl -X GET http://127.0.0.1:9612/metrics/Coherence.Cluster.Size.json Both requests will display something like this: <markup lang=\"bash\" >[{\"name\":\"Coherence.Cluster.Size\",\"tags\":{\"cluster\":\"storage\",\"version\":\"21.12.1\"},\"scope\":\"VENDOR\",\"value\":3}] We have now verified that the Pods in the cluster are producing metrics. ",
            "title": "Retrieve Metrics"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " The simplest way to install Prometheus is to follow the instructions in the Prometheus Operator Quick Start page. Prometheus can then be accessed as documented in the Access Prometheus section of the Quick Start page. As described in the Prometheus docs we can create a port-forward process to the Prometheus Service . <markup lang=\"bash\" >kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 Then point our browser to http://localhost:9090 to access the Prometheus UI. At this stage there will be no Coherence metrics, but we&#8217;ll change that in the next section. ",
            "title": "Install Prometheus"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " The out of the box Prometheus install uses ServiceMonitor resources to determine which Pods to scrape metrics from. We can therefore configure Prometheus to scrape our Coherence cluster metrics by adding a Service and ServiceMonitor . A Prometheus ServiceMonitor , as the name suggests, monitors a Service so we need to create a Service to expose the metrics port. We are not going to access this Service ourselves, so it does not need to be a load balancer, in fact it can just be a headless service. Prometheus uses the Service to locate the Pods that it should scrape. The yaml below is a simple headless service that has a selector that matches labels in our Coherence cluster Pods . <markup lang=\"yaml\" title=\"prometheus-metrics.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-metrics labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: metrics-service spec: type: ClusterIP ports: - name: metrics port: 9612 targetPort: metrics selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage We can now create a Prometheus ServiceMonitor that tells Prometheus about the Service to use. <markup lang=\"yaml\" title=\"prometheus-metrics.yaml\" >apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: storage-metrics labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: service-monitor spec: endpoints: - port: metrics selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: metrics-service The ServiceMonitor above contains a single endpoint that scrapes the port named metrics in any Service with labels matching those in the matchLabels array, which in this case are the labels we applied to the storage-metrics service above. The full specification of what can be in a ServiceMonitor can be found in the Prometheus ServiceMonitorSpec documentation. We can combine both of the above pieces of yaml into a single file and deploy them. The example source code contains a file named prometheus-metrics.yaml that contains the yaml above. Create the Service and ServiceMonitor in the same Kubernetes namespace as the Coherence cluster. <markup lang=\"bash\" >kubectl apply -f prometheus-metrics.yaml It can sometimes take a minute or two for Prometheus to discover the ServiceMonitor and start to scrape metrics from the Pods. Once this happens it should be possible to see Coherence metrics for the cluster in Prometheus. As shown above, the vendor:coherence_cluster_size metric has been scraped from all three Pods and as expected all Pods have a cluster size value of 3 . ",
            "title": "Create a ServiceMonitor"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " Prometheus Federation is the recommended way to scale Prometheus and to make metrics from inside Kubernetes available in a Prometheus instance outside of Kubernetes. Instead of the external Prometheus instance needing to be configured to locate and connect to Pods inside Kubernetes, it only needs an ingress into Prometheus running inside Kubernetes and can scrape all the metrics from there. More details can be found in the Prometheus Federation documentation. We can install a local Prometheus instance as described in the Prometheus Getting Started guide. In the Prometheus installation directory we can edit the prometheus.yml file to configure Prometheus to scrape the federation endpoint of Prometheus inside Kubernetes. We need to add the federation configuration to the scrape_configs: section as shown below: <markup lang=\"yaml\" title=\"prometheus.yml\" >scrape_configs: - job_name: 'federate' scrape_interval: 15s honor_labels: true metrics_path: '/federate' params: 'match[]': - '{__name__=~\"vendor:coherence_.*\"}' static_configs: - targets: - '127.0.0.1:9091' You will notice that we have used 127.0.0.1:9091 as the target address. This is because when we run our local Prometheus instance it will bind to port 9090 so when we run the port-forward process to allow connections into Prometheus in the cluster we cannot use port 9090 , so we will forward local port 9091 to the Prometheus service port 9090 in Kubernetes. In the params: section we have specified that the 'match[]': field only federates metrics that have a name that starts with vendor:coherence_ so in this example we only federate Coherence metrics. Run the port-forward process so that when we start our local Prometheus instance it can connect to Prometheus in Kubernetes. <markup lang=\"bash\" >kubectl --namespace monitoring port-forward svc/prometheus-k8s 9091:9090 We&#8217;re now forwarding local port 9091 to Prometheus service port 9090 so we can run the local Prometheus instance. As described in the Prometheus documentation, from the Prometheus installation directory run the command: <markup lang=\"bash\" >./prometheus --config.file=prometheus.yml Once Prometheus starts we can point our browser to http://localhost:9090 to access the prometheus UI. After a short pause, Prometheus should start to scrap emetrics from inside Kubernetes and we should see them in the UI ",
            "title": "Federated Prometheus Metrics"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " One of the most common ways to analyse metrics in Kubernetes is by using Prometheus. The recommended way to do this is to deploy Prometheus inside your Kubernetes cluster so that it can scrape metrics directly from Pods . Whilst Prometheus can be installed outside the Kubernetes cluster, this introduces a much more complicated set-up. If using Prometheus externally to the Kubernetes cluster, the approach recommended by Prometheus is to use federation, which we show in an example below. Install Prometheus The simplest way to install Prometheus is to follow the instructions in the Prometheus Operator Quick Start page. Prometheus can then be accessed as documented in the Access Prometheus section of the Quick Start page. As described in the Prometheus docs we can create a port-forward process to the Prometheus Service . <markup lang=\"bash\" >kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 Then point our browser to http://localhost:9090 to access the Prometheus UI. At this stage there will be no Coherence metrics, but we&#8217;ll change that in the next section. Create a ServiceMonitor The out of the box Prometheus install uses ServiceMonitor resources to determine which Pods to scrape metrics from. We can therefore configure Prometheus to scrape our Coherence cluster metrics by adding a Service and ServiceMonitor . A Prometheus ServiceMonitor , as the name suggests, monitors a Service so we need to create a Service to expose the metrics port. We are not going to access this Service ourselves, so it does not need to be a load balancer, in fact it can just be a headless service. Prometheus uses the Service to locate the Pods that it should scrape. The yaml below is a simple headless service that has a selector that matches labels in our Coherence cluster Pods . <markup lang=\"yaml\" title=\"prometheus-metrics.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-metrics labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: metrics-service spec: type: ClusterIP ports: - name: metrics port: 9612 targetPort: metrics selector: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage We can now create a Prometheus ServiceMonitor that tells Prometheus about the Service to use. <markup lang=\"yaml\" title=\"prometheus-metrics.yaml\" >apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: storage-metrics labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: service-monitor spec: endpoints: - port: metrics selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: metrics-service The ServiceMonitor above contains a single endpoint that scrapes the port named metrics in any Service with labels matching those in the matchLabels array, which in this case are the labels we applied to the storage-metrics service above. The full specification of what can be in a ServiceMonitor can be found in the Prometheus ServiceMonitorSpec documentation. We can combine both of the above pieces of yaml into a single file and deploy them. The example source code contains a file named prometheus-metrics.yaml that contains the yaml above. Create the Service and ServiceMonitor in the same Kubernetes namespace as the Coherence cluster. <markup lang=\"bash\" >kubectl apply -f prometheus-metrics.yaml It can sometimes take a minute or two for Prometheus to discover the ServiceMonitor and start to scrape metrics from the Pods. Once this happens it should be possible to see Coherence metrics for the cluster in Prometheus. As shown above, the vendor:coherence_cluster_size metric has been scraped from all three Pods and as expected all Pods have a cluster size value of 3 . Federated Prometheus Metrics Prometheus Federation is the recommended way to scale Prometheus and to make metrics from inside Kubernetes available in a Prometheus instance outside of Kubernetes. Instead of the external Prometheus instance needing to be configured to locate and connect to Pods inside Kubernetes, it only needs an ingress into Prometheus running inside Kubernetes and can scrape all the metrics from there. More details can be found in the Prometheus Federation documentation. We can install a local Prometheus instance as described in the Prometheus Getting Started guide. In the Prometheus installation directory we can edit the prometheus.yml file to configure Prometheus to scrape the federation endpoint of Prometheus inside Kubernetes. We need to add the federation configuration to the scrape_configs: section as shown below: <markup lang=\"yaml\" title=\"prometheus.yml\" >scrape_configs: - job_name: 'federate' scrape_interval: 15s honor_labels: true metrics_path: '/federate' params: 'match[]': - '{__name__=~\"vendor:coherence_.*\"}' static_configs: - targets: - '127.0.0.1:9091' You will notice that we have used 127.0.0.1:9091 as the target address. This is because when we run our local Prometheus instance it will bind to port 9090 so when we run the port-forward process to allow connections into Prometheus in the cluster we cannot use port 9090 , so we will forward local port 9091 to the Prometheus service port 9090 in Kubernetes. In the params: section we have specified that the 'match[]': field only federates metrics that have a name that starts with vendor:coherence_ so in this example we only federate Coherence metrics. Run the port-forward process so that when we start our local Prometheus instance it can connect to Prometheus in Kubernetes. <markup lang=\"bash\" >kubectl --namespace monitoring port-forward svc/prometheus-k8s 9091:9090 We&#8217;re now forwarding local port 9091 to Prometheus service port 9090 so we can run the local Prometheus instance. As described in the Prometheus documentation, from the Prometheus installation directory run the command: <markup lang=\"bash\" >./prometheus --config.file=prometheus.yml Once Prometheus starts we can point our browser to http://localhost:9090 to access the prometheus UI. After a short pause, Prometheus should start to scrap emetrics from inside Kubernetes and we should see them in the UI ",
            "title": "Using Prometheus"
        },
        {
            "location": "/examples/no-operator/02_metrics/README",
            "text": " We could now install Grafana and configure it to connect to Prometheus, either the local instance or the instance inside Kubernetes. The Coherence Operator provides a number of dashboards that can imported into Grafana. See the Operator Import Grafana Dashboards documentation. ",
            "title": "Grafana"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " Kubernetes does not have a deny all policy, but this can be achieved with a regular network policy that specifies a policyTypes of both 'Ingress` and Egress but omits any definitions. A wild-card podSelector: {} applies the policy to all Pods in the namespace. <markup lang=\"yaml\" title=\"manifests/deny-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress - Egress ingress: [] egress: [] The policy above can be installed into the coherence namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/deny-all.yaml After installing the deny-all policy, any Pod in the coherence namespace will not be allowed either ingress, nor egress. Very secure, but probably impractical for almost all use cases. After applying the deny-all policy more polices can be added to gradually open up the required access to run the Coherence Operator and Coherence clusters. ",
            "title": "Deny All Policy"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " When enforcing egress, such as with the deny-all policy above, it is important to remember that virtually every Pod needs to communicate with other Pods or Services, and will therefore need to access DNS. The policy below allows all Pods (using podSelector: {} ) egress to both TCP and UDP on port 53 in all namespaces. <markup lang=\"yaml\" title=\"manifests/allow-dns.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns spec: podSelector: { } policyTypes: - Egress egress: - to: - namespaceSelector: { } ports: - protocol: UDP port: 53 # - protocol: TCP # port: 53 If allowing DNS egress to all namespaces is overly permissive, DNS could be further restricted to just the kube-system namespace, therefore restricting DNS lookups to only Kubernetes internal DNS. Kubernetes applies the kubernetes.io/metadata.name label to namespaces, and sets its value to the namespace name, so this can be used in label matchers. With the policy below, Pods will be able to use internal Kubernetes DNS only. <markup lang=\"yaml\" title=\"manifests/allow-dns-kube-system.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns spec: podSelector: { } policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system ports: - protocol: UDP port: 53 # - protocol: TCP # port: 53 The policy above can be installed into the coherence namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-dns-kube-system.yaml Tip Some documentation regarding allowing DNS with Kubernetes network policies only shows opening up UDP connections. During our testing with network policies, we discovered that with only UDP allowed any lookup for a fully qualified name would fail. For example nslookup my-service.my-namespace.svc would work, but the fully qualified nslookup my-service.my-namespace.svc.cluster.local would not. Adding TCP to the DNS policy allowed DNS lookups with .cluster.local to also work. Neither the Coherence Operator, nor Coherence itself use a fully qualified service name for a DNS lookup. It appears that Java&#8217;s InetAddress.findAllByName() method still works only with UDP, albeit extremely slowly. By default, the service name used for the Coherence WKA setting uses just the .svc suffix. ",
            "title": "Allow DNS"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " Kubernetes network policies specify the access permissions for groups of pods, similar to security groups in the cloud are used to control access to VM instances and similar to firewalls. The default behaviour of a Kubernetes cluster is to allow all Pods to freely talk to each other. Whilst this sounds insecure, originally Kubernetes was designed to orchestrate services that communicated with each other, it was only later that network policies were added. A network policy is applied to a Kubernetes namespace and controls ingress into and egress out of Pods in that namespace. The ports specified in a NetworkPolicy are the ports exposed by the Pods , they are not any ports that may be exposed by any Service that exposes the Pod ports. For example, if a Pod exposed port 8080 and a Service exposing the Pod mapped port 80 in the Service to port 8080 in the Pod , the NetworkPolicy ingress rule would be for the Pod port 8080. Network polices would typically end up being dictated by corporate security standards where different companies may apply stricter or looser rules than others. The examples in this document start from the premise that everything will be blocked by a \"deny all\" policy and then opened up as needed. This is the most secure use of network policies, and hence the examples can easily be tweaked if looser rules are applied. This example has the following sections: Deny All Policy - denying all ingress and egress Allow DNS - almost every use case will require egress to DNS Coherence Operator Policies - the network policies required to run the Coherence Operator Kubernetes API Server - allow the Operator egress to the Kubernetes API server Coherence Clusters Pods - allow the Operator egress to the Coherence cluster Pods Web Hooks - allow ingress to the Operator&#8217;s web hook port Coherence Cluster Policies - the network policies required to run Coherence clusters Inter-Cluster Access - allow Coherence cluster Pods to communicate Coherence Operator - allow Coherence cluster Pods to communicate with the Operator Clients - allows access by Extend and gRPC clients Metrics - allow Coherence cluster member metrics to be scraped Testing Connectivity - using the Operator&#8217;s network connectivity test utility to test policies Deny All Policy Kubernetes does not have a deny all policy, but this can be achieved with a regular network policy that specifies a policyTypes of both 'Ingress` and Egress but omits any definitions. A wild-card podSelector: {} applies the policy to all Pods in the namespace. <markup lang=\"yaml\" title=\"manifests/deny-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress - Egress ingress: [] egress: [] The policy above can be installed into the coherence namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/deny-all.yaml After installing the deny-all policy, any Pod in the coherence namespace will not be allowed either ingress, nor egress. Very secure, but probably impractical for almost all use cases. After applying the deny-all policy more polices can be added to gradually open up the required access to run the Coherence Operator and Coherence clusters. Allow DNS When enforcing egress, such as with the deny-all policy above, it is important to remember that virtually every Pod needs to communicate with other Pods or Services, and will therefore need to access DNS. The policy below allows all Pods (using podSelector: {} ) egress to both TCP and UDP on port 53 in all namespaces. <markup lang=\"yaml\" title=\"manifests/allow-dns.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns spec: podSelector: { } policyTypes: - Egress egress: - to: - namespaceSelector: { } ports: - protocol: UDP port: 53 # - protocol: TCP # port: 53 If allowing DNS egress to all namespaces is overly permissive, DNS could be further restricted to just the kube-system namespace, therefore restricting DNS lookups to only Kubernetes internal DNS. Kubernetes applies the kubernetes.io/metadata.name label to namespaces, and sets its value to the namespace name, so this can be used in label matchers. With the policy below, Pods will be able to use internal Kubernetes DNS only. <markup lang=\"yaml\" title=\"manifests/allow-dns-kube-system.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns spec: podSelector: { } policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system ports: - protocol: UDP port: 53 # - protocol: TCP # port: 53 The policy above can be installed into the coherence namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-dns-kube-system.yaml Tip Some documentation regarding allowing DNS with Kubernetes network policies only shows opening up UDP connections. During our testing with network policies, we discovered that with only UDP allowed any lookup for a fully qualified name would fail. For example nslookup my-service.my-namespace.svc would work, but the fully qualified nslookup my-service.my-namespace.svc.cluster.local would not. Adding TCP to the DNS policy allowed DNS lookups with .cluster.local to also work. Neither the Coherence Operator, nor Coherence itself use a fully qualified service name for a DNS lookup. It appears that Java&#8217;s InetAddress.findAllByName() method still works only with UDP, albeit extremely slowly. By default, the service name used for the Coherence WKA setting uses just the .svc suffix. ",
            "title": "Introduction"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " The Coherence Operator uses Kubernetes APIs to manage various resources in the Kubernetes cluster. For this to work, the Operator Pod must be allowed egress to the Kubernetes API server. Configuring access to the API server is not as straight forward as other network policies. The reason for this is that there is no Pod available with labels that can be used in the configuration, instead, the IP address of the API server itself must be used. There are various methods to find the IP address of the API server. The exact method required may vary depending on the type of Kubernetes cluster being used, for example a simple development cluster running in KinD on a laptop may differ from a cluster running in a cloud provider&#8217;s infrastructure. The common way to find the API server&#8217;s IP address is to use kubectl cluster-info as follows: <markup lang=\"bash\" >$ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 In the above case the IP address of the API server would be 192.168.99.100 and the port is 8443 . In a simple KinD development cluster, the API server IP address can be obtained using kubectl as shown below: <markup lang=\"bash\" >$ kubectl -n default get endpoints kubernetes -o json { \"apiVersion\": \"v1\", \"kind\": \"Endpoints\", \"metadata\": { \"creationTimestamp\": \"2023-02-08T10:31:26Z\", \"labels\": { \"endpointslice.kubernetes.io/skip-mirror\": \"true\" }, \"name\": \"kubernetes\", \"namespace\": \"default\", \"resourceVersion\": \"196\", \"uid\": \"68b0a7de-c0db-4524-a1a2-9d29eb137f28\" }, \"subsets\": [ { \"addresses\": [ { \"ip\": \"192.168.49.2\" } ], \"ports\": [ { \"name\": \"https\", \"port\": 8443, \"protocol\": \"TCP\" } ] } ] } In the above case the IP address of the API server would be 192.168.49.2 and the port is 8443 . The IP address displayed for the API server can then be used in the network policy. The policy shown below allows Pods with the app.kubernetes.io/name: coherence-operator label (which the Operator has) egress access to the API server. <markup lang=\"yaml\" title=\"manifests/allow-k8s-api-server.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: operator-to-apiserver-egress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Egress - Ingress egress: - to: - ipBlock: cidr: 172.18.0.2/24 - ipBlock: cidr: 10.96.0.1/24 ports: - port: 6443 protocol: TCP - port: 443 protocol: TCP The allow-k8s-api-server.yaml policy can be installed into the coherence namespace to allow the Operator to communicate with the API server. <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-k8s-api-server.yaml With the allow-k8s-api-server.yaml policy applied, the Coherence Operator should now start correctly and its Pods should reach the \"ready\" state. ",
            "title": "Access the Kubernetes API Server"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " When a Coherence cluster is deployed, on start-up of a Pod the cluster member will connect to the Operator&#8217;s REST endpoint to query the site name and rack name, based on the Node the Coherence member is running on. To allow this to happen the Operator needs to be configured with the relevant ingress policy. The coherence-operator-rest-ingress policy applies to the Operator Pod, as it has a podSelector label of app.kubernetes.io/name: coherence-operator , which is a label applied to the Operator Pod. The policy allows any Pod with the label coherenceComponent: coherencePod ingress into the operator REST port. When the Operator creates a Coherence cluster, it applies the label coherenceComponent: coherencePod to all the Coherence cluster Pods. The policy below allows access from all namespaces using namespaceSelector: { } but it could be tightened up to specific namespaces if required. <markup lang=\"yaml\" title=\"manifests/allow-operator-rest-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-rest-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: operator protocol: TCP During operations such as scaling and shutting down of a Coherence cluster, the Operator needs to connect to the health endpoint of the Coherence cluster Pods. The coherence-operator-cluster-member-egress policy below applies to the Operator Pod, as it has a podSelector label of app.kubernetes.io/name: coherence-operator , which is a label applied to the Operator Pod. The policy allows egress to the health port in any Pod with the label coherenceComponent: coherencePod . When the Operator creates a Coherence cluster, it applies the label coherenceComponent: coherencePod to all the Coherence cluster Pods. The policy below allows egress to Coherence Pods in all namespaces using namespaceSelector: { } but it could be tightened up to specific namespaces if required. <markup lang=\"yaml\" title=\"manifests/allow-operator-cluster-member-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-cluster-member-egress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Egress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: health protocol: TCP The two policies can be applied to the coherence namespace. <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-operator-rest-ingress.yaml kubectl -n coherence apply -f manifests/allow-operator-cluster-member-egress.yaml ",
            "title": "Ingress From and Egress Into Coherence Cluster Member Pods"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " With all the above policies in place, the Operator is able to work correctly, but if a Coherence resource is now created Kubernetes will be unable to call the Operator&#8217;s webhook without the correct ingress policy. The following example demonstrates this. Assume there is a minimal`Coherence` yaml file named minimal.yaml that will create a single member Coherence cluster. <markup lang=\"yaml\" title=\"minimal.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 1 If minimal.yaml is applied using kubectl with a small timeout of 10 seconds, the creation of the resource will fail due to Kubernetes not having access to the Coherence Operator webhook. <markup lang=\"bash\" >$ kubectl apply --timeout=10s -f minimal.yaml Error from server (InternalError): error when creating \"minimal.yaml\": Internal error occurred: failed calling webhook \"coherence.oracle.com\": failed to call webhook: Post \"https://coherence-operator-webhook.operator-test.svc:443/mutate-coherence-oracle-com-v1-coherence?timeout=10s\": context deadline exceeded The simplest solution is to allow ingress from any IP address to the webhook on port, with a policy like that shown below. This policy uses and empty from: [] attribute, which allows access from anywhere to the webhook-server port in the Pod. <markup lang=\"yaml\" title=\"manifests/allow-webhook-ingress-from-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: apiserver-to-operator-webhook-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: [] ports: - port: webhook-server protocol: TCP Allowing access to the webhook from anywhere is not very secure, so a more restrictive from attribute could be used to limit access to the IP address (or addresses) of the Kubernetes API server. As with the API server policy above, the trick here is knowing the API server addresses to use. The policy below only allows access from specific addresses: <markup lang=\"yaml\" title=\"manifests/allow-webhook-ingress-from-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: apiserver-to-operator-webhook-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 172.18.0.2/24 - ipBlock: cidr: 10.96.0.1/24 ports: - port: webhook-server protocol: TCP - port: 443 protocol: TCP ",
            "title": "Webhook Ingress"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " Assuming the coherence namespace exists, and the deny-all and allow-dns policies described above have been applied, if the Coherence Operator is installed, it wil fail to start as it has no access to endpoints it needs to operate. The following sections will add network polices to allow the Coherence Operator to access Kubernetes services and Pods it requires. Access the Kubernetes API Server The Coherence Operator uses Kubernetes APIs to manage various resources in the Kubernetes cluster. For this to work, the Operator Pod must be allowed egress to the Kubernetes API server. Configuring access to the API server is not as straight forward as other network policies. The reason for this is that there is no Pod available with labels that can be used in the configuration, instead, the IP address of the API server itself must be used. There are various methods to find the IP address of the API server. The exact method required may vary depending on the type of Kubernetes cluster being used, for example a simple development cluster running in KinD on a laptop may differ from a cluster running in a cloud provider&#8217;s infrastructure. The common way to find the API server&#8217;s IP address is to use kubectl cluster-info as follows: <markup lang=\"bash\" >$ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 In the above case the IP address of the API server would be 192.168.99.100 and the port is 8443 . In a simple KinD development cluster, the API server IP address can be obtained using kubectl as shown below: <markup lang=\"bash\" >$ kubectl -n default get endpoints kubernetes -o json { \"apiVersion\": \"v1\", \"kind\": \"Endpoints\", \"metadata\": { \"creationTimestamp\": \"2023-02-08T10:31:26Z\", \"labels\": { \"endpointslice.kubernetes.io/skip-mirror\": \"true\" }, \"name\": \"kubernetes\", \"namespace\": \"default\", \"resourceVersion\": \"196\", \"uid\": \"68b0a7de-c0db-4524-a1a2-9d29eb137f28\" }, \"subsets\": [ { \"addresses\": [ { \"ip\": \"192.168.49.2\" } ], \"ports\": [ { \"name\": \"https\", \"port\": 8443, \"protocol\": \"TCP\" } ] } ] } In the above case the IP address of the API server would be 192.168.49.2 and the port is 8443 . The IP address displayed for the API server can then be used in the network policy. The policy shown below allows Pods with the app.kubernetes.io/name: coherence-operator label (which the Operator has) egress access to the API server. <markup lang=\"yaml\" title=\"manifests/allow-k8s-api-server.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: operator-to-apiserver-egress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Egress - Ingress egress: - to: - ipBlock: cidr: 172.18.0.2/24 - ipBlock: cidr: 10.96.0.1/24 ports: - port: 6443 protocol: TCP - port: 443 protocol: TCP The allow-k8s-api-server.yaml policy can be installed into the coherence namespace to allow the Operator to communicate with the API server. <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-k8s-api-server.yaml With the allow-k8s-api-server.yaml policy applied, the Coherence Operator should now start correctly and its Pods should reach the \"ready\" state. Ingress From and Egress Into Coherence Cluster Member Pods When a Coherence cluster is deployed, on start-up of a Pod the cluster member will connect to the Operator&#8217;s REST endpoint to query the site name and rack name, based on the Node the Coherence member is running on. To allow this to happen the Operator needs to be configured with the relevant ingress policy. The coherence-operator-rest-ingress policy applies to the Operator Pod, as it has a podSelector label of app.kubernetes.io/name: coherence-operator , which is a label applied to the Operator Pod. The policy allows any Pod with the label coherenceComponent: coherencePod ingress into the operator REST port. When the Operator creates a Coherence cluster, it applies the label coherenceComponent: coherencePod to all the Coherence cluster Pods. The policy below allows access from all namespaces using namespaceSelector: { } but it could be tightened up to specific namespaces if required. <markup lang=\"yaml\" title=\"manifests/allow-operator-rest-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-rest-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: operator protocol: TCP During operations such as scaling and shutting down of a Coherence cluster, the Operator needs to connect to the health endpoint of the Coherence cluster Pods. The coherence-operator-cluster-member-egress policy below applies to the Operator Pod, as it has a podSelector label of app.kubernetes.io/name: coherence-operator , which is a label applied to the Operator Pod. The policy allows egress to the health port in any Pod with the label coherenceComponent: coherencePod . When the Operator creates a Coherence cluster, it applies the label coherenceComponent: coherencePod to all the Coherence cluster Pods. The policy below allows egress to Coherence Pods in all namespaces using namespaceSelector: { } but it could be tightened up to specific namespaces if required. <markup lang=\"yaml\" title=\"manifests/allow-operator-cluster-member-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-cluster-member-egress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Egress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: health protocol: TCP The two policies can be applied to the coherence namespace. <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-operator-rest-ingress.yaml kubectl -n coherence apply -f manifests/allow-operator-cluster-member-egress.yaml Webhook Ingress With all the above policies in place, the Operator is able to work correctly, but if a Coherence resource is now created Kubernetes will be unable to call the Operator&#8217;s webhook without the correct ingress policy. The following example demonstrates this. Assume there is a minimal`Coherence` yaml file named minimal.yaml that will create a single member Coherence cluster. <markup lang=\"yaml\" title=\"minimal.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 1 If minimal.yaml is applied using kubectl with a small timeout of 10 seconds, the creation of the resource will fail due to Kubernetes not having access to the Coherence Operator webhook. <markup lang=\"bash\" >$ kubectl apply --timeout=10s -f minimal.yaml Error from server (InternalError): error when creating \"minimal.yaml\": Internal error occurred: failed calling webhook \"coherence.oracle.com\": failed to call webhook: Post \"https://coherence-operator-webhook.operator-test.svc:443/mutate-coherence-oracle-com-v1-coherence?timeout=10s\": context deadline exceeded The simplest solution is to allow ingress from any IP address to the webhook on port, with a policy like that shown below. This policy uses and empty from: [] attribute, which allows access from anywhere to the webhook-server port in the Pod. <markup lang=\"yaml\" title=\"manifests/allow-webhook-ingress-from-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: apiserver-to-operator-webhook-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: [] ports: - port: webhook-server protocol: TCP Allowing access to the webhook from anywhere is not very secure, so a more restrictive from attribute could be used to limit access to the IP address (or addresses) of the Kubernetes API server. As with the API server policy above, the trick here is knowing the API server addresses to use. The policy below only allows access from specific addresses: <markup lang=\"yaml\" title=\"manifests/allow-webhook-ingress-from-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: apiserver-to-operator-webhook-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 172.18.0.2/24 - ipBlock: cidr: 10.96.0.1/24 ports: - port: webhook-server protocol: TCP - port: 443 protocol: TCP ",
            "title": "Coherence Operator Policies"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " All Pods in a Coherence cluster must be able to talk to each other (otherwise they wouldn&#8217;t be a cluster). This means that there needs to be ingress and egress policies to allow this. Cluster port : The default cluster port is 7574, and there is almost never any need to change this, especially in a containerised environment where there is little chance of port conflicts. Unicast ports : Unicast uses TMB (default) and UDP. Each cluster member listens on one UDP and one TCP port and both ports need to be opened in the network policy. The default behaviour of Coherence is for the unicast ports to be automatically assigned from the operating system&#8217;s available ephemeral port range. When securing Coherence with network policies, the use of ephemeral ports will not work, so a range of ports can be specified for coherence to operate within. The Coherence Operator sets values for both unicast ports so that ephemeral ports will not be used. The default values are 7575 and 7576 . The two unicast ports can be changed in the Coherence spec by setting the spec.coherence.localPort field, and the spec.coherence.localPortAdjust field for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: localPort: 9000 localPortAdjust: 9001 Alternatively the values can also be configured using environment variables <markup lang=\"yaml\" >env: - name: COHERENCE_LOCALPORT value: \"9000\" - name: COHERENCE_LOCALPORT_ADJUST value: \"9001\" Echo port 7 : The default TCP port of the IpMonitor component that is used for detecting hardware failure of cluster members. Coherence doesn&#8217;t bind to this port, it only tries to connect to it as a means of pinging remote machines, or in this case Pods. The Coherence Operator applies the coherenceComponent: coherencePod label to all Coherence Pods, so this can be used in the network policy podSelector , to apply the policy to only the Coherence Pods. The policy below works with the default ports configured by the Operator. <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-access.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-coherence-cluster spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 endPort: 7576 protocol: TCP - port: 7574 endPort: 7576 protocol: UDP - port: 7 protocol: TCP egress: - to: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 endPort: 7576 protocol: TCP - port: 7574 endPort: 7576 protocol: UDP - port: 7 protocol: TCP If the Coherence local port and local port adjust values are changed, then the policy would need to be amended. For example, if COHERENCE_LOCALPORT=9000 and COHERENCE_LOCALPORT_ADJUST=9100 <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-access-non-default.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-coherence-cluster spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 protocol: TCP - port: 7574 protocol: UDP - port: 9000 endPort: 9100 protocol: TCP - port: 9000 endPort: 9100 protocol: UDP - port: 7 protocol: TCP egress: - to: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 protocol: TCP - port: 7574 protocol: UDP - port: 9000 endPort: 9100 protocol: TCP - port: 9000 endPort: 9100 protocol: UDP - port: 7 protocol: TCP Both of the policies above should be applied to the namespace where the Coherence cluster will be deployed. With the two policies above in place, the Coherence Pods will be able to communicate. ",
            "title": "Access Other Cluster Members"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " When a Coherence Pod starts Coherence calls back to the Operator to obtain the site name and rack name based on the Node the Pod is scheduled onto. For this to work, there needs to be an egress policy to allow Coherence Pods to access the Operator. During certain operations the Operator needs to call the Coherence members health endpoint to check health and status. For this to work there needs to be an ingress policy to allow the Operator access to the health endpoint in the Coherence Pods The policy below applies to Pods with the coherenceComponent: coherencePod label, which will match Coherence cluster member Pods. The policy allows ingress from the Operator to the Coherence Pod health port from namespace coherence using the namespace selector label kubernetes.io/metadata.name: coherence and Pod selector label app.kubernetes.io/name: coherence-operator The policy allows egress from the Coherence pods to the Operator&#8217;s REST server operator port. <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-operator-access.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-cluster-member-access spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: coherence podSelector: matchLabels: app.kubernetes.io/name: coherence-operator ports: - port: health protocol: TCP egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: coherence podSelector: matchLabels: app.kubernetes.io/name: coherence-operator ports: - port: operator protocol: TCP If the Operator is not running in the coherence namespace then the namespace match label can be changed to the required value. The policy above should be applied to the namespace where the Coherence cluster will be deployed. ",
            "title": "Egress to and Ingress From the Coherence Operator"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " If Coherence Extend is being used, then first the Extend Proxy must be configured to use a fixed port. The default behaviour of Coherence is to bind the Extend proxy to an ephemeral port and clients use the Coherence NameService to look up the port to use. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Extend proxy is already configured to run on a fixed port 20000 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_EXTEND_PORT environment variable. When using the Coherence Concurrent extensions over Extend, the Concurrent Extend proxy also needs to be configured with a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Concurrent Extend proxy is already configured to run on a fixed port 20001 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_CONCURRENT_EXTEND_PORT environment variable. For the examples below, a Coherence deployment has the following configuration. This will expose Extend on a port named extend with a port number of 20000 , and a port named extend-atomics with a port number of 20001 . The polices described below will then use the port names, so if required the port number could be changed and the policies would still work. <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: ports: - name: extend port: 20000 - name: extend-atomics port: 20001 The ingress policy below will work with the default Coherence image and allow ingress into the Coherence Pods to both the default Extend port and Coherence Concurrent Extend port. The policy allows ingress from Pods that have the coherence.oracle.com/extendClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-extend-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. Instead of using fixed port numbers in the The egress policy below will work with the default Coherence image and allow egress from Pods with the coherence.oracle.com/extendClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . on both the default Extend port and Coherence Concurrent Extend port. <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . ",
            "title": "Coherence Extend Access"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " If Coherence gRPC is being used, then first the gRPC Proxy must be configured to use a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the gRPC proxy is already configured to run on a fixed port 1408 . The gRPC proxy port can be changed by setting the COHERENCE_GRPC_PORT environment variable. The ingress policy below will allow ingress into the Coherence Pods gRPC port. The policy allows ingress from Pods that have the coherence.oracle.com/grpcClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-grpc-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-grpc-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/grpcClient: \"true\" ports: - port: grpc protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. The egress policy below will allow egress to the gRPC port from Pods with the coherence.oracle.com/grpcClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . ",
            "title": "Coherence gRPC Access"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " A typical Coherence cluster does not run in isolation but as part of a larger application. If the application has other Pods that are Coherence clients, then they will need access to the Coherence cluster. This would usually mean creating ingress and egress policies for the Coherence Extend port and gRPC port, depending on which Coherence APIs are being used. Instead of using actual port numbers, a NetworkPolicy can be made more flexible by using port names. When ports are defined in a container spec of a Pod, they are usually named. By using the names of the ports in the NetworkPolicy instead of port numbers, the real port numbers can be changed without affecting the network policy. Coherence Extend Access If Coherence Extend is being used, then first the Extend Proxy must be configured to use a fixed port. The default behaviour of Coherence is to bind the Extend proxy to an ephemeral port and clients use the Coherence NameService to look up the port to use. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Extend proxy is already configured to run on a fixed port 20000 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_EXTEND_PORT environment variable. When using the Coherence Concurrent extensions over Extend, the Concurrent Extend proxy also needs to be configured with a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Concurrent Extend proxy is already configured to run on a fixed port 20001 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_CONCURRENT_EXTEND_PORT environment variable. For the examples below, a Coherence deployment has the following configuration. This will expose Extend on a port named extend with a port number of 20000 , and a port named extend-atomics with a port number of 20001 . The polices described below will then use the port names, so if required the port number could be changed and the policies would still work. <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: ports: - name: extend port: 20000 - name: extend-atomics port: 20001 The ingress policy below will work with the default Coherence image and allow ingress into the Coherence Pods to both the default Extend port and Coherence Concurrent Extend port. The policy allows ingress from Pods that have the coherence.oracle.com/extendClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-extend-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. Instead of using fixed port numbers in the The egress policy below will work with the default Coherence image and allow egress from Pods with the coherence.oracle.com/extendClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . on both the default Extend port and Coherence Concurrent Extend port. <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . Coherence gRPC Access If Coherence gRPC is being used, then first the gRPC Proxy must be configured to use a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the gRPC proxy is already configured to run on a fixed port 1408 . The gRPC proxy port can be changed by setting the COHERENCE_GRPC_PORT environment variable. The ingress policy below will allow ingress into the Coherence Pods gRPC port. The policy allows ingress from Pods that have the coherence.oracle.com/grpcClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-grpc-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-grpc-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/grpcClient: \"true\" ports: - port: grpc protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. The egress policy below will allow egress to the gRPC port from Pods with the coherence.oracle.com/grpcClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . ",
            "title": "Client Access (Coherence*Extend and gRPC)"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " If Coherence metrics is enabled there will need to be an ingress policy to allow connections from metrics clients. There would also need to be a similar egress policy in the metrics client&#8217;s namespace to allow it to access the Coherence metrics endpoints. A simple Coherence resource that will create a cluster with metrics enabled is shown below. This yaml will create a Coherence cluster with a port names metrics that maps to the default metrics port of '9612`. <markup lang=\"yaml\" title=\"manifests/coherence-cluster-with-metrics.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: metrics: enabled: true ports: - name: metrics serviceMonitor: enabled: true The example below will assume that metrics will be scraped by Prometheus, and that Prometheus is installed into a namespace called monitoring . An ingress policy must be created in the namespace where the Coherence cluster is deployed allowing ingress to the metrics port from the Prometheus Pods. The Pods running Prometheus have a label app.kubernetes.io/name: prometheus so this can be used in the policy&#8217;s Pod selector. This policy should be applied to the namespace where the Coherence cluster is running. <markup lang=\"yaml\" title=\"manifests/allow-metrics-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-metrics-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: monitoring podSelector: matchLabels: app.kubernetes.io/name: prometheus ports: - port: metrics protocol: TCP If the monitoring namespace also has a \"deny-all\" policy and needs egress opening up for Prometheus to scrape metrics then an egress policy will need to be added to the monitoring namespace. The policy below will allow Pods with the label app.kubernetes.io/name: prometheus egress to Pods with the coherenceComponent: coherencePod label in any namespace. The policy could be further tightened up by adding a namespace selector to restrict egress to the specific namespace where the Coherence cluster is running. <markup lang=\"yaml\" title=\"manifests/allow-metrics-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-metrics-egress spec: podSelector: matchLabels: app.kubernetes.io/name: prometheus policyTypes: - Egress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: metrics protocol: TCP ",
            "title": "Coherence Metrics"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " Once the policies are in place to allow the Coherence Operator to work, the policies to allow Coherence clusters to run can be put in place. The exact set of policies requires will vary depending on the Coherence functionality being used. If Coherence is embedded in another application, such as a web-server, then additional policies may also be needed to allow ingress to other endpoints. Conversely, if the Coherence application needs access to other services, for example a database, then additional egress policies may need to be created. This example is only going to cover Coherence use cases, but it should be simple enough to apply the same techniques to policies for other applications. Access Other Cluster Members All Pods in a Coherence cluster must be able to talk to each other (otherwise they wouldn&#8217;t be a cluster). This means that there needs to be ingress and egress policies to allow this. Cluster port : The default cluster port is 7574, and there is almost never any need to change this, especially in a containerised environment where there is little chance of port conflicts. Unicast ports : Unicast uses TMB (default) and UDP. Each cluster member listens on one UDP and one TCP port and both ports need to be opened in the network policy. The default behaviour of Coherence is for the unicast ports to be automatically assigned from the operating system&#8217;s available ephemeral port range. When securing Coherence with network policies, the use of ephemeral ports will not work, so a range of ports can be specified for coherence to operate within. The Coherence Operator sets values for both unicast ports so that ephemeral ports will not be used. The default values are 7575 and 7576 . The two unicast ports can be changed in the Coherence spec by setting the spec.coherence.localPort field, and the spec.coherence.localPortAdjust field for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: localPort: 9000 localPortAdjust: 9001 Alternatively the values can also be configured using environment variables <markup lang=\"yaml\" >env: - name: COHERENCE_LOCALPORT value: \"9000\" - name: COHERENCE_LOCALPORT_ADJUST value: \"9001\" Echo port 7 : The default TCP port of the IpMonitor component that is used for detecting hardware failure of cluster members. Coherence doesn&#8217;t bind to this port, it only tries to connect to it as a means of pinging remote machines, or in this case Pods. The Coherence Operator applies the coherenceComponent: coherencePod label to all Coherence Pods, so this can be used in the network policy podSelector , to apply the policy to only the Coherence Pods. The policy below works with the default ports configured by the Operator. <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-access.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-coherence-cluster spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 endPort: 7576 protocol: TCP - port: 7574 endPort: 7576 protocol: UDP - port: 7 protocol: TCP egress: - to: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 endPort: 7576 protocol: TCP - port: 7574 endPort: 7576 protocol: UDP - port: 7 protocol: TCP If the Coherence local port and local port adjust values are changed, then the policy would need to be amended. For example, if COHERENCE_LOCALPORT=9000 and COHERENCE_LOCALPORT_ADJUST=9100 <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-access-non-default.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-coherence-cluster spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 protocol: TCP - port: 7574 protocol: UDP - port: 9000 endPort: 9100 protocol: TCP - port: 9000 endPort: 9100 protocol: UDP - port: 7 protocol: TCP egress: - to: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 protocol: TCP - port: 7574 protocol: UDP - port: 9000 endPort: 9100 protocol: TCP - port: 9000 endPort: 9100 protocol: UDP - port: 7 protocol: TCP Both of the policies above should be applied to the namespace where the Coherence cluster will be deployed. With the two policies above in place, the Coherence Pods will be able to communicate. Egress to and Ingress From the Coherence Operator When a Coherence Pod starts Coherence calls back to the Operator to obtain the site name and rack name based on the Node the Pod is scheduled onto. For this to work, there needs to be an egress policy to allow Coherence Pods to access the Operator. During certain operations the Operator needs to call the Coherence members health endpoint to check health and status. For this to work there needs to be an ingress policy to allow the Operator access to the health endpoint in the Coherence Pods The policy below applies to Pods with the coherenceComponent: coherencePod label, which will match Coherence cluster member Pods. The policy allows ingress from the Operator to the Coherence Pod health port from namespace coherence using the namespace selector label kubernetes.io/metadata.name: coherence and Pod selector label app.kubernetes.io/name: coherence-operator The policy allows egress from the Coherence pods to the Operator&#8217;s REST server operator port. <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-operator-access.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-cluster-member-access spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: coherence podSelector: matchLabels: app.kubernetes.io/name: coherence-operator ports: - port: health protocol: TCP egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: coherence podSelector: matchLabels: app.kubernetes.io/name: coherence-operator ports: - port: operator protocol: TCP If the Operator is not running in the coherence namespace then the namespace match label can be changed to the required value. The policy above should be applied to the namespace where the Coherence cluster will be deployed. Client Access (Coherence*Extend and gRPC) A typical Coherence cluster does not run in isolation but as part of a larger application. If the application has other Pods that are Coherence clients, then they will need access to the Coherence cluster. This would usually mean creating ingress and egress policies for the Coherence Extend port and gRPC port, depending on which Coherence APIs are being used. Instead of using actual port numbers, a NetworkPolicy can be made more flexible by using port names. When ports are defined in a container spec of a Pod, they are usually named. By using the names of the ports in the NetworkPolicy instead of port numbers, the real port numbers can be changed without affecting the network policy. Coherence Extend Access If Coherence Extend is being used, then first the Extend Proxy must be configured to use a fixed port. The default behaviour of Coherence is to bind the Extend proxy to an ephemeral port and clients use the Coherence NameService to look up the port to use. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Extend proxy is already configured to run on a fixed port 20000 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_EXTEND_PORT environment variable. When using the Coherence Concurrent extensions over Extend, the Concurrent Extend proxy also needs to be configured with a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Concurrent Extend proxy is already configured to run on a fixed port 20001 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_CONCURRENT_EXTEND_PORT environment variable. For the examples below, a Coherence deployment has the following configuration. This will expose Extend on a port named extend with a port number of 20000 , and a port named extend-atomics with a port number of 20001 . The polices described below will then use the port names, so if required the port number could be changed and the policies would still work. <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: ports: - name: extend port: 20000 - name: extend-atomics port: 20001 The ingress policy below will work with the default Coherence image and allow ingress into the Coherence Pods to both the default Extend port and Coherence Concurrent Extend port. The policy allows ingress from Pods that have the coherence.oracle.com/extendClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-extend-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. Instead of using fixed port numbers in the The egress policy below will work with the default Coherence image and allow egress from Pods with the coherence.oracle.com/extendClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . on both the default Extend port and Coherence Concurrent Extend port. <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . Coherence gRPC Access If Coherence gRPC is being used, then first the gRPC Proxy must be configured to use a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the gRPC proxy is already configured to run on a fixed port 1408 . The gRPC proxy port can be changed by setting the COHERENCE_GRPC_PORT environment variable. The ingress policy below will allow ingress into the Coherence Pods gRPC port. The policy allows ingress from Pods that have the coherence.oracle.com/grpcClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-grpc-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-grpc-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/grpcClient: \"true\" ports: - port: grpc protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. The egress policy below will allow egress to the gRPC port from Pods with the coherence.oracle.com/grpcClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . Coherence Metrics If Coherence metrics is enabled there will need to be an ingress policy to allow connections from metrics clients. There would also need to be a similar egress policy in the metrics client&#8217;s namespace to allow it to access the Coherence metrics endpoints. A simple Coherence resource that will create a cluster with metrics enabled is shown below. This yaml will create a Coherence cluster with a port names metrics that maps to the default metrics port of '9612`. <markup lang=\"yaml\" title=\"manifests/coherence-cluster-with-metrics.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: metrics: enabled: true ports: - name: metrics serviceMonitor: enabled: true The example below will assume that metrics will be scraped by Prometheus, and that Prometheus is installed into a namespace called monitoring . An ingress policy must be created in the namespace where the Coherence cluster is deployed allowing ingress to the metrics port from the Prometheus Pods. The Pods running Prometheus have a label app.kubernetes.io/name: prometheus so this can be used in the policy&#8217;s Pod selector. This policy should be applied to the namespace where the Coherence cluster is running. <markup lang=\"yaml\" title=\"manifests/allow-metrics-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-metrics-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: monitoring podSelector: matchLabels: app.kubernetes.io/name: prometheus ports: - port: metrics protocol: TCP If the monitoring namespace also has a \"deny-all\" policy and needs egress opening up for Prometheus to scrape metrics then an egress policy will need to be added to the monitoring namespace. The policy below will allow Pods with the label app.kubernetes.io/name: prometheus egress to Pods with the coherenceComponent: coherencePod label in any namespace. The policy could be further tightened up by adding a namespace selector to restrict egress to the specific namespace where the Coherence cluster is running. <markup lang=\"yaml\" title=\"manifests/allow-metrics-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-metrics-egress spec: podSelector: matchLabels: app.kubernetes.io/name: prometheus policyTypes: - Egress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: metrics protocol: TCP ",
            "title": "Coherence Cluster Member Policies"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " In this example we will assume the Operator will eventually be running in a namespace called coherence and the Coherence cluster will run in a namespace called coh-test . We can create the namespaces using kubectl <markup lang=\"bash\" >kubectl create ns coherence <markup lang=\"bash\" >kubectl create ns coh-test At this point there are no network policies installed, this will allow us to confirm the connectivity tests work. ",
            "title": "Create the Test Namespaces"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " The Operator simulator server should run in the coherence namespace. It can be created using the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator-server.yaml ",
            "title": "Start the Operator Simulator"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " The Coherence cluster member simulator server should run in the coh-test namespace. It can be created using the following command: <markup lang=\"bash\" >kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence-server.yaml ",
            "title": "Start the Coherence Cluster Simulator"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " We can now run the Operator test Job. This wil run a Kubernetes Job that simulates the Operator connecting to the Kubernetes API server and to the Operator Pods. <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml The test Job should complete very quickly as it is only testing connectivity to various ports. The results of the test can be seen by looking at the Pod log. The command below will display the log: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) The output from a successful test will look like this: <markup >1.6727606592497227e+09 INFO runner Operator Version: 3.3.2 1.6727606592497835e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727606592500978e+09 INFO runner Operator Built By: jonathanknight 1.6727606592501197e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727606592501485e+09 INFO runner Go Version: go1.19.2 1.6727606592501757e+09 INFO runner Go OS/Arch: linux/amd64 1.6727606592504115e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727606592504556e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727606592664087e+09 INFO net-test Testing connectivity PASSED {\"PortName\": \"K8s API Server\", \"Version\": \"v1.24.7\"} 1.6727606592674055e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727606592770455e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} We can see that the test has connected to the Kubernetes API server and has connected to the health port on the Coherence cluster test server in the coh-test namespace. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml ",
            "title": "Run the Operator Test"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " The cluster member test simulates a Coherence cluster member connecting to other cluster members in the same namespace and also making calls to the Operator&#8217;s REST endpoint. <markup lang=\"bash\" >kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence.yaml Again, the test should complete quickly as it is just connecting to various ports. The results of the test can be seen by looking at the Pod log. The command below will display the log: <markup lang=\"bash\" >kubectl -n coh-test logs $(kubectl -n coh-test get pod -l 'coherenceNetTest=coherence-client' -o name) The output from a successful test will look like this: <markup >1.6727631152848177e+09 INFO runner Operator Version: 3.3.2 1.6727631152849226e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727631152849536e+09 INFO runner Operator Built By: jonathanknight 1.6727631152849755e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727631152849965e+09 INFO runner Go Version: go1.19.2 1.6727631152850187e+09 INFO runner Go OS/Arch: linux/amd64 1.6727631152852216e+09 INFO net-test Starting test {\"Name\": \"Cluster Member Simulator\"} 1.6727631152852666e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort1\", \"Port\": 7575} 1.6727631152997334e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort1\", \"Port\": 7575} 1.6727631152998908e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort2\", \"Port\": 7576} 1.6727631153059115e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort2\", \"Port\": 7576} 1.6727631153063197e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Management\", \"Port\": 30000} 1.6727631153116117e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Management\", \"Port\": 30000} 1.6727631153119817e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Metrics\", \"Port\": 9612} 1.6727631153187876e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Metrics\", \"Port\": 9612} 1.6727631153189638e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"OperatorRest\", \"Port\": 8000} 1.6727631153265746e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"OperatorRest\", \"Port\": 8000} 1.6727631153267298e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Echo\", \"Port\": 7} 1.6727631153340726e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Echo\", \"Port\": 7} 1.6727631153342876e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"ClusterPort\", \"Port\": 7574} 1.6727631153406997e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"ClusterPort\", \"Port\": 7574} The test client successfully connected to the Coherence cluster port (7475), the two unicast ports (7575 and 7576), the Coherence management port (30000), the Coherence metrics port (9612), the Operator REST port (8000), and the echo port (7). The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence.yaml ",
            "title": "Run the Cluster Member Test"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " The Operator has a web-hook that k8s calls to validate Coherence resource configurations and to provide default values. Web hooks in Kubernetes use TLS by default and listen on port 443. The Operator server simulator also listens on port 443 to allow this connectivity to be tested. The network policy in this example that allows ingress to the web-hook allows any client to connect. This is because it is not always simple to work out the IP address that the API server will connect to the web-hook from. We can use the network tester to simulate this by running a Job that will connect to the web hook port. The web-hook test job in this example does not label the Pod and can be run from the default namespace to simulate a random external connection. <markup lang=\"bash\" >kubectl -n default apply -f examples/095_network_policies/manifests/net-test-webhook.yaml We can then check the results of the Job by looking at the Pod log. <markup lang=\"bash\" >kubectl -n default logs $(kubectl -n default get pod -l 'coherenceNetTest=webhook-client' -o name) The output from a successful test will look like this: <markup >1.6727639834559627e+09 INFO runner Operator Version: 3.3.2 1.6727639834562948e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727639834563956e+09 INFO runner Operator Built By: jonathanknight 1.6727639834565024e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727639834566057e+09 INFO runner Go Version: go1.19.2 1.6727639834567096e+09 INFO runner Go OS/Arch: linux/amd64 1.6727639834570327e+09 INFO net-test Starting test {\"Name\": \"Web-Hook Client\"} 1.6727639834571698e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"WebHook\", \"Port\": 443} 1.6727639834791095e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"WebHook\", \"Port\": 443} We can see that the client successfully connected to port 443. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n default delete -f examples/095_network_policies/manifests/net-test-webhook.yaml ",
            "title": "Testing the Operator Web Hook"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " The test client is able to test connectivity to any host and port. For example suppose we want to simulate a Prometheus Pod connecting to the metrics port of a Coherence cluster. The server simulator is listening on port 9612, so we need to run the client to connect to that port. We can create a Job yaml file to run the test client. As the test will simulate a Prometheus client we add the labels that a standard Prometheus Pod would have and that we also use in the network policies in this example. In the Job yaml, we need to set the HOST , PORT and optionally the PROTOCOL environment variables. In this test, the host is the DNS name for the Service created for the Coherence server simulator net-test-coherence-server.coh-test.svc , the port is the metrics port 9612 and the protocol is tcp . <markup lang=\"yaml\" title=\"manifests/net-test-client.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: test-client labels: app.kubernetes.io/name: prometheus coherenceNetTest: client spec: template: metadata: labels: app.kubernetes.io/name: prometheus coherenceNetTest: client spec: containers: - name: net-test image: ghcr.io/oracle/coherence-operator:3.3.4 env: - name: HOST value: net-test-coherence-server.coh-test.svc - name: PORT value: \"9612\" - name: PROTOCOL value: tcp command: - /files/runner args: - net-test - client restartPolicy: Never backoffLimit: 4 We need to run the test Job in the monitoring namespace, which is the same namespace that Prometheus is usually deployed into. <markup lang=\"bash\" >kubectl -n monitoring apply -f examples/095_network_policies/manifests/net-test-client.yaml We can then check the results of the Job by looking at the Pod log. <markup lang=\"bash\" >kubectl -n monitoring logs $(kubectl -n monitoring get pod -l 'coherenceNetTest=client' -o name) The output from a successful test will look like this: <markup >1.6727665901488597e+09 INFO runner Operator Version: 3.3.2 1.6727665901497366e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727665901498337e+09 INFO runner Operator Built By: jonathanknight 1.6727665901498716e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727665901498966e+09 INFO runner Go Version: go1.19.2 1.6727665901499205e+09 INFO runner Go OS/Arch: linux/amd64 1.6727665901501486e+09 INFO net-test Starting test {\"Name\": \"Simple Client\"} 1.6727665901501985e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"net-test-coherence-server.coh-test.svc-9612\", \"Port\": 9612} 1.6727665901573336e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"net-test-coherence-server.coh-test.svc-9612\", \"Port\": 9612} We can see that the test client successfully connected to the Coherence cluster member simulator on port 9612. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n monitoring delete -f examples/095_network_policies/manifests/net-test-client.yaml ",
            "title": "Testing Ad-Hoc Ports"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " All the above tests ran successfully without any network policies. We can now start to apply policies and re-run the tests to see what happens. In a secure environment we would start with a policy that blocks all access and then gradually open up required ports. We can apply the deny-all.yaml policy and then re-run the tests. We should apply the policy to both of the namespaces we are using in this example: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/deny-all.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/deny-all.yaml Now, re-run the Operator test client: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml and check the result: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) <markup >1.6727671834237397e+09 INFO runner Operator Version: 3.3.2 1.6727671834238796e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727671834239576e+09 INFO runner Operator Built By: jonathanknight 1.6727671834240365e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727671834240875e+09 INFO runner Go Version: go1.19.2 1.6727671834241736e+09 INFO runner Go OS/Arch: linux/amd64 1.6727671834244306e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727671834245417e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727672134268515e+09 INFO net-test Testing connectivity FAILED {\"PortName\": \"K8s API Server\", \"Error\": \"Get \\\"https://10.96.0.1:443/version?timeout=32s\\\": dial tcp 10.96.0.1:443: i/o timeout\"} 1.6727672134269848e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727672234281697e+09 INFO net-test Testing connectivity FAILED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676, \"Error\": \"dial tcp: lookup net-test-coherence-server.coh-test.svc: i/o timeout\"} We can see that the test client failed to connect to the Kubernetes API server and failed to connect to the Coherence cluster health port. This means the deny-all policy is working. We can now apply the various polices to fix the test <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-dns.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-k8s-api-server.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-cluster-member-egress.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-rest-ingress.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-webhook-ingress-from-all.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-dns.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-access.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-operator-access.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-metrics-ingress.yaml Now, delete and re-run the Operator test client: <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml and check the result: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) Now with the policies applied the test should have passed. <markup >1.6727691273634596e+09 INFO runner Operator Version: 3.3.2 1.6727691273635025e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727691273635256e+09 INFO runner Operator Built By: jonathanknight 1.6727691273635616e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727691273637156e+09 INFO runner Go Version: go1.19.2 1.6727691273637407e+09 INFO runner Go OS/Arch: linux/amd64 1.6727691273639407e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727691273639877e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727691273857167e+09 INFO net-test Testing connectivity PASSED {\"PortName\": \"K8s API Server\", \"Version\": \"v1.24.7\"} 1.6727691273858056e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727691273933685e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} The other tests can also be re-run and should also pass. ",
            "title": "Test with Network Policies"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " Once the tests are completed, the test servers and Jobs can be deleted. <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator-server.yaml kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence-server.yaml ",
            "title": "Clean-Up"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " At the time of writing this documentation, Kubernetes provides no way to verify the correctness of network policies. It is easy to mess up a policy, in which case policies will either block too much traffic, in which case your application will work, or worse they will not be blocking access and leave a security hole. As we have had various requests for help from customers who cannot get Coherence to work with network policies enabled, the Operator has a simple utility to test connectivity outside of Coherence. This will allow testing pf policies without the complications of having to start a Coherence server. This example includes some simple yaml files that will create simulator Pods that listen on all the ports used by the Operator and by a Coherence cluster member. These simulator Pods are configured with the same labels that the real Operator and Coherence Pods would have and the same labels used by the network policies in this example. Also included are some yaml files that start a test client, that simulates either the Operator connecting to Coherence Pods or a Coherence Pod connecting to the Operator and to other Coherence Pods. To run these tests, the Operator does not have to be installed. Create the Test Namespaces In this example we will assume the Operator will eventually be running in a namespace called coherence and the Coherence cluster will run in a namespace called coh-test . We can create the namespaces using kubectl <markup lang=\"bash\" >kubectl create ns coherence <markup lang=\"bash\" >kubectl create ns coh-test At this point there are no network policies installed, this will allow us to confirm the connectivity tests work. Start the Operator Simulator The Operator simulator server should run in the coherence namespace. It can be created using the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator-server.yaml Start the Coherence Cluster Simulator The Coherence cluster member simulator server should run in the coh-test namespace. It can be created using the following command: <markup lang=\"bash\" >kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence-server.yaml Run the Operator Test We can now run the Operator test Job. This wil run a Kubernetes Job that simulates the Operator connecting to the Kubernetes API server and to the Operator Pods. <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml The test Job should complete very quickly as it is only testing connectivity to various ports. The results of the test can be seen by looking at the Pod log. The command below will display the log: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) The output from a successful test will look like this: <markup >1.6727606592497227e+09 INFO runner Operator Version: 3.3.2 1.6727606592497835e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727606592500978e+09 INFO runner Operator Built By: jonathanknight 1.6727606592501197e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727606592501485e+09 INFO runner Go Version: go1.19.2 1.6727606592501757e+09 INFO runner Go OS/Arch: linux/amd64 1.6727606592504115e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727606592504556e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727606592664087e+09 INFO net-test Testing connectivity PASSED {\"PortName\": \"K8s API Server\", \"Version\": \"v1.24.7\"} 1.6727606592674055e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727606592770455e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} We can see that the test has connected to the Kubernetes API server and has connected to the health port on the Coherence cluster test server in the coh-test namespace. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml Run the Cluster Member Test The cluster member test simulates a Coherence cluster member connecting to other cluster members in the same namespace and also making calls to the Operator&#8217;s REST endpoint. <markup lang=\"bash\" >kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence.yaml Again, the test should complete quickly as it is just connecting to various ports. The results of the test can be seen by looking at the Pod log. The command below will display the log: <markup lang=\"bash\" >kubectl -n coh-test logs $(kubectl -n coh-test get pod -l 'coherenceNetTest=coherence-client' -o name) The output from a successful test will look like this: <markup >1.6727631152848177e+09 INFO runner Operator Version: 3.3.2 1.6727631152849226e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727631152849536e+09 INFO runner Operator Built By: jonathanknight 1.6727631152849755e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727631152849965e+09 INFO runner Go Version: go1.19.2 1.6727631152850187e+09 INFO runner Go OS/Arch: linux/amd64 1.6727631152852216e+09 INFO net-test Starting test {\"Name\": \"Cluster Member Simulator\"} 1.6727631152852666e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort1\", \"Port\": 7575} 1.6727631152997334e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort1\", \"Port\": 7575} 1.6727631152998908e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort2\", \"Port\": 7576} 1.6727631153059115e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort2\", \"Port\": 7576} 1.6727631153063197e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Management\", \"Port\": 30000} 1.6727631153116117e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Management\", \"Port\": 30000} 1.6727631153119817e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Metrics\", \"Port\": 9612} 1.6727631153187876e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Metrics\", \"Port\": 9612} 1.6727631153189638e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"OperatorRest\", \"Port\": 8000} 1.6727631153265746e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"OperatorRest\", \"Port\": 8000} 1.6727631153267298e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Echo\", \"Port\": 7} 1.6727631153340726e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Echo\", \"Port\": 7} 1.6727631153342876e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"ClusterPort\", \"Port\": 7574} 1.6727631153406997e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"ClusterPort\", \"Port\": 7574} The test client successfully connected to the Coherence cluster port (7475), the two unicast ports (7575 and 7576), the Coherence management port (30000), the Coherence metrics port (9612), the Operator REST port (8000), and the echo port (7). The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence.yaml Testing the Operator Web Hook The Operator has a web-hook that k8s calls to validate Coherence resource configurations and to provide default values. Web hooks in Kubernetes use TLS by default and listen on port 443. The Operator server simulator also listens on port 443 to allow this connectivity to be tested. The network policy in this example that allows ingress to the web-hook allows any client to connect. This is because it is not always simple to work out the IP address that the API server will connect to the web-hook from. We can use the network tester to simulate this by running a Job that will connect to the web hook port. The web-hook test job in this example does not label the Pod and can be run from the default namespace to simulate a random external connection. <markup lang=\"bash\" >kubectl -n default apply -f examples/095_network_policies/manifests/net-test-webhook.yaml We can then check the results of the Job by looking at the Pod log. <markup lang=\"bash\" >kubectl -n default logs $(kubectl -n default get pod -l 'coherenceNetTest=webhook-client' -o name) The output from a successful test will look like this: <markup >1.6727639834559627e+09 INFO runner Operator Version: 3.3.2 1.6727639834562948e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727639834563956e+09 INFO runner Operator Built By: jonathanknight 1.6727639834565024e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727639834566057e+09 INFO runner Go Version: go1.19.2 1.6727639834567096e+09 INFO runner Go OS/Arch: linux/amd64 1.6727639834570327e+09 INFO net-test Starting test {\"Name\": \"Web-Hook Client\"} 1.6727639834571698e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"WebHook\", \"Port\": 443} 1.6727639834791095e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"WebHook\", \"Port\": 443} We can see that the client successfully connected to port 443. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n default delete -f examples/095_network_policies/manifests/net-test-webhook.yaml Testing Ad-Hoc Ports The test client is able to test connectivity to any host and port. For example suppose we want to simulate a Prometheus Pod connecting to the metrics port of a Coherence cluster. The server simulator is listening on port 9612, so we need to run the client to connect to that port. We can create a Job yaml file to run the test client. As the test will simulate a Prometheus client we add the labels that a standard Prometheus Pod would have and that we also use in the network policies in this example. In the Job yaml, we need to set the HOST , PORT and optionally the PROTOCOL environment variables. In this test, the host is the DNS name for the Service created for the Coherence server simulator net-test-coherence-server.coh-test.svc , the port is the metrics port 9612 and the protocol is tcp . <markup lang=\"yaml\" title=\"manifests/net-test-client.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: test-client labels: app.kubernetes.io/name: prometheus coherenceNetTest: client spec: template: metadata: labels: app.kubernetes.io/name: prometheus coherenceNetTest: client spec: containers: - name: net-test image: ghcr.io/oracle/coherence-operator:3.3.4 env: - name: HOST value: net-test-coherence-server.coh-test.svc - name: PORT value: \"9612\" - name: PROTOCOL value: tcp command: - /files/runner args: - net-test - client restartPolicy: Never backoffLimit: 4 We need to run the test Job in the monitoring namespace, which is the same namespace that Prometheus is usually deployed into. <markup lang=\"bash\" >kubectl -n monitoring apply -f examples/095_network_policies/manifests/net-test-client.yaml We can then check the results of the Job by looking at the Pod log. <markup lang=\"bash\" >kubectl -n monitoring logs $(kubectl -n monitoring get pod -l 'coherenceNetTest=client' -o name) The output from a successful test will look like this: <markup >1.6727665901488597e+09 INFO runner Operator Version: 3.3.2 1.6727665901497366e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727665901498337e+09 INFO runner Operator Built By: jonathanknight 1.6727665901498716e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727665901498966e+09 INFO runner Go Version: go1.19.2 1.6727665901499205e+09 INFO runner Go OS/Arch: linux/amd64 1.6727665901501486e+09 INFO net-test Starting test {\"Name\": \"Simple Client\"} 1.6727665901501985e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"net-test-coherence-server.coh-test.svc-9612\", \"Port\": 9612} 1.6727665901573336e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"net-test-coherence-server.coh-test.svc-9612\", \"Port\": 9612} We can see that the test client successfully connected to the Coherence cluster member simulator on port 9612. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n monitoring delete -f examples/095_network_policies/manifests/net-test-client.yaml Test with Network Policies All the above tests ran successfully without any network policies. We can now start to apply policies and re-run the tests to see what happens. In a secure environment we would start with a policy that blocks all access and then gradually open up required ports. We can apply the deny-all.yaml policy and then re-run the tests. We should apply the policy to both of the namespaces we are using in this example: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/deny-all.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/deny-all.yaml Now, re-run the Operator test client: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml and check the result: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) <markup >1.6727671834237397e+09 INFO runner Operator Version: 3.3.2 1.6727671834238796e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727671834239576e+09 INFO runner Operator Built By: jonathanknight 1.6727671834240365e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727671834240875e+09 INFO runner Go Version: go1.19.2 1.6727671834241736e+09 INFO runner Go OS/Arch: linux/amd64 1.6727671834244306e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727671834245417e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727672134268515e+09 INFO net-test Testing connectivity FAILED {\"PortName\": \"K8s API Server\", \"Error\": \"Get \\\"https://10.96.0.1:443/version?timeout=32s\\\": dial tcp 10.96.0.1:443: i/o timeout\"} 1.6727672134269848e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727672234281697e+09 INFO net-test Testing connectivity FAILED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676, \"Error\": \"dial tcp: lookup net-test-coherence-server.coh-test.svc: i/o timeout\"} We can see that the test client failed to connect to the Kubernetes API server and failed to connect to the Coherence cluster health port. This means the deny-all policy is working. We can now apply the various polices to fix the test <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-dns.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-k8s-api-server.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-cluster-member-egress.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-rest-ingress.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-webhook-ingress-from-all.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-dns.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-access.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-operator-access.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-metrics-ingress.yaml Now, delete and re-run the Operator test client: <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml and check the result: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) Now with the policies applied the test should have passed. <markup >1.6727691273634596e+09 INFO runner Operator Version: 3.3.2 1.6727691273635025e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727691273635256e+09 INFO runner Operator Built By: jonathanknight 1.6727691273635616e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727691273637156e+09 INFO runner Go Version: go1.19.2 1.6727691273637407e+09 INFO runner Go OS/Arch: linux/amd64 1.6727691273639407e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727691273639877e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727691273857167e+09 INFO net-test Testing connectivity PASSED {\"PortName\": \"K8s API Server\", \"Version\": \"v1.24.7\"} 1.6727691273858056e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727691273933685e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} The other tests can also be re-run and should also pass. Clean-Up Once the tests are completed, the test servers and Jobs can be deleted. <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator-server.yaml kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence-server.yaml ",
            "title": "Testing Network Policies"
        },
        {
            "location": "/examples/095_network_policies/README",
            "text": " This example covers running the Coherence Operator and Coherence clusters in Kubernetes with network policies. In Kubernetes, a Network Policy is an application-centric construct which allow you to specify how a pod is allowed to communicate with various network \"entities\" (we use the word \"entity\" here to avoid overloading the more common terms such as \"endpoints\" and \"services\", which have specific Kubernetes connotations) over the network. Note Network policies in Kubernetes are easy to get wrong if you are not careful. In this case a policy will either block traffic it should not, in which case your application will not work, or it will let traffic through it should block, which will be an invisible security hole. It is obviously important to test your policies, but Kubernetes offers next to zero visibility into what the policies are actually doing, as it is typically the network CNI extensions that are providing the policy implementation and each of these may work in a different way. Introduction Kubernetes network policies specify the access permissions for groups of pods, similar to security groups in the cloud are used to control access to VM instances and similar to firewalls. The default behaviour of a Kubernetes cluster is to allow all Pods to freely talk to each other. Whilst this sounds insecure, originally Kubernetes was designed to orchestrate services that communicated with each other, it was only later that network policies were added. A network policy is applied to a Kubernetes namespace and controls ingress into and egress out of Pods in that namespace. The ports specified in a NetworkPolicy are the ports exposed by the Pods , they are not any ports that may be exposed by any Service that exposes the Pod ports. For example, if a Pod exposed port 8080 and a Service exposing the Pod mapped port 80 in the Service to port 8080 in the Pod , the NetworkPolicy ingress rule would be for the Pod port 8080. Network polices would typically end up being dictated by corporate security standards where different companies may apply stricter or looser rules than others. The examples in this document start from the premise that everything will be blocked by a \"deny all\" policy and then opened up as needed. This is the most secure use of network policies, and hence the examples can easily be tweaked if looser rules are applied. This example has the following sections: Deny All Policy - denying all ingress and egress Allow DNS - almost every use case will require egress to DNS Coherence Operator Policies - the network policies required to run the Coherence Operator Kubernetes API Server - allow the Operator egress to the Kubernetes API server Coherence Clusters Pods - allow the Operator egress to the Coherence cluster Pods Web Hooks - allow ingress to the Operator&#8217;s web hook port Coherence Cluster Policies - the network policies required to run Coherence clusters Inter-Cluster Access - allow Coherence cluster Pods to communicate Coherence Operator - allow Coherence cluster Pods to communicate with the Operator Clients - allows access by Extend and gRPC clients Metrics - allow Coherence cluster member metrics to be scraped Testing Connectivity - using the Operator&#8217;s network connectivity test utility to test policies Deny All Policy Kubernetes does not have a deny all policy, but this can be achieved with a regular network policy that specifies a policyTypes of both 'Ingress` and Egress but omits any definitions. A wild-card podSelector: {} applies the policy to all Pods in the namespace. <markup lang=\"yaml\" title=\"manifests/deny-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress - Egress ingress: [] egress: [] The policy above can be installed into the coherence namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/deny-all.yaml After installing the deny-all policy, any Pod in the coherence namespace will not be allowed either ingress, nor egress. Very secure, but probably impractical for almost all use cases. After applying the deny-all policy more polices can be added to gradually open up the required access to run the Coherence Operator and Coherence clusters. Allow DNS When enforcing egress, such as with the deny-all policy above, it is important to remember that virtually every Pod needs to communicate with other Pods or Services, and will therefore need to access DNS. The policy below allows all Pods (using podSelector: {} ) egress to both TCP and UDP on port 53 in all namespaces. <markup lang=\"yaml\" title=\"manifests/allow-dns.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns spec: podSelector: { } policyTypes: - Egress egress: - to: - namespaceSelector: { } ports: - protocol: UDP port: 53 # - protocol: TCP # port: 53 If allowing DNS egress to all namespaces is overly permissive, DNS could be further restricted to just the kube-system namespace, therefore restricting DNS lookups to only Kubernetes internal DNS. Kubernetes applies the kubernetes.io/metadata.name label to namespaces, and sets its value to the namespace name, so this can be used in label matchers. With the policy below, Pods will be able to use internal Kubernetes DNS only. <markup lang=\"yaml\" title=\"manifests/allow-dns-kube-system.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns spec: podSelector: { } policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system ports: - protocol: UDP port: 53 # - protocol: TCP # port: 53 The policy above can be installed into the coherence namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-dns-kube-system.yaml Tip Some documentation regarding allowing DNS with Kubernetes network policies only shows opening up UDP connections. During our testing with network policies, we discovered that with only UDP allowed any lookup for a fully qualified name would fail. For example nslookup my-service.my-namespace.svc would work, but the fully qualified nslookup my-service.my-namespace.svc.cluster.local would not. Adding TCP to the DNS policy allowed DNS lookups with .cluster.local to also work. Neither the Coherence Operator, nor Coherence itself use a fully qualified service name for a DNS lookup. It appears that Java&#8217;s InetAddress.findAllByName() method still works only with UDP, albeit extremely slowly. By default, the service name used for the Coherence WKA setting uses just the .svc suffix. Coherence Operator Policies Assuming the coherence namespace exists, and the deny-all and allow-dns policies described above have been applied, if the Coherence Operator is installed, it wil fail to start as it has no access to endpoints it needs to operate. The following sections will add network polices to allow the Coherence Operator to access Kubernetes services and Pods it requires. Access the Kubernetes API Server The Coherence Operator uses Kubernetes APIs to manage various resources in the Kubernetes cluster. For this to work, the Operator Pod must be allowed egress to the Kubernetes API server. Configuring access to the API server is not as straight forward as other network policies. The reason for this is that there is no Pod available with labels that can be used in the configuration, instead, the IP address of the API server itself must be used. There are various methods to find the IP address of the API server. The exact method required may vary depending on the type of Kubernetes cluster being used, for example a simple development cluster running in KinD on a laptop may differ from a cluster running in a cloud provider&#8217;s infrastructure. The common way to find the API server&#8217;s IP address is to use kubectl cluster-info as follows: <markup lang=\"bash\" >$ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 In the above case the IP address of the API server would be 192.168.99.100 and the port is 8443 . In a simple KinD development cluster, the API server IP address can be obtained using kubectl as shown below: <markup lang=\"bash\" >$ kubectl -n default get endpoints kubernetes -o json { \"apiVersion\": \"v1\", \"kind\": \"Endpoints\", \"metadata\": { \"creationTimestamp\": \"2023-02-08T10:31:26Z\", \"labels\": { \"endpointslice.kubernetes.io/skip-mirror\": \"true\" }, \"name\": \"kubernetes\", \"namespace\": \"default\", \"resourceVersion\": \"196\", \"uid\": \"68b0a7de-c0db-4524-a1a2-9d29eb137f28\" }, \"subsets\": [ { \"addresses\": [ { \"ip\": \"192.168.49.2\" } ], \"ports\": [ { \"name\": \"https\", \"port\": 8443, \"protocol\": \"TCP\" } ] } ] } In the above case the IP address of the API server would be 192.168.49.2 and the port is 8443 . The IP address displayed for the API server can then be used in the network policy. The policy shown below allows Pods with the app.kubernetes.io/name: coherence-operator label (which the Operator has) egress access to the API server. <markup lang=\"yaml\" title=\"manifests/allow-k8s-api-server.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: operator-to-apiserver-egress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Egress - Ingress egress: - to: - ipBlock: cidr: 172.18.0.2/24 - ipBlock: cidr: 10.96.0.1/24 ports: - port: 6443 protocol: TCP - port: 443 protocol: TCP The allow-k8s-api-server.yaml policy can be installed into the coherence namespace to allow the Operator to communicate with the API server. <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-k8s-api-server.yaml With the allow-k8s-api-server.yaml policy applied, the Coherence Operator should now start correctly and its Pods should reach the \"ready\" state. Ingress From and Egress Into Coherence Cluster Member Pods When a Coherence cluster is deployed, on start-up of a Pod the cluster member will connect to the Operator&#8217;s REST endpoint to query the site name and rack name, based on the Node the Coherence member is running on. To allow this to happen the Operator needs to be configured with the relevant ingress policy. The coherence-operator-rest-ingress policy applies to the Operator Pod, as it has a podSelector label of app.kubernetes.io/name: coherence-operator , which is a label applied to the Operator Pod. The policy allows any Pod with the label coherenceComponent: coherencePod ingress into the operator REST port. When the Operator creates a Coherence cluster, it applies the label coherenceComponent: coherencePod to all the Coherence cluster Pods. The policy below allows access from all namespaces using namespaceSelector: { } but it could be tightened up to specific namespaces if required. <markup lang=\"yaml\" title=\"manifests/allow-operator-rest-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-rest-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: operator protocol: TCP During operations such as scaling and shutting down of a Coherence cluster, the Operator needs to connect to the health endpoint of the Coherence cluster Pods. The coherence-operator-cluster-member-egress policy below applies to the Operator Pod, as it has a podSelector label of app.kubernetes.io/name: coherence-operator , which is a label applied to the Operator Pod. The policy allows egress to the health port in any Pod with the label coherenceComponent: coherencePod . When the Operator creates a Coherence cluster, it applies the label coherenceComponent: coherencePod to all the Coherence cluster Pods. The policy below allows egress to Coherence Pods in all namespaces using namespaceSelector: { } but it could be tightened up to specific namespaces if required. <markup lang=\"yaml\" title=\"manifests/allow-operator-cluster-member-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-cluster-member-egress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Egress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: health protocol: TCP The two policies can be applied to the coherence namespace. <markup lang=\"bash\" >kubectl -n coherence apply -f manifests/allow-operator-rest-ingress.yaml kubectl -n coherence apply -f manifests/allow-operator-cluster-member-egress.yaml Webhook Ingress With all the above policies in place, the Operator is able to work correctly, but if a Coherence resource is now created Kubernetes will be unable to call the Operator&#8217;s webhook without the correct ingress policy. The following example demonstrates this. Assume there is a minimal`Coherence` yaml file named minimal.yaml that will create a single member Coherence cluster. <markup lang=\"yaml\" title=\"minimal.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 1 If minimal.yaml is applied using kubectl with a small timeout of 10 seconds, the creation of the resource will fail due to Kubernetes not having access to the Coherence Operator webhook. <markup lang=\"bash\" >$ kubectl apply --timeout=10s -f minimal.yaml Error from server (InternalError): error when creating \"minimal.yaml\": Internal error occurred: failed calling webhook \"coherence.oracle.com\": failed to call webhook: Post \"https://coherence-operator-webhook.operator-test.svc:443/mutate-coherence-oracle-com-v1-coherence?timeout=10s\": context deadline exceeded The simplest solution is to allow ingress from any IP address to the webhook on port, with a policy like that shown below. This policy uses and empty from: [] attribute, which allows access from anywhere to the webhook-server port in the Pod. <markup lang=\"yaml\" title=\"manifests/allow-webhook-ingress-from-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: apiserver-to-operator-webhook-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: [] ports: - port: webhook-server protocol: TCP Allowing access to the webhook from anywhere is not very secure, so a more restrictive from attribute could be used to limit access to the IP address (or addresses) of the Kubernetes API server. As with the API server policy above, the trick here is knowing the API server addresses to use. The policy below only allows access from specific addresses: <markup lang=\"yaml\" title=\"manifests/allow-webhook-ingress-from-all.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: apiserver-to-operator-webhook-ingress spec: podSelector: matchLabels: app.kubernetes.io/name: coherence-operator policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 172.18.0.2/24 - ipBlock: cidr: 10.96.0.1/24 ports: - port: webhook-server protocol: TCP - port: 443 protocol: TCP Coherence Cluster Member Policies Once the policies are in place to allow the Coherence Operator to work, the policies to allow Coherence clusters to run can be put in place. The exact set of policies requires will vary depending on the Coherence functionality being used. If Coherence is embedded in another application, such as a web-server, then additional policies may also be needed to allow ingress to other endpoints. Conversely, if the Coherence application needs access to other services, for example a database, then additional egress policies may need to be created. This example is only going to cover Coherence use cases, but it should be simple enough to apply the same techniques to policies for other applications. Access Other Cluster Members All Pods in a Coherence cluster must be able to talk to each other (otherwise they wouldn&#8217;t be a cluster). This means that there needs to be ingress and egress policies to allow this. Cluster port : The default cluster port is 7574, and there is almost never any need to change this, especially in a containerised environment where there is little chance of port conflicts. Unicast ports : Unicast uses TMB (default) and UDP. Each cluster member listens on one UDP and one TCP port and both ports need to be opened in the network policy. The default behaviour of Coherence is for the unicast ports to be automatically assigned from the operating system&#8217;s available ephemeral port range. When securing Coherence with network policies, the use of ephemeral ports will not work, so a range of ports can be specified for coherence to operate within. The Coherence Operator sets values for both unicast ports so that ephemeral ports will not be used. The default values are 7575 and 7576 . The two unicast ports can be changed in the Coherence spec by setting the spec.coherence.localPort field, and the spec.coherence.localPortAdjust field for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: localPort: 9000 localPortAdjust: 9001 Alternatively the values can also be configured using environment variables <markup lang=\"yaml\" >env: - name: COHERENCE_LOCALPORT value: \"9000\" - name: COHERENCE_LOCALPORT_ADJUST value: \"9001\" Echo port 7 : The default TCP port of the IpMonitor component that is used for detecting hardware failure of cluster members. Coherence doesn&#8217;t bind to this port, it only tries to connect to it as a means of pinging remote machines, or in this case Pods. The Coherence Operator applies the coherenceComponent: coherencePod label to all Coherence Pods, so this can be used in the network policy podSelector , to apply the policy to only the Coherence Pods. The policy below works with the default ports configured by the Operator. <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-access.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-coherence-cluster spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 endPort: 7576 protocol: TCP - port: 7574 endPort: 7576 protocol: UDP - port: 7 protocol: TCP egress: - to: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 endPort: 7576 protocol: TCP - port: 7574 endPort: 7576 protocol: UDP - port: 7 protocol: TCP If the Coherence local port and local port adjust values are changed, then the policy would need to be amended. For example, if COHERENCE_LOCALPORT=9000 and COHERENCE_LOCALPORT_ADJUST=9100 <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-access-non-default.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-coherence-cluster spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 protocol: TCP - port: 7574 protocol: UDP - port: 9000 endPort: 9100 protocol: TCP - port: 9000 endPort: 9100 protocol: UDP - port: 7 protocol: TCP egress: - to: - podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: 7574 protocol: TCP - port: 7574 protocol: UDP - port: 9000 endPort: 9100 protocol: TCP - port: 9000 endPort: 9100 protocol: UDP - port: 7 protocol: TCP Both of the policies above should be applied to the namespace where the Coherence cluster will be deployed. With the two policies above in place, the Coherence Pods will be able to communicate. Egress to and Ingress From the Coherence Operator When a Coherence Pod starts Coherence calls back to the Operator to obtain the site name and rack name based on the Node the Pod is scheduled onto. For this to work, there needs to be an egress policy to allow Coherence Pods to access the Operator. During certain operations the Operator needs to call the Coherence members health endpoint to check health and status. For this to work there needs to be an ingress policy to allow the Operator access to the health endpoint in the Coherence Pods The policy below applies to Pods with the coherenceComponent: coherencePod label, which will match Coherence cluster member Pods. The policy allows ingress from the Operator to the Coherence Pod health port from namespace coherence using the namespace selector label kubernetes.io/metadata.name: coherence and Pod selector label app.kubernetes.io/name: coherence-operator The policy allows egress from the Coherence pods to the Operator&#8217;s REST server operator port. <markup lang=\"yaml\" title=\"manifests/allow-cluster-member-operator-access.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-operator-cluster-member-access spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: coherence podSelector: matchLabels: app.kubernetes.io/name: coherence-operator ports: - port: health protocol: TCP egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: coherence podSelector: matchLabels: app.kubernetes.io/name: coherence-operator ports: - port: operator protocol: TCP If the Operator is not running in the coherence namespace then the namespace match label can be changed to the required value. The policy above should be applied to the namespace where the Coherence cluster will be deployed. Client Access (Coherence*Extend and gRPC) A typical Coherence cluster does not run in isolation but as part of a larger application. If the application has other Pods that are Coherence clients, then they will need access to the Coherence cluster. This would usually mean creating ingress and egress policies for the Coherence Extend port and gRPC port, depending on which Coherence APIs are being used. Instead of using actual port numbers, a NetworkPolicy can be made more flexible by using port names. When ports are defined in a container spec of a Pod, they are usually named. By using the names of the ports in the NetworkPolicy instead of port numbers, the real port numbers can be changed without affecting the network policy. Coherence Extend Access If Coherence Extend is being used, then first the Extend Proxy must be configured to use a fixed port. The default behaviour of Coherence is to bind the Extend proxy to an ephemeral port and clients use the Coherence NameService to look up the port to use. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Extend proxy is already configured to run on a fixed port 20000 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_EXTEND_PORT environment variable. When using the Coherence Concurrent extensions over Extend, the Concurrent Extend proxy also needs to be configured with a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the Concurrent Extend proxy is already configured to run on a fixed port 20001 . When using this image, or any image that uses the default Coherence cache configuration file, this port can be changed by setting the COHERENCE_CONCURRENT_EXTEND_PORT environment variable. For the examples below, a Coherence deployment has the following configuration. This will expose Extend on a port named extend with a port number of 20000 , and a port named extend-atomics with a port number of 20001 . The polices described below will then use the port names, so if required the port number could be changed and the policies would still work. <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: ports: - name: extend port: 20000 - name: extend-atomics port: 20001 The ingress policy below will work with the default Coherence image and allow ingress into the Coherence Pods to both the default Extend port and Coherence Concurrent Extend port. The policy allows ingress from Pods that have the coherence.oracle.com/extendClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-extend-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. Instead of using fixed port numbers in the The egress policy below will work with the default Coherence image and allow egress from Pods with the coherence.oracle.com/extendClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . on both the default Extend port and Coherence Concurrent Extend port. <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . Coherence gRPC Access If Coherence gRPC is being used, then first the gRPC Proxy must be configured to use a fixed port. When using the default Coherence images, for example ghcr.io/oracle/coherence-ce:22.06 the gRPC proxy is already configured to run on a fixed port 1408 . The gRPC proxy port can be changed by setting the COHERENCE_GRPC_PORT environment variable. The ingress policy below will allow ingress into the Coherence Pods gRPC port. The policy allows ingress from Pods that have the coherence.oracle.com/grpcClient: true label, from any namespace. It could be tightened further by using a more specific namespace selector. <markup lang=\"yaml\" title=\"manifests/allow-grpc-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-grpc-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: {} podSelector: matchLabels: coherence.oracle.com/grpcClient: \"true\" ports: - port: grpc protocol: TCP The policy above should be applied to the namespace where the Coherence cluster is running. The egress policy below will allow egress to the gRPC port from Pods with the coherence.oracle.com/grpcClient: true label to Coherence Pods with the label coherenceComponent: coherencePod . <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-extend-egress spec: podSelector: matchLabels: coherence.oracle.com/extendClient: \"true\" policyTypes: - Ingress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: extend protocol: TCP - port: extend-atomics protocol: TCP The policy above allows egress to Coherence Pods in any namespace. This would ideally be tightened up to the specific namespace that the Coherence cluster is deployed in. For example, if the Coherence cluster is deployed in the datastore namespace, then the to section of policy could be changed as follows: <markup lang=\"yaml\" title=\"manifests/allow-extend-egress.yaml\" >- to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: datastore podSelector: matchLabels: coherenceComponent: coherencePod This policy must be applied to the namespace where the client Pods will be deployed . Coherence Metrics If Coherence metrics is enabled there will need to be an ingress policy to allow connections from metrics clients. There would also need to be a similar egress policy in the metrics client&#8217;s namespace to allow it to access the Coherence metrics endpoints. A simple Coherence resource that will create a cluster with metrics enabled is shown below. This yaml will create a Coherence cluster with a port names metrics that maps to the default metrics port of '9612`. <markup lang=\"yaml\" title=\"manifests/coherence-cluster-with-metrics.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: metrics: enabled: true ports: - name: metrics serviceMonitor: enabled: true The example below will assume that metrics will be scraped by Prometheus, and that Prometheus is installed into a namespace called monitoring . An ingress policy must be created in the namespace where the Coherence cluster is deployed allowing ingress to the metrics port from the Prometheus Pods. The Pods running Prometheus have a label app.kubernetes.io/name: prometheus so this can be used in the policy&#8217;s Pod selector. This policy should be applied to the namespace where the Coherence cluster is running. <markup lang=\"yaml\" title=\"manifests/allow-metrics-ingress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-metrics-ingress spec: podSelector: matchLabels: coherenceComponent: coherencePod policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: monitoring podSelector: matchLabels: app.kubernetes.io/name: prometheus ports: - port: metrics protocol: TCP If the monitoring namespace also has a \"deny-all\" policy and needs egress opening up for Prometheus to scrape metrics then an egress policy will need to be added to the monitoring namespace. The policy below will allow Pods with the label app.kubernetes.io/name: prometheus egress to Pods with the coherenceComponent: coherencePod label in any namespace. The policy could be further tightened up by adding a namespace selector to restrict egress to the specific namespace where the Coherence cluster is running. <markup lang=\"yaml\" title=\"manifests/allow-metrics-egress.yaml\" >apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: coherence-metrics-egress spec: podSelector: matchLabels: app.kubernetes.io/name: prometheus policyTypes: - Egress egress: - to: - namespaceSelector: { } podSelector: matchLabels: coherenceComponent: coherencePod ports: - port: metrics protocol: TCP Testing Network Policies At the time of writing this documentation, Kubernetes provides no way to verify the correctness of network policies. It is easy to mess up a policy, in which case policies will either block too much traffic, in which case your application will work, or worse they will not be blocking access and leave a security hole. As we have had various requests for help from customers who cannot get Coherence to work with network policies enabled, the Operator has a simple utility to test connectivity outside of Coherence. This will allow testing pf policies without the complications of having to start a Coherence server. This example includes some simple yaml files that will create simulator Pods that listen on all the ports used by the Operator and by a Coherence cluster member. These simulator Pods are configured with the same labels that the real Operator and Coherence Pods would have and the same labels used by the network policies in this example. Also included are some yaml files that start a test client, that simulates either the Operator connecting to Coherence Pods or a Coherence Pod connecting to the Operator and to other Coherence Pods. To run these tests, the Operator does not have to be installed. Create the Test Namespaces In this example we will assume the Operator will eventually be running in a namespace called coherence and the Coherence cluster will run in a namespace called coh-test . We can create the namespaces using kubectl <markup lang=\"bash\" >kubectl create ns coherence <markup lang=\"bash\" >kubectl create ns coh-test At this point there are no network policies installed, this will allow us to confirm the connectivity tests work. Start the Operator Simulator The Operator simulator server should run in the coherence namespace. It can be created using the following command: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator-server.yaml Start the Coherence Cluster Simulator The Coherence cluster member simulator server should run in the coh-test namespace. It can be created using the following command: <markup lang=\"bash\" >kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence-server.yaml Run the Operator Test We can now run the Operator test Job. This wil run a Kubernetes Job that simulates the Operator connecting to the Kubernetes API server and to the Operator Pods. <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml The test Job should complete very quickly as it is only testing connectivity to various ports. The results of the test can be seen by looking at the Pod log. The command below will display the log: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) The output from a successful test will look like this: <markup >1.6727606592497227e+09 INFO runner Operator Version: 3.3.2 1.6727606592497835e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727606592500978e+09 INFO runner Operator Built By: jonathanknight 1.6727606592501197e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727606592501485e+09 INFO runner Go Version: go1.19.2 1.6727606592501757e+09 INFO runner Go OS/Arch: linux/amd64 1.6727606592504115e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727606592504556e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727606592664087e+09 INFO net-test Testing connectivity PASSED {\"PortName\": \"K8s API Server\", \"Version\": \"v1.24.7\"} 1.6727606592674055e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727606592770455e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} We can see that the test has connected to the Kubernetes API server and has connected to the health port on the Coherence cluster test server in the coh-test namespace. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml Run the Cluster Member Test The cluster member test simulates a Coherence cluster member connecting to other cluster members in the same namespace and also making calls to the Operator&#8217;s REST endpoint. <markup lang=\"bash\" >kubectl -n coh-test apply -f examples/095_network_policies/manifests/net-test-coherence.yaml Again, the test should complete quickly as it is just connecting to various ports. The results of the test can be seen by looking at the Pod log. The command below will display the log: <markup lang=\"bash\" >kubectl -n coh-test logs $(kubectl -n coh-test get pod -l 'coherenceNetTest=coherence-client' -o name) The output from a successful test will look like this: <markup >1.6727631152848177e+09 INFO runner Operator Version: 3.3.2 1.6727631152849226e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727631152849536e+09 INFO runner Operator Built By: jonathanknight 1.6727631152849755e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727631152849965e+09 INFO runner Go Version: go1.19.2 1.6727631152850187e+09 INFO runner Go OS/Arch: linux/amd64 1.6727631152852216e+09 INFO net-test Starting test {\"Name\": \"Cluster Member Simulator\"} 1.6727631152852666e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort1\", \"Port\": 7575} 1.6727631152997334e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort1\", \"Port\": 7575} 1.6727631152998908e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort2\", \"Port\": 7576} 1.6727631153059115e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"UnicastPort2\", \"Port\": 7576} 1.6727631153063197e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Management\", \"Port\": 30000} 1.6727631153116117e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Management\", \"Port\": 30000} 1.6727631153119817e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Metrics\", \"Port\": 9612} 1.6727631153187876e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Metrics\", \"Port\": 9612} 1.6727631153189638e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"OperatorRest\", \"Port\": 8000} 1.6727631153265746e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"OperatorRest\", \"Port\": 8000} 1.6727631153267298e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Echo\", \"Port\": 7} 1.6727631153340726e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Echo\", \"Port\": 7} 1.6727631153342876e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"ClusterPort\", \"Port\": 7574} 1.6727631153406997e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"ClusterPort\", \"Port\": 7574} The test client successfully connected to the Coherence cluster port (7475), the two unicast ports (7575 and 7576), the Coherence management port (30000), the Coherence metrics port (9612), the Operator REST port (8000), and the echo port (7). The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence.yaml Testing the Operator Web Hook The Operator has a web-hook that k8s calls to validate Coherence resource configurations and to provide default values. Web hooks in Kubernetes use TLS by default and listen on port 443. The Operator server simulator also listens on port 443 to allow this connectivity to be tested. The network policy in this example that allows ingress to the web-hook allows any client to connect. This is because it is not always simple to work out the IP address that the API server will connect to the web-hook from. We can use the network tester to simulate this by running a Job that will connect to the web hook port. The web-hook test job in this example does not label the Pod and can be run from the default namespace to simulate a random external connection. <markup lang=\"bash\" >kubectl -n default apply -f examples/095_network_policies/manifests/net-test-webhook.yaml We can then check the results of the Job by looking at the Pod log. <markup lang=\"bash\" >kubectl -n default logs $(kubectl -n default get pod -l 'coherenceNetTest=webhook-client' -o name) The output from a successful test will look like this: <markup >1.6727639834559627e+09 INFO runner Operator Version: 3.3.2 1.6727639834562948e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727639834563956e+09 INFO runner Operator Built By: jonathanknight 1.6727639834565024e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727639834566057e+09 INFO runner Go Version: go1.19.2 1.6727639834567096e+09 INFO runner Go OS/Arch: linux/amd64 1.6727639834570327e+09 INFO net-test Starting test {\"Name\": \"Web-Hook Client\"} 1.6727639834571698e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"WebHook\", \"Port\": 443} 1.6727639834791095e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-operator-server.coherence.svc\", \"PortName\": \"WebHook\", \"Port\": 443} We can see that the client successfully connected to port 443. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n default delete -f examples/095_network_policies/manifests/net-test-webhook.yaml Testing Ad-Hoc Ports The test client is able to test connectivity to any host and port. For example suppose we want to simulate a Prometheus Pod connecting to the metrics port of a Coherence cluster. The server simulator is listening on port 9612, so we need to run the client to connect to that port. We can create a Job yaml file to run the test client. As the test will simulate a Prometheus client we add the labels that a standard Prometheus Pod would have and that we also use in the network policies in this example. In the Job yaml, we need to set the HOST , PORT and optionally the PROTOCOL environment variables. In this test, the host is the DNS name for the Service created for the Coherence server simulator net-test-coherence-server.coh-test.svc , the port is the metrics port 9612 and the protocol is tcp . <markup lang=\"yaml\" title=\"manifests/net-test-client.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: test-client labels: app.kubernetes.io/name: prometheus coherenceNetTest: client spec: template: metadata: labels: app.kubernetes.io/name: prometheus coherenceNetTest: client spec: containers: - name: net-test image: ghcr.io/oracle/coherence-operator:3.3.4 env: - name: HOST value: net-test-coherence-server.coh-test.svc - name: PORT value: \"9612\" - name: PROTOCOL value: tcp command: - /files/runner args: - net-test - client restartPolicy: Never backoffLimit: 4 We need to run the test Job in the monitoring namespace, which is the same namespace that Prometheus is usually deployed into. <markup lang=\"bash\" >kubectl -n monitoring apply -f examples/095_network_policies/manifests/net-test-client.yaml We can then check the results of the Job by looking at the Pod log. <markup lang=\"bash\" >kubectl -n monitoring logs $(kubectl -n monitoring get pod -l 'coherenceNetTest=client' -o name) The output from a successful test will look like this: <markup >1.6727665901488597e+09 INFO runner Operator Version: 3.3.2 1.6727665901497366e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727665901498337e+09 INFO runner Operator Built By: jonathanknight 1.6727665901498716e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727665901498966e+09 INFO runner Go Version: go1.19.2 1.6727665901499205e+09 INFO runner Go OS/Arch: linux/amd64 1.6727665901501486e+09 INFO net-test Starting test {\"Name\": \"Simple Client\"} 1.6727665901501985e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"net-test-coherence-server.coh-test.svc-9612\", \"Port\": 9612} 1.6727665901573336e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"net-test-coherence-server.coh-test.svc-9612\", \"Port\": 9612} We can see that the test client successfully connected to the Coherence cluster member simulator on port 9612. The test Job can then be deleted: <markup lang=\"bash\" >kubectl -n monitoring delete -f examples/095_network_policies/manifests/net-test-client.yaml Test with Network Policies All the above tests ran successfully without any network policies. We can now start to apply policies and re-run the tests to see what happens. In a secure environment we would start with a policy that blocks all access and then gradually open up required ports. We can apply the deny-all.yaml policy and then re-run the tests. We should apply the policy to both of the namespaces we are using in this example: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/deny-all.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/deny-all.yaml Now, re-run the Operator test client: <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml and check the result: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) <markup >1.6727671834237397e+09 INFO runner Operator Version: 3.3.2 1.6727671834238796e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727671834239576e+09 INFO runner Operator Built By: jonathanknight 1.6727671834240365e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727671834240875e+09 INFO runner Go Version: go1.19.2 1.6727671834241736e+09 INFO runner Go OS/Arch: linux/amd64 1.6727671834244306e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727671834245417e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727672134268515e+09 INFO net-test Testing connectivity FAILED {\"PortName\": \"K8s API Server\", \"Error\": \"Get \\\"https://10.96.0.1:443/version?timeout=32s\\\": dial tcp 10.96.0.1:443: i/o timeout\"} 1.6727672134269848e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727672234281697e+09 INFO net-test Testing connectivity FAILED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676, \"Error\": \"dial tcp: lookup net-test-coherence-server.coh-test.svc: i/o timeout\"} We can see that the test client failed to connect to the Kubernetes API server and failed to connect to the Coherence cluster health port. This means the deny-all policy is working. We can now apply the various polices to fix the test <markup lang=\"bash\" >kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-dns.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-k8s-api-server.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-cluster-member-egress.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-operator-rest-ingress.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/allow-webhook-ingress-from-all.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-dns.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-access.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-cluster-member-operator-access.yaml kubectl -n coh-test apply -f examples/095_network_policies/manifests/allow-metrics-ingress.yaml Now, delete and re-run the Operator test client: <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator.yaml kubectl -n coherence apply -f examples/095_network_policies/manifests/net-test-operator.yaml and check the result: <markup lang=\"bash\" >kubectl -n coherence logs $(kubectl -n coherence get pod -l 'coherenceNetTest=operator-client' -o name) Now with the policies applied the test should have passed. <markup >1.6727691273634596e+09 INFO runner Operator Version: 3.3.2 1.6727691273635025e+09 INFO runner Operator Build Date: 2023-01-03T12:25:58Z 1.6727691273635256e+09 INFO runner Operator Built By: jonathanknight 1.6727691273635616e+09 INFO runner Operator Git Commit: c8118585b8f3d72b083ab1209211bcea364c85c5 1.6727691273637156e+09 INFO runner Go Version: go1.19.2 1.6727691273637407e+09 INFO runner Go OS/Arch: linux/amd64 1.6727691273639407e+09 INFO net-test Starting test {\"Name\": \"Operator Simulator\"} 1.6727691273639877e+09 INFO net-test Testing connectivity {\"PortName\": \"K8s API Server\"} 1.6727691273857167e+09 INFO net-test Testing connectivity PASSED {\"PortName\": \"K8s API Server\", \"Version\": \"v1.24.7\"} 1.6727691273858056e+09 INFO net-test Testing connectivity {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} 1.6727691273933685e+09 INFO net-test Testing connectivity PASSED {\"Host\": \"net-test-coherence-server.coh-test.svc\", \"PortName\": \"Health\", \"Port\": 6676} The other tests can also be re-run and should also pass. Clean-Up Once the tests are completed, the test servers and Jobs can be deleted. <markup lang=\"bash\" >kubectl -n coherence delete -f examples/095_network_policies/manifests/net-test-operator-server.yaml kubectl -n coh-test delete -f examples/095_network_policies/manifests/net-test-coherence-server.yaml ",
            "title": "Using Network Policies"
        },
        {
            "location": "/docs/jvm/010_overview",
            "text": " Classpath Default classpath settings and options for setting a custom classpath. JVM Arguments Adding arbitrary JVM arguments and system properties. Garbage Collection Configuring the garbage collector. Heap & Memory Settings Configuring the heap size and other memory settings. Debugger Using debugger settings. Use Container Limits Configuring the JVM to respect container resource limits. ",
            "title": "Guides to JVM Settings"
        },
        {
            "location": "/docs/jvm/010_overview",
            "text": " The Coherence Operator allows full control over the configuration of the JVM used to run the Coherence application. The jvm section of the Coherence CRD spec has a number of fields to easily configure specific aspects of the JVM as well as a catch-all jvm.args list that allows any arbitrary argument to be passed to the JVM. Whilst every configuration setting could, in theory, be set only by specifying JVM arguments in the jvm.args field of the Coherence CRD, the other configuration fields provide simpler means to set configuration without having to remember specific JVM argument names or system property names to set. You are, of course, free to use whichever approach best suits your requirements; but obviously it is better to choose one approach and be consistent. Guides to JVM Settings Classpath Default classpath settings and options for setting a custom classpath. JVM Arguments Adding arbitrary JVM arguments and system properties. Garbage Collection Configuring the garbage collector. Heap & Memory Settings Configuring the heap size and other memory settings. Debugger Using debugger settings. Use Container Limits Configuring the JVM to respect container resource limits. ",
            "title": "Overview"
        },
        {
            "location": "/docs/other/030_labels",
            "text": " Additional labels can be added to the Pods managed by the Operator. Additional labels should be added to the labels map in the Coherence CRD spec. The entries in the labels map should confirm to the recommendations and rules in the Kubernetes Labels documentation. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: labels: tier: backend environment: dev Two labels will be added to the Pods , tier=backend and environment=dev ",
            "title": "Pod Labels"
        },
        {
            "location": "/docs/installation/05_private_repos",
            "text": " Sometimes the images used by a Coherence cluster need to be pulled from a private image registry that requires credentials. The Coherence Operator supports supplying credentials in the Coherence CRD configuration. The Kubernetes documentation on using a private registries gives a number of options for supplying credentials. ",
            "title": "Using Private Image Registries"
        },
        {
            "location": "/docs/installation/05_private_repos",
            "text": " Kubernetes supports configuring pods to use imagePullSecrets for pulling images. If possible, this is the preferable, and most portable route. See the kubernetes docs for this. Once secrets have been created in the namespace that the Coherence resource is to be installed in then the secret name can be specified in the Coherence spec . It is possible to specify multiple secrets in the case where the different images being used will be pulled from different registries. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: imagePullSecrets: - name: coherence-secret The coherence-secret will be used for pulling images from the registry associated to the secret The imagePullSecrets field is a list of values in the same format that they would be specified in Kubernetes Pod specs, so multiple secrets can be specified for different authenticated registries in the case where the Coherence cluster will use images from different authenticated registries.. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: imagePullSecrets: - name: coherence-secret - name: ocr-secret The imagePullSecrets list specifies two secrets to use coherence-secret and ocr-secret ",
            "title": "Use ImagePullSecrets"
        },
        {
            "location": "/examples/000_overview",
            "text": " There are a number of examples which show you how to build and deploy applications for the Coherence Operator. Tip The complete source code for the examples is in the Coherence Operator GitHub repository. Simple Coherence Image using JIB Building a simple Coherence server image with JIB using Maven or Gradle. Simple Coherence Image using a Dockerfile Building a simple Coherence image with a Dockerfile, that works out of the box with the Operator. Hello World Deploying the most basic Coherence cluster using the Operator. Coherence*Extend Clients An example demonstrating various ways to configure and use Coherence*Extend with Kubernetes. Deployment This example shows how to deploy Coherence applications using the Coherence Operator. TLS Securing Coherence clusters using TLS. Network Policies An example covering the use of Kubernetes NetworkPolicy rules with the Operator and Coherence clusters. Federation This is a simple Coherence federation example. The federation feature requires Coherence Grid Edition. Autoscaling Scaling Coherence clusters using the horizontal Pod Autoscaler. Helm Manage Coherence resources using Helm. Istio Istio Support Coherence Demo App Deploying the Coherence demo application. ",
            "title": "Examples Overview"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " When installing the Coherence Operator into Kubernetes clusters with RBAC enabled, the Operator will require certain roles to work properly. Both the Operator Helm chart, and the Operator k8s manifest files will install all the required roles, role bindings and create a service account. ",
            "title": "RBAC Roles"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " By default, both install methods will create ClusterRole resources and ClusterRoleBinding resources to bind those roles to the Operator ServiceAccount. Some Kubernetes administrators are wary of letting arbitrary installations have ClusterRole permissions and try to discourage it. The Coherence Operator can run without ClusterRole permissions, but it is important to understand what this means from an operational point of view. Cluster roles are used for a number of operator features: Installing the CRDs - the Operator automatically ensures that the CRDs it requires are installed when it starts. Installing the Web-Hook - the Operator automatically installs the defaulting and validating web-hooks for the Coherence resource when it starts. Without the validating web-hook a lot more care must be taken to ensure that only valid Coherence resource yaml is added to k8s. In the worst case, invalid yaml may ultimately cause the Operator to panic where invalid yaml would normally have been disallowed by the web-hook. Coherence CLuster site and rack information - the Operator is used to supply site and rack values for the Coherence clusters that it manages. These values come from Node labels that the Operator must be able to look up. Without this information a Coherence cluster will have empty values for the coherence.site and coherence.rack system properties, meaning that Coherence will be unable to make data site-safe in k8s clusters that have multiple availability zones. Monitoring multiple namespaces - if the Operator is to monitor multiple namespaces it must have cluster wide roles to do this Assuming that all the above reductions in features are acceptable then the Operator can be installed without creating cluster roles. ",
            "title": "Cluster Roles"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " Important Before installing the Operator, with either method described below, the CRDs MUST be manually installed from the Operator manifest files. The manifest files are published with the GitHub release at this link: 3.3.4 Manifests You MUST ensure that the CRD manifests match the version of the Operator being installed. Download the manifests and unpack them. In the directory that the .tar.gz file the was unpacked the crd/ directory will the Coherence CRD. The CRD can be installed with kubectl <markup lang=\"bash\" >kubectl create -f crd/coherence.oracle.com_coherence.yaml To update an existing CRD install use the replace command: <markup lang=\"bash\" >kubectl replace -f crd/coherence.oracle.com_coherence.yaml Installing the CRD Using kubectl apply The default Coherence CRD cannot be installed using kubectl apply as it is larger than the 1MB limit imposed by Etcd. For customers who cannot use the kubectl create/replace combination, a smaller version of the CRD is available. This small CRD has no description fields which makes is smaller to install, but less useful for validating the yaml in an IDE. The small CRD can be found in the coherence-operator-manifests.tar.gz file in the crd-small/ directory. <markup lang=\"bash\" >kubectl apply -f crd-small/coherence.oracle.com_coherence.yaml ",
            "title": "Manually Install CRDs"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " The Helm chart allows the Operator to be installed with a single ClusterRole allowing it to read k8s Node information. This is used to provide site, and rack labels, for Coherence cluster members. In environments where Kubernetes administrators are happy to allow the Operator read-only access to Node information the nodeRoles value can be set to true . <markup lang=\"bash\" >helm install \\ --set clusterRoles=false --set nodeRoles=true --namespace &lt;namespace&gt; \\ coherence \\ coherence/coherence-operator The clusterRoles value is set to false . The nodeRoles value is set to true , so a single ClusterRole will be applied to the Operator&#8217;s service account ",
            "title": "Allow Node Lookup"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " The Operator can be installed from the Helm chart, as described in the Install Guide . The Helm chart contains values that control whether cluster roles are created when installing the chart. To install the chart without any cluster roles set the clusterRoles value to false . <markup lang=\"bash\" >helm install \\ --set clusterRoles=false --namespace &lt;namespace&gt; \\ coherence \\ coherence/coherence-operator The clusterRoles value is set to false. The &lt;namespace&gt; value is the namespace that the Coherence Operator will be installed into and without cluster roles will be the only namespace that the Operator monitors. Allow Node Lookup The Helm chart allows the Operator to be installed with a single ClusterRole allowing it to read k8s Node information. This is used to provide site, and rack labels, for Coherence cluster members. In environments where Kubernetes administrators are happy to allow the Operator read-only access to Node information the nodeRoles value can be set to true . <markup lang=\"bash\" >helm install \\ --set clusterRoles=false --set nodeRoles=true --namespace &lt;namespace&gt; \\ coherence \\ coherence/coherence-operator The clusterRoles value is set to false . The nodeRoles value is set to true , so a single ClusterRole will be applied to the Operator&#8217;s service account ",
            "title": "Install Using Helm"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " To install without cluster roles, after unpacking the manifests .tar.gz edit the config/kustomization.yaml file to comment out the inclusion of the cluster role bindings. For example: <markup lang=\"yaml\" title=\"kustomization.yaml\" >resources: - service_account.yaml - role.yaml - role_binding.yaml #- node_viewer_role.yaml #- node_viewer_role_binding.yaml #- cluster_role.yaml #- cluster_role_binding.yaml ",
            "title": "Exclude the ClusterRole Manifests"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " The Operator would normally install validating and defaulting web-hooks as well as ensuring that the Coherence CRDs are installed. Without cluster roles this must be disabled by editing the manager/manager.yaml file in the manifests. Edit the Operator container args section of the deployment yaml to add command line arguments to --enable-webhook=false to disable web-hook creation and --install-crd=false to disable CRD installation. For example, change the section of the manager/manager.yaml file that looks like this: <markup lang=\"yaml\" title=\"manager/manager.yaml\" > command: - /files/runner args: - --enable-leader-election envFrom: to be: <markup lang=\"yaml\" title=\"manager/manager.yaml\" > command: - /files/runner args: - --enable-leader-election - --enable-webhook=false - --install-crd=false envFrom: ",
            "title": "Disable Web-Hooks and CRD Installation"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " The Operator will require a role and role binding to work in a single namespace. Edit the config/role.yaml to change its type from ClusterRole to Role . For example, change: <markup lang=\"yaml\" title=\"role.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: creationTimestamp: null name: manager-role to be: <markup lang=\"yaml\" title=\"role.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: null name: manager-role ClusterRole has been changed to Role Edit the config/role_binding.yaml to change its type from ClusterRoleBinding to RoleBinding . For example change: <markup lang=\"yaml\" title=\"role_binding.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: manager-rolebinding labels: control-plane: coherence roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: manager-role subjects: - kind: ServiceAccount name: coherence-operator namespace: default to be: <markup lang=\"yaml\" title=\"role_binding.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: manager-rolebinding labels: control-plane: coherence roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: manager-role subjects: - kind: ServiceAccount name: coherence-operator namespace: default The type has been changed from ClusterRoleBinding to RoleBinding The role being bound has been changed from ClusterRole to Role . ",
            "title": "Edit the Operator ClusterRole &amp; ClusterRoleBinding"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " In environments where Kubernetes administrators are happy to allow the Operator read-only access to Node information, the required ClusterRole can be created by leaving the relevant lines uncommented in the config/kustomization.yaml file. For example: <markup lang=\"yaml\" title=\"kustomization.yaml\" >resources: - service_account.yaml - role.yaml - role_binding.yaml - node_viewer_role.yaml - node_viewer_role_binding.yaml #- cluster_role.yaml #- cluster_role_binding.yaml The node_viewer_role.yaml and node_viewer_role_binding.yaml will now be left in the installation. ",
            "title": "Allow Node Lookup"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " The Operator can be installed using Kustomize with the manifest files, as described in the Install Guide . Exclude the ClusterRole Manifests To install without cluster roles, after unpacking the manifests .tar.gz edit the config/kustomization.yaml file to comment out the inclusion of the cluster role bindings. For example: <markup lang=\"yaml\" title=\"kustomization.yaml\" >resources: - service_account.yaml - role.yaml - role_binding.yaml #- node_viewer_role.yaml #- node_viewer_role_binding.yaml #- cluster_role.yaml #- cluster_role_binding.yaml Disable Web-Hooks and CRD Installation The Operator would normally install validating and defaulting web-hooks as well as ensuring that the Coherence CRDs are installed. Without cluster roles this must be disabled by editing the manager/manager.yaml file in the manifests. Edit the Operator container args section of the deployment yaml to add command line arguments to --enable-webhook=false to disable web-hook creation and --install-crd=false to disable CRD installation. For example, change the section of the manager/manager.yaml file that looks like this: <markup lang=\"yaml\" title=\"manager/manager.yaml\" > command: - /files/runner args: - --enable-leader-election envFrom: to be: <markup lang=\"yaml\" title=\"manager/manager.yaml\" > command: - /files/runner args: - --enable-leader-election - --enable-webhook=false - --install-crd=false envFrom: Edit the Operator ClusterRole &amp; ClusterRoleBinding The Operator will require a role and role binding to work in a single namespace. Edit the config/role.yaml to change its type from ClusterRole to Role . For example, change: <markup lang=\"yaml\" title=\"role.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: creationTimestamp: null name: manager-role to be: <markup lang=\"yaml\" title=\"role.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: null name: manager-role ClusterRole has been changed to Role Edit the config/role_binding.yaml to change its type from ClusterRoleBinding to RoleBinding . For example change: <markup lang=\"yaml\" title=\"role_binding.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: manager-rolebinding labels: control-plane: coherence roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: manager-role subjects: - kind: ServiceAccount name: coherence-operator namespace: default to be: <markup lang=\"yaml\" title=\"role_binding.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: manager-rolebinding labels: control-plane: coherence roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: manager-role subjects: - kind: ServiceAccount name: coherence-operator namespace: default The type has been changed from ClusterRoleBinding to RoleBinding The role being bound has been changed from ClusterRole to Role . Allow Node Lookup In environments where Kubernetes administrators are happy to allow the Operator read-only access to Node information, the required ClusterRole can be created by leaving the relevant lines uncommented in the config/kustomization.yaml file. For example: <markup lang=\"yaml\" title=\"kustomization.yaml\" >resources: - service_account.yaml - role.yaml - role_binding.yaml - node_viewer_role.yaml - node_viewer_role_binding.yaml #- cluster_role.yaml #- cluster_role_binding.yaml The node_viewer_role.yaml and node_viewer_role_binding.yaml will now be left in the installation. ",
            "title": "Install Using Kustomize"
        },
        {
            "location": "/docs/installation/09_RBAC",
            "text": " The two methods of installing the Operator discussed in the Install Guide can be used to install the Operator without ClusterRoles. Manually Install CRDs Important Before installing the Operator, with either method described below, the CRDs MUST be manually installed from the Operator manifest files. The manifest files are published with the GitHub release at this link: 3.3.4 Manifests You MUST ensure that the CRD manifests match the version of the Operator being installed. Download the manifests and unpack them. In the directory that the .tar.gz file the was unpacked the crd/ directory will the Coherence CRD. The CRD can be installed with kubectl <markup lang=\"bash\" >kubectl create -f crd/coherence.oracle.com_coherence.yaml To update an existing CRD install use the replace command: <markup lang=\"bash\" >kubectl replace -f crd/coherence.oracle.com_coherence.yaml Installing the CRD Using kubectl apply The default Coherence CRD cannot be installed using kubectl apply as it is larger than the 1MB limit imposed by Etcd. For customers who cannot use the kubectl create/replace combination, a smaller version of the CRD is available. This small CRD has no description fields which makes is smaller to install, but less useful for validating the yaml in an IDE. The small CRD can be found in the coherence-operator-manifests.tar.gz file in the crd-small/ directory. <markup lang=\"bash\" >kubectl apply -f crd-small/coherence.oracle.com_coherence.yaml Install Using Helm The Operator can be installed from the Helm chart, as described in the Install Guide . The Helm chart contains values that control whether cluster roles are created when installing the chart. To install the chart without any cluster roles set the clusterRoles value to false . <markup lang=\"bash\" >helm install \\ --set clusterRoles=false --namespace &lt;namespace&gt; \\ coherence \\ coherence/coherence-operator The clusterRoles value is set to false. The &lt;namespace&gt; value is the namespace that the Coherence Operator will be installed into and without cluster roles will be the only namespace that the Operator monitors. Allow Node Lookup The Helm chart allows the Operator to be installed with a single ClusterRole allowing it to read k8s Node information. This is used to provide site, and rack labels, for Coherence cluster members. In environments where Kubernetes administrators are happy to allow the Operator read-only access to Node information the nodeRoles value can be set to true . <markup lang=\"bash\" >helm install \\ --set clusterRoles=false --set nodeRoles=true --namespace &lt;namespace&gt; \\ coherence \\ coherence/coherence-operator The clusterRoles value is set to false . The nodeRoles value is set to true , so a single ClusterRole will be applied to the Operator&#8217;s service account Install Using Kustomize The Operator can be installed using Kustomize with the manifest files, as described in the Install Guide . Exclude the ClusterRole Manifests To install without cluster roles, after unpacking the manifests .tar.gz edit the config/kustomization.yaml file to comment out the inclusion of the cluster role bindings. For example: <markup lang=\"yaml\" title=\"kustomization.yaml\" >resources: - service_account.yaml - role.yaml - role_binding.yaml #- node_viewer_role.yaml #- node_viewer_role_binding.yaml #- cluster_role.yaml #- cluster_role_binding.yaml Disable Web-Hooks and CRD Installation The Operator would normally install validating and defaulting web-hooks as well as ensuring that the Coherence CRDs are installed. Without cluster roles this must be disabled by editing the manager/manager.yaml file in the manifests. Edit the Operator container args section of the deployment yaml to add command line arguments to --enable-webhook=false to disable web-hook creation and --install-crd=false to disable CRD installation. For example, change the section of the manager/manager.yaml file that looks like this: <markup lang=\"yaml\" title=\"manager/manager.yaml\" > command: - /files/runner args: - --enable-leader-election envFrom: to be: <markup lang=\"yaml\" title=\"manager/manager.yaml\" > command: - /files/runner args: - --enable-leader-election - --enable-webhook=false - --install-crd=false envFrom: Edit the Operator ClusterRole &amp; ClusterRoleBinding The Operator will require a role and role binding to work in a single namespace. Edit the config/role.yaml to change its type from ClusterRole to Role . For example, change: <markup lang=\"yaml\" title=\"role.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: creationTimestamp: null name: manager-role to be: <markup lang=\"yaml\" title=\"role.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: null name: manager-role ClusterRole has been changed to Role Edit the config/role_binding.yaml to change its type from ClusterRoleBinding to RoleBinding . For example change: <markup lang=\"yaml\" title=\"role_binding.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: manager-rolebinding labels: control-plane: coherence roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: manager-role subjects: - kind: ServiceAccount name: coherence-operator namespace: default to be: <markup lang=\"yaml\" title=\"role_binding.yaml\" >apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: manager-rolebinding labels: control-plane: coherence roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: manager-role subjects: - kind: ServiceAccount name: coherence-operator namespace: default The type has been changed from ClusterRoleBinding to RoleBinding The role being bound has been changed from ClusterRole to Role . Allow Node Lookup In environments where Kubernetes administrators are happy to allow the Operator read-only access to Node information, the required ClusterRole can be created by leaving the relevant lines uncommented in the config/kustomization.yaml file. For example: <markup lang=\"yaml\" title=\"kustomization.yaml\" >resources: - service_account.yaml - role.yaml - role_binding.yaml - node_viewer_role.yaml - node_viewer_role_binding.yaml #- cluster_role.yaml #- cluster_role_binding.yaml The node_viewer_role.yaml and node_viewer_role_binding.yaml will now be left in the installation. ",
            "title": "Install the Operator Without ClusterRoles"
        },
        {
            "location": "/docs/other/040_annotations",
            "text": " Annotations can be added to the Coherence cluster&#8217;s StatefulSet and the Pods . See the official Kubernetes Annotations documentation for more details on applying annotations to resources. ",
            "title": "preambule"
        },
        {
            "location": "/docs/other/040_annotations",
            "text": " The default behaviour of the Operator is to copy any annotations added to the Coherence resource to the StatefulSet . For example: <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage annotations: key1: value1 key2: value2 This will result in a StatefulSet with the following annotations: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage annotations: key1: value1 key2: value2 Alternatively, if the StatefulSet should have different annotations to the Coherence resource, the annotations for the StatefulSet can be specified in the spec.statefulSetAnnotations field of the Coherence resource. For example: <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage annotations: key1: value1 key2: value2 spec: replicas: 3 statefulSetAnnotations: key3: value3 key4: value4 This will result in a StatefulSet with the following annotations: <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage annotations: key3: value3 key4: value4 ",
            "title": "StatefulSet Annotations"
        },
        {
            "location": "/docs/other/040_annotations",
            "text": " Additional annotations can be added to the Pods managed by the Operator. Annotations should be added to the annotations map in the Coherence CRD spec. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: annotations: key1: value1 key2: value2 The annotations will be added the Pods : <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 annotations: key1: value1 key2: value2 ",
            "title": "Pod Annotations"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " Install the Coherence Operator Create the example namespace Create image pull and config store secrets Run the Example Cleaning Up ",
            "title": "What the Example will Cover"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " To run the examples below, you will need to have installed the Coherence Operator, do this using whatever method you prefer from the Installation Guide . Once you complete, confirm the operator is running, for example: <markup lang=\"bash\" >kubectl get pods -n coherence NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 27s ",
            "title": "Install the Coherence Operator"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " This simple example demonstrates the Coherence federation feature. It shows how to deploy two Coherence clusters that federating data between them using the Coherence Operator. The Coherence federation feature requires Coherence Grid Edition. See Obtain Coherence Images on how to get a commercial Coherence image. Tip The complete source code for this example is in the Coherence Operator GitHub repository. What the Example will Cover Install the Coherence Operator Create the example namespace Create image pull and config store secrets Run the Example Cleaning Up Install the Coherence Operator To run the examples below, you will need to have installed the Coherence Operator, do this using whatever method you prefer from the Installation Guide . Once you complete, confirm the operator is running, for example: <markup lang=\"bash\" >kubectl get pods -n coherence NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 27s ",
            "title": "Coherence Federation"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " This example reqires two secrets: An image pull secret named ocr-pull-secret containing your OCR credentials to be used by the example. Use a command similar to the following to create the image pull secret: <markup lang=\"bash\" >kubectl create secret docker-registry ocr-pull-secret \\ --docker-server=container-registry.oracle.com \\ --docker-username=\"&lt;username&gt;\" --docker-password=\"&lt;password&gt;\" \\ --docker-email=\"&lt;email&gt;\" -n coherence-example A configure store secret named storage-config to store the Coherence configuration files. Run the following command to create the configure store secret: <markup lang=\"bash\" >kubectl create secret generic storage-config -n coherence-example \\ --from-file=src/main/resources/tangosol-coherence-override.xml \\ --from-file=src/main/resources/storage-cache-config.xml ",
            "title": "Create image pull and configure store secrets"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " Run the following commands to create the primary and secondary clusters: <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/primary-cluster.yaml coherence.coherence.oracle.com/primary-cluster created <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/secondary-cluster.yaml coherence.coherence.oracle.com/secondary-cluster created ",
            "title": "1. Install the Coherence clusters"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " Run the following command to list the clusters: <markup lang=\"bash\" >kubectl -n coherence-example get coherence NAME CLUSTER ROLE REPLICAS READY PHASE primary-cluster primary-cluster primary-cluster 2 2 Ready secondary-cluster secondary-cluster secondary-cluster 2 2 Ready To see the Coherence cache configuration file loaded from the secret volumn we defined, run the following command: <markup lang=\"bash\" >kubectl logs -n coherence-example primary-cluster-0 | grep \"Loaded cache\" ... Oracle Coherence GE 14.1.1.0.0 &lt;Info&gt; (thread=main, member=n/a): Loaded cache configuration from \"file:/config/storage-cache-config.xml\" ",
            "title": "2. List the created Coherence clusters"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " Run the following command to view the Pods: <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE primary-cluster-0 1/1 Running 0 83s primary-cluster-1 1/1 Running 0 83s secondary-cluster-0 1/1 Running 0 74s secondary-cluster-1 1/1 Running 0 73s ",
            "title": "3. View the running pods"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " We will connect via Coherence console to add some data using the following commands: <markup lang=\"bash\" >kubectl exec -it -n coherence-example primary-cluster-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to add an entry with \"primarykey\" and \"primaryvalue\": <markup lang=\"bash\" >put \"primarykey\" \"primaryvalue\" Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. It should be 10001. Type bye to exit the console. ",
            "title": "4. Connect to the Coherence Console inside the primary cluster to add data"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " We will connect via Coherence console to confirm that the data we added to the primary cluster is federated to the secondary cluster. <markup lang=\"bash\" >kubectl exec -it -n coherence-example secondary-cluster-0 /coherence-operator/utils/runner console At the prompt type the following to set the cache to test : <markup lang=\"bash\" >cache test Use the following to get entry with \"primarykey\": <markup lang=\"bash\" >get \"primarykey\" primaryvalue Issue the command size to verify the cache entry count. It should be 10001. Our federation has Active/Active topology. So, the data changes in both primary and secondary clusters are federated between the clusters. Use the following to add an entry with \"secondarykey\" and \"secondaryvalue\": <markup lang=\"bash\" >put \"secondarykey\" \"secondaryvalue\" ",
            "title": "6. Connect to the Coherence Console inside the secondary cluster to verify that data is federated from primary cluster"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " Follow the command in the previous section to connect to the Coherence Console inside the primary cluster. Use the following command to confirm that entry with \"secondarykey\" is federated to primary cluster: <markup lang=\"bash\" >get \"secondarykey\" secondaryvalue ",
            "title": "7. Confirm the primary cluster also received \"secondarykey\", \"secondaryvalue\" entry"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " Ensure you are in the examples/federation directory to run the example. This example uses the yaml files src/main/yaml/primary-cluster.yaml and src/main/yaml/secondary-cluster.yaml , which define a primary cluster and a secondary cluster. 1. Install the Coherence clusters Run the following commands to create the primary and secondary clusters: <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/primary-cluster.yaml coherence.coherence.oracle.com/primary-cluster created <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/secondary-cluster.yaml coherence.coherence.oracle.com/secondary-cluster created 2. List the created Coherence clusters Run the following command to list the clusters: <markup lang=\"bash\" >kubectl -n coherence-example get coherence NAME CLUSTER ROLE REPLICAS READY PHASE primary-cluster primary-cluster primary-cluster 2 2 Ready secondary-cluster secondary-cluster secondary-cluster 2 2 Ready To see the Coherence cache configuration file loaded from the secret volumn we defined, run the following command: <markup lang=\"bash\" >kubectl logs -n coherence-example primary-cluster-0 | grep \"Loaded cache\" ... Oracle Coherence GE 14.1.1.0.0 &lt;Info&gt; (thread=main, member=n/a): Loaded cache configuration from \"file:/config/storage-cache-config.xml\" 3. View the running pods Run the following command to view the Pods: <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE primary-cluster-0 1/1 Running 0 83s primary-cluster-1 1/1 Running 0 83s secondary-cluster-0 1/1 Running 0 74s secondary-cluster-1 1/1 Running 0 73s 4. Connect to the Coherence Console inside the primary cluster to add data We will connect via Coherence console to add some data using the following commands: <markup lang=\"bash\" >kubectl exec -it -n coherence-example primary-cluster-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to add an entry with \"primarykey\" and \"primaryvalue\": <markup lang=\"bash\" >put \"primarykey\" \"primaryvalue\" Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. It should be 10001. Type bye to exit the console. 6. Connect to the Coherence Console inside the secondary cluster to verify that data is federated from primary cluster We will connect via Coherence console to confirm that the data we added to the primary cluster is federated to the secondary cluster. <markup lang=\"bash\" >kubectl exec -it -n coherence-example secondary-cluster-0 /coherence-operator/utils/runner console At the prompt type the following to set the cache to test : <markup lang=\"bash\" >cache test Use the following to get entry with \"primarykey\": <markup lang=\"bash\" >get \"primarykey\" primaryvalue Issue the command size to verify the cache entry count. It should be 10001. Our federation has Active/Active topology. So, the data changes in both primary and secondary clusters are federated between the clusters. Use the following to add an entry with \"secondarykey\" and \"secondaryvalue\": <markup lang=\"bash\" >put \"secondarykey\" \"secondaryvalue\" 7. Confirm the primary cluster also received \"secondarykey\", \"secondaryvalue\" entry Follow the command in the previous section to connect to the Coherence Console inside the primary cluster. Use the following command to confirm that entry with \"secondarykey\" is federated to primary cluster: <markup lang=\"bash\" >get \"secondarykey\" secondaryvalue ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " Use the following commands to delete the primary and secondary clusters: <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/primary-cluster.yaml kubectl -n coherence-example delete -f src/main/yaml/secondary-cluster.yaml Uninstall the Coherence operator using the undeploy commands for whichever method you chose to install it. ",
            "title": "Cleaning up"
        },
        {
            "location": "/examples/100_federation/README",
            "text": " First, run the following command to create the namespace, coherence-example, for the example: <markup lang=\"bash\" >kubectl create namespace coherence-example namespace/coherence-example created Create image pull and configure store secrets This example reqires two secrets: An image pull secret named ocr-pull-secret containing your OCR credentials to be used by the example. Use a command similar to the following to create the image pull secret: <markup lang=\"bash\" >kubectl create secret docker-registry ocr-pull-secret \\ --docker-server=container-registry.oracle.com \\ --docker-username=\"&lt;username&gt;\" --docker-password=\"&lt;password&gt;\" \\ --docker-email=\"&lt;email&gt;\" -n coherence-example A configure store secret named storage-config to store the Coherence configuration files. Run the following command to create the configure store secret: <markup lang=\"bash\" >kubectl create secret generic storage-config -n coherence-example \\ --from-file=src/main/resources/tangosol-coherence-override.xml \\ --from-file=src/main/resources/storage-cache-config.xml Run the Example Ensure you are in the examples/federation directory to run the example. This example uses the yaml files src/main/yaml/primary-cluster.yaml and src/main/yaml/secondary-cluster.yaml , which define a primary cluster and a secondary cluster. 1. Install the Coherence clusters Run the following commands to create the primary and secondary clusters: <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/primary-cluster.yaml coherence.coherence.oracle.com/primary-cluster created <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/secondary-cluster.yaml coherence.coherence.oracle.com/secondary-cluster created 2. List the created Coherence clusters Run the following command to list the clusters: <markup lang=\"bash\" >kubectl -n coherence-example get coherence NAME CLUSTER ROLE REPLICAS READY PHASE primary-cluster primary-cluster primary-cluster 2 2 Ready secondary-cluster secondary-cluster secondary-cluster 2 2 Ready To see the Coherence cache configuration file loaded from the secret volumn we defined, run the following command: <markup lang=\"bash\" >kubectl logs -n coherence-example primary-cluster-0 | grep \"Loaded cache\" ... Oracle Coherence GE 14.1.1.0.0 &lt;Info&gt; (thread=main, member=n/a): Loaded cache configuration from \"file:/config/storage-cache-config.xml\" 3. View the running pods Run the following command to view the Pods: <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE primary-cluster-0 1/1 Running 0 83s primary-cluster-1 1/1 Running 0 83s secondary-cluster-0 1/1 Running 0 74s secondary-cluster-1 1/1 Running 0 73s 4. Connect to the Coherence Console inside the primary cluster to add data We will connect via Coherence console to add some data using the following commands: <markup lang=\"bash\" >kubectl exec -it -n coherence-example primary-cluster-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to add an entry with \"primarykey\" and \"primaryvalue\": <markup lang=\"bash\" >put \"primarykey\" \"primaryvalue\" Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. It should be 10001. Type bye to exit the console. 6. Connect to the Coherence Console inside the secondary cluster to verify that data is federated from primary cluster We will connect via Coherence console to confirm that the data we added to the primary cluster is federated to the secondary cluster. <markup lang=\"bash\" >kubectl exec -it -n coherence-example secondary-cluster-0 /coherence-operator/utils/runner console At the prompt type the following to set the cache to test : <markup lang=\"bash\" >cache test Use the following to get entry with \"primarykey\": <markup lang=\"bash\" >get \"primarykey\" primaryvalue Issue the command size to verify the cache entry count. It should be 10001. Our federation has Active/Active topology. So, the data changes in both primary and secondary clusters are federated between the clusters. Use the following to add an entry with \"secondarykey\" and \"secondaryvalue\": <markup lang=\"bash\" >put \"secondarykey\" \"secondaryvalue\" 7. Confirm the primary cluster also received \"secondarykey\", \"secondaryvalue\" entry Follow the command in the previous section to connect to the Coherence Console inside the primary cluster. Use the following command to confirm that entry with \"secondarykey\" is federated to primary cluster: <markup lang=\"bash\" >get \"secondarykey\" secondaryvalue Cleaning up Use the following commands to delete the primary and secondary clusters: <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/primary-cluster.yaml kubectl -n coherence-example delete -f src/main/yaml/secondary-cluster.yaml Uninstall the Coherence operator using the undeploy commands for whichever method you chose to install it. ",
            "title": "Create the example namespace"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " This example showcases how to deploy Coherence applications using the Coherence Operator. This example shows how to use the Kubernetes Horizontal Pod Autoscaler to scale Coherence clusters. Tip The complete source code for this example is in the Coherence Operator GitHub repository. The following scenarios are covered: Prerequisites Create the example namespace Clone the GitHub repository Install the Coherence Operator Run the Examples Example 1 - Coherence cluster only Example 2 - Adding a Proxy tier Example 3 - Adding a User application tier Example 4 - Enabling Persistence View Cluster Metrics using Prometheus and Grafana Cleaning Up After the initial installation of the Coherence cluster, the following examples build on the previous ones by issuing a kubectl apply to modify the installation adding additional tiers. You can use kubectl create for any of the examples to install that one directly. ",
            "title": "Coherence Operator Deployment Example"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Ensure you have the following software installed: Java 11+ JDK either [OpenJDK]( https://adoptopenjdk.net/ ) or [Oracle JDK]( https://www.oracle.com/java/technologies/javase-downloads.html ) Docker version 17.03+. Access to a Kubernetes v1.14.0+ cluster. kubectl version matching your Kubernetes cluster. This example requires Java 11+ because it creates a Helidon web application and Helidon requires Java 11+. Coherence and running Coherence in Kubernetes only requires Java 8+. ",
            "title": "Prerequisites"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " You need to create the namespace for the first time to run any of the examples. Create your target namespace: <markup lang=\"bash\" >kubectl create namespace coherence-example namespace/coherence-example created Note In the examples, a Kubernetes namespace called coherence-example is used. If you want to change this namespace, ensure that you change any references to this namespace to match your selected namespace when running the examples. ",
            "title": "Create the example namespace"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " These examples exist in the examples/021_deployment directory in the Coherence Operator GitHub repository . Clone the repository: <markup lang=\"bash\" >git clone https://github.com/oracle/coherence-operator cd coherence-operator/examples/021_deployment Ensure you have Docker running and JDK 11+ build environment set and use the following command from the deployment example directory to build the project and associated Docker image: <markup lang=\"bash\" >./mvnw package jib:dockerBuild Note If you are running behind a corporate proxy and receive the following message building the Docker image: Connect to gcr.io:443 [gcr.io/172.217.212.82] failed: connect timed out you must modify the build command to add the proxy hosts and ports to be used by the jib-maven-plugin as shown below: <markup lang=\"bash\" >mvn package jib:dockerBuild -Dhttps.proxyHost=host \\ -Dhttps.proxyPort=80 -Dhttp.proxyHost=host -Dhttp.proxyPort=80 This will result in the following Docker image being created which contains the configuration and server-side artifacts to be use by all deployments. <markup >deployment-example:1.0.0 Note If you are running against a remote Kubernetes cluster, you need to tag and push the Docker image to your repository accessible to that cluster. You also need to prefix the image name in the yaml files below. ",
            "title": "Clone the GitHub repository"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Install the Coherence Operator using your preferred method in the Operator Installation Guide Confirm the operator is running, for example if the operator is installed into the coherence-example namespace: <markup lang=\"bash\" >kubectl get pods -n coherence-example NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 27s ",
            "title": "Install the Coherence Operator"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster.yaml coherence.coherence.oracle.com/example-cluster-storage created ",
            "title": "1. Install the Coherence cluster storage tier"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example get coherence NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-storage example-cluster example-cluster-storage 3 Created NAME AGE coherencerole.coherence.oracle.com/example-cluster-storage 18s ",
            "title": "2. List the created Coherence cluster"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Run the following command to view the Pods: <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 6m46s example-cluster-storage-0 0/1 Running 0 119s example-cluster-storage-1 1/1 Running 0 119s example-cluster-storage-2 0/1 Running 0 118s ",
            "title": "3. View the running pods"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Since we cannot yet access the cluster via Coherence*Extend, we will connect via Coherence console to add data. <markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. ",
            "title": "Connect to the Coherence Console inside the cluster to add data"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " To scale up the cluster the kubectl scale command can be used: <markup lang=\"bash\" >kubectl -n coherence-example scale coherence/example-cluster-storage --replicas=6 Use the following to verify all 6 nodes are Running and READY before continuing. <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 53m example-cluster-storage-0 1/1 Running 0 49m example-cluster-storage-1 1/1 Running 0 49m example-cluster-storage-2 1/1 Running 0 49m example-cluster-storage-3 1/1 Running 0 54s example-cluster-storage-4 1/1 Running 0 54s example-cluster-storage-5 1/1 Running 0 54s ",
            "title": "Scale the storage tier to 6 members"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Re-run step 3 above and just use the cache test and size commands to confirm the number of entries is still 10,000. This confirms that the scale-out was done in a safe manner ensuring no data loss. ",
            "title": "Confirm the cache count"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " The first example uses the yaml file src/main/yaml/example-cluster.yaml , which defines a single tier storage which will store cluster data. If you have pushed your Docker image to a remote repository, ensure you update the above file to prefix the image. 1. Install the Coherence cluster storage tier <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster.yaml coherence.coherence.oracle.com/example-cluster-storage created 2. List the created Coherence cluster <markup lang=\"bash\" >kubectl -n coherence-example get coherence NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-storage example-cluster example-cluster-storage 3 Created NAME AGE coherencerole.coherence.oracle.com/example-cluster-storage 18s 3. View the running pods Run the following command to view the Pods: <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 6m46s example-cluster-storage-0 0/1 Running 0 119s example-cluster-storage-1 1/1 Running 0 119s example-cluster-storage-2 0/1 Running 0 118s Connect to the Coherence Console inside the cluster to add data Since we cannot yet access the cluster via Coherence*Extend, we will connect via Coherence console to add data. <markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. Scale the storage tier to 6 members To scale up the cluster the kubectl scale command can be used: <markup lang=\"bash\" >kubectl -n coherence-example scale coherence/example-cluster-storage --replicas=6 Use the following to verify all 6 nodes are Running and READY before continuing. <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 53m example-cluster-storage-0 1/1 Running 0 49m example-cluster-storage-1 1/1 Running 0 49m example-cluster-storage-2 1/1 Running 0 49m example-cluster-storage-3 1/1 Running 0 54s example-cluster-storage-4 1/1 Running 0 54s example-cluster-storage-5 1/1 Running 0 54s Confirm the cache count Re-run step 3 above and just use the cache test and size commands to confirm the number of entries is still 10,000. This confirms that the scale-out was done in a safe manner ensuring no data loss. ",
            "title": "Example 1 - Coherence cluster only"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " To scale back doewn to three members run the following command: <markup lang=\"bash\" >kubectl -n coherence-example scale coherence/example-cluster-storage --replicas=3 By using the following, you will see that the number of members will gradually scale back to 3 during which the is done in a safe manner ensuring no data loss. <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE example-cluster-storage-0 1/1 Running 0 19m example-cluster-storage-1 1/1 Running 0 19m example-cluster-storage-2 1/1 Running 0 19m example-cluster-storage-3 1/1 Running 0 3m41s example-cluster-storage-4 0/1 Terminating 0 3m41s ",
            "title": "Scale the storage tier back to 3 members"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" > kubectl -n coherence-example apply -f src/main/yaml/example-cluster-proxy.yaml kubectl get coherence -n coherence-example NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-proxy example-cluster example-cluster-proxy 1 1 Ready example-cluster-storage example-cluster example-cluster-storage 3 3 Ready ",
            "title": "Install the proxy tier"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example get pods NAME READY STATUS RESTARTS AGE coherence-operator-578497bb5b-w89kt 1/1 Running 0 68m example-cluster-proxy-0 1/1 Running 0 2m41s example-cluster-storage-0 1/1 Running 0 29m example-cluster-storage-1 1/1 Running 0 29m example-cluster-storage-2 1/1 Running 0 2m43s Ensure the example-cluster-proxy-0 pod is Running and READY before continuing. ",
            "title": "View the running pods"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " In a separate terminal, run the following: <markup lang=\"bash\" > kubectl port-forward -n coherence-example example-cluster-proxy-0 20000:20000 ",
            "title": "Port forward the proxy port"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " In a separate terminal, change to the examples/021_deployments directory and run the following to start Coherence Query Language (CohQL): <markup lang=\"bash\" > mvn exec:java Coherence Command Line Tool CohQL&gt; Run the following CohQL commands to view and insert data into the cluster. <markup >CohQL&gt; select count() from 'test'; Results 10000 CohQL&gt; insert into 'test' key('key-1') value('value-1'); CohQL&gt; select key(), value() from 'test' where key() = 'key-1'; Results [\"key-1\", \"value-1\"] CohQL&gt; select count() from 'test'; Results 10001 CohQL&gt; quit The above results will show that you can see the data previously inserted and can add new data into the cluster using Coherence*Extend. ",
            "title": "Connect via CohQL and add data"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " The second example uses the yaml file src/main/yaml/example-cluster-proxy.yaml , which adds a proxy server example-cluster-proxy to allow for Coherence*Extend connections via a Proxy server. The additional yaml added below shows: A port called proxy being exposed on 20000 The tier being set as storage-disabled A different cache config being used which will start a Proxy Server. See [here](src/main/resources/proxy-cache-config.xml) for details <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: example-cluster-proxy spec: cluster: example-cluster jvm: memory: heapSize: 512m ports: - name: metrics port: 9612 serviceMonitor: enabled: true - name: proxy port: 20000 coherence: cacheConfig: proxy-cache-config.xml storageEnabled: false metrics: enabled: true image: deployment-example:1.0.0 imagePullPolicy: Always replicas: 1 Install the proxy tier <markup lang=\"bash\" > kubectl -n coherence-example apply -f src/main/yaml/example-cluster-proxy.yaml kubectl get coherence -n coherence-example NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-proxy example-cluster example-cluster-proxy 1 1 Ready example-cluster-storage example-cluster example-cluster-storage 3 3 Ready View the running pods <markup lang=\"bash\" >kubectl -n coherence-example get pods NAME READY STATUS RESTARTS AGE coherence-operator-578497bb5b-w89kt 1/1 Running 0 68m example-cluster-proxy-0 1/1 Running 0 2m41s example-cluster-storage-0 1/1 Running 0 29m example-cluster-storage-1 1/1 Running 0 29m example-cluster-storage-2 1/1 Running 0 2m43s Ensure the example-cluster-proxy-0 pod is Running and READY before continuing. Port forward the proxy port In a separate terminal, run the following: <markup lang=\"bash\" > kubectl port-forward -n coherence-example example-cluster-proxy-0 20000:20000 Connect via CohQL and add data In a separate terminal, change to the examples/021_deployments directory and run the following to start Coherence Query Language (CohQL): <markup lang=\"bash\" > mvn exec:java Coherence Command Line Tool CohQL&gt; Run the following CohQL commands to view and insert data into the cluster. <markup >CohQL&gt; select count() from 'test'; Results 10000 CohQL&gt; insert into 'test' key('key-1') value('value-1'); CohQL&gt; select key(), value() from 'test' where key() = 'key-1'; Results [\"key-1\", \"value-1\"] CohQL&gt; select count() from 'test'; Results 10001 CohQL&gt; quit The above results will show that you can see the data previously inserted and can add new data into the cluster using Coherence*Extend. ",
            "title": "Example 2 - Adding a Proxy tier"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Install the yaml with the following command: <markup lang=\"bash\" >kubectl -n coherence-example apply -f src/main/yaml/example-cluster-app.yaml kubectl get coherence -n coherence-example NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-proxy example-cluster example-cluster-proxy 1 1 Ready example-cluster-rest example-cluster example-cluster-rest 1 1 Ready example-cluster-storage example-cluster example-cluster-storage 3 3 Ready ",
            "title": "Install the rest tier"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example get pods NAME READY STATUS RESTARTS AGE coherence-operator-578497bb5b-w89kt 1/1 Running 0 90m example-cluster-proxy-0 1/1 Running 0 3m57s example-cluster-rest-0 1/1 Running 0 3m57s example-cluster-storage-0 1/1 Running 0 3m59s example-cluster-storage-1 1/1 Running 0 3m58s example-cluster-storage-2 1/1 Running 0 3m58s ",
            "title": "View the running pods"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " In a separate terminal, run the following: <markup lang=\"bash\" >kubectl port-forward -n coherence-example example-cluster-rest-0 8080:8080 ",
            "title": "Port forward the application port"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Use the various CohQL commands via the /query endpoint to access, and mutate data in the Coherence cluster. <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache foo\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:29:40 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"insert into foo key(\\\"foo\\\") value(\\\"bar\\\")\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:29:44 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select key(),value() from foo\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Content-Type: application/json Date: Fri, 19 Jun 2020 06:29:55 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"{foo=[foo, bar]}\"} <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache test\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:30:00 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select count() from test\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Content-Type: application/json Date: Fri, 19 Jun 2020 06:30:20 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"10001\"} ",
            "title": "Access the custom /query endpoint"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " The third example uses the yaml file src/main/yaml/example-cluster-app.yaml , which adds a new tier rest . This tier defines a user application which uses Helidon to create a /query endpoint allowing the user to send CohQL commands via this endpoint. The additional yaml added below shows: A port called http being exposed on 8080 for the application The tier being set as storage-disabled Using the storage-cache-config.xml but as storage-disabled An alternate main class to run - com.oracle.coherence.examples.Main <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: example-cluster-rest spec: cluster: example-cluster jvm: memory: heapSize: 512m ports: - name: metrics port: 9612 serviceMonitor: enabled: true - name: http port: 8080 coherence: cacheConfig: storage-cache-config.xml storageEnabled: false metrics: enabled: true image: deployment-example:1.0.0 imagePullPolicy: Always application: main: com.oracle.coherence.examples.Main Install the rest tier Install the yaml with the following command: <markup lang=\"bash\" >kubectl -n coherence-example apply -f src/main/yaml/example-cluster-app.yaml kubectl get coherence -n coherence-example NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-proxy example-cluster example-cluster-proxy 1 1 Ready example-cluster-rest example-cluster example-cluster-rest 1 1 Ready example-cluster-storage example-cluster example-cluster-storage 3 3 Ready View the running pods <markup lang=\"bash\" >kubectl -n coherence-example get pods NAME READY STATUS RESTARTS AGE coherence-operator-578497bb5b-w89kt 1/1 Running 0 90m example-cluster-proxy-0 1/1 Running 0 3m57s example-cluster-rest-0 1/1 Running 0 3m57s example-cluster-storage-0 1/1 Running 0 3m59s example-cluster-storage-1 1/1 Running 0 3m58s example-cluster-storage-2 1/1 Running 0 3m58s Port forward the application port In a separate terminal, run the following: <markup lang=\"bash\" >kubectl port-forward -n coherence-example example-cluster-rest-0 8080:8080 Access the custom /query endpoint Use the various CohQL commands via the /query endpoint to access, and mutate data in the Coherence cluster. <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache foo\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:29:40 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"insert into foo key(\\\"foo\\\") value(\\\"bar\\\")\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:29:44 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select key(),value() from foo\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Content-Type: application/json Date: Fri, 19 Jun 2020 06:29:55 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"{foo=[foo, bar]}\"} <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache test\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:30:00 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select count() from test\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Content-Type: application/json Date: Fri, 19 Jun 2020 06:30:20 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"10001\"} ",
            "title": "Example 3 - Adding a User application tier"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " We must first delete the existing deployment as we need to redeploy to enable Active Persistence. <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-app.yaml Ensure all the pods have terminated before you continue. ",
            "title": "Delete the existing deployment"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml ",
            "title": "Install the cluster with Persistence enabled"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE example-cluster-rest-0 1/1 Running 0 5s example-cluster-proxy-0 1/1 Running 0 5m1s example-cluster-storage-0 1/1 Running 0 5m3s example-cluster-storage-1 1/1 Running 0 5m3s example-cluster-storage-2 1/1 Running 0 5m3s Check the Persistent Volumes and PVC are automatically created. <markup lang=\"bash\" >kubectl get pvc -n coherence-example <markup lang=\"bash\" >NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-example-cluster-storage-0 Bound pvc-15b46996-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s persistence-volume-example-cluster-storage-1 Bound pvc-15bd99e9-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s persistence-volume-example-cluster-storage-2 Bound pvc-15e55b6b-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s Wait until all nodes are Running and READY before continuing. ",
            "title": "View the running pods and PVC&#8217;s"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Use the following to view the logs of the example-cluster-storage-0 pod and validate that Active Persistence is enabled. <markup lang=\"bash\" >kubectl logs example-cluster-storage-0 -c coherence -n coherence-example | grep 'Created persistent' <markup lang=\"bash\" >... 019-10-10 04:52:00.179/77.023 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/126-2-16db40199bc-4 2019-10-10 04:52:00.247/77.091 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/127-2-16db40199bc-4 ... If you see output similar to above then Active Persistence is enabled. ",
            "title": "Check Active Persistence is enabled"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. ",
            "title": "Connect to the Coherence Console to add data"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " This will not delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml Use kubectl get pods -n coherence-example to confirm the pods have terminated. ",
            "title": "Delete the cluster"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl get pvc -n coherence-example <markup lang=\"bash\" >NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-example-cluster-storage-0 Bound pvc-730f86fe-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s persistence-volume-example-cluster-storage-1 Bound pvc-73191751-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s persistence-volume-example-cluster-storage-2 Bound pvc-73230889-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s ",
            "title": "Confirm the PVC&#8217;s are still present"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml ",
            "title": "Re-install the cluster"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl logs example-cluster-storage-0 -c coherence -n coherence-example -f You should see a message regarding recovering partitions, similar to the following: <markup lang=\"bash\" >2019-10-10 05:00:14.255/32.206 Oracle Coherence GE 12.2.1.4.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=1): Recovering 86 partitions ... 2019-10-10 05:00:17.417/35.368 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1): Created persistent store /persistence/active/example-cluster/PartitionedCache/50-3-16db409d035-1 from SafeBerkeleyDBStore(50-2-16db40199bc-4, /persistence/active/example-cluster/PartitionedCache/50-2-16db40199bc-4) ... Finally, you should see the following indicating active recovery has completed. <markup lang=\"bash\" >2019-10-10 08:18:04.870/59.565 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1): Recovered PartitionSet{172..256} from active persistent store ",
            "title": "Follow the logs for Persistence messages"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Lastly issue the command size to verify the cache entry count is 10,000 meaning the data has been recovered. Type bye to exit the console. ",
            "title": "Confirm the data has been recovered"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " The fourth example uses the yaml file src/main/yaml/example-cluster-persistence.yaml , which enabled Active Persistence for the storage tier by adding a persistence: element. The additional yaml added to the storage tier below shows: Active Persistence being enabled via persistence.enabled=true Various Persistence Volume Claim (PVC) values being set under persistentVolumeClaim <markup lang=\"yaml\" > coherence: cacheConfig: storage-cache-config.xml metrics: enabled: true persistence: enabled: true persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi NOTE:By default, when you enable Coherence Persistence, the required infrastructure in terms of persistent volumes (PV) and persistent volume claims (PVC) is set up automatically. Also, the persistence-mode is set to active . This allows the Coherence cluster to be restarted, and the data to be retained. Delete the existing deployment We must first delete the existing deployment as we need to redeploy to enable Active Persistence. <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-app.yaml Ensure all the pods have terminated before you continue. Install the cluster with Persistence enabled <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml View the running pods and PVC&#8217;s <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE example-cluster-rest-0 1/1 Running 0 5s example-cluster-proxy-0 1/1 Running 0 5m1s example-cluster-storage-0 1/1 Running 0 5m3s example-cluster-storage-1 1/1 Running 0 5m3s example-cluster-storage-2 1/1 Running 0 5m3s Check the Persistent Volumes and PVC are automatically created. <markup lang=\"bash\" >kubectl get pvc -n coherence-example <markup lang=\"bash\" >NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-example-cluster-storage-0 Bound pvc-15b46996-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s persistence-volume-example-cluster-storage-1 Bound pvc-15bd99e9-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s persistence-volume-example-cluster-storage-2 Bound pvc-15e55b6b-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s Wait until all nodes are Running and READY before continuing. Check Active Persistence is enabled Use the following to view the logs of the example-cluster-storage-0 pod and validate that Active Persistence is enabled. <markup lang=\"bash\" >kubectl logs example-cluster-storage-0 -c coherence -n coherence-example | grep 'Created persistent' <markup lang=\"bash\" >... 019-10-10 04:52:00.179/77.023 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/126-2-16db40199bc-4 2019-10-10 04:52:00.247/77.091 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/127-2-16db40199bc-4 ... If you see output similar to above then Active Persistence is enabled. Connect to the Coherence Console to add data <markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. Delete the cluster This will not delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml Use kubectl get pods -n coherence-example to confirm the pods have terminated. Confirm the PVC&#8217;s are still present <markup lang=\"bash\" >kubectl get pvc -n coherence-example <markup lang=\"bash\" >NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-example-cluster-storage-0 Bound pvc-730f86fe-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s persistence-volume-example-cluster-storage-1 Bound pvc-73191751-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s persistence-volume-example-cluster-storage-2 Bound pvc-73230889-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s Re-install the cluster <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml Follow the logs for Persistence messages <markup lang=\"bash\" >kubectl logs example-cluster-storage-0 -c coherence -n coherence-example -f You should see a message regarding recovering partitions, similar to the following: <markup lang=\"bash\" >2019-10-10 05:00:14.255/32.206 Oracle Coherence GE 12.2.1.4.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=1): Recovering 86 partitions ... 2019-10-10 05:00:17.417/35.368 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1): Created persistent store /persistence/active/example-cluster/PartitionedCache/50-3-16db409d035-1 from SafeBerkeleyDBStore(50-2-16db40199bc-4, /persistence/active/example-cluster/PartitionedCache/50-2-16db40199bc-4) ... Finally, you should see the following indicating active recovery has completed. <markup lang=\"bash\" >2019-10-10 08:18:04.870/59.565 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1): Recovered PartitionSet{172..256} from active persistent store Confirm the data has been recovered <markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Lastly issue the command size to verify the cache entry count is 10,000 meaning the data has been recovered. Type bye to exit the console. ",
            "title": "Example 4 - Enabling Persistence"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Install the Prometheus Operator, as documented in the Prometheus Operator Quick Start page. Prometheus can then be accessed as documented in the Access Prometheus section of the Quick Start page. Note Using RBAC If installing Prometheus into RBAC enabled k8s clusters, you may need to create the required RBAC resources as described in the Prometheus RBAC documentation. The Coherence Operator contains an example that works with the out-of-the-box Prometheus Operator install that we use for testing prometheus-rbac.yaml This yaml creates a ClusterRole with the required permissions and a ClusterRoleBinding that binds the role to the prometheus-k8s service account (which is the name of the account created, and used by the Prometheus Operator). This yaml file can be installed into k8s before installing the Prometheus Operator. ",
            "title": "Install Prometheus Operator"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " The Prometheus Operator also installs Grafana. Grafana can be accessed as documented in the Access Grafana section of the Quick Start page. Note that the default credentials are specified in that section of the documentation. ",
            "title": "Access Grafana"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " To import the Coherence Grafana dashboards follow the instructions in the Operator documentation section Importing Grafana Dashboards . After importing the dashboards into Grafana and with the port-forward still running the Coherence dashboards can be accessed at localhost:3000/d/coh-main/coherence-dashboard-main ",
            "title": "Import the Grafana Dashboards"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " It may take up to 5 minutes for data to start appearing in Grafana. If you are not seeing data after 5 minutes, access the Prometheus endpoint as described above. Ensure that the endpoints named coherence-example/example-cluster-storage-metrics/0 (3/3 up) are up. If the endpoints are not up then wait 60 seconds and refresh the browser. If you do not see any values in the Cluster Name dropdown in Grafana, ensure the endpoints are up as described above and click on Manage Alerts and then Back to Main Dashboard . This will re-query the data and load the list of clusters. ",
            "title": "Troubleshooting"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " If you wish to view metrics via Grafana, you must carry out the following steps before you install any of the examples above. Install Prometheus Operator Install the Prometheus Operator, as documented in the Prometheus Operator Quick Start page. Prometheus can then be accessed as documented in the Access Prometheus section of the Quick Start page. Note Using RBAC If installing Prometheus into RBAC enabled k8s clusters, you may need to create the required RBAC resources as described in the Prometheus RBAC documentation. The Coherence Operator contains an example that works with the out-of-the-box Prometheus Operator install that we use for testing prometheus-rbac.yaml This yaml creates a ClusterRole with the required permissions and a ClusterRoleBinding that binds the role to the prometheus-k8s service account (which is the name of the account created, and used by the Prometheus Operator). This yaml file can be installed into k8s before installing the Prometheus Operator. Access Grafana The Prometheus Operator also installs Grafana. Grafana can be accessed as documented in the Access Grafana section of the Quick Start page. Note that the default credentials are specified in that section of the documentation. Import the Grafana Dashboards To import the Coherence Grafana dashboards follow the instructions in the Operator documentation section Importing Grafana Dashboards . After importing the dashboards into Grafana and with the port-forward still running the Coherence dashboards can be accessed at localhost:3000/d/coh-main/coherence-dashboard-main Troubleshooting It may take up to 5 minutes for data to start appearing in Grafana. If you are not seeing data after 5 minutes, access the Prometheus endpoint as described above. Ensure that the endpoints named coherence-example/example-cluster-storage-metrics/0 (3/3 up) are up. If the endpoints are not up then wait 60 seconds and refresh the browser. If you do not see any values in the Cluster Name dropdown in Grafana, ensure the endpoints are up as described above and click on Manage Alerts and then Back to Main Dashboard . This will re-query the data and load the list of clusters. ",
            "title": "View Cluster Metrics Using Prometheus and Grafana"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": "<markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml ",
            "title": "Delete the cluster"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Ensure all the pods have all terminated before you delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl get pvc -n coherence-example | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n coherence-example ",
            "title": "Delete the PVC&#8217;s"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Uninstall the Coherence operator using the undeploy commands for whichever method you chose to install it. ",
            "title": "Remove the Coherence Operator"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Uninstall the Prometheus Operator as documented in the Remove kube-prometheus section of the Quick Start page. ",
            "title": "Delete Prometheus Operator"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Delete the cluster <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml Delete the PVC&#8217;s Ensure all the pods have all terminated before you delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl get pvc -n coherence-example | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n coherence-example Remove the Coherence Operator Uninstall the Coherence operator using the undeploy commands for whichever method you chose to install it. Delete Prometheus Operator Uninstall the Prometheus Operator as documented in the Remove kube-prometheus section of the Quick Start page. ",
            "title": "Cleaning Up"
        },
        {
            "location": "/examples/021_deployment/README",
            "text": " Ensure you are in the examples/021_deployment directory to run the following commands. Example 1 - Coherence cluster only The first example uses the yaml file src/main/yaml/example-cluster.yaml , which defines a single tier storage which will store cluster data. If you have pushed your Docker image to a remote repository, ensure you update the above file to prefix the image. 1. Install the Coherence cluster storage tier <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster.yaml coherence.coherence.oracle.com/example-cluster-storage created 2. List the created Coherence cluster <markup lang=\"bash\" >kubectl -n coherence-example get coherence NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-storage example-cluster example-cluster-storage 3 Created NAME AGE coherencerole.coherence.oracle.com/example-cluster-storage 18s 3. View the running pods Run the following command to view the Pods: <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 6m46s example-cluster-storage-0 0/1 Running 0 119s example-cluster-storage-1 1/1 Running 0 119s example-cluster-storage-2 0/1 Running 0 118s Connect to the Coherence Console inside the cluster to add data Since we cannot yet access the cluster via Coherence*Extend, we will connect via Coherence console to add data. <markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. Scale the storage tier to 6 members To scale up the cluster the kubectl scale command can be used: <markup lang=\"bash\" >kubectl -n coherence-example scale coherence/example-cluster-storage --replicas=6 Use the following to verify all 6 nodes are Running and READY before continuing. <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-74d49cd9f9-sgzjr 1/1 Running 1 53m example-cluster-storage-0 1/1 Running 0 49m example-cluster-storage-1 1/1 Running 0 49m example-cluster-storage-2 1/1 Running 0 49m example-cluster-storage-3 1/1 Running 0 54s example-cluster-storage-4 1/1 Running 0 54s example-cluster-storage-5 1/1 Running 0 54s Confirm the cache count Re-run step 3 above and just use the cache test and size commands to confirm the number of entries is still 10,000. This confirms that the scale-out was done in a safe manner ensuring no data loss. Scale the storage tier back to 3 members To scale back doewn to three members run the following command: <markup lang=\"bash\" >kubectl -n coherence-example scale coherence/example-cluster-storage --replicas=3 By using the following, you will see that the number of members will gradually scale back to 3 during which the is done in a safe manner ensuring no data loss. <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE example-cluster-storage-0 1/1 Running 0 19m example-cluster-storage-1 1/1 Running 0 19m example-cluster-storage-2 1/1 Running 0 19m example-cluster-storage-3 1/1 Running 0 3m41s example-cluster-storage-4 0/1 Terminating 0 3m41s Example 2 - Adding a Proxy tier The second example uses the yaml file src/main/yaml/example-cluster-proxy.yaml , which adds a proxy server example-cluster-proxy to allow for Coherence*Extend connections via a Proxy server. The additional yaml added below shows: A port called proxy being exposed on 20000 The tier being set as storage-disabled A different cache config being used which will start a Proxy Server. See [here](src/main/resources/proxy-cache-config.xml) for details <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: example-cluster-proxy spec: cluster: example-cluster jvm: memory: heapSize: 512m ports: - name: metrics port: 9612 serviceMonitor: enabled: true - name: proxy port: 20000 coherence: cacheConfig: proxy-cache-config.xml storageEnabled: false metrics: enabled: true image: deployment-example:1.0.0 imagePullPolicy: Always replicas: 1 Install the proxy tier <markup lang=\"bash\" > kubectl -n coherence-example apply -f src/main/yaml/example-cluster-proxy.yaml kubectl get coherence -n coherence-example NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-proxy example-cluster example-cluster-proxy 1 1 Ready example-cluster-storage example-cluster example-cluster-storage 3 3 Ready View the running pods <markup lang=\"bash\" >kubectl -n coherence-example get pods NAME READY STATUS RESTARTS AGE coherence-operator-578497bb5b-w89kt 1/1 Running 0 68m example-cluster-proxy-0 1/1 Running 0 2m41s example-cluster-storage-0 1/1 Running 0 29m example-cluster-storage-1 1/1 Running 0 29m example-cluster-storage-2 1/1 Running 0 2m43s Ensure the example-cluster-proxy-0 pod is Running and READY before continuing. Port forward the proxy port In a separate terminal, run the following: <markup lang=\"bash\" > kubectl port-forward -n coherence-example example-cluster-proxy-0 20000:20000 Connect via CohQL and add data In a separate terminal, change to the examples/021_deployments directory and run the following to start Coherence Query Language (CohQL): <markup lang=\"bash\" > mvn exec:java Coherence Command Line Tool CohQL&gt; Run the following CohQL commands to view and insert data into the cluster. <markup >CohQL&gt; select count() from 'test'; Results 10000 CohQL&gt; insert into 'test' key('key-1') value('value-1'); CohQL&gt; select key(), value() from 'test' where key() = 'key-1'; Results [\"key-1\", \"value-1\"] CohQL&gt; select count() from 'test'; Results 10001 CohQL&gt; quit The above results will show that you can see the data previously inserted and can add new data into the cluster using Coherence*Extend. Example 3 - Adding a User application tier The third example uses the yaml file src/main/yaml/example-cluster-app.yaml , which adds a new tier rest . This tier defines a user application which uses Helidon to create a /query endpoint allowing the user to send CohQL commands via this endpoint. The additional yaml added below shows: A port called http being exposed on 8080 for the application The tier being set as storage-disabled Using the storage-cache-config.xml but as storage-disabled An alternate main class to run - com.oracle.coherence.examples.Main <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: example-cluster-rest spec: cluster: example-cluster jvm: memory: heapSize: 512m ports: - name: metrics port: 9612 serviceMonitor: enabled: true - name: http port: 8080 coherence: cacheConfig: storage-cache-config.xml storageEnabled: false metrics: enabled: true image: deployment-example:1.0.0 imagePullPolicy: Always application: main: com.oracle.coherence.examples.Main Install the rest tier Install the yaml with the following command: <markup lang=\"bash\" >kubectl -n coherence-example apply -f src/main/yaml/example-cluster-app.yaml kubectl get coherence -n coherence-example NAME CLUSTER ROLE REPLICAS READY PHASE example-cluster-proxy example-cluster example-cluster-proxy 1 1 Ready example-cluster-rest example-cluster example-cluster-rest 1 1 Ready example-cluster-storage example-cluster example-cluster-storage 3 3 Ready View the running pods <markup lang=\"bash\" >kubectl -n coherence-example get pods NAME READY STATUS RESTARTS AGE coherence-operator-578497bb5b-w89kt 1/1 Running 0 90m example-cluster-proxy-0 1/1 Running 0 3m57s example-cluster-rest-0 1/1 Running 0 3m57s example-cluster-storage-0 1/1 Running 0 3m59s example-cluster-storage-1 1/1 Running 0 3m58s example-cluster-storage-2 1/1 Running 0 3m58s Port forward the application port In a separate terminal, run the following: <markup lang=\"bash\" >kubectl port-forward -n coherence-example example-cluster-rest-0 8080:8080 Access the custom /query endpoint Use the various CohQL commands via the /query endpoint to access, and mutate data in the Coherence cluster. <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache foo\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:29:40 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"insert into foo key(\\\"foo\\\") value(\\\"bar\\\")\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:29:44 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select key(),value() from foo\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Content-Type: application/json Date: Fri, 19 Jun 2020 06:29:55 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"{foo=[foo, bar]}\"} <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache test\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Fri, 19 Jun 2020 06:30:00 GMT transfer-encoding: chunked connection: keep-alive <markup lang=\"bash\" >curl -i -w '' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select count() from test\"}' <markup lang=\"bash\" >HTTP/1.1 200 OK Content-Type: application/json Date: Fri, 19 Jun 2020 06:30:20 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"10001\"} Example 4 - Enabling Persistence The fourth example uses the yaml file src/main/yaml/example-cluster-persistence.yaml , which enabled Active Persistence for the storage tier by adding a persistence: element. The additional yaml added to the storage tier below shows: Active Persistence being enabled via persistence.enabled=true Various Persistence Volume Claim (PVC) values being set under persistentVolumeClaim <markup lang=\"yaml\" > coherence: cacheConfig: storage-cache-config.xml metrics: enabled: true persistence: enabled: true persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi NOTE:By default, when you enable Coherence Persistence, the required infrastructure in terms of persistent volumes (PV) and persistent volume claims (PVC) is set up automatically. Also, the persistence-mode is set to active . This allows the Coherence cluster to be restarted, and the data to be retained. Delete the existing deployment We must first delete the existing deployment as we need to redeploy to enable Active Persistence. <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-app.yaml Ensure all the pods have terminated before you continue. Install the cluster with Persistence enabled <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml View the running pods and PVC&#8217;s <markup lang=\"bash\" >kubectl -n coherence-example get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE example-cluster-rest-0 1/1 Running 0 5s example-cluster-proxy-0 1/1 Running 0 5m1s example-cluster-storage-0 1/1 Running 0 5m3s example-cluster-storage-1 1/1 Running 0 5m3s example-cluster-storage-2 1/1 Running 0 5m3s Check the Persistent Volumes and PVC are automatically created. <markup lang=\"bash\" >kubectl get pvc -n coherence-example <markup lang=\"bash\" >NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-example-cluster-storage-0 Bound pvc-15b46996-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s persistence-volume-example-cluster-storage-1 Bound pvc-15bd99e9-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s persistence-volume-example-cluster-storage-2 Bound pvc-15e55b6b-eb35-11e9-9b4b-025000000001 1Gi RWO hostpath 55s Wait until all nodes are Running and READY before continuing. Check Active Persistence is enabled Use the following to view the logs of the example-cluster-storage-0 pod and validate that Active Persistence is enabled. <markup lang=\"bash\" >kubectl logs example-cluster-storage-0 -c coherence -n coherence-example | grep 'Created persistent' <markup lang=\"bash\" >... 019-10-10 04:52:00.179/77.023 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/126-2-16db40199bc-4 2019-10-10 04:52:00.247/77.091 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/127-2-16db40199bc-4 ... If you see output similar to above then Active Persistence is enabled. Connect to the Coherence Console to add data <markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Use the following to create 10,000 entries of 100 bytes: <markup lang=\"bash\" >bulkput 10000 100 0 100 Lastly issue the command size to verify the cache entry count. Type bye to exit the console. Delete the cluster This will not delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml Use kubectl get pods -n coherence-example to confirm the pods have terminated. Confirm the PVC&#8217;s are still present <markup lang=\"bash\" >kubectl get pvc -n coherence-example <markup lang=\"bash\" >NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-example-cluster-storage-0 Bound pvc-730f86fe-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s persistence-volume-example-cluster-storage-1 Bound pvc-73191751-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s persistence-volume-example-cluster-storage-2 Bound pvc-73230889-eb19-11e9-9b4b-025000000001 1Gi RWO hostpath 116s Re-install the cluster <markup lang=\"bash\" >kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml Follow the logs for Persistence messages <markup lang=\"bash\" >kubectl logs example-cluster-storage-0 -c coherence -n coherence-example -f You should see a message regarding recovering partitions, similar to the following: <markup lang=\"bash\" >2019-10-10 05:00:14.255/32.206 Oracle Coherence GE 12.2.1.4.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=1): Recovering 86 partitions ... 2019-10-10 05:00:17.417/35.368 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1): Created persistent store /persistence/active/example-cluster/PartitionedCache/50-3-16db409d035-1 from SafeBerkeleyDBStore(50-2-16db40199bc-4, /persistence/active/example-cluster/PartitionedCache/50-2-16db40199bc-4) ... Finally, you should see the following indicating active recovery has completed. <markup lang=\"bash\" >2019-10-10 08:18:04.870/59.565 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1): Recovered PartitionSet{172..256} from active persistent store Confirm the data has been recovered <markup lang=\"bash\" >kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console At the prompt type the following to create a cache called test : <markup lang=\"bash\" >cache test Lastly issue the command size to verify the cache entry count is 10,000 meaning the data has been recovered. Type bye to exit the console. View Cluster Metrics Using Prometheus and Grafana If you wish to view metrics via Grafana, you must carry out the following steps before you install any of the examples above. Install Prometheus Operator Install the Prometheus Operator, as documented in the Prometheus Operator Quick Start page. Prometheus can then be accessed as documented in the Access Prometheus section of the Quick Start page. Note Using RBAC If installing Prometheus into RBAC enabled k8s clusters, you may need to create the required RBAC resources as described in the Prometheus RBAC documentation. The Coherence Operator contains an example that works with the out-of-the-box Prometheus Operator install that we use for testing prometheus-rbac.yaml This yaml creates a ClusterRole with the required permissions and a ClusterRoleBinding that binds the role to the prometheus-k8s service account (which is the name of the account created, and used by the Prometheus Operator). This yaml file can be installed into k8s before installing the Prometheus Operator. Access Grafana The Prometheus Operator also installs Grafana. Grafana can be accessed as documented in the Access Grafana section of the Quick Start page. Note that the default credentials are specified in that section of the documentation. Import the Grafana Dashboards To import the Coherence Grafana dashboards follow the instructions in the Operator documentation section Importing Grafana Dashboards . After importing the dashboards into Grafana and with the port-forward still running the Coherence dashboards can be accessed at localhost:3000/d/coh-main/coherence-dashboard-main Troubleshooting It may take up to 5 minutes for data to start appearing in Grafana. If you are not seeing data after 5 minutes, access the Prometheus endpoint as described above. Ensure that the endpoints named coherence-example/example-cluster-storage-metrics/0 (3/3 up) are up. If the endpoints are not up then wait 60 seconds and refresh the browser. If you do not see any values in the Cluster Name dropdown in Grafana, ensure the endpoints are up as described above and click on Manage Alerts and then Back to Main Dashboard . This will re-query the data and load the list of clusters. Cleaning Up Delete the cluster <markup lang=\"bash\" >kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml Delete the PVC&#8217;s Ensure all the pods have all terminated before you delete the PVC&#8217;s. <markup lang=\"bash\" >kubectl get pvc -n coherence-example | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n coherence-example Remove the Coherence Operator Uninstall the Coherence operator using the undeploy commands for whichever method you chose to install it. Delete Prometheus Operator Uninstall the Prometheus Operator as documented in the Remove kube-prometheus section of the Quick Start page. ",
            "title": "Run the Examples"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " The JVM has an option -XX:MaxRAM=N the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). When using resource limited containers it is useful to set the max RAM option to avoid the JVM exceeding the container limits. The Coherence CRD allows the max RAM option to be set using the jvm.memory.maxRAM field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: maxRAM: 10g ",
            "title": "Max RAM"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the JVM runs with a fixed size heap. The Coherence CRD provides the jvm.memory.percentage field for this use-case. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: percentage: 10 In this case the percentage field has been set to 10 , so the options passed to the JVM will be -XX:InitialRAMPercentage=10 -XX:MinRAMPercentage=10 -XX:MaxRAMPercentage=10 meaning the heap size will be fixed at 10% of max RAM. Setting the jvm.memory.percentage field will cause individual RAM percentage fields to be ignored. The JVM documentation states that \"If you set a value for -Xms , the -XX:InitialRAMPercentage , -XX:MinRAMPercentage and -XX:MaxRAMPercentage options will be ignored\" . So if the Coherence CRD fields detailed below for explictly setting the heap size as a bytes value are used then we can assume that the RAM percentage fields detailed here will be ignored by the JVM. The Coherence Operator will pass both the percentage and explicit values to the JVM. Due to CRDs not supporting decimal fields the RAM percentage fields are of type resource.Quantity, see the Kubernetes Quantity API docs for details of the different number formats allowed. ",
            "title": "Set Heap Percentages From a Single Value"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " There are three JVM options that can be used to control the JVM heap as a percentage of the available memory. These options can be useful when controlling memory as a percentage of container memory in combination with resource limits on containers. JVM Option Description -XX:InitialRAMPercentage=N Sets the initial amount of memory that the JVM will use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 1.5625 percent. '-XX:MaxRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 25 percent. Specifying this option disables automatic use of compressed oops if the combined result of this and other options influencing the maximum amount of memory is larger than the range of memory addressable by compressed oops. '-XX:MinRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option for small heaps. A small heap is a heap of approximately 125 MB. The default value is 50 percent. Where N is a decimal value between 0 and 100. For example, 12.3456. When running in a container, and the -XX:+UseContainerSupport is set (which it is by default for the Coherence container), both the default heap size for containers, the -XX:InitialRAMPercentage option, the -XX:MaxRAMPercentage option, and the -XX:MaxRAMPercentage option, will be based on the available container memory. Some JVMs may not support these options. The Coherence CRD allows these options to be set with the jvm.memory.initialRAMPercentage , jvm.memory.minRAMPercentage , and jvm.memory.maxRAMPercentage fields. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: initialRAMPercentage: 10 minRAMPercentage: 5.75 maxRAMPercentage: 75 Set Heap Percentages From a Single Value Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the JVM runs with a fixed size heap. The Coherence CRD provides the jvm.memory.percentage field for this use-case. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: percentage: 10 In this case the percentage field has been set to 10 , so the options passed to the JVM will be -XX:InitialRAMPercentage=10 -XX:MinRAMPercentage=10 -XX:MaxRAMPercentage=10 meaning the heap size will be fixed at 10% of max RAM. Setting the jvm.memory.percentage field will cause individual RAM percentage fields to be ignored. The JVM documentation states that \"If you set a value for -Xms , the -XX:InitialRAMPercentage , -XX:MinRAMPercentage and -XX:MaxRAMPercentage options will be ignored\" . So if the Coherence CRD fields detailed below for explictly setting the heap size as a bytes value are used then we can assume that the RAM percentage fields detailed here will be ignored by the JVM. The Coherence Operator will pass both the percentage and explicit values to the JVM. Due to CRDs not supporting decimal fields the RAM percentage fields are of type resource.Quantity, see the Kubernetes Quantity API docs for details of the different number formats allowed. ",
            "title": "Heap Size as a Percentage of Container Memory"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the JVM runs with a fixed size heap. The Coherence CRD provides the jvm.memory.heapSize field for this use-case. To set the JVM both the initial amd max heap sizes to the same value, set the jvm.memory.heapSize field. The value of the field can be any value that can be used with the JVM -XX:InitialHeapSize and -XX:MaxHeapSize (or -Xmx and -Xms ) arguments. The value of the jvm.memory.heapSize field will be used to set both the -XX:InitialHeapSize , and the -XX:MaxHeapSize arguments to the same value, so the heap will be a fixed size. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: heapSize: 10g Setting jvm.memory.heapSize to 10g will effectively pass -XX:InitialHeapSize=10g -XX:MaxHeapSize=10g to the JVM. ",
            "title": "Set Initial and Max Heap Size With a Single Value"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " There are two JVM options that can be used to control the JVM heap as an explicit size in bytes value. These options can be useful when controlling memory of container memory in combination with resource limits on containers. JVM Option Description -XX:InitialHeapSize=&lt;size&gt; Set initial heap size -XX:MaxHeapSize=&lt;size&gt; Set maximum heap size The &lt;size&gt; parameter is a numeric integer followed by a suffix to the size value: \"k\" or \"K\" to indicate kilobytes, \"m\" or \"M\" to indicate megabytes, \"g\" or \"G\" to indicate gigabytes, or, \"t\" or \"T\" to indicate terabytes. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: initialHeapSize: 5g maxHeapSize: 10g The initial heap size to 5g , passing the -XX:InitialHeapSize=5g option to the JVM. The max heap size to 10g , passing the -XX:MaxHeapSize=10g option to the JVM. Setting the jvm.memory.heapSize field will cause individual jvm.memory.initialHeapSize and jvm.memory.maxHeapSize fields to be ignored. Set Initial and Max Heap Size With a Single Value Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the JVM runs with a fixed size heap. The Coherence CRD provides the jvm.memory.heapSize field for this use-case. To set the JVM both the initial amd max heap sizes to the same value, set the jvm.memory.heapSize field. The value of the field can be any value that can be used with the JVM -XX:InitialHeapSize and -XX:MaxHeapSize (or -Xmx and -Xms ) arguments. The value of the jvm.memory.heapSize field will be used to set both the -XX:InitialHeapSize , and the -XX:MaxHeapSize arguments to the same value, so the heap will be a fixed size. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: heapSize: 10g Setting jvm.memory.heapSize to 10g will effectively pass -XX:InitialHeapSize=10g -XX:MaxHeapSize=10g to the JVM. ",
            "title": "Set Heap Size Explicitly"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " Direct memory size is used to limit on memory that can be reserved for all Direct Byte Buffers. If a value is set for this option, the sum of all Direct Byte Buffer sizes cannot exceed the limit. After the limit is reached, a new Direct Byte Buffer can be allocated only when enough old buffers are freed to provide enough space to allocate the new buffer. By default, the VM limits the amount of heap memory used for Direct Byte Buffers to approximately 85% of the maximum heap size. To set a value for the direct memory size use the jvm.memory.directMemorySize field. This wil set the value of the -XX:MaxDirectMemorySize JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: directMemorySize: 10g The direct memory size is set to 10g which will pass -XX:MaxDirectMemorySize=10g to the JVM. ",
            "title": "Direct Memory Size (NIO Memory)"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " Metaspace is memory the VM uses to store class metadata. Class metadata are the runtime representation of java classes within a JVM process - basically any information the JVM needs to work with a Java class. That includes, but is not limited to, runtime representation of data from the JVM class file format. To set the size of the metaspace use the jvm.memory.metaspaceSize field in the Coherence CRD. Setting this field sets both the -XX:MetaspaceSize and -XX:MaxMetaspaceSize JVM options to this value giving a fixed size metaspace. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: metaspaceSize: 100m Set the metaspace size to 100m which will pass -XX:MetaspaceSize=100m -XX:MaxMetaspaceSize=100m to the JVM. ",
            "title": "Metaspace Size"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " Thread stacks are memory areas allocated for each Java thread for their internal use. This is where the thread stores its local execution state. The current default size for a linux JVM is 1MB. To set the stack size use the jvm.memory.stackSize field in the Coherence CRD. Setting this value sets the -Xss JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: stackSize: 500k The stack size will be set to 500k , passing -Xss500k to the JVM. ",
            "title": "Stack Size"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " The Coherence CRD allows two optional behaviours to be specified if the JVM throws an out of memory error. The jvm.memory.onOutOfMemory.heapDump is a bool field that when set to true will pass the -XX:+HeapDumpOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off heap dumps on OOM set the specifically field to be false . The jvm.memory.onOutOfMemory.exit is a bool field that when set to true will pass the -XX:+ExitOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off killing the JVM on OOM set the specifically field to be false . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: onOutOfMemory: heapDump: true exit: true The JVM will create a heap dump on OOM The JVM will exit on OOM ",
            "title": "Out Of Memory Behaviour"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " The Native Memory Tracking (NMT) is a Java VM feature that tracks internal memory usage for a JVM. The Coherence CRD allows native memory tracking to be configured using the jvm.memory.nativeMemoryTracking field. Setting this field sets the -XX:NativeMemoryTracking JVM option. There are three valid values, off , summary or detail . If not specified the default value used by the operator is summary <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: nativeMemoryTracking: detail Native memory tracking is set to detail which will pass the -XX:NativeMemoryTracking=detail option to the JVM. ",
            "title": "Native Memory Tracking"
        },
        {
            "location": "/docs/jvm/050_memory",
            "text": " The JVM has a number of arguments that set the sizes of different memory regions; the most commonly set is the heap size but there are a number of others. The Coherence CRD spec has fields that allow some of these to sizes to be set. The Coherence CRD also has settings to control the behaviour of the JVM if an out of memory error occurs. For example, killing the container, creating a heap dump etc. Max RAM The JVM has an option -XX:MaxRAM=N the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). When using resource limited containers it is useful to set the max RAM option to avoid the JVM exceeding the container limits. The Coherence CRD allows the max RAM option to be set using the jvm.memory.maxRAM field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: maxRAM: 10g Heap Size as a Percentage of Container Memory There are three JVM options that can be used to control the JVM heap as a percentage of the available memory. These options can be useful when controlling memory as a percentage of container memory in combination with resource limits on containers. JVM Option Description -XX:InitialRAMPercentage=N Sets the initial amount of memory that the JVM will use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 1.5625 percent. '-XX:MaxRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 25 percent. Specifying this option disables automatic use of compressed oops if the combined result of this and other options influencing the maximum amount of memory is larger than the range of memory addressable by compressed oops. '-XX:MinRAMPercentage=N' Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a percentage of the maximum amount determined as described in the -XX:MaxRAM option for small heaps. A small heap is a heap of approximately 125 MB. The default value is 50 percent. Where N is a decimal value between 0 and 100. For example, 12.3456. When running in a container, and the -XX:+UseContainerSupport is set (which it is by default for the Coherence container), both the default heap size for containers, the -XX:InitialRAMPercentage option, the -XX:MaxRAMPercentage option, and the -XX:MaxRAMPercentage option, will be based on the available container memory. Some JVMs may not support these options. The Coherence CRD allows these options to be set with the jvm.memory.initialRAMPercentage , jvm.memory.minRAMPercentage , and jvm.memory.maxRAMPercentage fields. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: initialRAMPercentage: 10 minRAMPercentage: 5.75 maxRAMPercentage: 75 Set Heap Percentages From a Single Value Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the JVM runs with a fixed size heap. The Coherence CRD provides the jvm.memory.percentage field for this use-case. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: percentage: 10 In this case the percentage field has been set to 10 , so the options passed to the JVM will be -XX:InitialRAMPercentage=10 -XX:MinRAMPercentage=10 -XX:MaxRAMPercentage=10 meaning the heap size will be fixed at 10% of max RAM. Setting the jvm.memory.percentage field will cause individual RAM percentage fields to be ignored. The JVM documentation states that \"If you set a value for -Xms , the -XX:InitialRAMPercentage , -XX:MinRAMPercentage and -XX:MaxRAMPercentage options will be ignored\" . So if the Coherence CRD fields detailed below for explictly setting the heap size as a bytes value are used then we can assume that the RAM percentage fields detailed here will be ignored by the JVM. The Coherence Operator will pass both the percentage and explicit values to the JVM. Due to CRDs not supporting decimal fields the RAM percentage fields are of type resource.Quantity, see the Kubernetes Quantity API docs for details of the different number formats allowed. Set Heap Size Explicitly There are two JVM options that can be used to control the JVM heap as an explicit size in bytes value. These options can be useful when controlling memory of container memory in combination with resource limits on containers. JVM Option Description -XX:InitialHeapSize=&lt;size&gt; Set initial heap size -XX:MaxHeapSize=&lt;size&gt; Set maximum heap size The &lt;size&gt; parameter is a numeric integer followed by a suffix to the size value: \"k\" or \"K\" to indicate kilobytes, \"m\" or \"M\" to indicate megabytes, \"g\" or \"G\" to indicate gigabytes, or, \"t\" or \"T\" to indicate terabytes. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: initialHeapSize: 5g maxHeapSize: 10g The initial heap size to 5g , passing the -XX:InitialHeapSize=5g option to the JVM. The max heap size to 10g , passing the -XX:MaxHeapSize=10g option to the JVM. Setting the jvm.memory.heapSize field will cause individual jvm.memory.initialHeapSize and jvm.memory.maxHeapSize fields to be ignored. Set Initial and Max Heap Size With a Single Value Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the JVM runs with a fixed size heap. The Coherence CRD provides the jvm.memory.heapSize field for this use-case. To set the JVM both the initial amd max heap sizes to the same value, set the jvm.memory.heapSize field. The value of the field can be any value that can be used with the JVM -XX:InitialHeapSize and -XX:MaxHeapSize (or -Xmx and -Xms ) arguments. The value of the jvm.memory.heapSize field will be used to set both the -XX:InitialHeapSize , and the -XX:MaxHeapSize arguments to the same value, so the heap will be a fixed size. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: heapSize: 10g Setting jvm.memory.heapSize to 10g will effectively pass -XX:InitialHeapSize=10g -XX:MaxHeapSize=10g to the JVM. Direct Memory Size (NIO Memory) Direct memory size is used to limit on memory that can be reserved for all Direct Byte Buffers. If a value is set for this option, the sum of all Direct Byte Buffer sizes cannot exceed the limit. After the limit is reached, a new Direct Byte Buffer can be allocated only when enough old buffers are freed to provide enough space to allocate the new buffer. By default, the VM limits the amount of heap memory used for Direct Byte Buffers to approximately 85% of the maximum heap size. To set a value for the direct memory size use the jvm.memory.directMemorySize field. This wil set the value of the -XX:MaxDirectMemorySize JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: directMemorySize: 10g The direct memory size is set to 10g which will pass -XX:MaxDirectMemorySize=10g to the JVM. Metaspace Size Metaspace is memory the VM uses to store class metadata. Class metadata are the runtime representation of java classes within a JVM process - basically any information the JVM needs to work with a Java class. That includes, but is not limited to, runtime representation of data from the JVM class file format. To set the size of the metaspace use the jvm.memory.metaspaceSize field in the Coherence CRD. Setting this field sets both the -XX:MetaspaceSize and -XX:MaxMetaspaceSize JVM options to this value giving a fixed size metaspace. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: metaspaceSize: 100m Set the metaspace size to 100m which will pass -XX:MetaspaceSize=100m -XX:MaxMetaspaceSize=100m to the JVM. Stack Size Thread stacks are memory areas allocated for each Java thread for their internal use. This is where the thread stores its local execution state. The current default size for a linux JVM is 1MB. To set the stack size use the jvm.memory.stackSize field in the Coherence CRD. Setting this value sets the -Xss JVM option. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: stackSize: 500k The stack size will be set to 500k , passing -Xss500k to the JVM. Out Of Memory Behaviour The Coherence CRD allows two optional behaviours to be specified if the JVM throws an out of memory error. The jvm.memory.onOutOfMemory.heapDump is a bool field that when set to true will pass the -XX:+HeapDumpOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off heap dumps on OOM set the specifically field to be false . The jvm.memory.onOutOfMemory.exit is a bool field that when set to true will pass the -XX:+ExitOnOutOfMemoryError option to the JVM. The default value of the field when not specified is true , hence to turn off killing the JVM on OOM set the specifically field to be false . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: onOutOfMemory: heapDump: true exit: true The JVM will create a heap dump on OOM The JVM will exit on OOM Native Memory Tracking The Native Memory Tracking (NMT) is a Java VM feature that tracks internal memory usage for a JVM. The Coherence CRD allows native memory tracking to be configured using the jvm.memory.nativeMemoryTracking field. Setting this field sets the -XX:NativeMemoryTracking JVM option. There are three valid values, off , summary or detail . If not specified the default value used by the operator is summary <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: memory: nativeMemoryTracking: detail Native memory tracking is set to detail which will pass the -XX:NativeMemoryTracking=detail option to the JVM. ",
            "title": "Heap &amp; Memory Settings"
        },
        {
            "location": "/docs/management/040_ssl",
            "text": " It is possible to configure Management over REST endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.metrics.ssl section of the CRD spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: management: enabled: true ssl: enabled: true keyStore: metrics-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: metrics-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: metrics-secret The enabled field when set to true enables SSL for metrics or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. The various files and keystores referred to in the configuration above can be any location accessible in the image used by the coherence container in the deployment&#8217;s Pods . Typically, for things such as SSL keys and certs, these would be provided by obtained from Secrets loaded as additional Pod Volumes . See Add Secrets Volumes for the documentation on how to specify secrets as additional volumes. ",
            "title": "SSL with Management over REST"
        },
        {
            "location": "/docs/applications/040_application_main",
            "text": " The Coherence container in the deployment&#8217;s Pods will, by default, run com.tangosol.net.DefaultCacheServer as the Java main class. It is possible to change this when running a custom application that requires a different main. The name of the main is set in the application.main field in the Coherence spec. For example, if the deployment is using a custom image catalogue:1.0.0 that requires a custom main class called com.acme.Catalogue the Coherence resource would look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: main: com.acme.Catalogue The com.acme.Catalogue will be run as the main class. The example would be equivalent to the Coherence container running: <markup lang=\"bash\" >$ java com.acme.Catalogue ",
            "title": "Set the Application Main"
        },
        {
            "location": "/docs/other/010_overview",
            "text": " Add Containers Adding side-car containers and init-containers. ",
            "title": "Containers"
        },
        {
            "location": "/docs/other/010_overview",
            "text": " Add Volumes Adding Volumes and volume mounts. Add ConfigMap Volumes Adding Volumes and volume mounts using ConfigMaps. Add Secret Volumes Adding Volumes and volume mounts using Secrets. ",
            "title": "Volumes"
        },
        {
            "location": "/docs/other/010_overview",
            "text": " Pod Scheduling Taints, Tolerations and node selectors. Resources Configuring Coherence container resource constraints. ",
            "title": "Pod Scheduling"
        },
        {
            "location": "/docs/other/010_overview",
            "text": " There are a number of miscellaneous configuration settings that can be added to containers and Pods controlled by the Coherence Operator. Environment Variables Adding environment variables to the Coherence container. Pod Labels Adding Pod labels. Pod Annotations Adding Pod annotations. Containers Add Containers Adding side-car containers and init-containers. Volumes Add Volumes Adding Volumes and volume mounts. Add ConfigMap Volumes Adding Volumes and volume mounts using ConfigMaps. Add Secret Volumes Adding Volumes and volume mounts using Secrets. Pod Scheduling Pod Scheduling Taints, Tolerations and node selectors. Resources Configuring Coherence container resource constraints. ",
            "title": "Overview"
        },
        {
            "location": "/docs/applications/050_application_args",
            "text": " When running a custom application there may be a requirement to pass arguments to the application&#8217;s main class. By default, there are no application arguments but any arguments required can be specified in the application.args list in the Coherence resource spec. The application.args is a list of string values, each value in the list is passed as an argument, in the order that they are specified in the list. For example, a deployment uses a custom image catalogue:1.0.0 that requires a custom main class called com.acme.Catalogue , and that class takes additional arguments. In this example we&#8217;ll use two fictitious arguments such as a name and a language for the catalogue. the Coherence resource would look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: main: com.acme.Catalogue args: - \"--name=Books\" - \"--language=en_GB\" The com.acme.Catalogue will be run as the main class. The arguments passed to the com.acme.Catalogue class will be --name=Books and --language=en_GB The example would be equivalent to the Coherence container running: <markup lang=\"bash\" >$ java com.acme.Catalogue --name=Books --language=en_GB ",
            "title": "Set Application Arguments"
        },
        {
            "location": "/docs/applications/050_application_args",
            "text": " The Operator supports environment variable expansion in program arguments. The runner in the Coherence container will replace ${var} or $var in the program arguments with the corresponding environment variable name. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: main: com.acme.Catalogue args: - \"${HOSTNAME}\" The argument passed to the com.acme.Catalogue main method will resolve to the value of the HOSTNAME environment variable. Any environment variable that is present when the Coherence container starts can be used, this would include variables created as part of the image and variables specified in the Coherence yaml. ",
            "title": "Environment Variable Expansion"
        },
        {
            "location": "/docs/ordering/010_overview",
            "text": " The startQuorum can specify a dependency on more than on deployment; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: data - deployment: proxy podCount: 1 The data and proxy deployments do not specify a startQuorum , so the StatefulSets for these deployments will be created immediately by the operator. The web deployment has a startQuorum the defines a dependency on both the data deployment and the proxy deployment. The proxy dependency also specifies a podCount of 1 . This means that the operator wil not create the web role&#8217;s StatefulSet until all 5 replicas of the data deployment are Ready and at least 1 of the proxy deployment&#8217;s Pods is Ready . ",
            "title": "Multiple Dependencies"
        },
        {
            "location": "/docs/ordering/010_overview",
            "text": " It is also possible to chain dependencies, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 startQuorum: - deployment: data --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: proxy podCount: 1 The data deployment does not specify a startQuorum so this deployment&#8217;s StatefulSet will be created immediately by the operator. The proxy deployment defines a dependency on the data deployment without a podCount so all five Pods of the data role must be in a Ready state before the operator will create the proxy deployment&#8217;s StatefulSet . The web deployment depends on the proxy deployment with a podCount of one, so the operator will not create the web deployment&#8217;s StatefulSet until at least one proxy deployment Pod is in a Ready state. The operator does not validate that a startQuorum makes sense. It is possible to declare a quorum with circular dependencies, in which case the roles will never start. It would also be possible to create a quorum with a podCount greater than the replicas value of the dependent deployment, in which case the quorum would never be met, and the role would not start. ",
            "title": "Chained Dependencies"
        },
        {
            "location": "/docs/ordering/010_overview",
            "text": " The default behaviour of the operator is to create the StatefulSet for a Coherence deployment immediately. Sometimes this behaviour is not suitable if, for example, when application code running in one deployment depends on the availability of another deployment. Typically, this might be storage disabled members having functionality that relies on the storage members being ready first. The Coherence CRD can be configured with a startQuorum that defines a deployment&#8217;s dependency on other deployments in the cluster. The startQuorum only applies when a cluster is initially being started by the operator, it does not apply in other functions such as upgrades, scaling, shut down etc. An individual deployment can depend on one or more other deployment. The dependency can be such that the deployment will not be created until all of the Pods of the dependent deployment are ready, or it can be configured so that just a single Pod of the dependent deployment must be ready. For example: In the yaml snippet below there are two Coherence deployments, data and proxy <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: startQuorum: - deployment: data podCount: 1 The data deployment does not specify a startQuorum so this role will be created immediately by the operator. The proxy deployment has a start quorum that means that the proxy deployment depends on the data deployment. The podCount field has been set to 1 meaning the proxy deployment&#8217;s StatefulSet will not be created until at least 1 of the data deployment&#8217;s Pods is in the Ready state. Omitting the podCount from the quorum means that the role will not start until all the configured replicas of the dependent deployment are ready; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: startQuorum: - deployment: data The proxy deployment&#8217;s startQuorum just specifies a dependency on the data deployment with no podCount so all 3 of the data deployment&#8217;s Pods must be Ready before the proxy deployment&#8217;s StatefulSet is created by the operator. Setting a podCount less than or equal to zero is the same as not specifying a count. Multiple Dependencies The startQuorum can specify a dependency on more than on deployment; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: data - deployment: proxy podCount: 1 The data and proxy deployments do not specify a startQuorum , so the StatefulSets for these deployments will be created immediately by the operator. The web deployment has a startQuorum the defines a dependency on both the data deployment and the proxy deployment. The proxy dependency also specifies a podCount of 1 . This means that the operator wil not create the web role&#8217;s StatefulSet until all 5 replicas of the data deployment are Ready and at least 1 of the proxy deployment&#8217;s Pods is Ready . Chained Dependencies It is also possible to chain dependencies, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data spec: replicas: 5 --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: proxy spec: replicas: 3 startQuorum: - deployment: data --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web spec: startQuorum: - deployment: proxy podCount: 1 The data deployment does not specify a startQuorum so this deployment&#8217;s StatefulSet will be created immediately by the operator. The proxy deployment defines a dependency on the data deployment without a podCount so all five Pods of the data role must be in a Ready state before the operator will create the proxy deployment&#8217;s StatefulSet . The web deployment depends on the proxy deployment with a podCount of one, so the operator will not create the web deployment&#8217;s StatefulSet until at least one proxy deployment Pod is in a Ready state. The operator does not validate that a startQuorum makes sense. It is possible to declare a quorum with circular dependencies, in which case the roles will never start. It would also be possible to create a quorum with a podCount greater than the replicas value of the dependent deployment, in which case the quorum would never be met, and the role would not start. ",
            "title": "Coherence Deployment Dependencies and Start Order"
        },
        {
            "location": "/docs/coherence/020_cluster_name",
            "text": " The default Coherence cluster name, used when the cluster field is empty, will be the same as the name of the Coherence resource, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test The name of this Coherence resource is test , which will also be used as the Coherence cluster name, effectively passing -Dcoherence.cluster=test to the JVM in the Coherence container. ",
            "title": "Default Cluster Name"
        },
        {
            "location": "/docs/coherence/020_cluster_name",
            "text": " In a use case where multiple Coherence resources will be created to form a single Coherence cluster, the cluster field in all the Coherence resources needs to be set to the same value. <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: cluster: test-cluster --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: front-end spec: cluster: test-cluster The yaml above contains two Coherence resources, one named storage and one named front-end . Both of these Coherence resources have the same value for the cluster field, test-cluster , so the Pods in both deployments will form a single Coherence cluster named test . ",
            "title": "Specify a Cluster Name"
        },
        {
            "location": "/docs/coherence/020_cluster_name",
            "text": " The name of the Coherence cluster that a Coherence resource is part of can be set with the cluster field in the Coherence.Spec . The cluster name is used to set the coherence.cluster system property in the JVM in the Coherence container. Default Cluster Name The default Coherence cluster name, used when the cluster field is empty, will be the same as the name of the Coherence resource, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test The name of this Coherence resource is test , which will also be used as the Coherence cluster name, effectively passing -Dcoherence.cluster=test to the JVM in the Coherence container. Specify a Cluster Name In a use case where multiple Coherence resources will be created to form a single Coherence cluster, the cluster field in all the Coherence resources needs to be set to the same value. <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: cluster: test-cluster --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: front-end spec: cluster: test-cluster The yaml above contains two Coherence resources, one named storage and one named front-end . Both of these Coherence resources have the same value for the cluster field, test-cluster , so the Pods in both deployments will form a single Coherence cluster named test . ",
            "title": "Set Coherence Cluster Name"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " Coherence persistence is a set of tools and technologies that manage the persistence and recovery of Coherence distributed caches. Cached data can be persisted so that it can be quickly recovered after a catastrophic failure or after a cluster restart due to planned maintenance. Persistence and federated caching can be used together as required. ",
            "title": "preambule"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " The Coherence CRD allows the default persistence mode, and the storage location of persistence data to be configured. Persistence can be configured in the spec.coherence.persistence section of the CRD. See the Coherence Persistence documentation for more details of how persistence works and its configuration. ",
            "title": "Configure Coherence Persistence"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " There are four default persistence modes available, active , active-async , active-backup , and on-demand ; the default mode is on-demand . The persistence mode will be set using the spec.coherence.persistence,mode field in the CRD. The value of this field will be used to set the coherence.distributed.persistence.mode system property in the Coherence JVM. active-backup persistence mode is only available in the most recent versions of Coherence. Please check the release notes for your version. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: mode: active The example above sets the persistence mode to active which will effectively pass -Dcoherence.distributed.persistence-mode=active to the Coherence JVM&#8217;s command line. ",
            "title": "Persistence Mode"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " The Coherence Operator creates a StatefulSet for each Coherence resource, so the logical place to store persistence data is in a PersistentVolumeClaim . The PVC used for persistence can be configured in the spec.coherence.persistence.persistentVolumeClaim section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: persistentVolumeClaim: storageClassName: \"SSD\" accessModes: - ReadWriteOnce resources: requests: storage: 50Gi The example above configures a 50GB PVC with a storage class name of \"SSD\" (assuming the Kubernetes cluster has a storage class of that name configured). The configuration under the spec.coherence.persistence.persistentVolumeClaim section is exactly the same as configuring a PVC for a normal Kubernetes Pod and all the possible options are beyond the scope of this document. For more details on configuring PVC, see the Kubernetes Persistent Volumes documentation. ",
            "title": "Using a PersistentVolumeClaim"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " An alternative to a PVC is to use a normal Kubernetes Volume to store the persistence data. An example of this use-case could be when the Kubernetes Nodes that the Coherence Pods are scheduled onto have locally attached fast SSD drives, which is ideal storage for persistence. In this case a normal Volume can be configured in the spec.coherence.persistence.volume section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence In the example above a Volume has been configured for persistence, in this case a HostPath volume pointing to the /mnt/ssd/coherence/persistence directory on the Node. The configuration under the spec.coherence.persistence.volume section is a normal Kubernetes VolumeSource so any valid VolumeSource configuration can be used. See the Kubernetes Volumes documentation for more details. ",
            "title": "Using a Normal Volume"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " The purpose of persistence in Coherence is to store data on disc so that it is available outside of the lifetime of the JVMs that make up the cluster. In a containerised environment like Kubernetes this means storing that data in storage that also lives outside of the containers. When persistence storage has been configured a VolumeMount will be added to the Coherence container mounted at /persistence , and the coherence.distributed.persistence.base.dir system property will be configured to point to the storage location. Using a PersistentVolumeClaim The Coherence Operator creates a StatefulSet for each Coherence resource, so the logical place to store persistence data is in a PersistentVolumeClaim . The PVC used for persistence can be configured in the spec.coherence.persistence.persistentVolumeClaim section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: persistentVolumeClaim: storageClassName: \"SSD\" accessModes: - ReadWriteOnce resources: requests: storage: 50Gi The example above configures a 50GB PVC with a storage class name of \"SSD\" (assuming the Kubernetes cluster has a storage class of that name configured). The configuration under the spec.coherence.persistence.persistentVolumeClaim section is exactly the same as configuring a PVC for a normal Kubernetes Pod and all the possible options are beyond the scope of this document. For more details on configuring PVC, see the Kubernetes Persistent Volumes documentation. Using a Normal Volume An alternative to a PVC is to use a normal Kubernetes Volume to store the persistence data. An example of this use-case could be when the Kubernetes Nodes that the Coherence Pods are scheduled onto have locally attached fast SSD drives, which is ideal storage for persistence. In this case a normal Volume can be configured in the spec.coherence.persistence.volume section of the CRD. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence In the example above a Volume has been configured for persistence, in this case a HostPath volume pointing to the /mnt/ssd/coherence/persistence directory on the Node. The configuration under the spec.coherence.persistence.volume section is a normal Kubernetes VolumeSource so any valid VolumeSource configuration can be used. See the Kubernetes Volumes documentation for more details. ",
            "title": "Persistence Storage"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " A PVC can be configured for persistence snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: persistentVolumeClaim: resources: requests: storage: 50Gi Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored in a 50GB PVC. ",
            "title": "Snapshots Using a PersistentVolumeClaim"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " A normal volume can be configured for snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: volume: hostPath: path: /mnt/ssd/coherence/snapshots Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored on a normal Volume using a different HostPath volume source. ",
            "title": "Snapshots Using a Normal Volumes"
        },
        {
            "location": "/docs/coherence/080_persistence",
            "text": " Coherence allows on-demand snapshots to be taken of cache data. With the default configuration the snapshot files will be stored under the same persistence root location as active persistence data. The Coherence spec allows a different location to be specified for storage of snapshot files so that active data and snapshot data can be stored in different locations and/or on different storage types in Kubernetes. The same two options are available for snapshot storage that are available for persistence storage, namely PVCs and normal Volumes. The spec.coherence.persistence.snapshots section is used to configure snapshot storage. When this is used a VolumeMount will be added to the Coherence container with a mount path of /snapshots , and the coherence.distributed.persistence.snapshot.dir system property will be set to point to this location. Snapshots Using a PersistentVolumeClaim A PVC can be configured for persistence snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: persistentVolumeClaim: resources: requests: storage: 50Gi Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored in a 50GB PVC. Snapshots Using a Normal Volumes A normal volume can be configured for snapshot data as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: persistence: volume: hostPath: path: /mnt/ssd/coherence/persistence snapshots: volume: hostPath: path: /mnt/ssd/coherence/snapshots Active persistence data will be stored on a normal Volume using a HostPath volume source. Snapshot data will be stored on a normal Volume using a different HostPath volume source. ",
            "title": "Snapshot Storage"
        },
        {
            "location": "/docs/ports/040_servicemonitors",
            "text": " The Coherence CRD ServiceMonitorSpec contains many of the fields from the Prometheus ServiceMonitorSpec and Prometheus Endpoint to allow the ServiceMonitor to be configured for most use-cases. In situations where the Coherence CRD does not have the required fields, for example when a different version of Prometheus has been installed to that used to build the Coherence Operator, then the solution would be to manually create ServiceMonitors instead of letting them be created by the Coherence Operator. ",
            "title": "Configure the ServiceMonitor"
        },
        {
            "location": "/docs/ports/040_servicemonitors",
            "text": " When a port exposed on a container is to be used to serve metrics to Prometheus this often requires the addition of a Prometheus ServiceMonitor resource. The Coherence Operator makes it simple to add a ServiceMonitor for an exposed port. The advantage of specifying the ServiceMonitor configuration in the Coherence CRD spec is that the ServiceMonitor resource will be created, updated and deleted as part of the lifecycle of the Coherence resource, and does not need to be managed separately. A ServiceMonitor is created for an exposed port by setting the serviceMonitor.enabled field to true . The Operator will create a ServiceMonitor with the same name as the Service . The ServiceMonitor created will have a single endpoint for the port being exposed. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 serviceMonitor: enabled: true With the serviceMonitor.enabled field set to true a ServiceMonitor resource will be created. The ServiceMonitor created from the spec above will look like this: For example: <markup lang=\"yaml\" >apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: test-cluster-rest labels: coherenceCluster: test-cluster coherenceComponent: coherence-service-monitor coherenceDeployment: test-cluster coherenceRole: test-cluster spec: endpoints: - port: rest relabelings: - action: labeldrop regex: (endpoint|instance|job|service) selector: matchLabels: coherenceCluster: test-cluster coherenceComponent: coherence-service coherenceDeployment: test-cluster coherencePort: rest coherenceRole: test-cluster Configure the ServiceMonitor The Coherence CRD ServiceMonitorSpec contains many of the fields from the Prometheus ServiceMonitorSpec and Prometheus Endpoint to allow the ServiceMonitor to be configured for most use-cases. In situations where the Coherence CRD does not have the required fields, for example when a different version of Prometheus has been installed to that used to build the Coherence Operator, then the solution would be to manually create ServiceMonitors instead of letting them be created by the Coherence Operator. ",
            "title": "Prometheus ServiceMonitors"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " This example shows how to build a simple Coherence server image using a Dockerfile . This image is built so that ot works out of the box with the Operator, with no additional configuration. This is an alternative to the Coherence Image using JIB example. There are many build tools and plugins for Maven and Gradle that are supposed to make building images easy. Sometimes though, a simple Dockerfile approach is required. A typical Coherence application image will still need to pull together various Coherence dependencies to add to the image. This simple application does not actually contain any code, a real application would likely contain code and other resources. Tip The complete source code for this example is in the Coherence Operator GitHub repository. ",
            "title": "Example Coherence Image using a Dockerfile"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " The Dockerfile for the example is shown below: <markup title=\"src/docker/Dockerfile\" >FROM gcr.io/distroless/java11-debian11 # Configure the image's health check command # Health checks will only work with Coherence 22.06 and later HEALTHCHECK --start-period=10s --interval=30s \\ CMD [\"java\", \\ \"-cp\", \"/app/libs/coherence.jar\", \\ \"com.tangosol.util.HealthCheckClient\", \\ \"http://127.0.0.1:6676/ready\", \\ \"||\", \"exit\", \"1\"] # Expose any default ports # The default Coherence Extend port EXPOSE 20000 # The default Coherence gRPC port EXPOSE 1408 # The default Coherence metrics port EXPOSE 9612 # The default Coherence health port EXPOSE 6676 # Set the entry point to be the Java command to run ENTRYPOINT [\"java\", \"-cp\", \"/app/classes:/app/libs/*\", \"com.tangosol.net.Coherence\"] # Set any environment variables # Set the health check port to a fixed value (corresponding to the command above) ENV COHERENCE_HEALTH_HTTP_PORT=6676 # Fix the Extend Proxy to listen on port 20000 ENV COHERENCE_EXTEND_PORT=20000 # Enable Coherence metics ENV COHERENCE_METRICS_HTTP_ENABLED=true # Set the Coherence log level to debug logging ENV COHERENCE_LOG_LEVEL=9 # Effectively disabled multicast cluster discovery, which does not work in containers ENV COHERENCE_TTL=0 # Copy all the application files into the /app directory in the image # This is the default structure supported by the Coherence Operator COPY app app Base Image The base image for this example is a distroless Java 11 image gcr.io/distroless/java11-debian11 Health Check The image is configured with a health check that uses the built-in Coherence health check on port 6676. Expose Ports A number of default Coherence ports are exposed. Entrypoint The image entry point will run com.tangosol.net.Coherence to run a Coherence storage enabled server. The classpath is set to /app/classes:/app/libs/* . This is the same classpath that the JIB plugin would add artifacts to and is also supported out of the box by the Coherence operator. Environment Variables A number of environment variables are set to configure Coherence. These values could have been set as system properties in the entry point, but using environment variables is a simpler option when running containers as they can easily be overridden at deploy time. Copy the Image Artifacts The Maven and Gradle build will copy all the classes and dependencies into a directory named app/ in the same directory as the Dockerfile . Using COPY app app will copy all the files into the image. ",
            "title": "The Dockerfile"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " In the example the coherence-bom is added to the &lt;dependencyManagement&gt; section as an import, to ensure consistent versioning of other Coherence modules. In the pom.xml we have a dependencyManagement section. <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; We can then add the coherence coherence-json and coherence-grpc-proxy modules as dependencies <markup lang=\"xml\" title=\"pom.xml\" > &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-json&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; ",
            "title": "Adding Dependencies"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " This example will use the Maven Assembly Plugin to gather all the dependencies and other files together into the target/docker directory. The assembly plugin is configured in the pom.xml file. The assembly plugin is configured to use the src/assembly/image-assembly.xml descriptor file to determine what to assemble. The &lt;finalName&gt; configuration element is set to docker so all the files will be assembled into a directory named docker/ under the target/ directory. The assembly plugin execution is bound to the package build phase. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.assembly.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;prepare-image&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;finalName&gt;docker&lt;/finalName&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptors&gt; &lt;descriptor&gt;${project.basedir}/src/assembly/image-assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;attach&gt;false&lt;/attach&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; The image-assembly.xml descriptor file is shown below, and configures the following: The &lt;format&gt;dir&lt;/format&gt; element tells the assembly plugin to assemble all the artifacts into a directory. There are two &lt;fileSets&gt; configured: The first copies any class files in target/classes to app/classes (which will actually be target/docker/app/classes ) The second copies all files under src/docker (i.e. the Dockerfile ) into target/docker The &lt;dependencySets&gt; configuration copies all the project dependencies (including transitive dependencies) to the app/libs directory (actually the target/docker/app/libs directory). Any version information will be stripped from the files, so coherence-22.06.7.jar would become coherence.jar . <markup lang=\"xml\" title=\"src/assembly/image-assembly.xml\" >&lt;assembly xmlns=\"http://maven.apache.org/ASSEMBLY/2.1.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/ASSEMBLY/2.1.0 http://maven.apache.org/xsd/assembly-2.1.0.xsd\"&gt; &lt;id&gt;image&lt;/id&gt; &lt;formats&gt; &lt;format&gt;dir&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;!-- copy the module's compiled classes --&gt; &lt;fileSet&gt; &lt;directory&gt;target/classes&lt;/directory&gt; &lt;outputDirectory&gt;app/classes&lt;/outputDirectory&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;filtered&gt;false&lt;/filtered&gt; &lt;/fileSet&gt; &lt;!-- copy the Dockerfile --&gt; &lt;fileSet&gt; &lt;directory&gt;${project.basedir}/src/docker&lt;/directory&gt; &lt;outputDirectory/&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;!-- copy the application dependencies --&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;outputDirectory&gt;app/libs&lt;/outputDirectory&gt; &lt;directoryMode&gt;755&lt;/directoryMode&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;unpack&gt;false&lt;/unpack&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; &lt;!-- strip the version from the jar files --&gt; &lt;outputFileNameMapping&gt;${artifact.artifactId}${dashClassifier?}.${artifact.extension}&lt;/outputFileNameMapping&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;/assembly&gt; Running the following command will pull all the required image artifacts and Dockerfile into the target/docker directory: <markup lang=\"bash\" >./mvnw package ",
            "title": "Assembling the Image Artifacts"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " To assemble the image artifacts using Maven, everything is configured in the Maven pom.xml file. The Maven build will pull all the artifacts required in the image, including the Dockerfile into a directory under target\\docker . Adding Dependencies In the example the coherence-bom is added to the &lt;dependencyManagement&gt; section as an import, to ensure consistent versioning of other Coherence modules. In the pom.xml we have a dependencyManagement section. <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; We can then add the coherence coherence-json and coherence-grpc-proxy modules as dependencies <markup lang=\"xml\" title=\"pom.xml\" > &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-json&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Assembling the Image Artifacts This example will use the Maven Assembly Plugin to gather all the dependencies and other files together into the target/docker directory. The assembly plugin is configured in the pom.xml file. The assembly plugin is configured to use the src/assembly/image-assembly.xml descriptor file to determine what to assemble. The &lt;finalName&gt; configuration element is set to docker so all the files will be assembled into a directory named docker/ under the target/ directory. The assembly plugin execution is bound to the package build phase. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.assembly.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;prepare-image&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;finalName&gt;docker&lt;/finalName&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptors&gt; &lt;descriptor&gt;${project.basedir}/src/assembly/image-assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;attach&gt;false&lt;/attach&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; The image-assembly.xml descriptor file is shown below, and configures the following: The &lt;format&gt;dir&lt;/format&gt; element tells the assembly plugin to assemble all the artifacts into a directory. There are two &lt;fileSets&gt; configured: The first copies any class files in target/classes to app/classes (which will actually be target/docker/app/classes ) The second copies all files under src/docker (i.e. the Dockerfile ) into target/docker The &lt;dependencySets&gt; configuration copies all the project dependencies (including transitive dependencies) to the app/libs directory (actually the target/docker/app/libs directory). Any version information will be stripped from the files, so coherence-22.06.7.jar would become coherence.jar . <markup lang=\"xml\" title=\"src/assembly/image-assembly.xml\" >&lt;assembly xmlns=\"http://maven.apache.org/ASSEMBLY/2.1.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/ASSEMBLY/2.1.0 http://maven.apache.org/xsd/assembly-2.1.0.xsd\"&gt; &lt;id&gt;image&lt;/id&gt; &lt;formats&gt; &lt;format&gt;dir&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;!-- copy the module's compiled classes --&gt; &lt;fileSet&gt; &lt;directory&gt;target/classes&lt;/directory&gt; &lt;outputDirectory&gt;app/classes&lt;/outputDirectory&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;filtered&gt;false&lt;/filtered&gt; &lt;/fileSet&gt; &lt;!-- copy the Dockerfile --&gt; &lt;fileSet&gt; &lt;directory&gt;${project.basedir}/src/docker&lt;/directory&gt; &lt;outputDirectory/&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;!-- copy the application dependencies --&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;outputDirectory&gt;app/libs&lt;/outputDirectory&gt; &lt;directoryMode&gt;755&lt;/directoryMode&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;unpack&gt;false&lt;/unpack&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; &lt;!-- strip the version from the jar files --&gt; &lt;outputFileNameMapping&gt;${artifact.artifactId}${dashClassifier?}.${artifact.extension}&lt;/outputFileNameMapping&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;/assembly&gt; Running the following command will pull all the required image artifacts and Dockerfile into the target/docker directory: <markup lang=\"bash\" >./mvnw package ",
            "title": "Using Maven"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " In the example the coherence-bom is added to the &lt;dependencyManagement&gt; section as an import, to ensure consistent versioning of other Coherence modules. In the build.gradle file we add the bom as a platform dependency and then add dependencies on coherence and coherence-json . <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"com.oracle.coherence.ce:coherence-bom:22.06.7\") implementation \"com.oracle.coherence.ce:coherence\" implementation \"com.oracle.coherence.ce:coherence-json\" implementation \"com.oracle.coherence.ce:coherence-grpc-proxy\" } ",
            "title": "Adding Dependencies"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " To assemble all the image artifacts into the build/docker directory, the Gradle copy task can be used. There will be multiple copy tasks to copy each type of artifact, the dependencies, any compile classes, and the Dockerfile . The following task named copyDependencies is added to build.gradle to copy the dependencies. This task has additional configuration to rename the jar files to strip off any version. <markup lang=\"groovy\" title=\"build.gradle\" >task copyDependencies(type: Copy) { from configurations.runtimeClasspath into \"$buildDir/docker/app/libs\" configurations.runtimeClasspath.resolvedConfiguration.resolvedArtifacts.each { rename \"${it.artifact.name}-${it.artifactId.componentIdentifier.version}\", \"${it.artifact.name}\" } } The following task named copyClasses copies any compiled classes (although this example does not actually have any). <markup lang=\"groovy\" title=\"build.gradle\" >task copyClasses(type: Copy) { dependsOn classes from \"$buildDir/classes/java/main\" into \"$buildDir/docker/app/classes\" } The final copy task named copyDocker copies the contents of the src/docker directory: <markup lang=\"groovy\" title=\"build.gradle\" >task copyDocker(type: Copy) { from \"src/docker\" into \"$buildDir/docker\" } To be able to run the image assembly as a single command, an empty task named `` is created that depends on all the copy tasks. Running the following command will pull all the required image artifacts and Dockerfile into the build/docker directory: <markup lang=\"bash\" >./gradlew assembleImage ",
            "title": "Assembling the Image Artifacts"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " To assemble the image artifacts using Maven, everything is configured in the Maven build.gradle file. The Gradle build will pull all the artifacts required in the image, including the Dockerfile into a directory under build\\docker . Adding Dependencies In the example the coherence-bom is added to the &lt;dependencyManagement&gt; section as an import, to ensure consistent versioning of other Coherence modules. In the build.gradle file we add the bom as a platform dependency and then add dependencies on coherence and coherence-json . <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"com.oracle.coherence.ce:coherence-bom:22.06.7\") implementation \"com.oracle.coherence.ce:coherence\" implementation \"com.oracle.coherence.ce:coherence-json\" implementation \"com.oracle.coherence.ce:coherence-grpc-proxy\" } Assembling the Image Artifacts To assemble all the image artifacts into the build/docker directory, the Gradle copy task can be used. There will be multiple copy tasks to copy each type of artifact, the dependencies, any compile classes, and the Dockerfile . The following task named copyDependencies is added to build.gradle to copy the dependencies. This task has additional configuration to rename the jar files to strip off any version. <markup lang=\"groovy\" title=\"build.gradle\" >task copyDependencies(type: Copy) { from configurations.runtimeClasspath into \"$buildDir/docker/app/libs\" configurations.runtimeClasspath.resolvedConfiguration.resolvedArtifacts.each { rename \"${it.artifact.name}-${it.artifactId.componentIdentifier.version}\", \"${it.artifact.name}\" } } The following task named copyClasses copies any compiled classes (although this example does not actually have any). <markup lang=\"groovy\" title=\"build.gradle\" >task copyClasses(type: Copy) { dependsOn classes from \"$buildDir/classes/java/main\" into \"$buildDir/docker/app/classes\" } The final copy task named copyDocker copies the contents of the src/docker directory: <markup lang=\"groovy\" title=\"build.gradle\" >task copyDocker(type: Copy) { from \"src/docker\" into \"$buildDir/docker\" } To be able to run the image assembly as a single command, an empty task named `` is created that depends on all the copy tasks. Running the following command will pull all the required image artifacts and Dockerfile into the build/docker directory: <markup lang=\"bash\" >./gradlew assembleImage ",
            "title": "Using Gradle"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " The next step is to assemble all the artifacts required to build the image. Looking at the Dockerfile above, this means copying any dependencies and other files into a directory named app/ in the same directory that the Dockerfile is in. This example contains both a Maven pom.xml file and Gradle build files, that show how to use these tools to gather all the files required for the image. There are other build tools such as make or ant or just plain scripts, but as the task involves pulling together all the Coherence jar files from Maven central, it is simplest to use Maven or Gradle. To build a Coherence application there will obviously be at a minimum a dependency on coherence.jar . Optionally we can also add dependencies on other Coherence modules and other dependencies, for example Coherence coul dbe configured to use SLF4J for logging. In this example we&#8217;re going to add json support to the application by adding a dependency on coherence-json and coherence-grpc-proxy . Jump to the relevant section, depending on the build tool being used: Using Maven Using Gradle Using Maven To assemble the image artifacts using Maven, everything is configured in the Maven pom.xml file. The Maven build will pull all the artifacts required in the image, including the Dockerfile into a directory under target\\docker . Adding Dependencies In the example the coherence-bom is added to the &lt;dependencyManagement&gt; section as an import, to ensure consistent versioning of other Coherence modules. In the pom.xml we have a dependencyManagement section. <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; We can then add the coherence coherence-json and coherence-grpc-proxy modules as dependencies <markup lang=\"xml\" title=\"pom.xml\" > &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-json&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Assembling the Image Artifacts This example will use the Maven Assembly Plugin to gather all the dependencies and other files together into the target/docker directory. The assembly plugin is configured in the pom.xml file. The assembly plugin is configured to use the src/assembly/image-assembly.xml descriptor file to determine what to assemble. The &lt;finalName&gt; configuration element is set to docker so all the files will be assembled into a directory named docker/ under the target/ directory. The assembly plugin execution is bound to the package build phase. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.assembly.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;prepare-image&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;finalName&gt;docker&lt;/finalName&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptors&gt; &lt;descriptor&gt;${project.basedir}/src/assembly/image-assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;attach&gt;false&lt;/attach&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; The image-assembly.xml descriptor file is shown below, and configures the following: The &lt;format&gt;dir&lt;/format&gt; element tells the assembly plugin to assemble all the artifacts into a directory. There are two &lt;fileSets&gt; configured: The first copies any class files in target/classes to app/classes (which will actually be target/docker/app/classes ) The second copies all files under src/docker (i.e. the Dockerfile ) into target/docker The &lt;dependencySets&gt; configuration copies all the project dependencies (including transitive dependencies) to the app/libs directory (actually the target/docker/app/libs directory). Any version information will be stripped from the files, so coherence-22.06.7.jar would become coherence.jar . <markup lang=\"xml\" title=\"src/assembly/image-assembly.xml\" >&lt;assembly xmlns=\"http://maven.apache.org/ASSEMBLY/2.1.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/ASSEMBLY/2.1.0 http://maven.apache.org/xsd/assembly-2.1.0.xsd\"&gt; &lt;id&gt;image&lt;/id&gt; &lt;formats&gt; &lt;format&gt;dir&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;!-- copy the module's compiled classes --&gt; &lt;fileSet&gt; &lt;directory&gt;target/classes&lt;/directory&gt; &lt;outputDirectory&gt;app/classes&lt;/outputDirectory&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;filtered&gt;false&lt;/filtered&gt; &lt;/fileSet&gt; &lt;!-- copy the Dockerfile --&gt; &lt;fileSet&gt; &lt;directory&gt;${project.basedir}/src/docker&lt;/directory&gt; &lt;outputDirectory/&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;!-- copy the application dependencies --&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;outputDirectory&gt;app/libs&lt;/outputDirectory&gt; &lt;directoryMode&gt;755&lt;/directoryMode&gt; &lt;fileMode&gt;755&lt;/fileMode&gt; &lt;unpack&gt;false&lt;/unpack&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; &lt;!-- strip the version from the jar files --&gt; &lt;outputFileNameMapping&gt;${artifact.artifactId}${dashClassifier?}.${artifact.extension}&lt;/outputFileNameMapping&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;/assembly&gt; Running the following command will pull all the required image artifacts and Dockerfile into the target/docker directory: <markup lang=\"bash\" >./mvnw package Using Gradle To assemble the image artifacts using Maven, everything is configured in the Maven build.gradle file. The Gradle build will pull all the artifacts required in the image, including the Dockerfile into a directory under build\\docker . Adding Dependencies In the example the coherence-bom is added to the &lt;dependencyManagement&gt; section as an import, to ensure consistent versioning of other Coherence modules. In the build.gradle file we add the bom as a platform dependency and then add dependencies on coherence and coherence-json . <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"com.oracle.coherence.ce:coherence-bom:22.06.7\") implementation \"com.oracle.coherence.ce:coherence\" implementation \"com.oracle.coherence.ce:coherence-json\" implementation \"com.oracle.coherence.ce:coherence-grpc-proxy\" } Assembling the Image Artifacts To assemble all the image artifacts into the build/docker directory, the Gradle copy task can be used. There will be multiple copy tasks to copy each type of artifact, the dependencies, any compile classes, and the Dockerfile . The following task named copyDependencies is added to build.gradle to copy the dependencies. This task has additional configuration to rename the jar files to strip off any version. <markup lang=\"groovy\" title=\"build.gradle\" >task copyDependencies(type: Copy) { from configurations.runtimeClasspath into \"$buildDir/docker/app/libs\" configurations.runtimeClasspath.resolvedConfiguration.resolvedArtifacts.each { rename \"${it.artifact.name}-${it.artifactId.componentIdentifier.version}\", \"${it.artifact.name}\" } } The following task named copyClasses copies any compiled classes (although this example does not actually have any). <markup lang=\"groovy\" title=\"build.gradle\" >task copyClasses(type: Copy) { dependsOn classes from \"$buildDir/classes/java/main\" into \"$buildDir/docker/app/classes\" } The final copy task named copyDocker copies the contents of the src/docker directory: <markup lang=\"groovy\" title=\"build.gradle\" >task copyDocker(type: Copy) { from \"src/docker\" into \"$buildDir/docker\" } To be able to run the image assembly as a single command, an empty task named `` is created that depends on all the copy tasks. Running the following command will pull all the required image artifacts and Dockerfile into the build/docker directory: <markup lang=\"bash\" >./gradlew assembleImage ",
            "title": "Assemble the Image Directory"
        },
        {
            "location": "/examples/016_simple_docker_image/README",
            "text": " After running the Maven or Gradle commands to assemble the image artifacts, Docker can be used to actually build the image from the relevant docker/ directory. Using Maven: <markup lang=\"bash\" >cd target/docker docker build -t simple-coherence-server:1.0.0 . Using Gradle: <markup lang=\"bash\" >cd build/docker docker build -t simple-coherence-server:1.0.0 . The command above will create an image named simple-coherence-server:1.0.0 . Listing the local images should show the new images, similar to the output below: <markup lang=\"bash\" >$ docker images | grep simple simple-coherence-server 1.0.0 1613cd3b894e 51 years ago 227MB ",
            "title": "Build the Image"
        },
        {
            "location": "/docs/metrics/030_importing",
            "text": " The Coherence Operator provides a set of dashboards for Coherence that may be imported into Grafana. The Coherence dashboards are explained in detail on the Coherence Grafana Dashboards page. There are two ways to obtain the dashboards: 1 - Download the .tar.gz dashboards package for the release you want to use. <markup lang=\"bash\" >curl https://oracle.github.io/coherence-operator/dashboards/latest/coherence-dashboards.tar.gz \\ -o coherence-dashboards.tar.gz tar -zxvf coherence-dashboards.tar.gz The above commands will download the coherence-dashboards.tar.gz file and unpack it resulting in a directory named dashboards/ in the current working directory. This dashboards/ directory will contain the various Coherence dashboard files. 2 - Clone the Coherence Operator GitHub repo, checkout the branch or tag for the version you want to use and then obtain the dashboards from the dashboards/ directory. The recommended versions of Grafana to use are: 8.5.6 or 6.7.4 . It is not yet recommended to use the 9.x versions of Grafana as there are a number of bugs that cause issues when using the dashboards. ",
            "title": "Importing the Coherence Dashboards"
        },
        {
            "location": "/docs/metrics/030_importing",
            "text": " The dashboard .json files can be manually imported into Grafana using the Grafana UI following the instructions in the Grafana Import Dashboards documentation. ",
            "title": "Manually Import Grafana Dashboards"
        },
        {
            "location": "/docs/metrics/030_importing",
            "text": " At the time of writing, for whatever reason, Grafana does not provide a simple way to bulk import a set of dashboard files. There are many examples and scripts on available in the community that show how to do this. The Coherence Operator source contains a script that can be used for this purpose grafana-import.sh The grafana-import.sh script requires the JQ utility to parse json. The commands below will download and run the shell script to import the dashboards. Change the &lt;GRAFANA-USER&gt; and &lt;GRAFANA_PWD&gt; to the Grafana credentials for your environment. For example if using the default Prometheus Operator installation they are as specified on the Access Grafana section of the Quick Start page. We do not document the credentials here as the default values have been known to change between Prometheus Operator and Grafana versions. <markup lang=\"bash\" >curl -Lo grafana-import.sh https://github.com/oracle/coherence-operator/raw/main/hack/grafana-import.sh chmod +x grafana-import.sh <markup lang=\"bash\" >./grafana-import.sh -u &lt;GRAFANA-USER&gt; -w &lt;GRAFANA_PWD&gt; -d dashboards/grafana -t localhost:3000 Note: the command above assumes you can reach Grafana on localhost:3000 (for example, if you have a kubectl port forward process running to forward localhost:3000 to the Grafana service in Kubernetes). You may need to change the host and port to match however you are exposing your Grafana instance. Coherence clusters can now be created as described in the Publish Metrics page, and metrics will eventually appear in Prometheus and Grafana. It can sometimes take a minute or so for Prometheus to start scraping metrics and for them to appear in Grafana. ",
            "title": "Bulk Import Grafana Dashboards"
        },
        {
            "location": "/docs/metrics/030_importing",
            "text": " This section shows you how to import the Grafana dashboards into your own Grafana instance. Once you have obtained the dashboards using one of the methods above, the Grafana dashboard .json files will be in the dashboards/grafana/ subdirectory Important By default, the Coherence dashboards require a datasource in Grafana named prometheus (which is case-sensitive). This datasource usually exists in an out-of-the-box Prometheus Operator installation. If your Grafana environment does not have this datasource, then there are two choices. Create a Prometheus datasource named prometheus as described in the Grafana Add a Datasource documentation and make this the default datasource. If you have an existing Prometheus datasource with a different name then you will need to edit the dashboard json files to change all occurrences of \"datasource\": \"prometheus\" to have the name of your Prometheus datasource. For example, running the script below in the directory containing the datasource .json files to be imported will change the datasource name from prometheus to Coherence-Prometheus . for file in *.json do sed -i '' -e 's/\"datasource\": \"prometheus\"/\"datasource\": \"Coherence-Prometheus\"/g' $file; done The above sed command works for MacOS, but if you are running on Linux you need to remove the '' after the -i . Manually Import Grafana Dashboards The dashboard .json files can be manually imported into Grafana using the Grafana UI following the instructions in the Grafana Import Dashboards documentation. Bulk Import Grafana Dashboards At the time of writing, for whatever reason, Grafana does not provide a simple way to bulk import a set of dashboard files. There are many examples and scripts on available in the community that show how to do this. The Coherence Operator source contains a script that can be used for this purpose grafana-import.sh The grafana-import.sh script requires the JQ utility to parse json. The commands below will download and run the shell script to import the dashboards. Change the &lt;GRAFANA-USER&gt; and &lt;GRAFANA_PWD&gt; to the Grafana credentials for your environment. For example if using the default Prometheus Operator installation they are as specified on the Access Grafana section of the Quick Start page. We do not document the credentials here as the default values have been known to change between Prometheus Operator and Grafana versions. <markup lang=\"bash\" >curl -Lo grafana-import.sh https://github.com/oracle/coherence-operator/raw/main/hack/grafana-import.sh chmod +x grafana-import.sh <markup lang=\"bash\" >./grafana-import.sh -u &lt;GRAFANA-USER&gt; -w &lt;GRAFANA_PWD&gt; -d dashboards/grafana -t localhost:3000 Note: the command above assumes you can reach Grafana on localhost:3000 (for example, if you have a kubectl port forward process running to forward localhost:3000 to the Grafana service in Kubernetes). You may need to change the host and port to match however you are exposing your Grafana instance. Coherence clusters can now be created as described in the Publish Metrics page, and metrics will eventually appear in Prometheus and Grafana. It can sometimes take a minute or so for Prometheus to start scraping metrics and for them to appear in Grafana. ",
            "title": "Import the Dashboards into Grafana."
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " The purpose of this page is to list troubleshooting guides and work-arounds for issues that you may run into when using the Coherence Operator. This page will be updated and maintained over time to include common issues we see from customers ",
            "title": "Troubleshooting Guide"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " I Uninstalled the Operator and Cannot Delete the Coherence Clusters Deleting a Namespace Containing Coherence Resource(s) is Stuck Pending Deletion Why Does the Operator Pod Restart Why are the Coherence Pods not reaching ready My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment My cache services will not reach Site Safe My Grafana Dashboards do not display any metrics I&#8217;m using Arm64 and Java 8 and the JVM will not start due to using G1GC ",
            "title": "Contents"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " The Coherence resources managed by the Operator are marked in k8s as being owned by the Operator, and have finalizers to stop them being deleted. In normal operation the Operator will remove the finalizer when it deletes a Coherence cluster. The Operator also installs a validating and mutating web-hook, which will also stop k8s allowing mutations and deletions to a Coherence resource if the Coherence Operator is not running. If the Operator has been uninstalled, first remove the two web-hooks. <markup lang=\"bash\" >kubectl delete mutatingwebhookconfiguration coherence-operator-mutating-webhook-configuration kubectl delete validatingwebhookconfiguration coherence-operator-validating-webhook-configuration Now patch and delete each Coherence resource to delete its finalizers using the command below and replacing &lt;NAMESPACE&gt; with the correct namespace the Coherence resource is in and &lt;COHERENCE_RESOURCE_NAME&gt; with the Coherence resource name. <markup lang=\"bash\" >kubectl -n &lt;NAMESPACE&gt; patch coherence/&lt;COHERENCE_RESOURCE_NAME&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge kubectl -n &lt;NAMESPACE&gt; delete coherence/&lt;COHERENCE_RESOURCE_NAME&gt; ",
            "title": "I Uninstalled the Operator and Cannot Delete the Coherence Clusters"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " If you have tried to delete a namespace using kubectl delete and the namespace is now stuck pending deletion, this could be related to the issue above. This is especially true if the Operator is either not running, or it is in the same namespace that is being deleted. If deleting a namespace containing the Operator and a running Coherence cluster, or deleting a namespace containing a running Coherence cluster when the Operator is stopped will mean the finalizers the Operator added to the Coherence resources cannot be removed so the namespace will remain in a pending deletion state. The solution to this is the same as the point above in I Uninstalled the Operator and Cannot Delete the Coherence Clusters Alternatively, if you are running the Operator in a CI/CD environment and just want to be able to clean up after tests you can run Coherence clusters with the allowUnsafeDelete option enabled. By setting the allowUnsafeDelete field to true in the Coherence resource the Operator will not add a finalizer to that Coherence resource, allowing it to be deleted if its namespace is deleted. For example: <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: unsafe-cluster spec: allowUnsafeDelete: true Caution Setting the allowUnsafeDelete field to true will mean that the Operator will not be able to intercept the deletion and shutdown of a Coherence cluster and ensure it has a clean, safe shutdown. This is usually ok in CI/CD environments where the cluster and namespace are being cleaned up at the end of a test. This options should not be used in a production cluster, especially where features such as Coherence persistence are being used, otherwise the cluster may not cleanly shut down and will then not be able to be restarted using the persisted data. ",
            "title": "Deleting a Namespace Containing Coherence Resource(s) is Stuck Pending Deletion"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " You might notice that when the Operator is installed that the Operator Pod starts, dies and is then restarted. This is expected behaviour. The Operator uses a K8s web-hook for defaulting and validation of the Coherence resources. A web-hook requires certificates to be present in a Secret mounted to the Operator Pod as a volume. If this Secret is not present the Operator creates it and populates it with self-signed certs. In order for the Secret to then be mounted correctly the Pod must be restarted. See the the WebHooks documentation. ",
            "title": "Why Does the Operator Pod Restart After Installation"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " The readiness/liveness probe used by the Operator in the Coherence Pods checks a number of things to determine whether the Pods is ready, one of these is whether the JVM is a cluster member. If your application uses a custom main class and is not properly bootstrapping Coherence then the Pod will not be ready until your application code actually touches a Coherence resource causing Coherence to start and join the cluster. When running in clusters with the Operator using custom main classes it is advisable to properly bootstrap Coherence from within your main method. This can be done using the new Coherence bootstrap API available from CE release 20.12 or by calling com.tangosol.net.DefaultCacheServer.startServerDaemon().waitForServiceStart(); ",
            "title": "Why are the Coherence Pods not reaching ready"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " If you try to create a Coherence deployment that has a replica count that is greater than your k8s cluster can actually provision then one or more Pods will fail to be created or can be left in a pending state. The obvious solution to this is to just scale down your Coherence deployment to a smaller size that can be provisioned. The issue here is that the safe scaling functionality built into the operator will not allow scaling down to take place because it cannot guarantee no parition/data loss. The Coherence deployment is now stuck in this state. The simplest solution would be to completely delete the the Coherence deployment and redeploy with a lower replica count. If this is not possible then the following steps will allow the deployment to be scaled down. 1 Update the stuck Coherence deployment&#8217;s scaling policy to be Parallel <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: policy: Parallel 2 Scale down the cluster to the required size using whatever scaling commands you want, i.e kubectl scale or just update the replica value of the Coherence deployment yaml. Note: If updating the Coherence yaml, this should not be done as part of step 1, above. 3 Once the Coherence deployment has scaled to the required size then change the scaling policy value back to the default by updating the Coherence yaml to have no scaling policy value in it. When using this work around to scale down a stuck deployment that contains data it is important that only the missing or pending Pods are removed. For example if a Coherence deployment is deployed with a replica count of 100 and 90 Pods are ready, but the other 10 are either missing or stuck pending then the replica value used in step 2 above must be 90. Because the scaling policy has been set to Parallel the operator will not check any Status HA values before scaling down Pods, so removing \"ready\" Pods that contain data will almost certainly result in data loss. To safely scale down lower, then first follow the three steps above then after changing the scaling policy back to the default further scaling down can be done as normal. ",
            "title": "My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " A Coherence deployment can become stuck where none of the Pods can start, for example the image used is incorrect and all Pods are stuck in ImagePullBackoff. It can then become impossible to delete the broken deployment. This is because the Operator has installed a finalizer but this finalizer cannot execute. For example, suppose we have deployed a Coherence deployment named my-cluster into namespace coherence-test . First try to delete the deployment as normal: <markup lang=\"console\" >kubectl -n coherence-test delete coherence/my-cluster If this command hangs, then press ctrl-c to exit and then run the following patch command. <markup lang=\"console\" >kubectl -n coherence-test patch coherence/my-cluster -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge This will remove the Operator&#8217;s finalizer from the Coherence deployment. At this point the my-cluster Coherence deployment might already have been removed, if not try the delete command again. ",
            "title": "My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " Coherence distributes data in a cluster to achieve the highest status HA value that it can, the best being site-safe. This is done using the various values configured for the site, rack, machine, and member names. The Coherence Operator configures these values for the Pods in a Coherence deployment. By default, the values for the site and rack names are taken from standard k8s labels applied to the Nodes in the k8s cluster. If the Nodes in the cluster do not have these labels set then the site and rack names will be unset and Coherence will not be able to reach rack or site safe. There are a few possible solutions to this, see the explanation in the documentation explaining Member Identity ",
            "title": "My cache services will not reach Site-Safe"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " If you have imported the Grafana dashboards provided by the Operator into Grafana, but they are not displaying any metric values, it may be that you have imported the wrong format dashboards. The Operator has multiple sets of dashboards, one for the default Coherence metric name format, one for Microprofile metric name format, and one for Micrometer metric name format. The simplest way to find out which version corresponds to your Coherence cluster is to query the metrics endpoint with something like curl . If the metric names are in the format vendor:coherence_cluster_size , i.e. prefixed with vendor: then this is the default Coherence format. If metric names are in the format vendor_Coherence_Cluster_Size , i.e. prefixed with vendor_ then this is Microprofile format. If the metric name has no vendor prefix then it is using Micrometer metrics. See: the Importing Grafana Dashboards documentation. ",
            "title": "My Grafana Dashboards do not display any metrics"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " If running Kubernetes on ARM processors and using Coherence images built on Java 8 for ARM, note that the G1 garbage collector in that version of Java on ARM is marked as experimental. By default, the Operator configures the Coherence JVM to use G1. This will cause errors on Arm64 Java 8 JMS unless the JVM option -XX:+UnlockExperimentalVMOptions is added in the Coherence resource spec (see Adding Arbitrary JVM Arguments ). Alternatively specify a different garbage collector, ideally on a version of Java this old, use CMS (see Garbage Collector Settings ). ",
            "title": "I&#8217;m using Arm64 and Java 8 and the JVM will not start due to using G1GC"
        },
        {
            "location": "/docs/troubleshooting/01_trouble-shooting",
            "text": " I Uninstalled the Operator and Cannot Delete the Coherence Clusters The Coherence resources managed by the Operator are marked in k8s as being owned by the Operator, and have finalizers to stop them being deleted. In normal operation the Operator will remove the finalizer when it deletes a Coherence cluster. The Operator also installs a validating and mutating web-hook, which will also stop k8s allowing mutations and deletions to a Coherence resource if the Coherence Operator is not running. If the Operator has been uninstalled, first remove the two web-hooks. <markup lang=\"bash\" >kubectl delete mutatingwebhookconfiguration coherence-operator-mutating-webhook-configuration kubectl delete validatingwebhookconfiguration coherence-operator-validating-webhook-configuration Now patch and delete each Coherence resource to delete its finalizers using the command below and replacing &lt;NAMESPACE&gt; with the correct namespace the Coherence resource is in and &lt;COHERENCE_RESOURCE_NAME&gt; with the Coherence resource name. <markup lang=\"bash\" >kubectl -n &lt;NAMESPACE&gt; patch coherence/&lt;COHERENCE_RESOURCE_NAME&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge kubectl -n &lt;NAMESPACE&gt; delete coherence/&lt;COHERENCE_RESOURCE_NAME&gt; Deleting a Namespace Containing Coherence Resource(s) is Stuck Pending Deletion If you have tried to delete a namespace using kubectl delete and the namespace is now stuck pending deletion, this could be related to the issue above. This is especially true if the Operator is either not running, or it is in the same namespace that is being deleted. If deleting a namespace containing the Operator and a running Coherence cluster, or deleting a namespace containing a running Coherence cluster when the Operator is stopped will mean the finalizers the Operator added to the Coherence resources cannot be removed so the namespace will remain in a pending deletion state. The solution to this is the same as the point above in I Uninstalled the Operator and Cannot Delete the Coherence Clusters Alternatively, if you are running the Operator in a CI/CD environment and just want to be able to clean up after tests you can run Coherence clusters with the allowUnsafeDelete option enabled. By setting the allowUnsafeDelete field to true in the Coherence resource the Operator will not add a finalizer to that Coherence resource, allowing it to be deleted if its namespace is deleted. For example: <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: unsafe-cluster spec: allowUnsafeDelete: true Caution Setting the allowUnsafeDelete field to true will mean that the Operator will not be able to intercept the deletion and shutdown of a Coherence cluster and ensure it has a clean, safe shutdown. This is usually ok in CI/CD environments where the cluster and namespace are being cleaned up at the end of a test. This options should not be used in a production cluster, especially where features such as Coherence persistence are being used, otherwise the cluster may not cleanly shut down and will then not be able to be restarted using the persisted data. Why Does the Operator Pod Restart After Installation You might notice that when the Operator is installed that the Operator Pod starts, dies and is then restarted. This is expected behaviour. The Operator uses a K8s web-hook for defaulting and validation of the Coherence resources. A web-hook requires certificates to be present in a Secret mounted to the Operator Pod as a volume. If this Secret is not present the Operator creates it and populates it with self-signed certs. In order for the Secret to then be mounted correctly the Pod must be restarted. See the the WebHooks documentation. Why are the Coherence Pods not reaching ready The readiness/liveness probe used by the Operator in the Coherence Pods checks a number of things to determine whether the Pods is ready, one of these is whether the JVM is a cluster member. If your application uses a custom main class and is not properly bootstrapping Coherence then the Pod will not be ready until your application code actually touches a Coherence resource causing Coherence to start and join the cluster. When running in clusters with the Operator using custom main classes it is advisable to properly bootstrap Coherence from within your main method. This can be done using the new Coherence bootstrap API available from CE release 20.12 or by calling com.tangosol.net.DefaultCacheServer.startServerDaemon().waitForServiceStart(); My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down If you try to create a Coherence deployment that has a replica count that is greater than your k8s cluster can actually provision then one or more Pods will fail to be created or can be left in a pending state. The obvious solution to this is to just scale down your Coherence deployment to a smaller size that can be provisioned. The issue here is that the safe scaling functionality built into the operator will not allow scaling down to take place because it cannot guarantee no parition/data loss. The Coherence deployment is now stuck in this state. The simplest solution would be to completely delete the the Coherence deployment and redeploy with a lower replica count. If this is not possible then the following steps will allow the deployment to be scaled down. 1 Update the stuck Coherence deployment&#8217;s scaling policy to be Parallel <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: policy: Parallel 2 Scale down the cluster to the required size using whatever scaling commands you want, i.e kubectl scale or just update the replica value of the Coherence deployment yaml. Note: If updating the Coherence yaml, this should not be done as part of step 1, above. 3 Once the Coherence deployment has scaled to the required size then change the scaling policy value back to the default by updating the Coherence yaml to have no scaling policy value in it. When using this work around to scale down a stuck deployment that contains data it is important that only the missing or pending Pods are removed. For example if a Coherence deployment is deployed with a replica count of 100 and 90 Pods are ready, but the other 10 are either missing or stuck pending then the replica value used in step 2 above must be 90. Because the scaling policy has been set to Parallel the operator will not check any Status HA values before scaling down Pods, so removing \"ready\" Pods that contain data will almost certainly result in data loss. To safely scale down lower, then first follow the three steps above then after changing the scaling policy back to the default further scaling down can be done as normal. My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment A Coherence deployment can become stuck where none of the Pods can start, for example the image used is incorrect and all Pods are stuck in ImagePullBackoff. It can then become impossible to delete the broken deployment. This is because the Operator has installed a finalizer but this finalizer cannot execute. For example, suppose we have deployed a Coherence deployment named my-cluster into namespace coherence-test . First try to delete the deployment as normal: <markup lang=\"console\" >kubectl -n coherence-test delete coherence/my-cluster If this command hangs, then press ctrl-c to exit and then run the following patch command. <markup lang=\"console\" >kubectl -n coherence-test patch coherence/my-cluster -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge This will remove the Operator&#8217;s finalizer from the Coherence deployment. At this point the my-cluster Coherence deployment might already have been removed, if not try the delete command again. My cache services will not reach Site-Safe Coherence distributes data in a cluster to achieve the highest status HA value that it can, the best being site-safe. This is done using the various values configured for the site, rack, machine, and member names. The Coherence Operator configures these values for the Pods in a Coherence deployment. By default, the values for the site and rack names are taken from standard k8s labels applied to the Nodes in the k8s cluster. If the Nodes in the cluster do not have these labels set then the site and rack names will be unset and Coherence will not be able to reach rack or site safe. There are a few possible solutions to this, see the explanation in the documentation explaining Member Identity My Grafana Dashboards do not display any metrics If you have imported the Grafana dashboards provided by the Operator into Grafana, but they are not displaying any metric values, it may be that you have imported the wrong format dashboards. The Operator has multiple sets of dashboards, one for the default Coherence metric name format, one for Microprofile metric name format, and one for Micrometer metric name format. The simplest way to find out which version corresponds to your Coherence cluster is to query the metrics endpoint with something like curl . If the metric names are in the format vendor:coherence_cluster_size , i.e. prefixed with vendor: then this is the default Coherence format. If metric names are in the format vendor_Coherence_Cluster_Size , i.e. prefixed with vendor_ then this is Microprofile format. If the metric name has no vendor prefix then it is using Micrometer metrics. See: the Importing Grafana Dashboards documentation. I&#8217;m using Arm64 and Java 8 and the JVM will not start due to using G1GC If running Kubernetes on ARM processors and using Coherence images built on Java 8 for ARM, note that the G1 garbage collector in that version of Java on ARM is marked as experimental. By default, the Operator configures the Coherence JVM to use G1. This will cause errors on Arm64 Java 8 JMS unless the JVM option -XX:+UnlockExperimentalVMOptions is added in the Coherence resource spec (see Adding Arbitrary JVM Arguments ). Alternatively specify a different garbage collector, ideally on a version of Java this old, use CMS (see Garbage Collector Settings ). ",
            "title": "Issues"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " The Coherence Operator is a Kubernetes Operator that is used to manage Oracle Coherence clusters in Kubernetes. The Coherence Operator takes on the tasks of that human DevOps resource might carry out when managing Coherence clusters, such as configuration, installation, safe scaling, management and metrics. The Coherence Operator is a Go based application built using the Operator SDK . It is distributed as a Docker image and Helm chart for easy installation and configuration. ",
            "title": "What is the Coherence Operator?"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " JVMs that run as Coherence cluster members need to discover the other members of the cluster. This is discussed in the Coherence Well Known Addressing section of the documentation. When using the Operator the well known addressing configuration for clusters is managed automatically to allow a Coherence deployment to create its own cluster or to join with other deployments to form larger clusters. ",
            "title": "Cluster Discovery"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " The Operator configures the Coherence site and rack properties for cluster members based on Kubernetes Node topology labels. This allows Coherence to better distribute data across sites when a Kubernetes cluster spans availability domains. ",
            "title": "Better Fault Tolerant Data Distribution"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " When scaling down a Coherence cluster, care must be taken to ensure that there will be no data loss. This typically means scaling down by a single Pod at a time and waiting for the cluster to become \"safe\" before scaling down the next Pod. The Operator has built in functionality to do this, so scaling a Coherence cluster is as simple as scaling any other Kubernetes Deployment or StatefulSet. ",
            "title": "Safe Scaling"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Alongside safe scaling, because the Coherence CRD supports the Kubernetes scale sub-resource it is possible to configure the Kubernetes Horizontal Pod Autoscaler to scale Coherence clusters based on metrics. ",
            "title": "Autoscaling"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " The Operator has an understanding of when a Coherence JVM is \"ready\", so it configures a readiness probe that k8s will use to signal whether a Pod is ready or not. ",
            "title": "Readiness Probes"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Using the Operator makes it simple to configure and use Coherence Persistence, storing data on Kubernetes Persistent Volumes to allow state to be maintained between cluster restarts. ",
            "title": "Persistence"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " When a Coherence cluster is deployed with persistence enabled, the Operator will gracefully shutdown a cluster by suspending services before stopping all the Pods. This ensures that all persistence files are properly closed and allows for quicker recovery and restart of the cluster. Without the Operator, if a cluster is shutdown, typically by removing the controlling StatefulSet from Kubernetes then the Pods will be shutdown but not all at the same time. It is obviously impossible for k8s to kill all the Pods at the exact same instant in time. As some Pods die the remaining storage enabled Pods will be trying to recover data for the lost Pods, this can cause a lot of needles work and moving of data over the network. It is much cleaner to suspend all the services before shutdown. ",
            "title": "Graceful Shutdown"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " The Coherence CRD is designed to make the more commonly used configuration parameters for Coherence, and the JVM simpler to configure. The Coherence CRD is simple to use, in fact none of its fields are mandatory, so an application can be deployed with nothing more than a name, and a container image. ",
            "title": "Simpler Configuration"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " By using the Operator to manage Coherence clusters all clusters are configured and managed the same way making it easier for DevOps to manage multiple clusters and applications. ",
            "title": "Consistency"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " The Operator has been built and tested by the Coherence engineering team, who understand Coherence and the various scenarios and edge cases that can occur when managing Coherence clusters at scale in Kubernetes. ",
            "title": "Expertise"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Using the Coherence Operator to manage Coherence clusters running in Kubernetes has many advantages over just deploying and running clusters with the resources provided by Kubernetes. Coherence can be treated as just another library that your application depends on and uses and hence, a Coherence application can run in Kubernetes without requiring the Operator, but in this case there are a number of things that the DevOps team for an application would need to build or do manually. Cluster Discovery JVMs that run as Coherence cluster members need to discover the other members of the cluster. This is discussed in the Coherence Well Known Addressing section of the documentation. When using the Operator the well known addressing configuration for clusters is managed automatically to allow a Coherence deployment to create its own cluster or to join with other deployments to form larger clusters. Better Fault Tolerant Data Distribution The Operator configures the Coherence site and rack properties for cluster members based on Kubernetes Node topology labels. This allows Coherence to better distribute data across sites when a Kubernetes cluster spans availability domains. Safe Scaling When scaling down a Coherence cluster, care must be taken to ensure that there will be no data loss. This typically means scaling down by a single Pod at a time and waiting for the cluster to become \"safe\" before scaling down the next Pod. The Operator has built in functionality to do this, so scaling a Coherence cluster is as simple as scaling any other Kubernetes Deployment or StatefulSet. Autoscaling Alongside safe scaling, because the Coherence CRD supports the Kubernetes scale sub-resource it is possible to configure the Kubernetes Horizontal Pod Autoscaler to scale Coherence clusters based on metrics. Readiness Probes The Operator has an understanding of when a Coherence JVM is \"ready\", so it configures a readiness probe that k8s will use to signal whether a Pod is ready or not. Persistence Using the Operator makes it simple to configure and use Coherence Persistence, storing data on Kubernetes Persistent Volumes to allow state to be maintained between cluster restarts. Graceful Shutdown When a Coherence cluster is deployed with persistence enabled, the Operator will gracefully shutdown a cluster by suspending services before stopping all the Pods. This ensures that all persistence files are properly closed and allows for quicker recovery and restart of the cluster. Without the Operator, if a cluster is shutdown, typically by removing the controlling StatefulSet from Kubernetes then the Pods will be shutdown but not all at the same time. It is obviously impossible for k8s to kill all the Pods at the exact same instant in time. As some Pods die the remaining storage enabled Pods will be trying to recover data for the lost Pods, this can cause a lot of needles work and moving of data over the network. It is much cleaner to suspend all the services before shutdown. Simpler Configuration The Coherence CRD is designed to make the more commonly used configuration parameters for Coherence, and the JVM simpler to configure. The Coherence CRD is simple to use, in fact none of its fields are mandatory, so an application can be deployed with nothing more than a name, and a container image. Consistency By using the Operator to manage Coherence clusters all clusters are configured and managed the same way making it easier for DevOps to manage multiple clusters and applications. Expertise The Operator has been built and tested by the Coherence engineering team, who understand Coherence and the various scenarios and edge cases that can occur when managing Coherence clusters at scale in Kubernetes. ",
            "title": "Why use the Coherence Kubernetes Operator"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " A Coherence cluster is a number of distributed Java Virtual Machines (JVMs) that communicate to form a single coherent cluster. In Kubernetes, this concept can be related to a number of Pods that form a single cluster. In each Pod is a JVM running a Coherence DefaultCacheServer , or a custom application using Coherence. The operator uses a Kubernetes Custom Resource Definition (CRD) to represent a group of members in a Coherence cluster. Typically, a deployment would be used to configure one or more members of a specific role in a cluster. Every field in the Coherence CRD Spec is optional, so a simple cluster can be defined in yaml as: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster In this case the metadata.name field in the Coherence resource yaml will be used as the Coherence cluster name. The operator will use default values for fields that have not been entered, so the above yaml will create a Coherence deployment using a StatefulSet with a replica count of three, which means that will be three storage enabled Coherence Pods . See the Coherence CRD spec page for details of all the fields in the CRD. In the above example no spec.image field has been set, so the Operator will use a publicly available Coherence CE image as its default. These images are meant for demos, POCs and experimentation, but for a production application you should build your own image. ",
            "title": "Coherence Clusters"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Whilst the Coherence CE version can be freely deployed anywhere, if your application image uses a commercial version of Oracle Coherence then you are responsible for making sure your deployment has been properly licensed. Oracle&#8217;s current policy is that a license will be required for each Kubernetes Node that images are to be pulled to. While an image exists on a node it is effectively the same as having installed the software on that node. One way to ensure that the Pods of a Coherence deployment only get scheduled onto nodes that meet the license requirement is to configure Pod scheduling, for example a node selector. Node selectors, and other scheduling, is simple to configure in the Coherence CRD, see the scheduling documentation For example, if a commercial Coherence license exists such that a sub-set of nodes in a Kubernetes cluster have been covered by the license then those nodes could all be given a label, e.g. coherenceLicense=true When creating a Coherence deployment specify a node selector to match the label: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: my-app:1.0.0 nodeSelector: coherenceLicense: 'true' The my-app:1.0.0 image contains a commercial Coherence version. The nodeSelector will ensure Pods only get scheduled to nodes with the coherenceLicense=true label. There are other ways to configure Pod scheduling supported by the Coherence Operator (such as taints and tolerations) and there are alternative ways to restrict nodes that Pods can be schedule to, for example a namespace in kubernetes can be restricted to a sub-set of the cluster&#8217;s nodes. Using a node selector as described above is probably the simplest approach. ",
            "title": "Using Commercial Coherence Versions"
        },
        {
            "location": "/docs/installation/04_obtain_coherence_images",
            "text": " For most use-cases we expect the developer to provide a suitable Coherence application image to be run by the operator. For POCs, demos and experimentation the Coherence Operator uses the OSS Coherence CE image when no image has been specified for a Coherence resource. Commercial Coherence images are not available from public image registries and must be pulled from the middleware section of Oracle Container Registry. ",
            "title": "preambule"
        },
        {
            "location": "/docs/installation/04_obtain_coherence_images",
            "text": " Get the Coherence Docker image from the Oracle Container Registry: In a web browser, navigate to Oracle Container Registry and click Sign In. Enter your Oracle credentials or create an account if you don&#8217;t have one. Search for coherence in the Search Oracle Container Registry field. Click coherence in the search result list. On the Oracle Coherence page, select the language from the drop-down list and click Continue. Click Accept on the Oracle Standard Terms and Conditions page. Once this is done the Oracle Container Registry credentials can be used to create Kubernetes secret to pull the Coherence image. ",
            "title": "Coherence Images from Oracle Container Registry"
        },
        {
            "location": "/docs/installation/04_obtain_coherence_images",
            "text": " Kubernetes supports configuring pods to use imagePullSecrets for pulling images. If possible, this is the preferable and most portable route. See the kubernetes docs for this. Once secrets have been created in the namespace that the Coherence resource is to be installed in then the secret name can be specified in the Coherence CRD spec . It is possible to specify multiple secrets in the case where the different images being used are pulled from different registries. For example to use the commercial Coherence 14.1.1.0.0 image from OCR specify the image and image pull secrets in the Coherence resource yaml <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0 imagePullSecrets: - name: coherence-secret The coherence-secret will be used for pulling images from the registry associated to the secret Also see Using Private Image Registries ",
            "title": "Use ImagePullSecrets"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " To build a Coherence application there will obviously be at a minimum a dependency on coherence.jar . Optionally we can also add dependencies on other Coherence modules. In this example we&#8217;re going to add json support to the application by adding a dependency on coherence-json . In the example we use the coherence-bom which ensures that we have consistent use of other Coherence modules. In the pom.xml we have a dependencyManagement section. <markup lang=\"xml\" title=\"pom.xml\" > &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; In the build.gradle file we add the bom as a platform dependency. <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"com.oracle.coherence.ce:coherence-bom:22.06.7\") We can then add the coherence and coherence-json modules as dependencies <markup lang=\"xml\" title=\"pom.xml\" > &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-json&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; In the build.gradle file we add the bom as a platform dependency. <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"com.oracle.coherence.ce:coherence-bom:22.06.7\") implementation \"com.oracle.coherence.ce:coherence\" implementation \"com.oracle.coherence.ce:coherence-json\" } ",
            "title": "Add Dependencies"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " To build the image using JIB we need to add the JIB plugin to the project. In the pom.xml file we add JIB to the plugins section. <markup lang=\"xml\" title=\"pom.xml\" > &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt; &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.4.0&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; In the build.gradle file we add JIB to the plugins section. <markup lang=\"groovy\" title=\"build.gradle\" >plugins { id 'java' id 'com.google.cloud.tools.jib' version '3.3.4' } ",
            "title": "Add the JIB Plugin"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " In the pom.xml file we configure the plugin where it is declared in the plugins section: <markup lang=\"xml\" title=\"pom.xml\" >&lt;plugin&gt; &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt; &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.plugin.jib}&lt;/version&gt; &lt;configuration&gt; &lt;from&gt; &lt;image&gt;gcr.io/distroless/java11-debian11&lt;/image&gt; &lt;/from&gt; &lt;to&gt; &lt;image&gt;${project.artifactId}&lt;/image&gt; &lt;tags&gt; &lt;tag&gt;${project.version}&lt;/tag&gt; &lt;tag&gt;latest&lt;/tag&gt; &lt;/tags&gt; &lt;/to&gt; &lt;container&gt; &lt;mainClass&gt;com.tangosol.net.Coherence&lt;/mainClass&gt; &lt;format&gt;OCI&lt;/format&gt; &lt;/container&gt; &lt;/configuration&gt; &lt;/plugin&gt; The base image will be gcr.io/distroless/java11-debian11 The image name is set to the Maven module name using the property ${project.artifactId} There will be two tags for the image, latest and the project version taken from the ${project.version} property. The main class to use when the image is run is set to com.tangosol.net.Coherence The image type is set to OCI ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " In the build.gradle file we configure JIB in the jib section: <markup lang=\"groovy\" title=\"build.gradle\" >jib { from { image = 'gcr.io/distroless/java11-debian11' } to { image = \"${project.name}\" tags = [\"${version}\", 'latest'] } container { mainClass = 'com.tangosol.net.Coherence' format = 'OCI' } } The base image will be gcr.io/distroless/java11-debian11 The image name is set to the Maven module name using the property ${project.artifactId} There will be two tags for the image, latest and the project version taken from the ${project.version} property. The main class to use when the image is run is set to com.tangosol.net.Coherence The image type is set to OCI ",
            "title": "Gradle Configuration"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " Now we can configure the JIB plugin with the properties specific to our image. In this example the configuration is very simple, the JIB plugin documentation shows many more options. We are going to set the following options: * The name and tags for the image we will build. * The main class that we will run as the entry point to the image - in this case com.tangosol.net.Coherence . * The base image. In this example we will us a distroless Java 11 image. A distroless image is more secure as it contains nothing more than core linux and a JRE. There is no shell or other tools to introduce CVEs. The downside of this is that there is no shell, so you cannot exec into the running container, or use a shell script as an entry point. If you don;t need those things a distroless image is a great choice. Maven Configuration In the pom.xml file we configure the plugin where it is declared in the plugins section: <markup lang=\"xml\" title=\"pom.xml\" >&lt;plugin&gt; &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt; &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.plugin.jib}&lt;/version&gt; &lt;configuration&gt; &lt;from&gt; &lt;image&gt;gcr.io/distroless/java11-debian11&lt;/image&gt; &lt;/from&gt; &lt;to&gt; &lt;image&gt;${project.artifactId}&lt;/image&gt; &lt;tags&gt; &lt;tag&gt;${project.version}&lt;/tag&gt; &lt;tag&gt;latest&lt;/tag&gt; &lt;/tags&gt; &lt;/to&gt; &lt;container&gt; &lt;mainClass&gt;com.tangosol.net.Coherence&lt;/mainClass&gt; &lt;format&gt;OCI&lt;/format&gt; &lt;/container&gt; &lt;/configuration&gt; &lt;/plugin&gt; The base image will be gcr.io/distroless/java11-debian11 The image name is set to the Maven module name using the property ${project.artifactId} There will be two tags for the image, latest and the project version taken from the ${project.version} property. The main class to use when the image is run is set to com.tangosol.net.Coherence The image type is set to OCI Gradle Configuration In the build.gradle file we configure JIB in the jib section: <markup lang=\"groovy\" title=\"build.gradle\" >jib { from { image = 'gcr.io/distroless/java11-debian11' } to { image = \"${project.name}\" tags = [\"${version}\", 'latest'] } container { mainClass = 'com.tangosol.net.Coherence' format = 'OCI' } } The base image will be gcr.io/distroless/java11-debian11 The image name is set to the Maven module name using the property ${project.artifactId} There will be two tags for the image, latest and the project version taken from the ${project.version} property. The main class to use when the image is run is set to com.tangosol.net.Coherence The image type is set to OCI ",
            "title": "Configure the JIB Plugin"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " To create the server image run the relevant commands as documented in the JIB plugin documentation. In this case we&#8217;re going to build the image using Docker, although JIB offers other alternatives. Using Maven we run: <markup lang=\"bash\" >./mvnw compile jib:dockerBuild Using Gradle we run: <markup lang=\"bash\" >./gradlew compileJava jibDockerBuild The command above will create an image named simple-coherence with two tags, latest and 1.0.0 . Listing the local images should show the new images. <markup lang=\"bash\" >$ docker images | grep simple simple-coherence 1.0.0 1613cd3b894e 51 years ago 227MB simple-coherence latest 1613cd3b894e 51 years ago 227MB ",
            "title": "Build the Image"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " The image just built can be run using Docker (or your chosen container tool). In this example we&#8217;ll run it interactively, just to prove it runs and starts Coherence. <markup lang=\"bash\" >docker run -it --rm simple-coherence:latest The console output should display Coherence starting and finally show the Coherence service list, which will look something like this: <markup lang=\"bash\" >Services ( ClusterService{Name=Cluster, State=(SERVICE_STARTED, STATE_JOINED), Id=0, OldestMemberId=1} TransportService{Name=TransportService, State=(SERVICE_STARTED), Id=1, OldestMemberId=1} InvocationService{Name=Management, State=(SERVICE_STARTED), Id=2, OldestMemberId=1} PartitionedCache{Name=$SYS:Config, State=(SERVICE_STARTED), Id=3, OldestMemberId=1, LocalStorage=enabled, PartitionCount=257, BackupCount=1, AssignedPartitions=257, BackupPartitions=0, CoordinatorId=1} PartitionedCache{Name=PartitionedCache, State=(SERVICE_STARTED), Id=4, OldestMemberId=1, LocalStorage=enabled, PartitionCount=257, BackupCount=1, AssignedPartitions=257, BackupPartitions=0, CoordinatorId=1} PartitionedCache{Name=PartitionedTopic, State=(SERVICE_STARTED), Id=5, OldestMemberId=1, LocalStorage=enabled, PartitionCount=257, BackupCount=1, AssignedPartitions=257, BackupPartitions=0, CoordinatorId=1} ProxyService{Name=Proxy, State=(SERVICE_STARTED), Id=6, OldestMemberId=1} ) Press ctrl-C to exit the container, the --rm option we used above wil automatically delete the stopped container. We now have a simple Coherence image we can use in other examples and when trying out the Coherence Operator. ",
            "title": "Run the Image"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " With recent Coherence versions, Coherence configuration items that can be set using system properties prefixed with coherence. can also be set using environment variables. This makes it simple to set those properties when running containers because environment variables can be set from the commandline. To set a property the system property name needs to be converted to an environment variable name. This is done by converting the name to uppercase and replacing dots ('.') with underscores ('_'). For example, to set the cluster name we would set the coherence.cluster system property. To run the image and set cluster name with an environment variable we convert coherence.cluster to COHERENCE_CLUSTER and run: <markup lang=\"bash\" >docker run -it --rm -e COHERENCE_CLUSTER=my-cluster simple-coherence:latest This is much simpler than trying to change the Java commandline the image entrypoint uses. ",
            "title": "Configuring the Image at Runtime"
        },
        {
            "location": "/examples/015_simple_image/README",
            "text": " This example shows how to build a simple Coherence server image using JIB with either Maven or Gradle. When building with Maven the project uses the JIB Maven Plugin . When building with Gradle the project uses the JIB Gradle Plugin . The Coherence Operator has out of the box support for images built with JIB, for example it can automatically detect the class path to use and run the correct main class. This simple application does not actually contain any code, a real application would obviously contain code and other resources. Tip The complete source code for this example is in the Coherence Operator GitHub repository. Add Dependencies To build a Coherence application there will obviously be at a minimum a dependency on coherence.jar . Optionally we can also add dependencies on other Coherence modules. In this example we&#8217;re going to add json support to the application by adding a dependency on coherence-json . In the example we use the coherence-bom which ensures that we have consistent use of other Coherence modules. In the pom.xml we have a dependencyManagement section. <markup lang=\"xml\" title=\"pom.xml\" > &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; In the build.gradle file we add the bom as a platform dependency. <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"com.oracle.coherence.ce:coherence-bom:22.06.7\") We can then add the coherence and coherence-json modules as dependencies <markup lang=\"xml\" title=\"pom.xml\" > &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-json&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; In the build.gradle file we add the bom as a platform dependency. <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"com.oracle.coherence.ce:coherence-bom:22.06.7\") implementation \"com.oracle.coherence.ce:coherence\" implementation \"com.oracle.coherence.ce:coherence-json\" } Add the JIB Plugin To build the image using JIB we need to add the JIB plugin to the project. In the pom.xml file we add JIB to the plugins section. <markup lang=\"xml\" title=\"pom.xml\" > &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt; &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.4.0&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; In the build.gradle file we add JIB to the plugins section. <markup lang=\"groovy\" title=\"build.gradle\" >plugins { id 'java' id 'com.google.cloud.tools.jib' version '3.3.4' } Configure the JIB Plugin Now we can configure the JIB plugin with the properties specific to our image. In this example the configuration is very simple, the JIB plugin documentation shows many more options. We are going to set the following options: * The name and tags for the image we will build. * The main class that we will run as the entry point to the image - in this case com.tangosol.net.Coherence . * The base image. In this example we will us a distroless Java 11 image. A distroless image is more secure as it contains nothing more than core linux and a JRE. There is no shell or other tools to introduce CVEs. The downside of this is that there is no shell, so you cannot exec into the running container, or use a shell script as an entry point. If you don;t need those things a distroless image is a great choice. Maven Configuration In the pom.xml file we configure the plugin where it is declared in the plugins section: <markup lang=\"xml\" title=\"pom.xml\" >&lt;plugin&gt; &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt; &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.plugin.jib}&lt;/version&gt; &lt;configuration&gt; &lt;from&gt; &lt;image&gt;gcr.io/distroless/java11-debian11&lt;/image&gt; &lt;/from&gt; &lt;to&gt; &lt;image&gt;${project.artifactId}&lt;/image&gt; &lt;tags&gt; &lt;tag&gt;${project.version}&lt;/tag&gt; &lt;tag&gt;latest&lt;/tag&gt; &lt;/tags&gt; &lt;/to&gt; &lt;container&gt; &lt;mainClass&gt;com.tangosol.net.Coherence&lt;/mainClass&gt; &lt;format&gt;OCI&lt;/format&gt; &lt;/container&gt; &lt;/configuration&gt; &lt;/plugin&gt; The base image will be gcr.io/distroless/java11-debian11 The image name is set to the Maven module name using the property ${project.artifactId} There will be two tags for the image, latest and the project version taken from the ${project.version} property. The main class to use when the image is run is set to com.tangosol.net.Coherence The image type is set to OCI Gradle Configuration In the build.gradle file we configure JIB in the jib section: <markup lang=\"groovy\" title=\"build.gradle\" >jib { from { image = 'gcr.io/distroless/java11-debian11' } to { image = \"${project.name}\" tags = [\"${version}\", 'latest'] } container { mainClass = 'com.tangosol.net.Coherence' format = 'OCI' } } The base image will be gcr.io/distroless/java11-debian11 The image name is set to the Maven module name using the property ${project.artifactId} There will be two tags for the image, latest and the project version taken from the ${project.version} property. The main class to use when the image is run is set to com.tangosol.net.Coherence The image type is set to OCI Build the Image To create the server image run the relevant commands as documented in the JIB plugin documentation. In this case we&#8217;re going to build the image using Docker, although JIB offers other alternatives. Using Maven we run: <markup lang=\"bash\" >./mvnw compile jib:dockerBuild Using Gradle we run: <markup lang=\"bash\" >./gradlew compileJava jibDockerBuild The command above will create an image named simple-coherence with two tags, latest and 1.0.0 . Listing the local images should show the new images. <markup lang=\"bash\" >$ docker images | grep simple simple-coherence 1.0.0 1613cd3b894e 51 years ago 227MB simple-coherence latest 1613cd3b894e 51 years ago 227MB Run the Image The image just built can be run using Docker (or your chosen container tool). In this example we&#8217;ll run it interactively, just to prove it runs and starts Coherence. <markup lang=\"bash\" >docker run -it --rm simple-coherence:latest The console output should display Coherence starting and finally show the Coherence service list, which will look something like this: <markup lang=\"bash\" >Services ( ClusterService{Name=Cluster, State=(SERVICE_STARTED, STATE_JOINED), Id=0, OldestMemberId=1} TransportService{Name=TransportService, State=(SERVICE_STARTED), Id=1, OldestMemberId=1} InvocationService{Name=Management, State=(SERVICE_STARTED), Id=2, OldestMemberId=1} PartitionedCache{Name=$SYS:Config, State=(SERVICE_STARTED), Id=3, OldestMemberId=1, LocalStorage=enabled, PartitionCount=257, BackupCount=1, AssignedPartitions=257, BackupPartitions=0, CoordinatorId=1} PartitionedCache{Name=PartitionedCache, State=(SERVICE_STARTED), Id=4, OldestMemberId=1, LocalStorage=enabled, PartitionCount=257, BackupCount=1, AssignedPartitions=257, BackupPartitions=0, CoordinatorId=1} PartitionedCache{Name=PartitionedTopic, State=(SERVICE_STARTED), Id=5, OldestMemberId=1, LocalStorage=enabled, PartitionCount=257, BackupCount=1, AssignedPartitions=257, BackupPartitions=0, CoordinatorId=1} ProxyService{Name=Proxy, State=(SERVICE_STARTED), Id=6, OldestMemberId=1} ) Press ctrl-C to exit the container, the --rm option we used above wil automatically delete the stopped container. We now have a simple Coherence image we can use in other examples and when trying out the Coherence Operator. Configuring the Image at Runtime With recent Coherence versions, Coherence configuration items that can be set using system properties prefixed with coherence. can also be set using environment variables. This makes it simple to set those properties when running containers because environment variables can be set from the commandline. To set a property the system property name needs to be converted to an environment variable name. This is done by converting the name to uppercase and replacing dots ('.') with underscores ('_'). For example, to set the cluster name we would set the coherence.cluster system property. To run the image and set cluster name with an environment variable we convert coherence.cluster to COHERENCE_CLUSTER and run: <markup lang=\"bash\" >docker run -it --rm -e COHERENCE_CLUSTER=my-cluster simple-coherence:latest This is much simpler than trying to change the Java commandline the image entrypoint uses. ",
            "title": "Example Coherence Image using JIB"
        },
        {
            "location": "/docs/installation/06_openshift",
            "text": " Whilst the Coherence Operator will run out of the box on OpenShift some earlier versions of the Coherence Docker image will not work without configuration changes. These earlier versions of the Coherence Docker images that Oracle publishes default the container user as oracle . When running the Oracle images or layered images that retain the default user as oracle with OpenShift, the anyuid security context constraint is required to ensure proper access to the file system within the Docker image. Later versions of the Coherence images have been modified to work without needing anyuid . To work with older image versions , the administrator must: Ensure the anyuid security content is granted Ensure that Coherence containers are annotated with openshift.io/scc: anyuid For example, to update the OpenShift policy, use: <markup lang=\"bash\" >oc adm policy add-scc-to-user anyuid -z default and to annotate the Coherence containers, update the Coherence resource to include annotations For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: annotations: openshift.io/scc: anyuid The openshift.io/scc: anyuid annotation will be applied to all of the Coherence Pods. For additional information about OpenShift requirements see the OpenShift documentation ",
            "title": "Coherence Clusters on OpenShift"
        },
        {
            "location": "/docs/other/110_readiness",
            "text": " The default endpoint used by the Operator for readiness checks that the Pod is a member of the Coherence cluster and that none of the partitioned cache services have a StatusHA value of endangered . If the Pod is the only cluster member at the time of the ready check the StatusHA check will be skipped. If a partitioned service has a backup count of zero the StatusHA check will be skipped for that service. There are scenarios where the StatusHA check can fail but should be ignored because the application does not care about data loss for caches on that particular cache service. Normally in this case the backup count for the cache service would be zero, and the service would automatically be skipped in the StatusHA test. The ready check used by the Operator can be configured to skip the StatusHA test for certain services. In the Coherence CRD the coherence.allowEndangeredForStatusHA is a list of string values that can be set to the names of partitioned cache services that should not be included in the StatusHA check. For a service to be skipped its name must exactly match one of the names in the allowEndangeredForStatusHA list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: allowEndangeredForStatusHA: - TempService The allowEndangeredForStatusHA field is a list of string values. In this case the TempService will not be checked for StatusHA in the ready check. ",
            "title": "Coherence Readiness"
        },
        {
            "location": "/docs/other/110_readiness",
            "text": " The Coherence CRD spec.readinessProbe field is identical to configuring a readiness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: readinessProbe: httpGet: port: 8080 path: \"/ready\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for readiness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . ",
            "title": "Configure Readiness"
        },
        {
            "location": "/docs/other/110_readiness",
            "text": " The Coherence CRD spec.livenessProbe field is identical to configuring a liveness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: livenessProbe: httpGet: port: 8080 path: \"/live\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for liveness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . ",
            "title": "Configure Liveness"
        },
        {
            "location": "/docs/other/110_readiness",
            "text": " The Coherence Operator injects a Readiness/Liveness endpoint into the Coherence container that is used as the default readiness and liveness check for the Pods deployed by the operator. This endpoint is suitable for most use-cases, but it is possible to configure a different readiness and liveness probe, or just change the timings of the probes if required. The readiness/liveness probe used by the Operator in the Coherence Pods checks a number of things to determine whether the Pods is ready, one of these is whether the JVM is a cluster member. If your application uses a custom main class and is not properly bootstrapping Coherence then the Pod will not be ready until your application code actually touches a Coherence resource causing Coherence to start and join the cluster. When running in clusters with the Operator using custom main classes it is advisable to properly bootstrap Coherence from within your main method. This can be done using the new Coherence bootstrap API available from CE release 20.12 or by calling com.tangosol.net.DefaultCacheServer.startServerDaemon().waitForServiceStart(); Coherence Readiness The default endpoint used by the Operator for readiness checks that the Pod is a member of the Coherence cluster and that none of the partitioned cache services have a StatusHA value of endangered . If the Pod is the only cluster member at the time of the ready check the StatusHA check will be skipped. If a partitioned service has a backup count of zero the StatusHA check will be skipped for that service. There are scenarios where the StatusHA check can fail but should be ignored because the application does not care about data loss for caches on that particular cache service. Normally in this case the backup count for the cache service would be zero, and the service would automatically be skipped in the StatusHA test. The ready check used by the Operator can be configured to skip the StatusHA test for certain services. In the Coherence CRD the coherence.allowEndangeredForStatusHA is a list of string values that can be set to the names of partitioned cache services that should not be included in the StatusHA check. For a service to be skipped its name must exactly match one of the names in the allowEndangeredForStatusHA list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: allowEndangeredForStatusHA: - TempService The allowEndangeredForStatusHA field is a list of string values. In this case the TempService will not be checked for StatusHA in the ready check. Configure Readiness The Coherence CRD spec.readinessProbe field is identical to configuring a readiness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: readinessProbe: httpGet: port: 8080 path: \"/ready\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for readiness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . Configure Liveness The Coherence CRD spec.livenessProbe field is identical to configuring a liveness probe for a Pod in Kubernetes; see Configure Liveness &amp; Readiness For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: livenessProbe: httpGet: port: 8080 path: \"/live\" timeoutSeconds: 60 initialDelaySeconds: 300 periodSeconds: 120 failureThreshold: 10 successThreshold: 1 The example above configures a http probe for liveness and sets different timings for the probe. The Coherence CRD supports the other types of readiness probe too, exec and tcpSocket . ",
            "title": "Readiness &amp; Liveness Probes"
        },
        {
            "location": "/docs/metrics/020_metrics",
            "text": " To expose metrics on a different port the alternative port value can be set in the coherence.metrics section, for example: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true port: 8080 ports: - name: metrics metrics will now be exposed on port 8080 ",
            "title": "Expose Metrics on a Different Port"
        },
        {
            "location": "/docs/metrics/020_metrics",
            "text": " To deploy a Coherence resource with metrics enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true ports: - name: metrics Setting the coherence.metrics.enabled field to true will enable metrics To expose metrics via a Service it is added to the ports list. The metrics port is a special case where the port number is optional so in this case metrics will bind to the default port 9612 . (see Exposing Ports for details) Expose Metrics on a Different Port To expose metrics on a different port the alternative port value can be set in the coherence.metrics section, for example: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true port: 8080 ports: - name: metrics metrics will now be exposed on port 8080 ",
            "title": "Deploy Coherence with Metrics Enabled"
        },
        {
            "location": "/docs/metrics/020_metrics",
            "text": " After installing the basic metrics-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f metrics-cluster.yaml coherence.coherence.oracle.com/metrics-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=metrics-cluster NAME READY STATUS RESTARTS AGE metrics-cluster-0 1/1 Running 0 36s metrics-cluster-1 1/1 Running 0 36s metrics-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward metrics-cluster-0 9612:9612 Forwarding from [::1]:9612 -&gt; 9612 Forwarding from 127.0.0.1:9612 -&gt; 9612 ",
            "title": "Port-forward the Metrics Port"
        },
        {
            "location": "/docs/metrics/020_metrics",
            "text": " Now that a port has been forwarded from localhost to a Pod in the cluster the metrics endpoint can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:9612/metrics ",
            "title": "Access the Metrics Endpoint"
        },
        {
            "location": "/docs/metrics/020_metrics",
            "text": " Since version 12.2.1.4 Coherence has had the ability to expose a http endpoint that can be used to scrape metrics. This would typically be used to expose metrics to something like Prometheus. The default metrics endpoint is disabled by default in Coherence clusters but can be enabled and configured by setting the relevant fields in the Coherence CRD. If your Coherence version is before CE 21.12.1 this example assumes that your application has included the coherence-metrics module as a dependency. See the Coherence product documentation for more details on enabling metrics in your application. The example below shows how to enable and access Coherence metrics. Once the metrics port has been exposed, for example via a load balancer or port-forward command, the metrics endpoint is available at http://host:port/metrics . See the Using Coherence Metrics documentation for full details on the available metrics. Deploy Coherence with Metrics Enabled To deploy a Coherence resource with metrics enabled and exposed on a port, the simplest yaml would look like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true ports: - name: metrics Setting the coherence.metrics.enabled field to true will enable metrics To expose metrics via a Service it is added to the ports list. The metrics port is a special case where the port number is optional so in this case metrics will bind to the default port 9612 . (see Exposing Ports for details) Expose Metrics on a Different Port To expose metrics on a different port the alternative port value can be set in the coherence.metrics section, for example: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true port: 8080 ports: - name: metrics metrics will now be exposed on port 8080 Port-forward the Metrics Port After installing the basic metrics-cluster.yaml from the first example above there would be a three member Coherence cluster installed into Kubernetes. For example, the cluster can be installed with kubectl <markup lang=\"bash\" >kubectl -n coherence-test create -f metrics-cluster.yaml coherence.coherence.oracle.com/metrics-cluster created The kubectl CLI can be used to list Pods for the cluster: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=metrics-cluster NAME READY STATUS RESTARTS AGE metrics-cluster-0 1/1 Running 0 36s metrics-cluster-1 1/1 Running 0 36s metrics-cluster-2 1/1 Running 0 36s In a test or development environment the simplest way to reach an exposed port is to use the kubectl port-forward command. For example to connect to the first Pod in the deployment: <markup lang=\"bash\" >kubectl -n coherence-test port-forward metrics-cluster-0 9612:9612 Forwarding from [::1]:9612 -&gt; 9612 Forwarding from 127.0.0.1:9612 -&gt; 9612 Access the Metrics Endpoint Now that a port has been forwarded from localhost to a Pod in the cluster the metrics endpoint can be accessed. Issue the following curl command to access the REST endpoint: <markup lang=\"bash\" >curl http://127.0.0.1:9612/metrics ",
            "title": "Publish Metrics"
        },
        {
            "location": "/docs/metrics/020_metrics",
            "text": " The operator can create a Prometheus ServiceMonitor for the metrics port so that Prometheus will automatically scrape metrics from the Pods in a Coherence deployment. <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: metrics-cluster spec: coherence: metrics: enabled: true ports: - name: metrics serviceMonitor: enabled: true The serviceMonitor.enabled field is set to true for the metrics port. See Exposing ports and Services - Service Monitors documentation for more details. ",
            "title": "Prometheus Service Monitor"
        },
        {
            "location": "/docs/jvm/020_classpath",
            "text": " If the image to be run has the CLASSPATH environment variable set this will be used as part of the classpath. ",
            "title": "The CLASSPATH Environment Variable"
        },
        {
            "location": "/docs/jvm/020_classpath",
            "text": " If the image to be run has the COHERENCE_HOME environment variable set this will be used to add the following elements to the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf These will be added to the end of the classpath. For example in an image that has CLASSPATH=/home/root/lib/* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf ",
            "title": "The COHERENCE_HOME Environment Variable"
        },
        {
            "location": "/docs/jvm/020_classpath",
            "text": " If the image is not a JIB image there could be occasions when automatically adding /app/libs/*:/app/classes:/app/resources to the classpath causes issues, for example one or more of those locations exists with files in that should not be on the classpath. In this case the Coherence CRD spec has a field to specify that the JIB classpath should not be used. The spec.jvm.useJibClasspath field can be set to false to exclude the JIB directories from the classpath (the default value is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. ",
            "title": "Exclude the JIB Classpath"
        },
        {
            "location": "/docs/jvm/020_classpath",
            "text": " A simple way to build Java images is using JIB . When JIB was with its Maven or Gradle plugin to produce an image it packages the application&#8217;s dependencies, classes and resources into a set of well-known locations: /app/libs/ - the jar files that the application depends on /app/classes - the application&#8217;s class files /app/resources - the application&#8217;s other resources By default, the Operator will add these locations to the classpath. These classpath elements will be added before any value set by the CLASSPATH or COHERENCE_HOME environment variables. For example in an image that has CLASSPATH=/home/root/lib/\\* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /app/libs/*:/app/classes:/app/resources:/home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf Exclude the JIB Classpath If the image is not a JIB image there could be occasions when automatically adding /app/libs/*:/app/classes:/app/resources to the classpath causes issues, for example one or more of those locations exists with files in that should not be on the classpath. In this case the Coherence CRD spec has a field to specify that the JIB classpath should not be used. The spec.jvm.useJibClasspath field can be set to false to exclude the JIB directories from the classpath (the default value is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. ",
            "title": "JIB Image Classpath"
        },
        {
            "location": "/docs/jvm/020_classpath",
            "text": " If an image will be used that has artifacts in locations other than the defaults discussed above then it is possible to specify additional elements to be added to the classpath. The jvm.classpath field in the Coherence CRD spec allows a list of extra classpath values to be provided. These elements will be added after the JIB classpath described above. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/lib/*\" - \"/data/config\" The classpath field adds /data/lib/* and /data/config to the classpath. In an image without the CLASSPATH or COHERENCE_HOME environment variables the effective classpath would be: There is no validation of the elements of the classpath. The elements will not be verified to ensure that the locations exist. As long as they are valid values to be used in a JVM classpath they will be accepted. ",
            "title": "Additional Classpath Elements"
        },
        {
            "location": "/docs/jvm/020_classpath",
            "text": " The Coherence container in the Pods in a Coherence resource deployment runs a Java application and as such requires a classpath with at a minimum coherence.jar . There are certain defaults that the Operator will use to work out the classpath to use but additional classpath elements can be provided to the configuration. The CLASSPATH Environment Variable If the image to be run has the CLASSPATH environment variable set this will be used as part of the classpath. The COHERENCE_HOME Environment Variable If the image to be run has the COHERENCE_HOME environment variable set this will be used to add the following elements to the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf These will be added to the end of the classpath. For example in an image that has CLASSPATH=/home/root/lib/* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf JIB Image Classpath A simple way to build Java images is using JIB . When JIB was with its Maven or Gradle plugin to produce an image it packages the application&#8217;s dependencies, classes and resources into a set of well-known locations: /app/libs/ - the jar files that the application depends on /app/classes - the application&#8217;s class files /app/resources - the application&#8217;s other resources By default, the Operator will add these locations to the classpath. These classpath elements will be added before any value set by the CLASSPATH or COHERENCE_HOME environment variables. For example in an image that has CLASSPATH=/home/root/lib/\\* and COHERENCE_HOME set to /oracle/coherence the effective classpath used will be: /app/libs/*:/app/classes:/app/resources:/home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf Exclude the JIB Classpath If the image is not a JIB image there could be occasions when automatically adding /app/libs/*:/app/classes:/app/resources to the classpath causes issues, for example one or more of those locations exists with files in that should not be on the classpath. In this case the Coherence CRD spec has a field to specify that the JIB classpath should not be used. The spec.jvm.useJibClasspath field can be set to false to exclude the JIB directories from the classpath (the default value is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. Additional Classpath Elements If an image will be used that has artifacts in locations other than the defaults discussed above then it is possible to specify additional elements to be added to the classpath. The jvm.classpath field in the Coherence CRD spec allows a list of extra classpath values to be provided. These elements will be added after the JIB classpath described above. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/lib/*\" - \"/data/config\" The classpath field adds /data/lib/* and /data/config to the classpath. In an image without the CLASSPATH or COHERENCE_HOME environment variables the effective classpath would be: There is no validation of the elements of the classpath. The elements will not be verified to ensure that the locations exist. As long as they are valid values to be used in a JVM classpath they will be accepted. ",
            "title": "Set the Classpath"
        },
        {
            "location": "/docs/jvm/020_classpath",
            "text": " The Operator supports environment variable expansion in classpath entries. The runner in the Coherence container will replace ${var} or $var in classpath entries with the corresponding environment variable name. For example if a container has an environment variable of APP_HOME set to /myapp then it could be used in the classpath like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"${APP_HOME}/lib/*\" The actual classpath entry at runtime will resolve to /myapp/lib/* Any environment variable that is present when the Coherence container starts can be used, this would include variables created as part of the image and variables specified in the Coherence yaml. ",
            "title": "Environment Variable Expansion"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " This example shows how to deploy a simple Coherence cluster in Kubernetes with Istio. Coherence can be configured to work with Istio , even if Istio is configured in Strict Mode. Coherence caches can be accessed from inside or outside the Kubernetes cluster via Coherence*Extend, REST, and other supported Coherence clients. Although Coherence itself can be configured to use TLS, when using Istio Coherence cluster members and clients can just use the default socket configurations and Istio will control and route all the traffic over mTLS. ",
            "title": "Running Coherence with Istio"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " For this example we make Istio run in \"strict\" mode so that it will not allow any traffic between Pods outside the Envoy proxy. If other modes are used, such as permissive, then Istio allows Pod to Pod communication so a cluster may appear to work in permissive mode, when it would not in strict mode. To set Istio to strict mode create the following yaml file. <markup lang=\"yaml\" title=\"istio-strict.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"default\" spec: mtls: mode: STRICT Install this yaml into the Istio system namespace with the following command: <markup lang=\"bash\" >kubectl -n istio-system apply istio-strict.yaml ",
            "title": "Enable Istio Strict Mode"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " The instructions assume that you are using a Kubernetes cluster with Istio installed and configured already. Enable Istio Strict Mode For this example we make Istio run in \"strict\" mode so that it will not allow any traffic between Pods outside the Envoy proxy. If other modes are used, such as permissive, then Istio allows Pod to Pod communication so a cluster may appear to work in permissive mode, when it would not in strict mode. To set Istio to strict mode create the following yaml file. <markup lang=\"yaml\" title=\"istio-strict.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"default\" spec: mtls: mode: STRICT Install this yaml into the Istio system namespace with the following command: <markup lang=\"bash\" >kubectl -n istio-system apply istio-strict.yaml ",
            "title": "Prerequisites"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " Istio is a \"Service Mesh\" so the clue to how Istio works in Kubernetes is in the name, it relies on the configuration of Kubernetes Services. This means that any ports than need to be accessed in Pods, including those using in \"Pod to Pod\" communication must be exposed via a Service. Usually a Pod can reach any port on another Pod even if it is not exposed in the container spec, but this is not the case when using Istio as only ports exposed by the Envoy proxy are allowed. For Coherence cluster membership, this means the cluster port and the local port must be exposed on a Service. To do this the local port must be configured to be a fixed port instead of the default ephemeral port. The default cluster port is 7574 and there is no reason to ever change this when running in containers. A fixed local port has to be configured for Coherence to work with Istio out of the box. Additional ports, management port, metrics port, etc. also need to be exposed if they are being used. Ideally, Coherence clusters are run as a StatefulSet in Kubernetes. This means that the Pods are configured with a host name and a subdomain based on the name of the StatefulSet headless service name, and it is this name that should be used to access Pods. Prerequisites The instructions assume that you are using a Kubernetes cluster with Istio installed and configured already. Enable Istio Strict Mode For this example we make Istio run in \"strict\" mode so that it will not allow any traffic between Pods outside the Envoy proxy. If other modes are used, such as permissive, then Istio allows Pod to Pod communication so a cluster may appear to work in permissive mode, when it would not in strict mode. To set Istio to strict mode create the following yaml file. <markup lang=\"yaml\" title=\"istio-strict.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"default\" spec: mtls: mode: STRICT Install this yaml into the Istio system namespace with the following command: <markup lang=\"bash\" >kubectl -n istio-system apply istio-strict.yaml ",
            "title": "How Does Coherence Work with Istio?"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " For Coherence cluster discovery to work in Kubernetes we have to configure Coherence well-known-addresses which requires a headless service. We cannot use the same headless service then we will create for the StatefulSet because the WKA service must have the publishNotReadyAddresses field set to true , whereas the StatefulSet service does not. We would not want the ports accessed via the StatefulSet service to route to unready Pods, but for cluster discovery we must allow unready Pods to be part of the Service. The discovery service can be created with yaml like that shown below. <markup lang=\"yaml\" title=\"wka-service.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-wka spec: clusterIP: None publishNotReadyAddresses: true selector: app: my-coherence-app version: 1.0.0 ports: - name: coherence port: 7574 targetPort: coherence appProtocol: tcp The service name is storeage-wka and this will be used to configure the Coherence WKA address in the cluster. The publishNotReadyAddresses field must be set to true The selector is configured to match a sub-set of the Pod labels configured in the StatefulSet We do not really need or care about the port for the cluster discovery service, but all Kubernetes services must have at least one port, so here we use the cluster port. We could use any random port, even one that nothing is listening on ",
            "title": "Cluster Discovery Service"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " All StatefulSets require a headless Service creating and the name of this Service is specified in the StatefulSet spec. All the ports mentioned above will be exposed on this service. The yaml for the service could look like this: <markup lang=\"yaml\" title=\"storage-service.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-headless spec: clusterIP: None selector: app: my-coherence-app version: 1.0.0 ports: - name: coherence port: 7574 targetPort: coherence appProtocol: tcp - name: coh-local port: 7575 targetPort: coh-local appProtocol: tcp - name: extend-proxy port: 20000 targetPort: extend-proxy appProtocol: tcp - name: grpc-proxy port: 1408 targetPort: grpc-proxy appProtocol: grpc - name: management port: 30000 targetPort: management appProtocol: http - name: metrics port: 9612 targetPort: metrics appProtocol: http The selector labels will match a sub-set of the labels specified for the Pods in the StatefulSet The Coherence cluster port 7574 is exposed with the name coherence mapping to the container port in the StatefulSet named coherence . This port has an appProtocol of tcp to tell Istio that the port traffic is raw TCP traffic. The Coherence local port 7575 is exposed with the name coh-local mapping to the container port in the StatefulSet named coh-local This port has an appProtocol of tcp to tell Istio that the port traffic is raw TCP traffic. The Coherence Extend proxy port 20000 is exposed with the name extend-proxy mapping to the container port in the StatefulSet named extend-proxy This port has an appProtocol of tcp to tell Istio that the port traffic is raw TCP traffic. The Coherence gRPC proxy port 1408 is exposed with the name grpc-proxy mapping to the container port in the StatefulSet named grpc-proxy This port has an appProtocol of grpc to tell Istio that the port traffic is gRPC traffic. The Coherence Management over REST port 30000 is exposed with the name management mapping to the container port in the StatefulSet named management This port has an appProtocol of http to tell Istio that the port traffic is http traffic. The Coherence Metrics port 9612 is exposed with the name metrics mapping to the container port in the StatefulSet named metrics This port has an appProtocol of http to tell Istio that the port traffic is http traffic. Note Istio requires ports to specify the protocol used for their traffic, and this can be done in two ways. Either using the appProtocol field for the ports, as shown above. Or, prefix the port name with the protocol, so instead of management the port name would be http-management ",
            "title": "StatefulSet Headless Service"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " With the two Services defined, the StatefulSet can now be configured. Istio <markup lang=\"yaml\" title=\"storage.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage labels: app: my-coherence-app version: 1.0.0 spec: selector: matchLabels: app: my-coherence-app version: 1.0.0 serviceName: storage-headless replicas: 3 podManagementPolicy: Parallel updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: app: my-coherence-app version: 1.0.0 spec: containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06.7 env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: NAMESPACE valueFrom: fieldRef: fieldPath: \"metadata.namespace\" - name: COHERENCE_WKA value: \"storage-wka.${NAMESPACE}.svc\" - name: COHERENCE_LOCALPORT value: \"7575\" - name: COHERENCE_LOCALHOST valueFrom: fieldRef: fieldPath: \"metadata.name\" - name: COHERENCE_MACHINE valueFrom: fieldRef: fieldPath: \"spec.nodeName\" - name: COHERENCE_MEMBER valueFrom: fieldRef: fieldPath: \"metadata.name\" - name: COHERENCE_EXTEND_PORT value: \"20000\" - name: COHERENCE_GRPC_SERVER_PORT value: \"1408\" ports: - name: coherence containerPort: 7574 - name: coh-local containerPort: 7575 - name: extend-proxy containerPort: 20000 - name: grpc-proxy containerPort: 1408 - name: management containerPort: 30000 - name: metrics containerPort: 9162 readinessProbe: httpGet: path: \"/ready\" port: 6676 scheme: \"HTTP\" livenessProbe: httpGet: path: \"/healthz\" port: 6676 scheme: \"HTTP\" All StatefulSets require a headless service, in this case the service will be named storage-headless to match the service above This example is using the CE 22.06 image The COHERENCE_CLUSTER environment variable sets the Coherence cluster name to test-cluster The NAMESPACE environment variable contains the namespace the StatefulSet is deployed into. The value is taken from the matadata.namespace field of the Pod. This is then used to create a fully qualified well known address value The COHERENCE_WKA environment variable sets address Coherence uses to perform a DNS lookup for cluster member IP addresses. In this case we use the name of the WKA service created above combined with the NAMESPACE environment variable to give a fully qualified service name. The COHERENCE_LOCALPORT environment variable sets the Coherence localport to 7575, which matches what was exposed in the Service ports and container ports The COHERENCE_LOCAHOST environment variable sets the hostname that Coherence binds to, in this case it will be the same as the Pod name by using the \"valueFrom\" setting to get the value from the Pod&#8217;s metadata.name field It is best practice to use the COHERENCE_MACHINE environment variable to set the Coherence machine label to the Kubernetes Node name. The machine name is used by Coherence when assigning backup partitions, so a backup of a partition will not be on the same Node as the primary owner of the partition. the same as the Pod name by using the \"valueFrom\" setting to get the value from the Pod&#8217;s metadata.name field It is best practice to use the COHERENCE_MEMBER environment variable to set the Coherence member name to the Pod name. All the ports required are exposed as container ports. The names must correspond to the names used for the container ports in the Service spec. As we are using Coherence CE 22.06 we can use Coherence built in health check endpoints for the readiness and liveness probes. Note The example above is using Coherence 22.06 which has built in health checks and health check endpoints which can be used as readiness and liveness probes in Kubernetes. These endpoints are only available if you start Coherence correctly using the Bootstrap API introduced in 22.06. Start Coherence using com.tangosol.net.Coherence as the main class. <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.net.Coherence Start Coherence in code: <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember().start().join(); See the Coherence Bootstrap API documentation for more details ",
            "title": "The StatefulSet"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " We will deploy the cluster into a Kubernetes namespace names coherence . Before deploying the cluster we need to ensure it has been labeled so that Istio will inject the Envoy proxy sidecar into the Pods. <markup lang=\"bash\" >kubectl create namespace coherence kubectl label namespace coherence istio-injection=enabled To deploy the cluster we just apply all three yaml files to Kubernetes. We could combine them into a single yaml file if we wanted to. <markup lang=\"bash\" >kubectl -n coherence apply -f wka-service.yaml kubectl -n coherence apply -f storage-service.yaml kubectl -n coherence apply -f storage.yaml If we list the services, we see the two services we created <markup lang=\"bash\" >$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE storage-headless ClusterIP None &lt;none&gt; 7574/TCP,7575/TCP,20000/TCP,1408/TCP,30000/TCP,9612/TCP 37m storage-wka ClusterIP None &lt;none&gt; 7574/TCP 16m If we list the Pods, we see three Pods, as the StatefulSet replicas field is set to three. <markup lang=\"bash\" >$ kubectl get pod NAME READY STATUS RESTARTS AGE storage-0 2/2 Running 0 7m47s storage-1 2/2 Running 0 7m47s storage-2 2/2 Running 0 7m47s We can use Istio&#8217;s Kiali dashboard to visualize the cluster we created. We labelled the resources with the app label with a value of my-coherence-app and we can see this application in the Kiali dashboard. The graph shows the cluster member Pods communicating with each other via the storage-headless service. The padlock icons show that this traffic is using mTLS even though Coherence has not been configured with TLS, this is being provided by Istio. ",
            "title": "Deploy the Cluster"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " The best way to run Coherence cluster members is to use a StatefulSet. Multiple StatefulSets can be created that are all part of the same Coherence cluster. In this example we will run a Coherence cluster using the CE image. This image starts Coherence with health checks enabled on port 6676, an Extend proxy listening on port 20000, a gRPC proxy on port 1408, the cluster port set to 7574. We will also enable Coherence Management over REST on port 30000, and metrics on port 9612. We will set the Coherence local port to a fixed value of 7575. Note Istio has a few requirements for how Kubernetes resources are configured. One of those is labels, where an app and version label are required to specify the application name that the resource is part of and the version of that application. All the resources in this example contains those labels. Cluster Discovery Service For Coherence cluster discovery to work in Kubernetes we have to configure Coherence well-known-addresses which requires a headless service. We cannot use the same headless service then we will create for the StatefulSet because the WKA service must have the publishNotReadyAddresses field set to true , whereas the StatefulSet service does not. We would not want the ports accessed via the StatefulSet service to route to unready Pods, but for cluster discovery we must allow unready Pods to be part of the Service. The discovery service can be created with yaml like that shown below. <markup lang=\"yaml\" title=\"wka-service.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-wka spec: clusterIP: None publishNotReadyAddresses: true selector: app: my-coherence-app version: 1.0.0 ports: - name: coherence port: 7574 targetPort: coherence appProtocol: tcp The service name is storeage-wka and this will be used to configure the Coherence WKA address in the cluster. The publishNotReadyAddresses field must be set to true The selector is configured to match a sub-set of the Pod labels configured in the StatefulSet We do not really need or care about the port for the cluster discovery service, but all Kubernetes services must have at least one port, so here we use the cluster port. We could use any random port, even one that nothing is listening on StatefulSet Headless Service All StatefulSets require a headless Service creating and the name of this Service is specified in the StatefulSet spec. All the ports mentioned above will be exposed on this service. The yaml for the service could look like this: <markup lang=\"yaml\" title=\"storage-service.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-headless spec: clusterIP: None selector: app: my-coherence-app version: 1.0.0 ports: - name: coherence port: 7574 targetPort: coherence appProtocol: tcp - name: coh-local port: 7575 targetPort: coh-local appProtocol: tcp - name: extend-proxy port: 20000 targetPort: extend-proxy appProtocol: tcp - name: grpc-proxy port: 1408 targetPort: grpc-proxy appProtocol: grpc - name: management port: 30000 targetPort: management appProtocol: http - name: metrics port: 9612 targetPort: metrics appProtocol: http The selector labels will match a sub-set of the labels specified for the Pods in the StatefulSet The Coherence cluster port 7574 is exposed with the name coherence mapping to the container port in the StatefulSet named coherence . This port has an appProtocol of tcp to tell Istio that the port traffic is raw TCP traffic. The Coherence local port 7575 is exposed with the name coh-local mapping to the container port in the StatefulSet named coh-local This port has an appProtocol of tcp to tell Istio that the port traffic is raw TCP traffic. The Coherence Extend proxy port 20000 is exposed with the name extend-proxy mapping to the container port in the StatefulSet named extend-proxy This port has an appProtocol of tcp to tell Istio that the port traffic is raw TCP traffic. The Coherence gRPC proxy port 1408 is exposed with the name grpc-proxy mapping to the container port in the StatefulSet named grpc-proxy This port has an appProtocol of grpc to tell Istio that the port traffic is gRPC traffic. The Coherence Management over REST port 30000 is exposed with the name management mapping to the container port in the StatefulSet named management This port has an appProtocol of http to tell Istio that the port traffic is http traffic. The Coherence Metrics port 9612 is exposed with the name metrics mapping to the container port in the StatefulSet named metrics This port has an appProtocol of http to tell Istio that the port traffic is http traffic. Note Istio requires ports to specify the protocol used for their traffic, and this can be done in two ways. Either using the appProtocol field for the ports, as shown above. Or, prefix the port name with the protocol, so instead of management the port name would be http-management The StatefulSet With the two Services defined, the StatefulSet can now be configured. Istio <markup lang=\"yaml\" title=\"storage.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage labels: app: my-coherence-app version: 1.0.0 spec: selector: matchLabels: app: my-coherence-app version: 1.0.0 serviceName: storage-headless replicas: 3 podManagementPolicy: Parallel updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: app: my-coherence-app version: 1.0.0 spec: containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06.7 env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: NAMESPACE valueFrom: fieldRef: fieldPath: \"metadata.namespace\" - name: COHERENCE_WKA value: \"storage-wka.${NAMESPACE}.svc\" - name: COHERENCE_LOCALPORT value: \"7575\" - name: COHERENCE_LOCALHOST valueFrom: fieldRef: fieldPath: \"metadata.name\" - name: COHERENCE_MACHINE valueFrom: fieldRef: fieldPath: \"spec.nodeName\" - name: COHERENCE_MEMBER valueFrom: fieldRef: fieldPath: \"metadata.name\" - name: COHERENCE_EXTEND_PORT value: \"20000\" - name: COHERENCE_GRPC_SERVER_PORT value: \"1408\" ports: - name: coherence containerPort: 7574 - name: coh-local containerPort: 7575 - name: extend-proxy containerPort: 20000 - name: grpc-proxy containerPort: 1408 - name: management containerPort: 30000 - name: metrics containerPort: 9162 readinessProbe: httpGet: path: \"/ready\" port: 6676 scheme: \"HTTP\" livenessProbe: httpGet: path: \"/healthz\" port: 6676 scheme: \"HTTP\" All StatefulSets require a headless service, in this case the service will be named storage-headless to match the service above This example is using the CE 22.06 image The COHERENCE_CLUSTER environment variable sets the Coherence cluster name to test-cluster The NAMESPACE environment variable contains the namespace the StatefulSet is deployed into. The value is taken from the matadata.namespace field of the Pod. This is then used to create a fully qualified well known address value The COHERENCE_WKA environment variable sets address Coherence uses to perform a DNS lookup for cluster member IP addresses. In this case we use the name of the WKA service created above combined with the NAMESPACE environment variable to give a fully qualified service name. The COHERENCE_LOCALPORT environment variable sets the Coherence localport to 7575, which matches what was exposed in the Service ports and container ports The COHERENCE_LOCAHOST environment variable sets the hostname that Coherence binds to, in this case it will be the same as the Pod name by using the \"valueFrom\" setting to get the value from the Pod&#8217;s metadata.name field It is best practice to use the COHERENCE_MACHINE environment variable to set the Coherence machine label to the Kubernetes Node name. The machine name is used by Coherence when assigning backup partitions, so a backup of a partition will not be on the same Node as the primary owner of the partition. the same as the Pod name by using the \"valueFrom\" setting to get the value from the Pod&#8217;s metadata.name field It is best practice to use the COHERENCE_MEMBER environment variable to set the Coherence member name to the Pod name. All the ports required are exposed as container ports. The names must correspond to the names used for the container ports in the Service spec. As we are using Coherence CE 22.06 we can use Coherence built in health check endpoints for the readiness and liveness probes. Note The example above is using Coherence 22.06 which has built in health checks and health check endpoints which can be used as readiness and liveness probes in Kubernetes. These endpoints are only available if you start Coherence correctly using the Bootstrap API introduced in 22.06. Start Coherence using com.tangosol.net.Coherence as the main class. <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.net.Coherence Start Coherence in code: <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember().start().join(); See the Coherence Bootstrap API documentation for more details Deploy the Cluster We will deploy the cluster into a Kubernetes namespace names coherence . Before deploying the cluster we need to ensure it has been labeled so that Istio will inject the Envoy proxy sidecar into the Pods. <markup lang=\"bash\" >kubectl create namespace coherence kubectl label namespace coherence istio-injection=enabled To deploy the cluster we just apply all three yaml files to Kubernetes. We could combine them into a single yaml file if we wanted to. <markup lang=\"bash\" >kubectl -n coherence apply -f wka-service.yaml kubectl -n coherence apply -f storage-service.yaml kubectl -n coherence apply -f storage.yaml If we list the services, we see the two services we created <markup lang=\"bash\" >$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE storage-headless ClusterIP None &lt;none&gt; 7574/TCP,7575/TCP,20000/TCP,1408/TCP,30000/TCP,9612/TCP 37m storage-wka ClusterIP None &lt;none&gt; 7574/TCP 16m If we list the Pods, we see three Pods, as the StatefulSet replicas field is set to three. <markup lang=\"bash\" >$ kubectl get pod NAME READY STATUS RESTARTS AGE storage-0 2/2 Running 0 7m47s storage-1 2/2 Running 0 7m47s storage-2 2/2 Running 0 7m47s We can use Istio&#8217;s Kiali dashboard to visualize the cluster we created. We labelled the resources with the app label with a value of my-coherence-app and we can see this application in the Kiali dashboard. The graph shows the cluster member Pods communicating with each other via the storage-headless service. The padlock icons show that this traffic is using mTLS even though Coherence has not been configured with TLS, this is being provided by Istio. ",
            "title": "Create a Coherence Cluster"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " To work correctly with Istio a Coherence Extend proxy in the server&#8217;s cache configuration file must be configured to use a fixed port. For example, the XML snippet below configures the proxy to bind to all interfaces ( 0.0.0.0 ) on port 20000. <markup lang=\"xml\" > &lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;local-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;0.0.0.0&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; The port could be changed by setting the COHERENCE_EXTEND_PORT environment variable in the server yaml. <markup lang=\"yaml\" > spec: containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06.7 env: - name: COHERENCE_EXTEND_PORT value: \"20001\" The Extend port should be exposed on the StatefulSet (as shown in the StatefulSet yaml above) and on the StatefulSet headless service so that clients can discover it and connect to it (as shown in the Service yaml above). Tip The default cache configuration file used by Coherence, and used in the Coherence images published on GitHub, contains an Extend Proxy service that uses the COHERENCE_EXTEND_PORT environment variable to set the port. ",
            "title": "Extend Proxy Configuration"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " The Coherence gRPC proxy binds to an ephemeral port by default. This port can be changed by using the COHERENCE_GRPC_SERVER_PORT environment variable; <markup lang=\"yaml\" > spec: containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06.7 env: - name: COHERENCE_GRPC_SERVER_PORT value: \"1408\" Tip The default configuration used by Coherence images published on GitHub sets the gRPC port to 1408. Once the server StatefulSet and Service have been properly configured the clients can be configured. The options available for this will depend on where the client will run. ",
            "title": "gRPC Proxy Configuration"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " For this example we need a simple client image that can be run with different configurations. Instead of building an application we will use a Coherence Image from GitHub combined with the utilities from the Coherence Operator. The simple Dockerfile below is a multistage build file. It uses the Operator image as a \"builder\" and then the Coherence image as the base. Various utilities are copied from the Operator image into the base. <markup title=\"Dockerfile\" >FROM ghcr.io/oracle/coherence-operator:3.3.4 AS Builder FROM ghcr.io/oracle/coherence-ce:22.06.7 COPY --from=Builder /files /files COPY --from=Builder /files/lib/coherence-operator.jar /app/libs/coherence-operator.jar COPY coherence-java-client-22.06.7.jar /app/libs/coherence-java-client-22.06.7.jar ENTRYPOINT [\"files/runner\"] CMD [\"-h\"] As we are going to show both the Coherence Extend client and gRPC client we need to add the Coherence gRPC client jar. We can download this with curl to the same directory as the Dockerfile. <markup lang=\"bash\" >curl -s https://repo1.maven.org/maven2/com/oracle/coherence/ce/coherence-java-client/22.06.7/coherence-java-client-22.06.7.jar \\ -o coherence-java-client-22.06.7.jar Build the image with the following command: <markup lang=\"bash\" >docker build -t coherence-client:1.0.0 -f Dockerfile . There will now be an imaged named coherence-client:1.0.0 which can be pushed somewhere Kubernetes can see it. We will use this example below. ",
            "title": "Build a Client Image"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " The minimal configuration in a client&#8217;s cache configuration file is shown below. This configuration will use the Coherence NameService to look up the endpoints for the Extend Proxy services running in the Coherence cluster. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;/remote-cache-scheme&gt; For the NameService to work in Kubernetes, the client must be configured with the same cluster name, the same well known addresses and same cluster port as the server. When using Istio the server&#8217;s cluster port, local port and Extend port should be exposed on the StatefulSet headless service. The client&#8217;s well known address is then set to the qualified Kubernetes DNS name for the server&#8217;s StatefulSet headless service. These can all be set using environment variables in the yaml for the client. For example, assuming the client will connect to the Coherence cluster configured in the StatefulSet above: <markup lang=\"yaml\" > env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: COHERENCE_WKA value: \"storage-headless.coherence.svc\" The cluster name is set to test-cluster the same as the StatefulSet The COHERENCE_WKA value is set to the DNS name of the StatefulSet headless service, which has the format &lt;service-name&gt;.&lt;namespace&gt;.svc so in this case storage-headless.coherence.svc ",
            "title": "Using the Coherence NameService Configuration"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " Using the coherence-client:1.0.0 image created above, we can run a simple Coherence client Pod. <markup lang=\"yaml\" title=\"extend-client-pod.yaml\" >apiVersion: v1 kind: Pod metadata: name: client labels: app: coherence-client version: 1.0.0 spec: containers: - name: coherence image: coherence-client:1.0.0 command: - /files/runner - sleep - \"15m\" env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: COHERENCE_WKA value: \"storage-headless.coherence.svc\" - name: COHERENCE_CLIENT value: \"remote\" The container image is set to the client image built above coherence-client:1.0.0 The command line the container will run is /files/runner sleep 15m which will just sleep for 15 minutes The Coherence cluster name is set to the same name as the server deployed above in the StatefulSet yaml The WKA address is set to the StatefulSet&#8217;s headless service name storage-headless.coherence.svc For this example the COHERENCE_CLIENT which sets the default cache configuration file to run as an Extend client, using the NameService to look up the proxies. We can deploy the client into Kubernetes <markup lang=\"bash\" >kubectl -n coherence apply -f extend-client-pod.yaml We deployed the client into the same namespace as the cluster, we could easily have deployed it to another namespace. If we list the Pods we will see the cluster and the client. All Pods has two containers, one being the Istio side-car. <markup lang=\"bash\" >$ k -n coherence get pod NAME READY STATUS RESTARTS AGE storage-0 2/2 Running 0 105m storage-1 2/2 Running 0 105m storage-2 2/2 Running 0 105m client 2/2 Running 0 8m27s Now we can exec into the Pod and start a Coherence QueryPlus console session using the following command: <markup lang=\"bash\" >kubectl -n coherence exec -it client -- /files/runner queryplus The QueryPlus session will start and eventually display the CohQL&gt; prompt: <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; A simple command to try is just creating a cache, so at the prompt type the command create cache test which will create a cache named test . If all is configured correctly this client will connect to the cluster over Extend and create the cache called test and return to the CohQL prompt. <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; create cache test We can also try selecting data from the cache using the CohQL query select * from test (which will return nothing as the cache is empty). <markup lang=\"bash\" >CohQL&gt; select * from test Results CohQL&gt; If we now look at the Kiali dashboard we can see that the client application has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. If we look at the Kiali dashboard traffic tab for the client application we can see the traffic was TCP over mTLS. To exit from the CohQL&gt; prompt type the bye command. The delete the client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f extend-client-pod.yaml ",
            "title": "Run an Extend Client Pod"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " We can run the same image as a gRPC client. For this example, instead of the NameService we will configure Coherence to <markup lang=\"yaml\" title=\"grpc-client-pod.yaml\" >apiVersion: v1 kind: Pod metadata: name: client labels: app: coherence-client version: 1.0.0 spec: containers: - name: coherence image: coherence-client:1.0.0 command: - /files/runner - sleep - \"15m\" env: - name: COHERENCE_CLIENT value: \"grpc-fixed\" - name: COHERENCE_GRPC_ADDRESS value: \"storage-headless.coherence.svc\" - name: COHERENCE_GRPC_PORT value: \"1408\" We can now deploy the gRPC client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f grpc-client-pod.yaml And exec into the Pod to create a QueryPlus session. <markup lang=\"bash\" >kubectl -n coherence exec -it client -- /files/runner queryplus We can run the same create cache test and select * from test command that we ran above to connect the client to the cluster. This time the client should be connecting over gRPC. If we now look at the Kiali dashboard we can see again that the client application has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. If we look at the Kiali dashboard traffic tab for the client application we can see that this time the traffic was gRPC over mTLS. To exit from the CohQL&gt; prompt type the bye command. The delete the client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f extend-client-pod.yaml ",
            "title": "Run a gRPC Client Pod"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " If the clients are also inside the cluster they can be configured to connect using the StatefulSet headless service as the hostname for the proxy endpoints. There are two options for configuring Extend and Clients inside Kubernetes can also use the minimal Coherence NameService configuration where the StatefulSet service name is used as the client&#8217;s WKA address and the same cluster name is configured. Clients external to the Kubernetes cluster can be configured using any of the ingress or gateway features of Istio and Kubernetes. All the different ways to do this are beyond the scope of this simple example as there are many, and they depend on the versions of Istio and Kubernetes being used. Build a Client Image For this example we need a simple client image that can be run with different configurations. Instead of building an application we will use a Coherence Image from GitHub combined with the utilities from the Coherence Operator. The simple Dockerfile below is a multistage build file. It uses the Operator image as a \"builder\" and then the Coherence image as the base. Various utilities are copied from the Operator image into the base. <markup title=\"Dockerfile\" >FROM ghcr.io/oracle/coherence-operator:3.3.4 AS Builder FROM ghcr.io/oracle/coherence-ce:22.06.7 COPY --from=Builder /files /files COPY --from=Builder /files/lib/coherence-operator.jar /app/libs/coherence-operator.jar COPY coherence-java-client-22.06.7.jar /app/libs/coherence-java-client-22.06.7.jar ENTRYPOINT [\"files/runner\"] CMD [\"-h\"] As we are going to show both the Coherence Extend client and gRPC client we need to add the Coherence gRPC client jar. We can download this with curl to the same directory as the Dockerfile. <markup lang=\"bash\" >curl -s https://repo1.maven.org/maven2/com/oracle/coherence/ce/coherence-java-client/22.06.7/coherence-java-client-22.06.7.jar \\ -o coherence-java-client-22.06.7.jar Build the image with the following command: <markup lang=\"bash\" >docker build -t coherence-client:1.0.0 -f Dockerfile . There will now be an imaged named coherence-client:1.0.0 which can be pushed somewhere Kubernetes can see it. We will use this example below. Using the Coherence NameService Configuration The minimal configuration in a client&#8217;s cache configuration file is shown below. This configuration will use the Coherence NameService to look up the endpoints for the Extend Proxy services running in the Coherence cluster. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;/remote-cache-scheme&gt; For the NameService to work in Kubernetes, the client must be configured with the same cluster name, the same well known addresses and same cluster port as the server. When using Istio the server&#8217;s cluster port, local port and Extend port should be exposed on the StatefulSet headless service. The client&#8217;s well known address is then set to the qualified Kubernetes DNS name for the server&#8217;s StatefulSet headless service. These can all be set using environment variables in the yaml for the client. For example, assuming the client will connect to the Coherence cluster configured in the StatefulSet above: <markup lang=\"yaml\" > env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: COHERENCE_WKA value: \"storage-headless.coherence.svc\" The cluster name is set to test-cluster the same as the StatefulSet The COHERENCE_WKA value is set to the DNS name of the StatefulSet headless service, which has the format &lt;service-name&gt;.&lt;namespace&gt;.svc so in this case storage-headless.coherence.svc Run an Extend Client Pod Using the coherence-client:1.0.0 image created above, we can run a simple Coherence client Pod. <markup lang=\"yaml\" title=\"extend-client-pod.yaml\" >apiVersion: v1 kind: Pod metadata: name: client labels: app: coherence-client version: 1.0.0 spec: containers: - name: coherence image: coherence-client:1.0.0 command: - /files/runner - sleep - \"15m\" env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: COHERENCE_WKA value: \"storage-headless.coherence.svc\" - name: COHERENCE_CLIENT value: \"remote\" The container image is set to the client image built above coherence-client:1.0.0 The command line the container will run is /files/runner sleep 15m which will just sleep for 15 minutes The Coherence cluster name is set to the same name as the server deployed above in the StatefulSet yaml The WKA address is set to the StatefulSet&#8217;s headless service name storage-headless.coherence.svc For this example the COHERENCE_CLIENT which sets the default cache configuration file to run as an Extend client, using the NameService to look up the proxies. We can deploy the client into Kubernetes <markup lang=\"bash\" >kubectl -n coherence apply -f extend-client-pod.yaml We deployed the client into the same namespace as the cluster, we could easily have deployed it to another namespace. If we list the Pods we will see the cluster and the client. All Pods has two containers, one being the Istio side-car. <markup lang=\"bash\" >$ k -n coherence get pod NAME READY STATUS RESTARTS AGE storage-0 2/2 Running 0 105m storage-1 2/2 Running 0 105m storage-2 2/2 Running 0 105m client 2/2 Running 0 8m27s Now we can exec into the Pod and start a Coherence QueryPlus console session using the following command: <markup lang=\"bash\" >kubectl -n coherence exec -it client -- /files/runner queryplus The QueryPlus session will start and eventually display the CohQL&gt; prompt: <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; A simple command to try is just creating a cache, so at the prompt type the command create cache test which will create a cache named test . If all is configured correctly this client will connect to the cluster over Extend and create the cache called test and return to the CohQL prompt. <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; create cache test We can also try selecting data from the cache using the CohQL query select * from test (which will return nothing as the cache is empty). <markup lang=\"bash\" >CohQL&gt; select * from test Results CohQL&gt; If we now look at the Kiali dashboard we can see that the client application has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. If we look at the Kiali dashboard traffic tab for the client application we can see the traffic was TCP over mTLS. To exit from the CohQL&gt; prompt type the bye command. The delete the client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f extend-client-pod.yaml Run a gRPC Client Pod We can run the same image as a gRPC client. For this example, instead of the NameService we will configure Coherence to <markup lang=\"yaml\" title=\"grpc-client-pod.yaml\" >apiVersion: v1 kind: Pod metadata: name: client labels: app: coherence-client version: 1.0.0 spec: containers: - name: coherence image: coherence-client:1.0.0 command: - /files/runner - sleep - \"15m\" env: - name: COHERENCE_CLIENT value: \"grpc-fixed\" - name: COHERENCE_GRPC_ADDRESS value: \"storage-headless.coherence.svc\" - name: COHERENCE_GRPC_PORT value: \"1408\" We can now deploy the gRPC client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f grpc-client-pod.yaml And exec into the Pod to create a QueryPlus session. <markup lang=\"bash\" >kubectl -n coherence exec -it client -- /files/runner queryplus We can run the same create cache test and select * from test command that we ran above to connect the client to the cluster. This time the client should be connecting over gRPC. If we now look at the Kiali dashboard we can see again that the client application has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. If we look at the Kiali dashboard traffic tab for the client application we can see that this time the traffic was gRPC over mTLS. To exit from the CohQL&gt; prompt type the bye command. The delete the client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f extend-client-pod.yaml ",
            "title": "Clients Inside Kubernetes"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " When connecting Coherence Extend or gRPC clients from outside Kubernetes, the Coherence NameService cannot be used by clients to look up the endpoints. The clients must be configured with fixed endpoints using the hostnames and ports of the configured ingress or gateway services. Exactly how this is done will depend on the versions of Istio and Kubernetes being used and whether Ingress or the Kubernetes Gateway API is used. The different options available make it impossible to build an example that can cover all these scenarios. ",
            "title": "Clients Outside Kubernetes"
        },
        {
            "location": "/examples/no-operator/04_istio/README",
            "text": " Coherence clients (Extend or gRPC) can be configured to connect to the Coherence cluster. Extend Proxy Configuration To work correctly with Istio a Coherence Extend proxy in the server&#8217;s cache configuration file must be configured to use a fixed port. For example, the XML snippet below configures the proxy to bind to all interfaces ( 0.0.0.0 ) on port 20000. <markup lang=\"xml\" > &lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;local-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;0.0.0.0&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; The port could be changed by setting the COHERENCE_EXTEND_PORT environment variable in the server yaml. <markup lang=\"yaml\" > spec: containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06.7 env: - name: COHERENCE_EXTEND_PORT value: \"20001\" The Extend port should be exposed on the StatefulSet (as shown in the StatefulSet yaml above) and on the StatefulSet headless service so that clients can discover it and connect to it (as shown in the Service yaml above). Tip The default cache configuration file used by Coherence, and used in the Coherence images published on GitHub, contains an Extend Proxy service that uses the COHERENCE_EXTEND_PORT environment variable to set the port. gRPC Proxy Configuration The Coherence gRPC proxy binds to an ephemeral port by default. This port can be changed by using the COHERENCE_GRPC_SERVER_PORT environment variable; <markup lang=\"yaml\" > spec: containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06.7 env: - name: COHERENCE_GRPC_SERVER_PORT value: \"1408\" Tip The default configuration used by Coherence images published on GitHub sets the gRPC port to 1408. Once the server StatefulSet and Service have been properly configured the clients can be configured. The options available for this will depend on where the client will run. Clients Inside Kubernetes If the clients are also inside the cluster they can be configured to connect using the StatefulSet headless service as the hostname for the proxy endpoints. There are two options for configuring Extend and Clients inside Kubernetes can also use the minimal Coherence NameService configuration where the StatefulSet service name is used as the client&#8217;s WKA address and the same cluster name is configured. Clients external to the Kubernetes cluster can be configured using any of the ingress or gateway features of Istio and Kubernetes. All the different ways to do this are beyond the scope of this simple example as there are many, and they depend on the versions of Istio and Kubernetes being used. Build a Client Image For this example we need a simple client image that can be run with different configurations. Instead of building an application we will use a Coherence Image from GitHub combined with the utilities from the Coherence Operator. The simple Dockerfile below is a multistage build file. It uses the Operator image as a \"builder\" and then the Coherence image as the base. Various utilities are copied from the Operator image into the base. <markup title=\"Dockerfile\" >FROM ghcr.io/oracle/coherence-operator:3.3.4 AS Builder FROM ghcr.io/oracle/coherence-ce:22.06.7 COPY --from=Builder /files /files COPY --from=Builder /files/lib/coherence-operator.jar /app/libs/coherence-operator.jar COPY coherence-java-client-22.06.7.jar /app/libs/coherence-java-client-22.06.7.jar ENTRYPOINT [\"files/runner\"] CMD [\"-h\"] As we are going to show both the Coherence Extend client and gRPC client we need to add the Coherence gRPC client jar. We can download this with curl to the same directory as the Dockerfile. <markup lang=\"bash\" >curl -s https://repo1.maven.org/maven2/com/oracle/coherence/ce/coherence-java-client/22.06.7/coherence-java-client-22.06.7.jar \\ -o coherence-java-client-22.06.7.jar Build the image with the following command: <markup lang=\"bash\" >docker build -t coherence-client:1.0.0 -f Dockerfile . There will now be an imaged named coherence-client:1.0.0 which can be pushed somewhere Kubernetes can see it. We will use this example below. Using the Coherence NameService Configuration The minimal configuration in a client&#8217;s cache configuration file is shown below. This configuration will use the Coherence NameService to look up the endpoints for the Extend Proxy services running in the Coherence cluster. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;/remote-cache-scheme&gt; For the NameService to work in Kubernetes, the client must be configured with the same cluster name, the same well known addresses and same cluster port as the server. When using Istio the server&#8217;s cluster port, local port and Extend port should be exposed on the StatefulSet headless service. The client&#8217;s well known address is then set to the qualified Kubernetes DNS name for the server&#8217;s StatefulSet headless service. These can all be set using environment variables in the yaml for the client. For example, assuming the client will connect to the Coherence cluster configured in the StatefulSet above: <markup lang=\"yaml\" > env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: COHERENCE_WKA value: \"storage-headless.coherence.svc\" The cluster name is set to test-cluster the same as the StatefulSet The COHERENCE_WKA value is set to the DNS name of the StatefulSet headless service, which has the format &lt;service-name&gt;.&lt;namespace&gt;.svc so in this case storage-headless.coherence.svc Run an Extend Client Pod Using the coherence-client:1.0.0 image created above, we can run a simple Coherence client Pod. <markup lang=\"yaml\" title=\"extend-client-pod.yaml\" >apiVersion: v1 kind: Pod metadata: name: client labels: app: coherence-client version: 1.0.0 spec: containers: - name: coherence image: coherence-client:1.0.0 command: - /files/runner - sleep - \"15m\" env: - name: COHERENCE_CLUSTER value: \"test-cluster\" - name: COHERENCE_WKA value: \"storage-headless.coherence.svc\" - name: COHERENCE_CLIENT value: \"remote\" The container image is set to the client image built above coherence-client:1.0.0 The command line the container will run is /files/runner sleep 15m which will just sleep for 15 minutes The Coherence cluster name is set to the same name as the server deployed above in the StatefulSet yaml The WKA address is set to the StatefulSet&#8217;s headless service name storage-headless.coherence.svc For this example the COHERENCE_CLIENT which sets the default cache configuration file to run as an Extend client, using the NameService to look up the proxies. We can deploy the client into Kubernetes <markup lang=\"bash\" >kubectl -n coherence apply -f extend-client-pod.yaml We deployed the client into the same namespace as the cluster, we could easily have deployed it to another namespace. If we list the Pods we will see the cluster and the client. All Pods has two containers, one being the Istio side-car. <markup lang=\"bash\" >$ k -n coherence get pod NAME READY STATUS RESTARTS AGE storage-0 2/2 Running 0 105m storage-1 2/2 Running 0 105m storage-2 2/2 Running 0 105m client 2/2 Running 0 8m27s Now we can exec into the Pod and start a Coherence QueryPlus console session using the following command: <markup lang=\"bash\" >kubectl -n coherence exec -it client -- /files/runner queryplus The QueryPlus session will start and eventually display the CohQL&gt; prompt: <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; A simple command to try is just creating a cache, so at the prompt type the command create cache test which will create a cache named test . If all is configured correctly this client will connect to the cluster over Extend and create the cache called test and return to the CohQL prompt. <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; create cache test We can also try selecting data from the cache using the CohQL query select * from test (which will return nothing as the cache is empty). <markup lang=\"bash\" >CohQL&gt; select * from test Results CohQL&gt; If we now look at the Kiali dashboard we can see that the client application has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. If we look at the Kiali dashboard traffic tab for the client application we can see the traffic was TCP over mTLS. To exit from the CohQL&gt; prompt type the bye command. The delete the client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f extend-client-pod.yaml Run a gRPC Client Pod We can run the same image as a gRPC client. For this example, instead of the NameService we will configure Coherence to <markup lang=\"yaml\" title=\"grpc-client-pod.yaml\" >apiVersion: v1 kind: Pod metadata: name: client labels: app: coherence-client version: 1.0.0 spec: containers: - name: coherence image: coherence-client:1.0.0 command: - /files/runner - sleep - \"15m\" env: - name: COHERENCE_CLIENT value: \"grpc-fixed\" - name: COHERENCE_GRPC_ADDRESS value: \"storage-headless.coherence.svc\" - name: COHERENCE_GRPC_PORT value: \"1408\" We can now deploy the gRPC client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f grpc-client-pod.yaml And exec into the Pod to create a QueryPlus session. <markup lang=\"bash\" >kubectl -n coherence exec -it client -- /files/runner queryplus We can run the same create cache test and select * from test command that we ran above to connect the client to the cluster. This time the client should be connecting over gRPC. If we now look at the Kiali dashboard we can see again that the client application has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. If we look at the Kiali dashboard traffic tab for the client application we can see that this time the traffic was gRPC over mTLS. To exit from the CohQL&gt; prompt type the bye command. The delete the client Pod <markup lang=\"bash\" >kubectl -n coherence delete -f extend-client-pod.yaml Clients Outside Kubernetes When connecting Coherence Extend or gRPC clients from outside Kubernetes, the Coherence NameService cannot be used by clients to look up the endpoints. The clients must be configured with fixed endpoints using the hostnames and ports of the configured ingress or gateway services. Exactly how this is done will depend on the versions of Istio and Kubernetes being used and whether Ingress or the Kubernetes Gateway API is used. The different options available make it impossible to build an example that can cover all these scenarios. ",
            "title": "Coherence Clients"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " The Kibana dashboard files are located in the Coherence operator source in the dashboards/kibana directory. The method of importing the dashboards into Kibana will depend on how Kibana is being run. The simplest method is just to import the json file using the Kibana web UI. An alternative approach is to load the dashboard into a ConfigMap in Kubernetes that is mounted into the Kibana Pod and then trigger an import when Kibana starts. As there are many ways to do this depending on the specifics of the version of Kibana being used, exact instructions are beyond the scope fo this guide. ",
            "title": "Importing Kibana Dashboards"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": "",
            "title": "Kibana Dashboards &amp; Searches"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " Dashboards Coherence Cluster - All Messages Coherence Cluster - Errors and Warnings Coherence Cluster - Persistence Coherence Cluster - Configuration Messages Coherence Cluster - Network Coherence Cluster - Partitions Coherence Cluster - Message Sources Searches ",
            "title": "Table of Contents"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. ",
            "title": "1. Coherence Cluster - All Messages"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. ",
            "title": "2. Coherence Cluster - Errors and Warnings"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " This dashboard shows Persistence related messages including failed and successful operations. ",
            "title": "3. Coherence Cluster - Persistence"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. ",
            "title": "4. Coherence Cluster - Configuration Messages"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " Information from all dashboards (and queries) can be filtered using the standard Kibana date/time filtering in the top right of the UI, as well as the Add a filter button. 1. Coherence Cluster - All Messages This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. 2. Coherence Cluster - Errors and Warnings This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. 3. Coherence Cluster - Persistence This dashboard shows Persistence related messages including failed and successful operations. 4. Coherence Cluster - Configuration Messages This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. ",
            "title": "Dashboards"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " Shows partition transfer and partition loss messages. ",
            "title": "6. Coherence Cluster - Partitions"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " Shows the source (thread) for messages Users can drill-down by cluster, host and message level. ",
            "title": "7. Coherence Cluster - Message Sources"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " This dashboard hows network related messages, such as communication delays and TCP ring disconnects. 6. Coherence Cluster - Partitions Shows partition transfer and partition loss messages. 7. Coherence Cluster - Message Sources Shows the source (thread) for messages Users can drill-down by cluster, host and message level. ",
            "title": "5. Coherence Cluster - Network"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " A number of searches are automatically includes which can help assist in diagnosis and troubleshooting a Coherence cluster. They can be accessed via the Discover side-bar and selecting `Open . These are grouped into the following general categories: Cluster - Cluster join, discovery, heartbeat, member joining and stopping messages Cache - Cache restarting, exceptions and index exception messages Configuration - Configuration loading and not loading messages Persistence - Persistence success and failure messages Network - Network communications delays, disconnects, timeouts and terminations Partition - Partition loss, ownership and transfer related messages Member - Member thread dump, join and leave messages Errors - All Error messages only Federation - Federation participant, disconnection, connection, errors and other messages ",
            "title": "Searches"
        },
        {
            "location": "/docs/logging/030_kibana",
            "text": " Kibana is often used to anyalze log files that have been collected into Elasticsearch. The Coherence Operator provides a number of Kibana dashboards and queries to allow you to view and analyze logs from your Coherence clusters. Importing Kibana Dashboards The Kibana dashboard files are located in the Coherence operator source in the dashboards/kibana directory. The method of importing the dashboards into Kibana will depend on how Kibana is being run. The simplest method is just to import the json file using the Kibana web UI. An alternative approach is to load the dashboard into a ConfigMap in Kubernetes that is mounted into the Kibana Pod and then trigger an import when Kibana starts. As there are many ways to do this depending on the specifics of the version of Kibana being used, exact instructions are beyond the scope fo this guide. Kibana Dashboards &amp; Searches Table of Contents Dashboards Coherence Cluster - All Messages Coherence Cluster - Errors and Warnings Coherence Cluster - Persistence Coherence Cluster - Configuration Messages Coherence Cluster - Network Coherence Cluster - Partitions Coherence Cluster - Message Sources Searches Dashboards Information from all dashboards (and queries) can be filtered using the standard Kibana date/time filtering in the top right of the UI, as well as the Add a filter button. 1. Coherence Cluster - All Messages This dashboard shows all messages captured for the given time period for all clusters. Users can drill-down by cluster, host, message level and thread. 2. Coherence Cluster - Errors and Warnings This dashboard shows errors and warning messages only. Users can drill-down by cluster, host, message level and thread. 3. Coherence Cluster - Persistence This dashboard shows Persistence related messages including failed and successful operations. 4. Coherence Cluster - Configuration Messages This dashboard shows configuration related messages such as loading of operational, cache configuration and POF configuration files. 5. Coherence Cluster - Network This dashboard hows network related messages, such as communication delays and TCP ring disconnects. 6. Coherence Cluster - Partitions Shows partition transfer and partition loss messages. 7. Coherence Cluster - Message Sources Shows the source (thread) for messages Users can drill-down by cluster, host and message level. Searches A number of searches are automatically includes which can help assist in diagnosis and troubleshooting a Coherence cluster. They can be accessed via the Discover side-bar and selecting `Open . These are grouped into the following general categories: Cluster - Cluster join, discovery, heartbeat, member joining and stopping messages Cache - Cache restarting, exceptions and index exception messages Configuration - Configuration loading and not loading messages Persistence - Persistence success and failure messages Network - Network communications delays, disconnects, timeouts and terminations Partition - Partition loss, ownership and transfer related messages Member - Member thread dump, join and leave messages Errors - All Error messages only Federation - Federation participant, disconnection, connection, errors and other messages ",
            "title": "Using Kibana Dashboards"
        },
        {
            "location": "/docs/other/020_environment",
            "text": " It is also possible to specify environment variables from a ConfigMap or Secret as you would for a Kubernetes container. For example, if there was a ConfigMap named special-config that contained environment variable values, it can be added to the Coherence spec as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: envFrom: - configMapRef: name: special-config ",
            "title": "Environment Variables From"
        },
        {
            "location": "/docs/other/020_environment",
            "text": " Environment variables can be added to the Coherence container in the Pods managed by the Operator. Additional variables should be added to the env list in the Coherence CRD spec. The entries in the env list are Kubernetes EnvVar values, exactly the same as when adding environment variables to a container spec. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: env: - name: VAR_ONE value: VALUE_ONE - name: VAR_TWO valueFrom: secretKeyRef: name: test-secret key: secret-key The VAR_ONE environment variable is a simple variable with a value of VALUE_ONE The VAR_TWO environment variable is variable that is loaded from a secret. Environment Variables From It is also possible to specify environment variables from a ConfigMap or Secret as you would for a Kubernetes container. For example, if there was a ConfigMap named special-config that contained environment variable values, it can be added to the Coherence spec as shown below. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: envFrom: - configMapRef: name: special-config ",
            "title": "Environment Variables"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " This example shows how to deploy a simple Coherence cluster in Kubernetes manually, and secure the Extend endpoint using TLS. This example expands on the StatefulSet used in the first simple deployment example. Tip The complete source code for this example is in the Coherence Operator GitHub repository. Prerequisites This example assumes that you have already built the example server image. There are a number of ways to use TLS to secure ingress in Kubernetes. We could use a load balancer Service and terminate TLS at the load balance, or we could use an add-on such as Istio to manage TLS ingress. Both of those approaches would require no changes to the Coherence server, as the server would not know TLS was being used. The Coherence Operator Examples contains examples of using TLS with Coherence and using Istio. The TLS example also shows how to use Kubernetes built in certificate management to create keys and certificates. In this example we are going to actually change the server to use TLS for its Extend endpoints. ",
            "title": "Secure Coherence Extend with TLS"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " To use TLS we will need some certificates and Java keystore files. For testing and examples, self-signed certs are fine. The source code for this example contains some keystores. * server.jks contains the server key and certificate files * trust.jks contains the CA certificate used to create the client and server certificates The keystores are password protected, the passwords are stored in files with the example source. We will use these files to securely provide the passwords to the client and server instead of hard coding or providing credentials via system properties or environment variables. * server-password.txt is the password to open the server.jks keystore * server-key-password.txt is the password for the key file stored in the server.jks keystore * trust-password.txt is the password to open the trust.jks` keystore. ",
            "title": "Create Certs and Java Keystores"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " If we look at the test-cache-config.xml file in the simple-server example project, we can see the configuration for the Extend proxy. <markup lang=\"xml\" title=\"test-cache-config.xml\" > &lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;local-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;0.0.0.0&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; The important item to note above is the socket-provider element, which is empty, but can be set using the coherence.extend.socket.provider system property (or the COHERENCE_EXTEND_SOCKET_PROVIDER environment variable). By default, a plain TCP socket will be used, but by setting the specified property a different socket can be used, in this case we&#8217;ll use one configured for TLS. ",
            "title": "Configure the Extend Proxy"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " In Coherence, socket providers can be configured in the operational configuration file, typically named tangosol-coherence-override.xml . The source code for the simple-server module contains this file with the TLS socket provider already configured. We need to configure two things in the operational configuration file, the socket provider and some password providers to supply the keystore credentials. The socket-provider section looks like this: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" >&lt;socket-providers&gt; &lt;socket-provider id=\"extend-tls\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.extend.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password-provider&gt; &lt;name&gt;identity-password-provider&lt;/name&gt; &lt;/password-provider&gt; &lt;type&gt;JKS&lt;/type&gt; &lt;/key-store&gt; &lt;password-provider&gt; &lt;name&gt;key-password-provider&lt;/name&gt; &lt;/password-provider&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.extend.truststore\"&gt;file:trust.jks&lt;/url&gt; &lt;password-provider&gt; &lt;name&gt;trust-password-provider&lt;/name&gt; &lt;/password-provider&gt; &lt;type&gt;JKS&lt;/type&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; There is a socket-provider with the id of extend-tls . This id is the value that must be used to tell the Extend proxy which socket provider to use, i.e. using the system property -Dcoherence.extend.socket.provider=extend-tls The &lt;identity-manager&gt; element specifies the keystore containing the key and certificate file that the proxy should use. This is set to file:server.jks but can be overridden using the coherence.extend.keystore system property, or corresponding environment variable. The password for the &lt;identity-manager&gt; keystore is configured to be provided by the password-provider named identity-password-provider . The password for the key file in the identity keystore is configured to be provided by the password-provider named key-password-provider . The &lt;trust-manager&gt; element contains the configuration for the trust keystore containing the CA certs used to validate client certificates. By default, the keystore name is file:trust.jks but this can be overridden using the coherence.extend.truststore system property or corresponding environment variable. The password for the trust keystore is configured to be provided by the password-provider named trust-password-provider . There are three &lt;password-provider&gt; elements in the configuration above, so we need to also configure these three password providers in the operational configuration file. <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" >&lt;password-providers&gt; &lt;password-provider id=\"trust-password-provider\"&gt; &lt;class-name&gt;com.oracle.coherence.examples.tls.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;fileName&lt;/param-name&gt; &lt;param-value system-property=\"coherence.trust.password.file\"&gt;trust-password.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;password-provider id=\"identity-password-provider\"&gt; &lt;class-name&gt;com.oracle.coherence.examples.tls.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;fileName&lt;/param-name&gt; &lt;param-value system-property=\"coherence.identity.password.file\"&gt;server-password.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;password-provider id=\"key-password-provider\"&gt; &lt;class-name&gt;com.oracle.coherence.examples.tls.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;fileName&lt;/param-name&gt; &lt;param-value system-property=\"coherence.key.password.file\"&gt;server-key-password.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/password-providers&gt; There are three password providers declared above, each with an 'id' attribute corresponding to the names used in the socket provider configuration. Each password provider is identical, they just have a different password file name. The class-name element refers to a class named com.oracle.coherence.examples.tls.FileBasedPasswordProvider , which is in the source code for both the server and client. This is an implementation of the com.tangosol.net.PasswordProvider interface which can read a password from a file. Each password provider&#8217;s password file name can be set using the relevant system property or environment variable. The name of the trust keystore password file is set using the coherence.trust.password.file system property. The name of the identity keystore is set using the coherence.identity.password.file system property. The nam eof the identity key file password file is set using the coherence.key.password.file system property. The simple server image has all the configuration above built in so there is nothing additional to do to use TLS other than set the system properties or environment variables. The test client uses the same configurations, so it can also be run using TLS by setting the relevant system properties. ",
            "title": "Socket Providers"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " The Coherence documentation explains how to Use TLS Secure Communication . This example is going to use a standard approach to securing Extend with TLS. To provide the keystores and credentials the example will make use of Kubernetes Secrets to mount those files as Volumes in the StatefulSet . This is much more flexible and secure than baking them into an application&#8217;s code or image. Configure the Extend Proxy If we look at the test-cache-config.xml file in the simple-server example project, we can see the configuration for the Extend proxy. <markup lang=\"xml\" title=\"test-cache-config.xml\" > &lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;local-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;0.0.0.0&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; The important item to note above is the socket-provider element, which is empty, but can be set using the coherence.extend.socket.provider system property (or the COHERENCE_EXTEND_SOCKET_PROVIDER environment variable). By default, a plain TCP socket will be used, but by setting the specified property a different socket can be used, in this case we&#8217;ll use one configured for TLS. Socket Providers In Coherence, socket providers can be configured in the operational configuration file, typically named tangosol-coherence-override.xml . The source code for the simple-server module contains this file with the TLS socket provider already configured. We need to configure two things in the operational configuration file, the socket provider and some password providers to supply the keystore credentials. The socket-provider section looks like this: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" >&lt;socket-providers&gt; &lt;socket-provider id=\"extend-tls\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.extend.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password-provider&gt; &lt;name&gt;identity-password-provider&lt;/name&gt; &lt;/password-provider&gt; &lt;type&gt;JKS&lt;/type&gt; &lt;/key-store&gt; &lt;password-provider&gt; &lt;name&gt;key-password-provider&lt;/name&gt; &lt;/password-provider&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.extend.truststore\"&gt;file:trust.jks&lt;/url&gt; &lt;password-provider&gt; &lt;name&gt;trust-password-provider&lt;/name&gt; &lt;/password-provider&gt; &lt;type&gt;JKS&lt;/type&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; There is a socket-provider with the id of extend-tls . This id is the value that must be used to tell the Extend proxy which socket provider to use, i.e. using the system property -Dcoherence.extend.socket.provider=extend-tls The &lt;identity-manager&gt; element specifies the keystore containing the key and certificate file that the proxy should use. This is set to file:server.jks but can be overridden using the coherence.extend.keystore system property, or corresponding environment variable. The password for the &lt;identity-manager&gt; keystore is configured to be provided by the password-provider named identity-password-provider . The password for the key file in the identity keystore is configured to be provided by the password-provider named key-password-provider . The &lt;trust-manager&gt; element contains the configuration for the trust keystore containing the CA certs used to validate client certificates. By default, the keystore name is file:trust.jks but this can be overridden using the coherence.extend.truststore system property or corresponding environment variable. The password for the trust keystore is configured to be provided by the password-provider named trust-password-provider . There are three &lt;password-provider&gt; elements in the configuration above, so we need to also configure these three password providers in the operational configuration file. <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" >&lt;password-providers&gt; &lt;password-provider id=\"trust-password-provider\"&gt; &lt;class-name&gt;com.oracle.coherence.examples.tls.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;fileName&lt;/param-name&gt; &lt;param-value system-property=\"coherence.trust.password.file\"&gt;trust-password.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;password-provider id=\"identity-password-provider\"&gt; &lt;class-name&gt;com.oracle.coherence.examples.tls.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;fileName&lt;/param-name&gt; &lt;param-value system-property=\"coherence.identity.password.file\"&gt;server-password.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;password-provider id=\"key-password-provider\"&gt; &lt;class-name&gt;com.oracle.coherence.examples.tls.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;fileName&lt;/param-name&gt; &lt;param-value system-property=\"coherence.key.password.file\"&gt;server-key-password.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/password-providers&gt; There are three password providers declared above, each with an 'id' attribute corresponding to the names used in the socket provider configuration. Each password provider is identical, they just have a different password file name. The class-name element refers to a class named com.oracle.coherence.examples.tls.FileBasedPasswordProvider , which is in the source code for both the server and client. This is an implementation of the com.tangosol.net.PasswordProvider interface which can read a password from a file. Each password provider&#8217;s password file name can be set using the relevant system property or environment variable. The name of the trust keystore password file is set using the coherence.trust.password.file system property. The name of the identity keystore is set using the coherence.identity.password.file system property. The nam eof the identity key file password file is set using the coherence.key.password.file system property. The simple server image has all the configuration above built in so there is nothing additional to do to use TLS other than set the system properties or environment variables. The test client uses the same configurations, so it can also be run using TLS by setting the relevant system properties. ",
            "title": "Configure Coherence Extend TLS"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " We first need to supply the keystores and credentials to the Coherence Pods . The secure way to do this in Kubernetes is to use a Secret . We can create a Secret from the command line using kubectl . From the 03_extend_tls/ directory containing the keystores and password file srun the following command: <markup lang=\"bash\" >kubectl create secret generic coherence-tls \\ --from-file=./server.jks \\ --from-file=./server-password.txt \\ --from-file=./server-key-password.txt \\ --from-file=./trust.jks \\ --from-file=./trust-password.txt The command above will create a Secret named coherence-tls containing the files specified. We can now use the Secret in the cluster&#8217;s StatefulSet ",
            "title": "Keystore Secret"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " We will expand on the StatefulSet created in the simple server example and add TLS. <markup lang=\"yaml\" title=\"coherence-tls.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: statefulset spec: selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage serviceName: storage-sts replicas: 3 template: metadata: labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage spec: volumes: - name: tls secret: secretName: coherence-tls containers: - name: coherence image: simple-coherence:1.0.0 volumeMounts: - mountPath: /certs name: tls command: - java args: - -cp - \"@/app/jib-classpath-file\" - -Xms1800m - -Xmx1800m - \"@/app/jib-main-class-file\" env: - name: COHERENCE_CLUSTER value: storage - name: COHERENCE_WKA value: storage-wka - name: COHERENCE_CACHECONFIG value: test-cache-config.xml - name: COHERENCE_EXTEND_SOCKET_PROVIDER value: extend-tls - name: COHERENCE_EXTEND_KEYSTORE value: file:/certs/server.jks - name: COHERENCE_IDENTITY_PASSWORD_FILE value: /certs/server-password.txt - name: COHERENCE_KEY_PASSWORD_FILE value: /certs/server-key-password.txt - name: COHERENCE_EXTEND_TRUSTSTORE value: file:/certs/trust.jks - name: COHERENCE_TRUST_PASSWORD_FILE value: /certs/trust-password.txt ports: - name: extend containerPort: 20000 The yaml above is identical to the simple server example with the following additions: A Volume has been added to the spec section. volumes: - name: tls secret: secretName: coherence-tls The volume name is tls and the files to mount to the file system in the Pod come from the coherence-tls secret we created above. A volumeMount has been added to the Coherence container to map the tls volume to the mount point /certs . volumeMounts: - mountPath: /certs name: tls A number of environment variables have been added to configure Coherence to use the extend-tls socket provider and the locations of the keystores and password files. - name: COHERENCE_EXTEND_SOCKET_PROVIDER value: extend-tls - name: COHERENCE_EXTEND_KEYSTORE value: file:/certs/server.jks - name: COHERENCE_IDENTITY_PASSWORD_FILE value: /certs/server-password.txt - name: COHERENCE_KEY_PASSWORD_FILE value: /certs/server-key-password.txt - name: COHERENCE_EXTEND_TRUSTSTORE value: file:/certs/trust.jks - name: COHERENCE_TRUST_PASSWORD_FILE value: /certs/trust-password.txt Note The COHERENCE_EXTEND_KEYSTORE and COHERENCE_EXTEND_TRUSTSTORE values must be URLs. In this case we refer to files usinf the file: prefix. ",
            "title": "StatefulSet"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " We can now create the resources we need to run the Cluster with TLS enabled. Keystore Secret We first need to supply the keystores and credentials to the Coherence Pods . The secure way to do this in Kubernetes is to use a Secret . We can create a Secret from the command line using kubectl . From the 03_extend_tls/ directory containing the keystores and password file srun the following command: <markup lang=\"bash\" >kubectl create secret generic coherence-tls \\ --from-file=./server.jks \\ --from-file=./server-password.txt \\ --from-file=./server-key-password.txt \\ --from-file=./trust.jks \\ --from-file=./trust-password.txt The command above will create a Secret named coherence-tls containing the files specified. We can now use the Secret in the cluster&#8217;s StatefulSet StatefulSet We will expand on the StatefulSet created in the simple server example and add TLS. <markup lang=\"yaml\" title=\"coherence-tls.yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: storage labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage coherence.oracle.com/component: statefulset spec: selector: matchLabels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage serviceName: storage-sts replicas: 3 template: metadata: labels: coherence.oracle.com/cluster: test-cluster coherence.oracle.com/deployment: storage spec: volumes: - name: tls secret: secretName: coherence-tls containers: - name: coherence image: simple-coherence:1.0.0 volumeMounts: - mountPath: /certs name: tls command: - java args: - -cp - \"@/app/jib-classpath-file\" - -Xms1800m - -Xmx1800m - \"@/app/jib-main-class-file\" env: - name: COHERENCE_CLUSTER value: storage - name: COHERENCE_WKA value: storage-wka - name: COHERENCE_CACHECONFIG value: test-cache-config.xml - name: COHERENCE_EXTEND_SOCKET_PROVIDER value: extend-tls - name: COHERENCE_EXTEND_KEYSTORE value: file:/certs/server.jks - name: COHERENCE_IDENTITY_PASSWORD_FILE value: /certs/server-password.txt - name: COHERENCE_KEY_PASSWORD_FILE value: /certs/server-key-password.txt - name: COHERENCE_EXTEND_TRUSTSTORE value: file:/certs/trust.jks - name: COHERENCE_TRUST_PASSWORD_FILE value: /certs/trust-password.txt ports: - name: extend containerPort: 20000 The yaml above is identical to the simple server example with the following additions: A Volume has been added to the spec section. volumes: - name: tls secret: secretName: coherence-tls The volume name is tls and the files to mount to the file system in the Pod come from the coherence-tls secret we created above. A volumeMount has been added to the Coherence container to map the tls volume to the mount point /certs . volumeMounts: - mountPath: /certs name: tls A number of environment variables have been added to configure Coherence to use the extend-tls socket provider and the locations of the keystores and password files. - name: COHERENCE_EXTEND_SOCKET_PROVIDER value: extend-tls - name: COHERENCE_EXTEND_KEYSTORE value: file:/certs/server.jks - name: COHERENCE_IDENTITY_PASSWORD_FILE value: /certs/server-password.txt - name: COHERENCE_KEY_PASSWORD_FILE value: /certs/server-key-password.txt - name: COHERENCE_EXTEND_TRUSTSTORE value: file:/certs/trust.jks - name: COHERENCE_TRUST_PASSWORD_FILE value: /certs/trust-password.txt Note The COHERENCE_EXTEND_KEYSTORE and COHERENCE_EXTEND_TRUSTSTORE values must be URLs. In this case we refer to files usinf the file: prefix. ",
            "title": "Create the Kubernetes Resources"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " The source code for this example contains a file named coherence-tls.yaml containing all the configuration above as well as the Services required to run Coherence and expose the Extend port. We can deploy it with the following command: <markup lang=\"bash\" >kubectl apply -f coherence-tls.yaml We can see all the resources created in Kubernetes are the same as for the simple server example. <markup lang=\"bash\" >kubectl get all Which will display something like the following: <markup >NAME READY STATUS RESTARTS AGE pod/storage-0 1/1 Running 0 19s pod/storage-1 1/1 Running 0 17s pod/storage-2 1/1 Running 0 16s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/storage-extend ClusterIP 10.105.78.34 &lt;none&gt; 20000/TCP 19s service/storage-sts ClusterIP None &lt;none&gt; 7/TCP 19s service/storage-wka ClusterIP None &lt;none&gt; 7/TCP 19s NAME READY AGE statefulset.apps/storage 3/3 19s ",
            "title": "Deploy to Kubernetes"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " Just like the server, the example test client contains the same operational configuration to configure a socket provider and password providers. The test client directory also contains copies of the keystores and password files. We can therefore run the client with the relevant system properties to enable it to use TLS and connect to the server. We just need to run the client from the test-client/ directory setting the socket provider system property. <markup lang=\"bash\" >cd test-client/ mvn exec:java -Dcoherence.extend.socket.provider=extend-tls After the client starts we can run the cache command, which should complete without an error. <markup >Map (?): cache test We can see from the output below that the client connected and created a remote cache. <markup >Cache Configuration: test SchemeName: remote ServiceName: RemoteCache ServiceDependencies: DefaultRemoteCacheServiceDependencies{RemoteCluster=null, RemoteService=Proxy, InitiatorDependencies=DefaultTcpInitiatorDependencies{EventDispatcherThreadPriority=10, RequestTimeoutMillis=30000, SerializerFactory=null, TaskHungThresholdMillis=0, TaskTimeoutMillis=0, ThreadPriority=10, WorkerThreadCount=0, WorkerThreadCountMax=2147483647, WorkerThreadCountMin=0, WorkerThreadPriority=5}{Codec=null, FilterList=[], PingIntervalMillis=0, PingTimeoutMillis=30000, MaxIncomingMessageSize=0, MaxOutgoingMessageSize=0}{ConnectTimeoutMillis=30000, RequestSendTimeoutMillis=30000}{LocalAddress=null, RemoteAddressProviderBldr=com.tangosol.coherence.config.builder.WrapperSocketAddressProviderBuilder@5431b4b4, SocketOptions=SocketOptions{LingerTimeout=0, KeepAlive=true, TcpNoDelay=true}, SocketProvideBuilderr=com.tangosol.coherence.config.builder.SocketProviderBuilder@52c85af7, isNameServiceAddressProvider=false}}{DeferKeyAssociationCheck=false} Map (test): Now the client is connected using TLS, we could do puts and gets, or other operations on the cache. To exit from the client press ctrl-C, and uninstall the cluster <markup lang=\"bash\" >kubectl delete -f coherence-tls.yaml ",
            "title": "Enable Client TLS"
        },
        {
            "location": "/examples/no-operator/03_extend_tls/README",
            "text": " If we run the test client using the same instructions as the simple server example, we will run an interactive Coherence console. <markup lang=\"bash\" >cd test-client/ mvn exec:java When the Map (?): prompt is displayed we can try to create a cache. <markup >Map (?): cache test This will not throw an exception because the client is not using TLS so the server rejected the connection. <markup >2021-09-17 18:19:39.182/12.090 Oracle Coherence CE 21.12.1 &lt;Error&gt; (thread=com.tangosol.net.CacheFactory.main(), member=1): Error while starting service \"RemoteCache\": com.tangosol.net.messaging.ConnectionException: could not establish a connection to one of the following addresses: [127.0.0.1:20000] at com.tangosol.coherence.component.util.daemon.queueProcessor.service.peer.initiator.TcpInitiator.openConnection(TcpInitiator.CDB:139) at com.tangosol.coherence.component.util.daemon.queueProcessor.service.peer.Initiator.ensureConnection(Initiator.CDB:11) at com.tangosol.coherence.component.net.extend.remoteService.RemoteCacheService.openChannel(RemoteCacheService.CDB:7) at com.tangosol.coherence.component.net.extend.RemoteService.ensureChannel(RemoteService.CDB:6) at com.tangosol.coherence.component.net.extend.RemoteService.doStart(RemoteService.CDB:11) Enable Client TLS Just like the server, the example test client contains the same operational configuration to configure a socket provider and password providers. The test client directory also contains copies of the keystores and password files. We can therefore run the client with the relevant system properties to enable it to use TLS and connect to the server. We just need to run the client from the test-client/ directory setting the socket provider system property. <markup lang=\"bash\" >cd test-client/ mvn exec:java -Dcoherence.extend.socket.provider=extend-tls After the client starts we can run the cache command, which should complete without an error. <markup >Map (?): cache test We can see from the output below that the client connected and created a remote cache. <markup >Cache Configuration: test SchemeName: remote ServiceName: RemoteCache ServiceDependencies: DefaultRemoteCacheServiceDependencies{RemoteCluster=null, RemoteService=Proxy, InitiatorDependencies=DefaultTcpInitiatorDependencies{EventDispatcherThreadPriority=10, RequestTimeoutMillis=30000, SerializerFactory=null, TaskHungThresholdMillis=0, TaskTimeoutMillis=0, ThreadPriority=10, WorkerThreadCount=0, WorkerThreadCountMax=2147483647, WorkerThreadCountMin=0, WorkerThreadPriority=5}{Codec=null, FilterList=[], PingIntervalMillis=0, PingTimeoutMillis=30000, MaxIncomingMessageSize=0, MaxOutgoingMessageSize=0}{ConnectTimeoutMillis=30000, RequestSendTimeoutMillis=30000}{LocalAddress=null, RemoteAddressProviderBldr=com.tangosol.coherence.config.builder.WrapperSocketAddressProviderBuilder@5431b4b4, SocketOptions=SocketOptions{LingerTimeout=0, KeepAlive=true, TcpNoDelay=true}, SocketProvideBuilderr=com.tangosol.coherence.config.builder.SocketProviderBuilder@52c85af7, isNameServiceAddressProvider=false}}{DeferKeyAssociationCheck=false} Map (test): Now the client is connected using TLS, we could do puts and gets, or other operations on the cache. To exit from the client press ctrl-C, and uninstall the cluster <markup lang=\"bash\" >kubectl delete -f coherence-tls.yaml ",
            "title": "Run the Client"
        },
        {
            "location": "/docs/other/100_resources",
            "text": " When creating a Coherence resource you can optionally specify how much CPU and memory (RAM) each Coherence Container is allowed to consume. The container resources are specified in the resources section of the Coherence spec; the format is exactly the same as documented in the Kubernetes documentation Managing Compute Resources for Containers . When setting resource limits, in particular memory limits, for a container it is important to ensure that the Coherence JVM is properly configured so that it does not consume more memory than the limits. If the JVM attempts to consume more memory than the resource limits allow the Pod can be killed by Kubernetes. See Configuring the JVM Memory for details on the different memory settings. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" The coherence container in the Pods will have requests of 0.25 cpu and 64MiB of memory, and limits of 0.5 cpu and 128MiB of memory. ",
            "title": "Container Resource Limits"
        },
        {
            "location": "/docs/other/100_resources",
            "text": " The Coherence Operator adds an init-container to the Pods that it manages. This init container does nothing more than copy some files and ensure some directories exist. In terms of resource use it is extremely light. Some customers have expressed a desire to still be able to set limits fo this init container, so this is possible using the spec.initResources field. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: initResources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" The coherence-k8s-utils init-container in the Pods will have requests of 0.25 cpu and 64MiB of memory, and limits of 0.5 cpu and 128MiB of memory. These resources only applies to the init-container that the Operator creates, any other init-containers added in the spec.initContainers section should have their own resources configured. ",
            "title": "InitContainer Resource Limits"
        },
        {
            "location": "/docs/other/090_pod_scheduling",
            "text": " In Kubernetes Pods can be configured to control how, and onto which nodes, Kubernetes will schedule those Pods ; the Coherence Operator allows the same control for Pods owned by a Coherence resource. The following settings can be configured: Field Description nodeSelector nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of role spec, it specifies a map of key-value pairs. For the Pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). See Assigning Pods to Nodes in the Kubernetes documentation affinity The affinity/anti-affinity feature, greatly expands the types of constraints you can express over just using labels in a nodeSelector . See Assigning Pods to Nodes in the Kubernetes documentation tolerations nodeSelector and affinity are properties of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite  they allow a node to repel a set of Pods . Taints and tolerations work together to ensure that Pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any Pods that do not tolerate the taints. Tolerations are applied to Pods , and allow (but do not require) the Pods to schedule onto nodes with matching taints. See Taints and Tolerations in the Kubernetes documentation. The nodeSelector , affinity and tolerations fields are all part of the Coherence CRD spec. The format of the fields is that same as documented in the Kubernetes documentation Assigning Pods to Nodes and Taints and Tolerations For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: tolerations: - key: \"example-key\" operator: \"Exists\" effect: \"NoSchedule\" nodeSelector: disktype: ssd affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 ",
            "title": "Configure Pod Scheduling"
        },
        {
            "location": "/docs/applications/020_build_application",
            "text": " The image does not need to have an EntryPoint or command specified, it does not need to actually be executable. If the image does have an EntryPoint , it will just be ignored. The Coherence Operator actually injects its own runner executable into the container which the container runs and which in turn builds the Java command line to execute. The runner process looks at arguments and environment variables configured for the Coherence container and from these constructs a Java command line that it then executes. The default command might look something like this: <markup lang=\"bash\" >java -cp `/app/resources:/app/classes:/app/libs/*` \\ &lt;JVM args&gt; \\ &lt;System Properties&gt; \\ com.tangosol.net.DefaultCacheServer The runner will work out the JVM&#8217;s classpath, args and system properties to add to the command line and execute the main class com.tangosol.net.DefaultCacheServer . All these are configurable in the Coherence resource spec. ",
            "title": "Image EntryPoint - What Does the Operator Run?"
        },
        {
            "location": "/docs/applications/020_build_application",
            "text": " If the CLASSPATH environment variable has been set in an image that classpath will be used when running the Coherence container. Other elements may also be added to the classpath depending on the configuration of the Coherence resource. ",
            "title": "Optional CLASSPATH Environment Variable"
        },
        {
            "location": "/docs/applications/020_build_application",
            "text": " An application image contains .jar files (at least coherence.jar ), possibly Java class files, also possibly other ad-hoc files, all of which need to be on the application&#8217;s classpath. There are certain classpath values that the operator supports out of the box without needing any extra configuration, but for occasions where the location of files in the image does not match the defaults a classpath can be specified. Images built with JIB have a default classpath of /app/resources:/app/classes:/app/libs/* . When the Coherence container starts if the directories /app/resources , /app/classes or /app/libs/ exist in the image they will automatically be added to the classpath of the JVM. In this way the Operator supports standard JIB images without requiring additional configuration. If the image is not a JIB image, or is a JIB image without the standard classpath but one or more of the /app/resources , /app/classes or /app/libs/ directories exist they will still be added to the classpath. This may be desired or in some cases it may cause issues. It is possible to disable automatically adding these directories in the Coherence resource spec by setting the jvm.useJibClasspath field to false (the default value of the field is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. If the image is not a JIB image, or is a JIB image without the standard classpath, then additional classpath entries can be configured as described in the setting the classpath documentation. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/libs/*\" - \"/data/config\" The jvm.classpath field will be used to add additional items to the classpath, the field is a list of strings. Each entry in the jvm.classpath will be appended to the classpath exactly as it is declared, so in this case the classpath will be /data/libs/*:/data/config ",
            "title": "Setting the Classpath"
        },
        {
            "location": "/docs/applications/020_build_application",
            "text": " The JAVA_HOME environment variable does not have to be set in the image. If it is set the JVM at that location will be used to run the application. If it is not set then the java executable must be on the PATH in the image. ",
            "title": "Optional JAVA_HOME Environment Variable"
        },
        {
            "location": "/docs/applications/020_build_application",
            "text": " The COHERENCE_HOME environment variable does not have to be set in an image. Typically, all the jar files, including coherence.jar would be packaged into a single directory which is then used as the classpath. It is possible to run official Coherence images published by Oracle, which have COHERENCE_HOME set, which is then used by the Operator to set the classpath. If the COHERENCE_HOME environment variable is set in an image the following entries will be added to the end of the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf ",
            "title": "Optional COHERENCE_HOME Environment Variable"
        },
        {
            "location": "/docs/applications/020_build_application",
            "text": " If the application requires access to external storage volumes in Kubernetes it is possible to add additional Volumes and VolumeMappings to the Pod and containers. There are three ways to add additional volumes: ConfigMaps - easily add a ConfigMap volume and volume mapping see: Add ConfigMap Volumes Secrets - easily add a Secret volume and volume mapping see: Add Secret Volumes Volumes - easily add any additional volume and volume mapping see: Add Volumes Both of ConfigMaps and Secrets have been treated as a special case because they are quite commonly used to provide configurations to Pods, so the Coherence spec provides a simpler way to declare them than for ad-hoc Volumes . ",
            "title": "Additional Data Volumes"
        },
        {
            "location": "/docs/applications/020_build_application",
            "text": " To deploy a Coherence application using the operator the application code must be packaged into an image that the Coherence container in the Pods will run. This image can be any image that contains a JVM as well as the application&#8217;s jar files, including obviously coherence.jar . There are many ways to build an image for a Java application so it would be of little value to document the exact steps for one of them here that might turn out to be used by very few people. One of the simplest ways to build a Java image is to use JIB . The Operator supports JIB images automatically but any image that meets the requirements of having a JVM and coherence.jar will be supported. Any version of Java which works with the version of coherence.jar in the image will be suitable. This can be a JRE, it does not need to be a full JDK. At a bare minimum the directories in an image might look like this example (obviously there would be more O/S related files and more JVM files, but they are not relevant for the example): <markup >/ |-- app | |-- libs | |-- application.jar | |-- coherence.jar |-- usr |-- bin | |-- java | |-- lib |-- jvm |-- java-11-openjdk The /app/libs directory contains the application jar files. This will be the classpath used to run the application. The /usr/bin/java file is the Java executable and on the PATH in the image (this would be a link to the actual Java executable location, in this example to /usr/lib/jvm/java-11-openjdk/bin/java . The /usr/lib/jvm/java-11-openjdk/ is the actual JVM install location. Image EntryPoint - What Does the Operator Run? The image does not need to have an EntryPoint or command specified, it does not need to actually be executable. If the image does have an EntryPoint , it will just be ignored. The Coherence Operator actually injects its own runner executable into the container which the container runs and which in turn builds the Java command line to execute. The runner process looks at arguments and environment variables configured for the Coherence container and from these constructs a Java command line that it then executes. The default command might look something like this: <markup lang=\"bash\" >java -cp `/app/resources:/app/classes:/app/libs/*` \\ &lt;JVM args&gt; \\ &lt;System Properties&gt; \\ com.tangosol.net.DefaultCacheServer The runner will work out the JVM&#8217;s classpath, args and system properties to add to the command line and execute the main class com.tangosol.net.DefaultCacheServer . All these are configurable in the Coherence resource spec. Optional CLASSPATH Environment Variable If the CLASSPATH environment variable has been set in an image that classpath will be used when running the Coherence container. Other elements may also be added to the classpath depending on the configuration of the Coherence resource. Setting the Classpath An application image contains .jar files (at least coherence.jar ), possibly Java class files, also possibly other ad-hoc files, all of which need to be on the application&#8217;s classpath. There are certain classpath values that the operator supports out of the box without needing any extra configuration, but for occasions where the location of files in the image does not match the defaults a classpath can be specified. Images built with JIB have a default classpath of /app/resources:/app/classes:/app/libs/* . When the Coherence container starts if the directories /app/resources , /app/classes or /app/libs/ exist in the image they will automatically be added to the classpath of the JVM. In this way the Operator supports standard JIB images without requiring additional configuration. If the image is not a JIB image, or is a JIB image without the standard classpath but one or more of the /app/resources , /app/classes or /app/libs/ directories exist they will still be added to the classpath. This may be desired or in some cases it may cause issues. It is possible to disable automatically adding these directories in the Coherence resource spec by setting the jvm.useJibClasspath field to false (the default value of the field is true ). <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useJibClasspath: false The useJibClasspath is set to false . Even if any of the the /app/resources , /app/classes or /app/libs/ directories exist in the image they will not be added to the classpath. If the image is not a JIB image, or is a JIB image without the standard classpath, then additional classpath entries can be configured as described in the setting the classpath documentation. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: classpath: - \"/data/libs/*\" - \"/data/config\" The jvm.classpath field will be used to add additional items to the classpath, the field is a list of strings. Each entry in the jvm.classpath will be appended to the classpath exactly as it is declared, so in this case the classpath will be /data/libs/*:/data/config Optional JAVA_HOME Environment Variable The JAVA_HOME environment variable does not have to be set in the image. If it is set the JVM at that location will be used to run the application. If it is not set then the java executable must be on the PATH in the image. Optional COHERENCE_HOME Environment Variable The COHERENCE_HOME environment variable does not have to be set in an image. Typically, all the jar files, including coherence.jar would be packaged into a single directory which is then used as the classpath. It is possible to run official Coherence images published by Oracle, which have COHERENCE_HOME set, which is then used by the Operator to set the classpath. If the COHERENCE_HOME environment variable is set in an image the following entries will be added to the end of the classpath: $COHERENCE_HOME/lib/coherence.jar $COHERENCE_HOME/conf Additional Data Volumes If the application requires access to external storage volumes in Kubernetes it is possible to add additional Volumes and VolumeMappings to the Pod and containers. There are three ways to add additional volumes: ConfigMaps - easily add a ConfigMap volume and volume mapping see: Add ConfigMap Volumes Secrets - easily add a Secret volume and volume mapping see: Add Secret Volumes Volumes - easily add any additional volume and volume mapping see: Add Volumes Both of ConfigMaps and Secrets have been treated as a special case because they are quite commonly used to provide configurations to Pods, so the Coherence spec provides a simpler way to declare them than for ad-hoc Volumes . ",
            "title": "Build Custom Application Images"
        },
        {
            "location": "/docs/jvm/040_gc",
            "text": " To enable GC logging set the jvm.gc.logging field to true . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: logging: true Setting the field to true adds the following JVM arguments to the JVM in the coherence container: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime If different GC logging arguments are required then the relevant JVM arguments can be added to either the jvm.args field or the jvm.gc.args field. ",
            "title": "Enable GC Logging"
        },
        {
            "location": "/docs/jvm/040_gc",
            "text": " The garbage collector to use can be set using the jvm.gc.collector field. This field can be set to either G1 , CMS or Parallel (the field is case-insensitive, invalid values will be silently ignored). The default collector set, if none has been specified, will be G1 . Parameter JVM Argument Set G1 -XX:+UseG1GC CMS -XX:+UseConcMarkSweepGC Parallel -XX:+UseParallelGC For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: collector: \"G1\" The example above will add -XX:+UseG1GC to the command line. ",
            "title": "Set the Garbage Collector"
        },
        {
            "location": "/docs/jvm/040_gc",
            "text": " Any arbitrary GC argument can be added to the jvm.gc.args field. These arguments will be passed verbatim to the JVM command line. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=200\" In the example above the -XX:MaxGCPauseMillis=200 JVM argument will be added to the command line. The jvm.gc.args field will add the provided arguments to the end of the command line exactly as they are in the args list. This field provides the same functionality as JVM Args but sometimes it might be useful to be able to separate the two gorups of arguments in the CRD spec. ",
            "title": "Adding Arbitrary GC Args"
        },
        {
            "location": "/docs/jvm/040_gc",
            "text": " The Coherence CRD has fields in the jvm.gc section to allow certain garbage collection parameters to be set. These include GC logging, setting the collector to use and arbitrary GC arguments. Important If running Kubernetes on ARM processors and using Coherence images built on Java 8 for ARM, note that the G1 garbage collector in that version of Java on ARM is marked as experimental. By default, the Operator configures the Coherence JVM to use G1. This will cause errors on Arm64 Java 8 JMS unless the JVM option -XX:+UnlockExperimentalVMOptions is added in the Coherence resource spec. Alternatively specify a different garbage collector, ideally on a version of Java this old, use CMS. Enable GC Logging To enable GC logging set the jvm.gc.logging field to true . For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: logging: true Setting the field to true adds the following JVM arguments to the JVM in the coherence container: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime If different GC logging arguments are required then the relevant JVM arguments can be added to either the jvm.args field or the jvm.gc.args field. Set the Garbage Collector The garbage collector to use can be set using the jvm.gc.collector field. This field can be set to either G1 , CMS or Parallel (the field is case-insensitive, invalid values will be silently ignored). The default collector set, if none has been specified, will be G1 . Parameter JVM Argument Set G1 -XX:+UseG1GC CMS -XX:+UseConcMarkSweepGC Parallel -XX:+UseParallelGC For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: collector: \"G1\" The example above will add -XX:+UseG1GC to the command line. Adding Arbitrary GC Args Any arbitrary GC argument can be added to the jvm.gc.args field. These arguments will be passed verbatim to the JVM command line. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: gc: args: - \"-XX:MaxGCPauseMillis=200\" In the example above the -XX:MaxGCPauseMillis=200 JVM argument will be added to the command line. The jvm.gc.args field will add the provided arguments to the end of the command line exactly as they are in the args list. This field provides the same functionality as JVM Args but sometimes it might be useful to be able to separate the two gorups of arguments in the CRD spec. ",
            "title": "Garbage Collector Settings"
        },
        {
            "location": "/docs/other/060_secret_volumes",
            "text": " Additional Volumes and VolumeMounts from Secrets can easily be added to a Coherence resource. ",
            "title": "preambule"
        },
        {
            "location": "/docs/other/060_secret_volumes",
            "text": " To add a Secret as an additional volume to the Pods of a Coherence deployment add entries to the secretVolumes list in the CRD spec. Each entry in the list has a mandatory name and mountPath field, all other fields are optional. The name field is the name of the Secret to mount and is also used as the volume name. The mountPath field is the path in the container to mount the volume to. Additional volumes added in this way (either Secrets shown here, or Secrets or plain Volumes ) will be added to all containers in the Pod . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: secretVolumes: - name: storage-config mountPath: /home/coherence/config The Secret named storage-config will be mounted to the Pod as an additional Volume named storage-config The Secret will be mounted at /home/coherence/config in the containers. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config secret: secretName: storage-config As already stated, if the Coherence resource has additional containers the Secret will be mounted in all of them. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" secretVolumes: - name: storage-config mountPath: /home/coherence/config In this example the storage-config Secret will be mounted as a Volume and mounted to both the coherence container and the fluentd container. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3\" volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config secret: secretName: storage-config ",
            "title": "Add Secrets Volumes"
        },
        {
            "location": "/docs/webhooks/01_introduction",
            "text": " Webhooks in Kubernetes are a cluster resource, not a namespaced scoped resource, so consequently there is typically only a single webhook installed for a given resource type. If the Coherence Operator is installed as a cluster scoped operator then this is not a problem but if multiple Coherence Operators are deployed then they could all attempt to install the webhooks and update or overwrite a previous configuration. This might not be an issue if all of the operators deployed in a Kubernetes cluster are the same version but different versions could cause issues. ",
            "title": "Webhook Scope"
        },
        {
            "location": "/docs/webhooks/01_introduction",
            "text": " The Coherence Operator uses Kubernetes admission control webhooks to validate and provide default values for Coherence resources (see the Kubernetes documentation for more details on web-hooks). The Coherence Operator webhooks will validate a Coherence resources when it is created or updated contain. For example, the replicas count is not negative. If a Coherence resource is invalid it will be rejected before it gets stored into Kubernetes. Webhook Scope Webhooks in Kubernetes are a cluster resource, not a namespaced scoped resource, so consequently there is typically only a single webhook installed for a given resource type. If the Coherence Operator is installed as a cluster scoped operator then this is not a problem but if multiple Coherence Operators are deployed then they could all attempt to install the webhooks and update or overwrite a previous configuration. This might not be an issue if all of the operators deployed in a Kubernetes cluster are the same version but different versions could cause issues. ",
            "title": "Coherence Operator Kubernetes Web-Hooks"
        },
        {
            "location": "/docs/webhooks/01_introduction",
            "text": " Kubernetes requires webhooks to expose an API over https and consequently this requires certificates to be created. By default, the Coherence Operator will create a self-signed CA certificate and key for use with its webhooks. Alternatively it is possible to use an external certificate manager such as the commonly used Cert Manager . Configuring and using Cert Manager is beyond the scope of this documentation. ",
            "title": "Webhook Certificates"
        },
        {
            "location": "/docs/metrics/010_overview",
            "text": " Coherence metrics can be used in a cluster deployed in Kubernetes. Note: Use of Coherence metrics is available only when using the operator with clusters running Coherence version 12.2.1.4 or later. Metrics Enabling the Coherence metrics feature. Importing Grafana Dashboards Importing the Coherence Grafana Dashboards. Using Grafana Dashboards Using the Coherence Grafana Dashboards. SSL Enable SSL on the metrics endpoint. ",
            "title": "Overview"
        },
        {
            "location": "/docs/logging/010_overview",
            "text": " The use of Elasticsearch, Fluentd and Kibana is a common approach. For this reason the Coherence Operator has a set of Kibana dashboards that support the common Coherence logging format. The logging guides below show one approach to shipping Coherence logs to Elasticsearch and importing the Coherence dashboards into Kibana. If this approach does not meet your needs you are obviously free to configure an alternative. Enabling Log Capture Capturing and viewing Coherence cluster Logs in Elasticsearch using a Fluentd side-car. Kibana Dashboards Importing and using the Kibana Dashboards available. ",
            "title": "Logging Guides"
        },
        {
            "location": "/docs/logging/010_overview",
            "text": " In a container environment like Kubernetes, or any cloud, it is often a requirement to centralize log files to allow easier analysis and debugging. There are many ways to do this, including collecting container logs, parsing and shipping log files with something like Fluentd, or using a specialized log appender specific to your logging framework. The Coherence Operator does not proscribe any particular method of log capture. The Coherence CRD is flexible enough to allow any method of log capture that an application or specific cloud environment requires. This could be as simple as adding JVM arguments to configure the Java logger, or it could be injecting a whole side-car container to run something like Fluentd. Different approaches have their own pros and cons that need to be weighed up on a case by case basis. Logging Guides The use of Elasticsearch, Fluentd and Kibana is a common approach. For this reason the Coherence Operator has a set of Kibana dashboards that support the common Coherence logging format. The logging guides below show one approach to shipping Coherence logs to Elasticsearch and importing the Coherence dashboards into Kibana. If this approach does not meet your needs you are obviously free to configure an alternative. Enabling Log Capture Capturing and viewing Coherence cluster Logs in Elasticsearch using a Fluentd side-car. Kibana Dashboards Importing and using the Kibana Dashboards available. ",
            "title": "Overview"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " The simplest way to build an application image to run with the Coherence Operator (including Spring Boot applications) is to use the JIB tool. JIB images will work out of the box with the operator, even for a Spring Boot application, as described in Building Applications and Deploying Applications . If you have used the Spring Maven or Gradle plugins to build the application into a fat jar, but you then build the image using the JIB plugin then JIB will detect the fat jar and package the image in an exploded form that will run out of the box with the operator. ",
            "title": "Using JIB Images"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " Another way to build a Spring Boot image is to explode the Spring Boot jar into a directory structure in the image. For example, if a Spring Boot jar has been exploded into a directory called /spring , the image contents might look like the diagram below; where you can see the /spring directory contains the Spring Boot application. <markup > bin  boot  dev  etc  home  lib  lib64  proc  root  run  sbin  spring   BOOT-INF    classes    classpath.idx    lib   META-INF    MANIFEST.MF    maven   org   springframework   boot  sys  tmp  usr  var This type of image can be run by the Coherence Operator by specifying an application type of spring in the spec.application.type field and by setting the working directory to the exploded directory, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: my-spring-app:1.0.0 application: type: spring workingDir: /spring The type field set to spring tells the Operator that this is a Spring Boot application. The working directory has been set to the directory containing the exploded Spring Boot application. When the Operator starts the application it will then run a command equivalent to: <markup lang=\"bash\" >cd /spring &amp;&amp; java org.springframework.boot.loader.PropertiesLauncher ",
            "title": "Using an Exploded Spring Boot Image"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " It is not recommended to build images containing fat jars for various reasons which can easily be found on the internet. If you feel that you must build your application as a Spring Boot fat jar then this can still work with the Coherence Operator. The Java command line to run a Spring Boot fat jar needs to be something like java -jar my-app.jar where my-app.jar is the fat jar. This means that the Operator&#8217;s launcher needs to know the location of the fat jar in the image, so this must be provided in the Coherence deployment yaml. For example, suppose that an application has been built into a fat jar names catalogue-1.0.0.jar which is in the /app/libs directory in the image, so the full path to the jar is /app/libs/catalogue-1.0.0.jar . This needs to be set in the spec.applicaton.springBootFatJar field of the Coherence yaml. The spec.application.type field also needs to be set to spring so that the Operator knows that this is a Spring Boot application <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring springBootFatJar: /app/libs/catalogue-1.0.0.jar The type field set to spring tells the Operator that this is a Spring Boot application. The location of the Spring Boot jar has been set. When the Operator starts the application it will then run a command equivalent to: <markup lang=\"bash\" >java -cp /app/libs/catalogue-1.0.0.jar org.springframework.boot.loader.PropertiesLauncher The Operator does not run the fat jar using the java -jar command because it needs to add various other JVM arguments and append to the classpath, so it has to run the org.springframework.boot.loader.PropertiesLauncher class as opposed to the org.springframework.boot.loader.JarLauncher that java -jar would run. ",
            "title": "Using a Spring Boot Fat Jar"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " If for some reason buildpacks auto-detection does not work properly the Coherence CRD contains a filed to force buildpacks to be enabled or disabled. The boolean field spec.application.cloudNativeBuildPack.enabled can be set to true to enable buildpacks or false to disable buildpack. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring cloudNativeBuildPack: enabled: true The application type has been set to spring so that the operator knows that this is a Spring Boot application The cloudNativeBuildPack.enabled field has been set to true to force the Operator to use the Buildpacks launcher. ",
            "title": "Buildpacks Detection"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " A Cloud Native Buildpacks image uses a launcher mechanism to run the executable(s) in the image. The Coherence Operator launcher will configure the application and then invoke the same buildpacks launcher. The Coherence Operator assumes that the buildpacks launcher is in the image in the location /cnb/lifecycle/launcher . If a buildpacks image has been built with the launcher in a different location then the Coherence CRD contains a field to set the new location. The spec.application.cloudNativeBuildPack.enabled field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring cloudNativeBuildPack: launcher: /buildpack/launcher The application type has been set to spring so that the operator knows that this is a Spring Boot application The buildpacks launcher that the Operator will invoke is located at /buildpack/launcher . ",
            "title": "Specify the Buildpacks Launcher"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " A typical Spring Boot buildpack launcher will attempt to configure options such as heap size based on the container resource limits configured, so this must be taken into account if using any of the memory options available in the Coherence CRD as there may be conflicting configurations. ",
            "title": "Buildpack JVM Arguments"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " If the Spring Boot Maven or Gradle plugin has been used to produce an image using Cloud Native Buildpacks these images can work with the Coherence Operator. Warning Due to limitation on the way that arguments can be passed to the JVM when using Buildpacks images the Coherence operator will only work with images containing a JVM greater than Java 11. Although the Buildpacks launcher will honour the JAVA_OPTS or JAVA_TOOL_OPTIONS environment variables there appear to be size limitations for the values of these variables that make it impractical for the Operator to use them. The Operator therefore creates a JVM arguments file to pass the arguments to the JVM. At the time of writing these docs, Java 8 (which is the default version of Java used by the Spring Boot plugin) does not support the use of argument files for the JVM. It is simple to configure the version of the JVM used by the Spring Boot plugin, for example in Maven: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.4.RELEASE&lt;/version&gt; &lt;configuration&gt; &lt;image&gt; &lt;env&gt; &lt;BP_JVM_VERSION&gt;11.*&lt;/BP_JVM_VERSION&gt; &lt;/env&gt; &lt;/image&gt; &lt;/configuration&gt; &lt;/plugin&gt; When creating a Coherence deployment for a Spring Boot Buildpacks image The application type must be set to spring . The Operator&#8217;s launcher will automatically detect that the image is a Buildpacks image and launch the application using the Buildpacks launcher. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring The application type has been set to spring so that the operator knows that this is a Spring Boot application, and the fact that the image is a Buildpacks image will be auto-discovered. When the Operator starts the application it will then run the buildpacks launcher with a command equivalent to this: <markup lang=\"bash\" >/cnb/lifecycle/launcher java @jvm-args-file org.springframework.boot.loader.PropertiesLauncher Buildpacks Detection If for some reason buildpacks auto-detection does not work properly the Coherence CRD contains a filed to force buildpacks to be enabled or disabled. The boolean field spec.application.cloudNativeBuildPack.enabled can be set to true to enable buildpacks or false to disable buildpack. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring cloudNativeBuildPack: enabled: true The application type has been set to spring so that the operator knows that this is a Spring Boot application The cloudNativeBuildPack.enabled field has been set to true to force the Operator to use the Buildpacks launcher. Specify the Buildpacks Launcher A Cloud Native Buildpacks image uses a launcher mechanism to run the executable(s) in the image. The Coherence Operator launcher will configure the application and then invoke the same buildpacks launcher. The Coherence Operator assumes that the buildpacks launcher is in the image in the location /cnb/lifecycle/launcher . If a buildpacks image has been built with the launcher in a different location then the Coherence CRD contains a field to set the new location. The spec.application.cloudNativeBuildPack.enabled field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring cloudNativeBuildPack: launcher: /buildpack/launcher The application type has been set to spring so that the operator knows that this is a Spring Boot application The buildpacks launcher that the Operator will invoke is located at /buildpack/launcher . Buildpack JVM Arguments A typical Spring Boot buildpack launcher will attempt to configure options such as heap size based on the container resource limits configured, so this must be taken into account if using any of the memory options available in the Coherence CRD as there may be conflicting configurations. ",
            "title": "Using Could Native Buildpacks"
        },
        {
            "location": "/docs/applications/070_spring",
            "text": " The Coherence Operator supports running images that contain Spring Boot applications. Exactly how easy this is depends on how the image has been built. When the operator runs an image it overrides the default image entrypoint and uses its own launcher. This allows the operator to properly configure various Coherence properties that the launcher then uses to build the command line to actually run your application. With some types of image this is not a straight forward Java command line so the Operator requires a bit more information adding to the Coherence deployment yaml. Using JIB Images The simplest way to build an application image to run with the Coherence Operator (including Spring Boot applications) is to use the JIB tool. JIB images will work out of the box with the operator, even for a Spring Boot application, as described in Building Applications and Deploying Applications . If you have used the Spring Maven or Gradle plugins to build the application into a fat jar, but you then build the image using the JIB plugin then JIB will detect the fat jar and package the image in an exploded form that will run out of the box with the operator. Using an Exploded Spring Boot Image Another way to build a Spring Boot image is to explode the Spring Boot jar into a directory structure in the image. For example, if a Spring Boot jar has been exploded into a directory called /spring , the image contents might look like the diagram below; where you can see the /spring directory contains the Spring Boot application. <markup > bin  boot  dev  etc  home  lib  lib64  proc  root  run  sbin  spring   BOOT-INF    classes    classpath.idx    lib   META-INF    MANIFEST.MF    maven   org   springframework   boot  sys  tmp  usr  var This type of image can be run by the Coherence Operator by specifying an application type of spring in the spec.application.type field and by setting the working directory to the exploded directory, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: my-spring-app:1.0.0 application: type: spring workingDir: /spring The type field set to spring tells the Operator that this is a Spring Boot application. The working directory has been set to the directory containing the exploded Spring Boot application. When the Operator starts the application it will then run a command equivalent to: <markup lang=\"bash\" >cd /spring &amp;&amp; java org.springframework.boot.loader.PropertiesLauncher Using a Spring Boot Fat Jar It is not recommended to build images containing fat jars for various reasons which can easily be found on the internet. If you feel that you must build your application as a Spring Boot fat jar then this can still work with the Coherence Operator. The Java command line to run a Spring Boot fat jar needs to be something like java -jar my-app.jar where my-app.jar is the fat jar. This means that the Operator&#8217;s launcher needs to know the location of the fat jar in the image, so this must be provided in the Coherence deployment yaml. For example, suppose that an application has been built into a fat jar names catalogue-1.0.0.jar which is in the /app/libs directory in the image, so the full path to the jar is /app/libs/catalogue-1.0.0.jar . This needs to be set in the spec.applicaton.springBootFatJar field of the Coherence yaml. The spec.application.type field also needs to be set to spring so that the Operator knows that this is a Spring Boot application <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring springBootFatJar: /app/libs/catalogue-1.0.0.jar The type field set to spring tells the Operator that this is a Spring Boot application. The location of the Spring Boot jar has been set. When the Operator starts the application it will then run a command equivalent to: <markup lang=\"bash\" >java -cp /app/libs/catalogue-1.0.0.jar org.springframework.boot.loader.PropertiesLauncher The Operator does not run the fat jar using the java -jar command because it needs to add various other JVM arguments and append to the classpath, so it has to run the org.springframework.boot.loader.PropertiesLauncher class as opposed to the org.springframework.boot.loader.JarLauncher that java -jar would run. Using Could Native Buildpacks If the Spring Boot Maven or Gradle plugin has been used to produce an image using Cloud Native Buildpacks these images can work with the Coherence Operator. Warning Due to limitation on the way that arguments can be passed to the JVM when using Buildpacks images the Coherence operator will only work with images containing a JVM greater than Java 11. Although the Buildpacks launcher will honour the JAVA_OPTS or JAVA_TOOL_OPTIONS environment variables there appear to be size limitations for the values of these variables that make it impractical for the Operator to use them. The Operator therefore creates a JVM arguments file to pass the arguments to the JVM. At the time of writing these docs, Java 8 (which is the default version of Java used by the Spring Boot plugin) does not support the use of argument files for the JVM. It is simple to configure the version of the JVM used by the Spring Boot plugin, for example in Maven: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.4.RELEASE&lt;/version&gt; &lt;configuration&gt; &lt;image&gt; &lt;env&gt; &lt;BP_JVM_VERSION&gt;11.*&lt;/BP_JVM_VERSION&gt; &lt;/env&gt; &lt;/image&gt; &lt;/configuration&gt; &lt;/plugin&gt; When creating a Coherence deployment for a Spring Boot Buildpacks image The application type must be set to spring . The Operator&#8217;s launcher will automatically detect that the image is a Buildpacks image and launch the application using the Buildpacks launcher. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring The application type has been set to spring so that the operator knows that this is a Spring Boot application, and the fact that the image is a Buildpacks image will be auto-discovered. When the Operator starts the application it will then run the buildpacks launcher with a command equivalent to this: <markup lang=\"bash\" >/cnb/lifecycle/launcher java @jvm-args-file org.springframework.boot.loader.PropertiesLauncher Buildpacks Detection If for some reason buildpacks auto-detection does not work properly the Coherence CRD contains a filed to force buildpacks to be enabled or disabled. The boolean field spec.application.cloudNativeBuildPack.enabled can be set to true to enable buildpacks or false to disable buildpack. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring cloudNativeBuildPack: enabled: true The application type has been set to spring so that the operator knows that this is a Spring Boot application The cloudNativeBuildPack.enabled field has been set to true to force the Operator to use the Buildpacks launcher. Specify the Buildpacks Launcher A Cloud Native Buildpacks image uses a launcher mechanism to run the executable(s) in the image. The Coherence Operator launcher will configure the application and then invoke the same buildpacks launcher. The Coherence Operator assumes that the buildpacks launcher is in the image in the location /cnb/lifecycle/launcher . If a buildpacks image has been built with the launcher in a different location then the Coherence CRD contains a field to set the new location. The spec.application.cloudNativeBuildPack.enabled field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: type: spring cloudNativeBuildPack: launcher: /buildpack/launcher The application type has been set to spring so that the operator knows that this is a Spring Boot application The buildpacks launcher that the Operator will invoke is located at /buildpack/launcher . Buildpack JVM Arguments A typical Spring Boot buildpack launcher will attempt to configure options such as heap size based on the container resource limits configured, so this must be taken into account if using any of the memory options available in the Coherence CRD as there may be conflicting configurations. ",
            "title": "Spring Boot Applications"
        },
        {
            "location": "/docs/installation/08_networking",
            "text": " In order for Coherence clusters to form correctly, the conntrack library must be installed. Most Kubernetes distributions will do this for you. If you have issues with clusters not forming, then you should check that conntrack is installed using this command (or equivalent): <markup lang=\"bash\" >rpm -qa | grep conntrack You should see output similar to that shown below. If you do not, then you should install conntrack using your operating system tools. <markup lang=\"bash\" >libnetfilter_conntrack-1.0.6-1.el7_3.x86_64 conntrack-tools-1.4.4-4.el7.x86_64 ",
            "title": "Operating System Library Requirements"
        },
        {
            "location": "/docs/installation/08_networking",
            "text": " The recommended way to make iptables updates permanent across reboots is to create a systemd service that applies the necessary updates during the startup process. Here is an example; you may need to adjust this to suit your own environment: Create a systemd service: <markup lang=\"bash\" >echo 'Set up systemd service to fix iptables nat chain at each reboot (so Coherence will work)...' mkdir -p /etc/systemd/system/ cat &gt; /etc/systemd/system/fix-iptables.service &lt;&lt; EOF [Unit] Description=Fix iptables After=firewalld.service After=docker.service [Service] ExecStart=/sbin/fix-iptables.sh [Install] WantedBy=multi-user.target EOF Create the script to update iptables : <markup lang=\"bash\" >cat &gt; /sbin/fix-iptables.sh &lt;&lt; EOF #!/bin/bash echo 'Fixing iptables rules for Coherence issue...' TIMES=$((`iptables -t nat -v -L POST_public_allow -n --line-number | wc -l` - 2)) COUNTER=1 while [ $COUNTER -le $TIMES ]; do iptables -t nat -v -D POST_public_allow 1 ((COUNTER++)) done EOF Start the service (or just reboot): <markup lang=\"bash\" >echo 'Start the systemd service to fix iptables nat chain...' systemctl enable --now fix-iptables ",
            "title": "Make iptables Updates Permanent Across Reboots"
        },
        {
            "location": "/docs/installation/08_networking",
            "text": " Some Kubernetes distributions create iptables rules that block some types of traffic that Coherence requires to form clusters. If you are not able to form clusters, then you can check for this issue using the following command: <markup lang=\"bash\" >iptables -t nat -v -L POST_public_allow -n If you see output similar to the example below: <markup lang=\"bash\" >Chain POST_public_allow (1 references) pkts bytes target prot opt in out source destination 164K 11M MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 0 0 MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 For example, if you see any entries in this chain, then you need to remove them. You can remove the entries using this command: <markup lang=\"bash\" >iptables -t nat -v -D POST_public_allow 1 Note that you will need to run that command for each line. So in the example above, you would need to run it twice. After you are done, you can run the previous command again and verify that the output is now an empty list. After making this change, restart your domains and the Coherence cluster should now form correctly. Make iptables Updates Permanent Across Reboots The recommended way to make iptables updates permanent across reboots is to create a systemd service that applies the necessary updates during the startup process. Here is an example; you may need to adjust this to suit your own environment: Create a systemd service: <markup lang=\"bash\" >echo 'Set up systemd service to fix iptables nat chain at each reboot (so Coherence will work)...' mkdir -p /etc/systemd/system/ cat &gt; /etc/systemd/system/fix-iptables.service &lt;&lt; EOF [Unit] Description=Fix iptables After=firewalld.service After=docker.service [Service] ExecStart=/sbin/fix-iptables.sh [Install] WantedBy=multi-user.target EOF Create the script to update iptables : <markup lang=\"bash\" >cat &gt; /sbin/fix-iptables.sh &lt;&lt; EOF #!/bin/bash echo 'Fixing iptables rules for Coherence issue...' TIMES=$((`iptables -t nat -v -L POST_public_allow -n --line-number | wc -l` - 2)) COUNTER=1 while [ $COUNTER -le $TIMES ]; do iptables -t nat -v -D POST_public_allow 1 ((COUNTER++)) done EOF Start the service (or just reboot): <markup lang=\"bash\" >echo 'Start the systemd service to fix iptables nat chain...' systemctl enable --now fix-iptables ",
            "title": "Firewall (iptables) Requirements"
        },
        {
            "location": "/docs/installation/08_networking",
            "text": " Operating System Library Requirements In order for Coherence clusters to form correctly, the conntrack library must be installed. Most Kubernetes distributions will do this for you. If you have issues with clusters not forming, then you should check that conntrack is installed using this command (or equivalent): <markup lang=\"bash\" >rpm -qa | grep conntrack You should see output similar to that shown below. If you do not, then you should install conntrack using your operating system tools. <markup lang=\"bash\" >libnetfilter_conntrack-1.0.6-1.el7_3.x86_64 conntrack-tools-1.4.4-4.el7.x86_64 Firewall (iptables) Requirements Some Kubernetes distributions create iptables rules that block some types of traffic that Coherence requires to form clusters. If you are not able to form clusters, then you can check for this issue using the following command: <markup lang=\"bash\" >iptables -t nat -v -L POST_public_allow -n If you see output similar to the example below: <markup lang=\"bash\" >Chain POST_public_allow (1 references) pkts bytes target prot opt in out source destination 164K 11M MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 0 0 MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 For example, if you see any entries in this chain, then you need to remove them. You can remove the entries using this command: <markup lang=\"bash\" >iptables -t nat -v -D POST_public_allow 1 Note that you will need to run that command for each line. So in the example above, you would need to run it twice. After you are done, you can run the previous command again and verify that the output is now an empty list. After making this change, restart your domains and the Coherence cluster should now form correctly. Make iptables Updates Permanent Across Reboots The recommended way to make iptables updates permanent across reboots is to create a systemd service that applies the necessary updates during the startup process. Here is an example; you may need to adjust this to suit your own environment: Create a systemd service: <markup lang=\"bash\" >echo 'Set up systemd service to fix iptables nat chain at each reboot (so Coherence will work)...' mkdir -p /etc/systemd/system/ cat &gt; /etc/systemd/system/fix-iptables.service &lt;&lt; EOF [Unit] Description=Fix iptables After=firewalld.service After=docker.service [Service] ExecStart=/sbin/fix-iptables.sh [Install] WantedBy=multi-user.target EOF Create the script to update iptables : <markup lang=\"bash\" >cat &gt; /sbin/fix-iptables.sh &lt;&lt; EOF #!/bin/bash echo 'Fixing iptables rules for Coherence issue...' TIMES=$((`iptables -t nat -v -L POST_public_allow -n --line-number | wc -l` - 2)) COUNTER=1 while [ $COUNTER -le $TIMES ]; do iptables -t nat -v -D POST_public_allow 1 ((COUNTER++)) done EOF Start the service (or just reboot): <markup lang=\"bash\" >echo 'Start the systemd service to fix iptables nat chain...' systemctl enable --now fix-iptables ",
            "title": "O/S Networking Configuration"
        },
        {
            "location": "/docs/management/025_coherence_cli",
            "text": " The Operator installs the CLI at the location /coherence-operator/utils/cohctl . Most official Coherence images are distroless images so they do not have a shell that can be used to create a session and execute commands. Each cohctl command will need to be executed as a separate kubectl exec command. Once a Pod is running is it simple to use the CLI. For example, the yaml below will create a simple three member cluster. <markup title=\"minimal.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 The cluster name is storage and there will be three Pods created, storage-0 , storage-1 and storage-2 . To list the services running in the storage-0 Pod the following command can be run: <markup lang=\"bash\" >kubectl exec storage-0 -c coherence -- /coherence-operator/utils/cohctl get services The -c coherence option tells kubectl to exec the command in the coherence container. By default this is the only container that will be running in the Pod, so the option could be omitted. If the option is omitted, kubectl will display a warning to say it assumes you mean the coherence container. Everything after the -- is the command to run in the Pod. In this case we execute: <markup lang=\"bash\" >/coherence-operator/utils/cohctl get services which runs the Coherence CLI binary at /coherence-operator/utils/cohctl with the command get services . The output displayed by the command will look something like this: <markup lang=\"bash\" >Using cluster connection 'default' from current context. SERVICE NAME TYPE MEMBERS STATUS HA STORAGE PARTITIONS \"$GRPC:GrpcProxy\" Proxy 3 n/a -1 -1 \"$SYS:Concurrent\" DistributedCache 3 NODE-SAFE 3 257 \"$SYS:ConcurrentProxy\" Proxy 3 n/a -1 -1 \"$SYS:Config\" DistributedCache 3 NODE-SAFE 3 257 \"$SYS:HealthHttpProxy\" Proxy 3 n/a -1 -1 \"$SYS:SystemProxy\" Proxy 3 n/a -1 -1 ManagementHttpProxy Proxy 3 n/a -1 -1 MetricsHttpProxy Proxy 3 n/a -1 -1 PartitionedCache DistributedCache 3 NODE-SAFE 3 257 PartitionedTopic PagedTopic 3 NODE-SAFE 3 257 Proxy Proxy 3 n/a -1 -1 The exact output will vary depending on the version of Coherence and the configurations being used. More CLI commands can be run by changing the CLI commands specified after /coherence-operator/utils/cohctl . For example, to list all the members of the cluster: <markup lang=\"bash\" >kubectl exec storage-0 -c coherence -- /coherence-operator/utils/cohctl get members ",
            "title": "Using the CLI in Pods"
        },
        {
            "location": "/docs/management/025_coherence_cli",
            "text": " There may be certain circumstances in which you wish to disable the use of the CLI in your cluster. To do this, add the CLI_DISABLED env variable to you config and set to true . <markup title=\"minimal.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 env: - name: \"CLI_DISABLED\" value: \"true\" If you try to run the CLI you will get the following message: <markup >cohctl has been disabled from running in the Coherence Operator ",
            "title": "Disabling CLI Access"
        },
        {
            "location": "/docs/management/025_coherence_cli",
            "text": " If Coherence Management over REST is enabled, it is possible to use the Coherence CLI to access management information. The Operator enables Coherence Management over REST by default, so unless it has specifically been disabled, the CLI can be used. See the Coherence CLI Documentation for more information on how to use the CLI. The Coherence CLI is automatically added to Coherence Pods by the Operator, so it is available as an executable that can be run using kubectl exec . At start-up of a Coherence container a default Coherence CLI configuration is created so that the CLI knows about the local cluster member. Using the CLI in Pods The Operator installs the CLI at the location /coherence-operator/utils/cohctl . Most official Coherence images are distroless images so they do not have a shell that can be used to create a session and execute commands. Each cohctl command will need to be executed as a separate kubectl exec command. Once a Pod is running is it simple to use the CLI. For example, the yaml below will create a simple three member cluster. <markup title=\"minimal.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 The cluster name is storage and there will be three Pods created, storage-0 , storage-1 and storage-2 . To list the services running in the storage-0 Pod the following command can be run: <markup lang=\"bash\" >kubectl exec storage-0 -c coherence -- /coherence-operator/utils/cohctl get services The -c coherence option tells kubectl to exec the command in the coherence container. By default this is the only container that will be running in the Pod, so the option could be omitted. If the option is omitted, kubectl will display a warning to say it assumes you mean the coherence container. Everything after the -- is the command to run in the Pod. In this case we execute: <markup lang=\"bash\" >/coherence-operator/utils/cohctl get services which runs the Coherence CLI binary at /coherence-operator/utils/cohctl with the command get services . The output displayed by the command will look something like this: <markup lang=\"bash\" >Using cluster connection 'default' from current context. SERVICE NAME TYPE MEMBERS STATUS HA STORAGE PARTITIONS \"$GRPC:GrpcProxy\" Proxy 3 n/a -1 -1 \"$SYS:Concurrent\" DistributedCache 3 NODE-SAFE 3 257 \"$SYS:ConcurrentProxy\" Proxy 3 n/a -1 -1 \"$SYS:Config\" DistributedCache 3 NODE-SAFE 3 257 \"$SYS:HealthHttpProxy\" Proxy 3 n/a -1 -1 \"$SYS:SystemProxy\" Proxy 3 n/a -1 -1 ManagementHttpProxy Proxy 3 n/a -1 -1 MetricsHttpProxy Proxy 3 n/a -1 -1 PartitionedCache DistributedCache 3 NODE-SAFE 3 257 PartitionedTopic PagedTopic 3 NODE-SAFE 3 257 Proxy Proxy 3 n/a -1 -1 The exact output will vary depending on the version of Coherence and the configurations being used. More CLI commands can be run by changing the CLI commands specified after /coherence-operator/utils/cohctl . For example, to list all the members of the cluster: <markup lang=\"bash\" >kubectl exec storage-0 -c coherence -- /coherence-operator/utils/cohctl get members Disabling CLI Access There may be certain circumstances in which you wish to disable the use of the CLI in your cluster. To do this, add the CLI_DISABLED env variable to you config and set to true . <markup title=\"minimal.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 env: - name: \"CLI_DISABLED\" value: \"true\" If you try to run the CLI you will get the following message: <markup >cohctl has been disabled from running in the Coherence Operator ",
            "title": "The Coherence CLI"
        },
        {
            "location": "/docs/performance/010_performance",
            "text": " Many customers use Coherence because they want access to data at in-memory speeds. Customers who want the best performance from their application typically embark on performance testing and load testing of their application. When doing this sort of testing on Kubernetes, it is useful to understand the ways that your Kubernetes environment can impact your test results. ",
            "title": "Performance Testing in Kubernetes"
        },
        {
            "location": "/docs/performance/010_performance",
            "text": " This example is going to talks about scheduling Pods to a single availability zone in a Kubernetes cluster in the cloud. Pod scheduling in this way uses Node labels, and in fact any label on the Nodes in your cluster could be used to fix the location of the Pods. To schedule all the Coherence Pods into a single zone we first need to know what zones we have and what labels have used. The standard Kubernetes Node label for the availability zone is topology.kubernetes.io/zone (as documented in the Kubernetes Labels Annotations and Taints documentation). To slightly confuse the situation, prior to Kubernetes 1.17 the label was failure-domain.beta.kubernetes.io/zone , which has now been deprecated. Some Kubernetes clusters, even after 1.17, still use the deprecated label, so you need to know what labels your Nodes have. Run the following command so list the nodes in a Kubernetes cluster with the value of the two zone labels for each node: <markup lang=\"bash\" >kubectl get nodes -L topology.kubernetes.io/zone,failure-domain.beta.kubernetes.io/zone The output will be something like this: <markup >NAME STATUS ROLES AGE VERSION ZONE ZONE node-1 Ready node 66d v1.19.7 US-ASHBURN-AD-1 node-2 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-3 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-4 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-5 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-6 Ready node 66d v1.19.7 US-ASHBURN-AD-1 In the output above the first Zone column has values, and the second does not. This means that the zone label used is the first in the label list in our kubectl command, i.e., topology.kubernetes.io/zone . If the nodes had been labeled with the second, deprecated, label in the kubectl command list failure-domain.beta.kubernetes.io/zone the output would look like this: <markup >NAME STATUS ROLES AGE VERSION ZONE ZONE node-1 Ready node 66d v1.19.7 US-ASHBURN-AD-1 node-2 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-3 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-4 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-5 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-6 Ready node 66d v1.19.7 US-ASHBURN-AD-1 From the list of nodes above we can see that there are three zones, US-ASHBURN-AD-1 , US-ASHBURN-AD-2 and US-ASHBURN-AD-3 . In this example we will schedule all the Pods to zome US-ASHBURN-AD-1 . ",
            "title": "Finding Node Zones"
        },
        {
            "location": "/docs/performance/010_performance",
            "text": " The Coherence CRD supports a number of ways to schedule Pods, as described in the Configure Pod Scheduling documentation. Using node labels is the simplest of the scheduling methods. In this case we need to schedule Pods onto nodes that have the label topology.kubernetes.io/zone=US-ASHBURN-AD-1 . In the Coherence yaml we use the nodeSelector field. <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 nodeSelector: topology.kubernetes.io/zone: US-ASHBURN-AD-1 When the yaml above is applied, a cluster of three Pods will be created, all scheduled onto nodes in the US-ASHBURN-AD-1 availability zone. ",
            "title": "Scheduling Pods of a Coherence Cluster"
        },
        {
            "location": "/docs/performance/010_performance",
            "text": " Depending on the Kubernetes cluster you are using there could be various other factors to bear in mind. Many Kubernetes clusters run on virtual machines, which can be poor for repeated performance comparisons unless you know what else might be running on the underlying hardware that the VM is on. If a test run happens at the same time as another VM is consuming a lot of the underlying hardware resource this can obviously impact the results. Unfortunately bear-metal hardware, the best for repeated performance tests, is not always available, so it is useful to bear this in mind if there is suddenly a strange outlier in the tests. ",
            "title": "Other Performance Factors"
        },
        {
            "location": "/docs/performance/010_performance",
            "text": " When an application has been deployed into Kubernetes, pods will typically be distributed over many nodes in the Kubernetes cluster. When deploying into Kubernetes cluster in the cloud, for example on Oracle OKE, the nodes can be distributed across different availability zones. These zones are effectively different data centers, meaning that the network speed can differ considerable between nodes in different zones. Performance testing in this sort of environment can be difficult if you use default Pod scheduling. Different test runs could distribute Pods to different nodes, in different zones, and skew results depending on how \"far\" test clients and servers are from each other. For example, when testing a simple Coherence EntryProcessor invocation in a Kubernetes cluster spread across zones, we saw the 95% response time when the client and server were in the same zone was 0.1 milli-seconds. When the client and server were in different zones, the 95% response time could be as high as 0.8 milli-seconds. This difference is purely down to the network distance between nodes. Depending on the actual use-cases being tested, this difference might not have much impact on overall response times, but for simple operations it can be a significant enough overhead to impact test results. The solution to the issue described above is to use Pod scheduling to fix the location of the Pods to be used for tests. In a cluster like Oracle OKE, this would ensure all the Pods will be scheduled into the same availability zone. Finding Node Zones This example is going to talks about scheduling Pods to a single availability zone in a Kubernetes cluster in the cloud. Pod scheduling in this way uses Node labels, and in fact any label on the Nodes in your cluster could be used to fix the location of the Pods. To schedule all the Coherence Pods into a single zone we first need to know what zones we have and what labels have used. The standard Kubernetes Node label for the availability zone is topology.kubernetes.io/zone (as documented in the Kubernetes Labels Annotations and Taints documentation). To slightly confuse the situation, prior to Kubernetes 1.17 the label was failure-domain.beta.kubernetes.io/zone , which has now been deprecated. Some Kubernetes clusters, even after 1.17, still use the deprecated label, so you need to know what labels your Nodes have. Run the following command so list the nodes in a Kubernetes cluster with the value of the two zone labels for each node: <markup lang=\"bash\" >kubectl get nodes -L topology.kubernetes.io/zone,failure-domain.beta.kubernetes.io/zone The output will be something like this: <markup >NAME STATUS ROLES AGE VERSION ZONE ZONE node-1 Ready node 66d v1.19.7 US-ASHBURN-AD-1 node-2 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-3 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-4 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-5 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-6 Ready node 66d v1.19.7 US-ASHBURN-AD-1 In the output above the first Zone column has values, and the second does not. This means that the zone label used is the first in the label list in our kubectl command, i.e., topology.kubernetes.io/zone . If the nodes had been labeled with the second, deprecated, label in the kubectl command list failure-domain.beta.kubernetes.io/zone the output would look like this: <markup >NAME STATUS ROLES AGE VERSION ZONE ZONE node-1 Ready node 66d v1.19.7 US-ASHBURN-AD-1 node-2 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-3 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-4 Ready node 66d v1.19.7 US-ASHBURN-AD-2 node-5 Ready node 66d v1.19.7 US-ASHBURN-AD-3 node-6 Ready node 66d v1.19.7 US-ASHBURN-AD-1 From the list of nodes above we can see that there are three zones, US-ASHBURN-AD-1 , US-ASHBURN-AD-2 and US-ASHBURN-AD-3 . In this example we will schedule all the Pods to zome US-ASHBURN-AD-1 . Scheduling Pods of a Coherence Cluster The Coherence CRD supports a number of ways to schedule Pods, as described in the Configure Pod Scheduling documentation. Using node labels is the simplest of the scheduling methods. In this case we need to schedule Pods onto nodes that have the label topology.kubernetes.io/zone=US-ASHBURN-AD-1 . In the Coherence yaml we use the nodeSelector field. <markup lang=\"yaml\" title=\"coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 nodeSelector: topology.kubernetes.io/zone: US-ASHBURN-AD-1 When the yaml above is applied, a cluster of three Pods will be created, all scheduled onto nodes in the US-ASHBURN-AD-1 availability zone. Other Performance Factors Depending on the Kubernetes cluster you are using there could be various other factors to bear in mind. Many Kubernetes clusters run on virtual machines, which can be poor for repeated performance comparisons unless you know what else might be running on the underlying hardware that the VM is on. If a test run happens at the same time as another VM is consuming a lot of the underlying hardware resource this can obviously impact the results. Unfortunately bear-metal hardware, the best for repeated performance tests, is not always available, so it is useful to bear this in mind if there is suddenly a strange outlier in the tests. ",
            "title": "Where are your Nodes?"
        },
        {
            "location": "/examples/020_hello_world/README",
            "text": " If you have not already done so, you need to install the Coherence Operator. There are a few simple ways to do this as described in the Installation Guide ",
            "title": "Install the Operator"
        },
        {
            "location": "/examples/020_hello_world/README",
            "text": " All the fields in the Coherence CRD spec are optional, the Operator will apply default values, if required, for fields not specified. For example, this is the minimum required yaml to run a Coherence cluster: <markup lang=\"yaml\" title=\"default-coherence.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test The yaml above could be installed into Kubernetes using kubectl: <markup lang=\"bash\" >kubectl create -f default-coherence.yaml The command above will create a Coherence cluster named test in the default Kubernetes namespace. Because no spec was specified in the yaml, the Operator will use its defaults for certain fields. The replicas field, which controls the number of Pods in the cluster, will default to 3 . The image used to run Coherence will be the default for this version of the Operator, typically this is the latest Coherence CE image released at the time the Operator version was released. No ports will be exposed on the container, and no additional services will be created. We can list the resources that have been created by the Operator. <markup lang=\"bash\" >kubectl get all Which should display something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/test-0 1/1 Running 0 81s pod/test-1 1/1 Running 0 81s pod/test-2 1/1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/test-sts ClusterIP None &lt;none&gt; 7/TCP 81s service/test-wka ClusterIP None &lt;none&gt; 7/TCP 81s NAME READY AGE statefulset.apps/test 3/3 81s We can see that the Operator has created a StatefulSet , with three Pods and there are two Services . The test-sts service is the headless service required for the StatefulSet . The test-wka service is the headless service that Coherence will use for well known address cluster discovery. We can now undeploy the cluster: <markup lang=\"bash\" >kubectl delete -f default-coherence.yaml ",
            "title": "A Default Coherence Cluster"
        },
        {
            "location": "/examples/020_hello_world/README",
            "text": " We can deploy a specific image by setting the spec.image field in the yaml. In this example we&#8217;ll deploy the simple-coherence:1.0.0 image built in the Build a Coherence Server Image example. To deploy a specific image we just need to set the spec.image field. <markup lang=\"yaml\" title=\"simple.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: simple spec: image: simple-coherence:1.0.0 replicas: 6 ports: - name: extend port: 20000 We have set the image to use to the Build a Coherence Server Image example simple-coherence:1.0.0 . We have set the replicas field to 6 , so this time there should only be six Pods. The simple image starts a Coherence Extend proxy on port 20000 , so we expose this port in the Coherence spec. The Operator will then expose the port on the Coherence container and create a Service for the port. We can deploy the simple cluster into Kubernetes using kubectl: <markup lang=\"bash\" >kubectl create -f simple.yaml Now list the resources the Operator has created. <markup lang=\"bash\" >kubectl get all Which this time should look something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/test-0 1/1 Running 0 4m49s pod/test-1 1/1 Running 0 4m49s pod/test-2 1/1 Running 0 4m49s pod/test-3 1/1 Running 0 4m49s pod/test-4 1/1 Running 0 4m49s pod/test-5 1/1 Running 0 4m49s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 164d service/test-extend ClusterIP 10.108.166.193 &lt;none&gt; 20000/TCP 4m49s service/test-sts ClusterIP None &lt;none&gt; 7/TCP 4m49s service/test-wka ClusterIP None &lt;none&gt; 7/TCP 4m49s NAME READY AGE statefulset.apps/test 6/6 4m49s We can see that the Operator has created a StatefulSet , with six Pods and there are three Services . The simple-sts service is the headless service required for the StatefulSet . The simple-wka service is the headless service that Coherence will use for well known address cluster discovery. The simple-extend service is the service that exposes the Extend port 20000 , and could be used by Extend clients to connect to the cluster. We can now delete the simple cluster: <markup lang=\"bash\" >kubectl delete -f simple.yaml ",
            "title": "Deploy the Simple Server Image"
        },
        {
            "location": "/examples/020_hello_world/README",
            "text": " This is the most basic example of how to deploy a simple Coherence cluster to Kubernetes using the Coherence Operator. Tip The complete source code for this example is in the Coherence Operator GitHub repository. Install the Operator If you have not already done so, you need to install the Coherence Operator. There are a few simple ways to do this as described in the Installation Guide A Default Coherence Cluster All the fields in the Coherence CRD spec are optional, the Operator will apply default values, if required, for fields not specified. For example, this is the minimum required yaml to run a Coherence cluster: <markup lang=\"yaml\" title=\"default-coherence.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test The yaml above could be installed into Kubernetes using kubectl: <markup lang=\"bash\" >kubectl create -f default-coherence.yaml The command above will create a Coherence cluster named test in the default Kubernetes namespace. Because no spec was specified in the yaml, the Operator will use its defaults for certain fields. The replicas field, which controls the number of Pods in the cluster, will default to 3 . The image used to run Coherence will be the default for this version of the Operator, typically this is the latest Coherence CE image released at the time the Operator version was released. No ports will be exposed on the container, and no additional services will be created. We can list the resources that have been created by the Operator. <markup lang=\"bash\" >kubectl get all Which should display something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/test-0 1/1 Running 0 81s pod/test-1 1/1 Running 0 81s pod/test-2 1/1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/test-sts ClusterIP None &lt;none&gt; 7/TCP 81s service/test-wka ClusterIP None &lt;none&gt; 7/TCP 81s NAME READY AGE statefulset.apps/test 3/3 81s We can see that the Operator has created a StatefulSet , with three Pods and there are two Services . The test-sts service is the headless service required for the StatefulSet . The test-wka service is the headless service that Coherence will use for well known address cluster discovery. We can now undeploy the cluster: <markup lang=\"bash\" >kubectl delete -f default-coherence.yaml Deploy the Simple Server Image We can deploy a specific image by setting the spec.image field in the yaml. In this example we&#8217;ll deploy the simple-coherence:1.0.0 image built in the Build a Coherence Server Image example. To deploy a specific image we just need to set the spec.image field. <markup lang=\"yaml\" title=\"simple.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: simple spec: image: simple-coherence:1.0.0 replicas: 6 ports: - name: extend port: 20000 We have set the image to use to the Build a Coherence Server Image example simple-coherence:1.0.0 . We have set the replicas field to 6 , so this time there should only be six Pods. The simple image starts a Coherence Extend proxy on port 20000 , so we expose this port in the Coherence spec. The Operator will then expose the port on the Coherence container and create a Service for the port. We can deploy the simple cluster into Kubernetes using kubectl: <markup lang=\"bash\" >kubectl create -f simple.yaml Now list the resources the Operator has created. <markup lang=\"bash\" >kubectl get all Which this time should look something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/test-0 1/1 Running 0 4m49s pod/test-1 1/1 Running 0 4m49s pod/test-2 1/1 Running 0 4m49s pod/test-3 1/1 Running 0 4m49s pod/test-4 1/1 Running 0 4m49s pod/test-5 1/1 Running 0 4m49s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 164d service/test-extend ClusterIP 10.108.166.193 &lt;none&gt; 20000/TCP 4m49s service/test-sts ClusterIP None &lt;none&gt; 7/TCP 4m49s service/test-wka ClusterIP None &lt;none&gt; 7/TCP 4m49s NAME READY AGE statefulset.apps/test 6/6 4m49s We can see that the Operator has created a StatefulSet , with six Pods and there are three Services . The simple-sts service is the headless service required for the StatefulSet . The simple-wka service is the headless service that Coherence will use for well known address cluster discovery. The simple-extend service is the service that exposes the Extend port 20000 , and could be used by Extend clients to connect to the cluster. We can now delete the simple cluster: <markup lang=\"bash\" >kubectl delete -f simple.yaml ",
            "title": "A \"Hello World\" Operator Example"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " fa-rocket Quick Start Quick start guide to running your first Coherence cluster using the Coherence Operator. fa-save Install Installing and running the Coherence Operator. cloud_upload Build and Deploy Applications Deploying Coherence Applications using the Coherence Operator. fa-question-circle Troubleshooting Hints and tips to troubleshoot common issues. ",
            "title": "Get Going"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " explore Guides & Examples There are a number of examples of using the Coherence Operator and different Coherence features in Kubernetes. fa-ban Non-Operator Guides & Examples Examples for those in the unlucky situation of not being able to install Operators, CRDs, RBAC cluster roles etc, and therfore need to manage Coherence clusters manually. ",
            "title": "Examples"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " fa-balance-scale Scaling Safe scaling of Coherence deployments the Coherence Operator. fa-cogs Configure Coherence Configuring Coherence behaviour. fa-cog Configure the JVM Configure the behaviour of the JVM. control_camera Expose Ports & Services Configure services to expose ports provided by the application. speed Metrics Enabling and working with Metrics in the Coherence Operator. fa-stethoscope Management and Diagnostics Management and Diagnostics in the Coherence Operator. find_in_page Logging Viewing and managing log files within using the Coherence Operator. widgets Coherence CRD Reference Coherence CRD reference guide. ",
            "title": "In Depth"
        },
        {
            "location": "/docs/coherence/050_storage_enabled",
            "text": " Partitioned cache services that manage Coherence caches are configured as storage enabled or storage disabled. Whilst it is possible to configure individual services to be storage enabled or disabled in the cache configuration file and have a mixture of modes in a single JVM, typically all the services in a JVM share the same mode by setting the coherence.distributed.localstorage system property to true for storage enabled members and to false for storage disabled members. The Coherence CRD allows this property to be set by specifying the spec.coherence.storageEnabled field to either true or false. The default value when nothing is specified is true . <markup lang=\"yaml\" title=\"storage enabled\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: storageEnabled: true The Coherence resource specifically sets coherence.distributed.localstorage to true <markup lang=\"yaml\" title=\"storage disabled\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: storageEnabled: false The Coherence resource specifically sets coherence.distributed.localstorage to false ",
            "title": "Storage Enabled or Disabled Deployments"
        },
        {
            "location": "/docs/installation/02_pre_release_versions",
            "text": " Pre-release version of the Coherence Operator are made available from time to time. ",
            "title": "preambule"
        },
        {
            "location": "/docs/installation/02_pre_release_versions",
            "text": " We cannot guarantee that pre-release versions of the Coherence Operator are bug free and hence they should not be used in production. We reserve the right to remove pre-release versions of the Helm chart and Docker images ant any time and without notice. We cannot guarantee that APIs and CRD specifications will remain stable or backwards compatible between pre-release versions. To access pre-release versions of the Helm chart add the unstable chart repository. <markup lang=\"bash\" >helm repo add coherence-unstable https://oracle.github.io/coherence-operator/charts-unstable helm repo update To list all the available Coherence Operator chart versions: <markup lang=\"bash\" >helm search coherence-operator -l The -l parameter shows all versions as opposed to just the latest versions if it was omitted. A specific pre-release version of the Helm chart can be installed using the --version argument, for example to use version 3.0.0-2005140315 : <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 3.0.0-2005140315 \\ --namespace &lt;namespace&gt; \\ --name coherence-operator The --version argument is used to specify the exact version of the chart The optional --namespace parameter to specify which namespace to install the operator into, if omitted then Helm will install into whichever is currently the default namespace for your Kubernetes configuration. When using pre-release versions of the Helm chart it is always advisable to install a specific version otherwise Helm will try to work out the latest version in the pre-release repo and as pre-release version numbers are not strictly sem-ver compliant this may be unreliable. ",
            "title": "Accessing Pre-Release Versions"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " To show Extend working the example will require a Coherence cluster to connect to. For the server the example will use the image built in the Build a Coherence Server Image using JIB example (or could also use the Build a Coherence Server Image using a Dockerfile example. If you have not already done so, you should build the image from that example, so it is available to deploy to your Kubernetes cluster. ",
            "title": "Server Image"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " If you have not already done so, you need to install the Coherence Operator. There are a few simple ways to do this as described in the Installation Guide ",
            "title": "Install the Operator"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " Server Image To show Extend working the example will require a Coherence cluster to connect to. For the server the example will use the image built in the Build a Coherence Server Image using JIB example (or could also use the Build a Coherence Server Image using a Dockerfile example. If you have not already done so, you should build the image from that example, so it is available to deploy to your Kubernetes cluster. Install the Operator If you have not already done so, you need to install the Coherence Operator. There are a few simple ways to do this as described in the Installation Guide ",
            "title": "Prerequisites"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " Coherence*Extend is the mechanism used by remote Coherence client applications to connect to a Coherence cluster. Coherence*Extend includes support for native Coherence clients (Java, C++, and .NET) and non-native Coherence clients (REST and Memcached). Coherence*Extend can be used to connect clients to Coherence clusters running in Kubernetes. There are two scenarios, the client could also be in kubernetes, or the client could be external connecting via a service or some other form of ingress. There are different ways to configure the client in these scenarios. These examples are not going to cover all the possible use-cases for Extend, the examples are specifically about different ways to connect a client to a Coherence cluster running inside kubernetes. Extend is extensively documented in the official Coherence documentation . Prerequisites Server Image To show Extend working the example will require a Coherence cluster to connect to. For the server the example will use the image built in the Build a Coherence Server Image using JIB example (or could also use the Build a Coherence Server Image using a Dockerfile example. If you have not already done so, you should build the image from that example, so it is available to deploy to your Kubernetes cluster. Install the Operator If you have not already done so, you need to install the Coherence Operator. There are a few simple ways to do this as described in the Installation Guide ",
            "title": "Coherence Extend Clients"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " To demonstrate different configurations and connectivity we need a simple Extend client application. Tip The complete source code for this example is in the Coherence Operator GitHub repository. As the client application only needs to demonstrate connectivity to Coherence using different configurations it is not going to do very much. There is a single class with a main method. In the main method the code obtains a NamedMap from Coherence via Extend and does some simple put and get operations. If these all function correctly the application exits with an exit code of zero. If there is an exception, the stack trace is printed and the application exits with an exit code of 1. There are also some different cache configuration files for the different ways to configure Extend, these are covered in the relevant examples below. ",
            "title": "The Client Application"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The image can be built using the JIB plugin with either Maven or Gradle, as described below. Using Maven we run: <markup lang=\"bash\" >./mvnw compile jib:dockerBuild Using Gradle we run: <markup lang=\"bash\" >./gradlew compileJava jibDockerBuild The command above will create an image named simple-extend-client with two tags, latest and 1.0.0 . Listing the local images should show the new images. <markup lang=\"bash\" >$ docker images | grep simple simple-extend-client 1.0.0 1613cd3b894e 51 years ago 220MB simple-extend-client latest 1613cd3b894e 51 years ago 220MB ",
            "title": "Using the Maven or Gradle JIB Plugin"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " Alternatively, if you cannot use the JIB plugin in your environment, the client image can be built using a simple Dockerfile and docker build command. We will still use Maven or Gradle to pull all the required dependencies together. Using Maven we run: <markup lang=\"bash\" >./mvnw package docker build -t simple-extend-client:1.0.0 -t simple-extend-client:latest target/docker Using Gradle we run: <markup lang=\"bash\" >./gradlew assembleImage docker build -t simple-extend-client:1.0.0 -t simple-extend-client:latest build/docker Again, the build should result in the Extend client images The command above will create an image named simple-extend-client with two tags, latest and 1.0.0 . Listing the local images should show the new images. <markup lang=\"bash\" >$ docker images | grep simple simple-extend-client 1.0.0 1613cd3b894e 2 minutes ago 220MB simple-extend-client latest 1613cd3b894e 2 minutes ago 220MB If we tried to run the application or image at this point it would fail with an exception as there is no cluster to connect to. ",
            "title": "Using a Dockerfile"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The client application is both a Maven and Gradle project, so you can use whichever you are comfortable with. The only dependency the client application needs is coherence.jar . Using the Maven or Gradle JIB Plugin The image can be built using the JIB plugin with either Maven or Gradle, as described below. Using Maven we run: <markup lang=\"bash\" >./mvnw compile jib:dockerBuild Using Gradle we run: <markup lang=\"bash\" >./gradlew compileJava jibDockerBuild The command above will create an image named simple-extend-client with two tags, latest and 1.0.0 . Listing the local images should show the new images. <markup lang=\"bash\" >$ docker images | grep simple simple-extend-client 1.0.0 1613cd3b894e 51 years ago 220MB simple-extend-client latest 1613cd3b894e 51 years ago 220MB Using a Dockerfile Alternatively, if you cannot use the JIB plugin in your environment, the client image can be built using a simple Dockerfile and docker build command. We will still use Maven or Gradle to pull all the required dependencies together. Using Maven we run: <markup lang=\"bash\" >./mvnw package docker build -t simple-extend-client:1.0.0 -t simple-extend-client:latest target/docker Using Gradle we run: <markup lang=\"bash\" >./gradlew assembleImage docker build -t simple-extend-client:1.0.0 -t simple-extend-client:latest build/docker Again, the build should result in the Extend client images The command above will create an image named simple-extend-client with two tags, latest and 1.0.0 . Listing the local images should show the new images. <markup lang=\"bash\" >$ docker images | grep simple simple-extend-client 1.0.0 1613cd3b894e 2 minutes ago 220MB simple-extend-client latest 1613cd3b894e 2 minutes ago 220MB If we tried to run the application or image at this point it would fail with an exception as there is no cluster to connect to. ",
            "title": "Building the Client Image"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The default cache configuration file, built into coherence.jar configures an Extend proxy that binds to an ephemeral port. The proxy-scheme configuration looks like this: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;autostart system-property=\"coherence.proxy.enabled\"&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; That is all that is required in a cache configuration file to create a proxy service that will bind to an ephemeral port. The proxy is enabled by default, but could be disabled by setting the system property coherence.proxy.enabled to false. ",
            "title": "Proxy Server Configuration"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " To run the NameService examples below the server needs to be deployed. The example includes a manifests/ directory containing Kubernetes yaml files used by the example. For the NameService examples below the server will use the default cache configuration file from coherence.jar which has the Proxy service configured above. The yaml to deploy the server cluster is in the manifests/default-server.yaml file. <markup lang=\"yaml\" title=\"manifests/default-server.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: image: simple-coherence:1.0.0 replicas: 3 coherence: cacheConfig: coherence-cache-config.xml The yaml above will deploy a three member cluster configured to use the default coherence-cache-config.xml configuration file. There are no additional ports exposed in the configuration. The Extend proxy will be listening on an ephemeral port, so we have no idea what that port will be. We can deploy the server into the default namespace in kubernetes with the following command: <markup lang=\"bash\" >kubectl apply -f manifests/default-server.yaml We can list the resources created by the Operator. <markup lang=\"bash\" >kubectl get all Which should display something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/storage-0 1/1 Running 0 81s pod/storage-1 1/1 Running 0 81s pod/storage-2 1/1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/storage-sts ClusterIP None &lt;none&gt; 7/TCP 81s service/storage-wka ClusterIP None &lt;none&gt; 7/TCP 81s NAME READY AGE statefulset.apps/storage 3/3 81s We can see that the Operator has created a StatefulSet , with three Pods and there are two Services . The storage-sts service is the headless service required for the StatefulSet . The storage-wka service is the headless service that Coherence will use for well known address cluster discovery. ",
            "title": "Deploy the Server"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The simplest way to run the Extend client in Kubernetes is as a Job . The client just connects to a cache and does a put , then exits, so a Job is ideal for this type of process. The example contains yaml to create a Job manifests/minimal-job.yaml that looks like this: <markup lang=\"yaml\" title=\"manifests/minimal-job.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: extend-client spec: template: spec: restartPolicy: Never containers: - name: client image: simple-extend-client:1.0.0 env: - name: COHERENCE_CACHE_CONFIG value: minimal-client-cache-config.xml - name: COHERENCE_WKA value: storage-wka - name: COHERENCE_CLUSTER value: storage To be able to run the client we need to set in three pieces of information. The name of the cache configuration file. We set this using the COHERENCE_CACHE_CONFIG environment variable, and set the value to minimal-client-cache-config.xml , which is the configuration file we&#8217;re using in this example. The client needs to be able to discover the storage Pods to connect to. Just like the server cluster uses well known addresses to discover a cluster, the client can do the same. We set the COHERENCE_WKA environment variable to the name of the WKA service created for the server when we deployed it above, in this case it is storage-wka . Finally, we set the name of the Coherence cluster the client will connect to. When we deployed the server we did not specify a name, so the default cluster name will be the same as the Coherence resource name, in this case storage . So we set the COHERENCE_CLUSTER environment variable to storage . The client Job can be deployed into the default namespace in Kubernetes with the following command: <markup lang=\"bash\" >kubectl apply -f manifests/minimal-job.yaml The Jobs deployed can then be listed <markup lang=\"bash\" >kubectl get job Which should display something like this: <markup lang=\"bash\" >NAME COMPLETIONS DURATION AGE extend-client 1/1 4s 5s The Job above completed very quickly, which we would expect as it is just doing a trivial put to a cache. We can list the Pods created for the Job and then look at the log from the client. All Pods associated to a Job have a label in the form job-name: &lt;name-of-job&gt; , so in our case the label will be job-name: extend-client . We can use this with kubectl to list Pods associated to the Job . If the Job ran successfully there should be only one Pod . If the Job failed and has a restart policy, or was restarted by Kubernetes for other reasons there could be multiple Pods . In this case we expect a single successful Pod . <markup lang=\"bash\" >kubectl get pod -l job-name=extend-client <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE extend-client-k7wfq 0/1 Completed 0 4m24s If we look at the log for the Pod we should see the last line printed to System.out by the client: <markup lang=\"bash\" >kubectl logs extend-client-k7wfq The last line of the log will be something like this: <markup lang=\"bash\" >Put key=key-1 value=0.9332279895860512 previous=null The values will be different as we put different random values each time the client runs. The previous value was null in this case as we have not run any other client with this cluster. If we re-ran the client Job the previous value would be displayed as the cache on the server now has data in it. Clean-Up We have shown a simple Extend client running in Kubernetes, connecting to a Coherence cluster using the NameService. We can now delete the Job using kubectl . <markup lang=\"bash\" >kubectl delete job extend-client We can also delete the server. <markup lang=\"bash\" >kubectl delete -f manifests/default-server.yaml ",
            "title": "Deploy the Client"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The configuration required for the Extend client is equally minimal. The example source code includes a configuration file named src/main/resources/minimal-client-cache-config.xml that can be used to connect to the proxy configured above. <markup lang=\"xml\" title=\"src/main/resources/minimal-client-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteService&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; There is a single cache-mapping that maps all cache names to the scheme named remote . The remote-scheme is named remote . The remote-scheme has a service name of RemoteService . The remote service will connect to a proxy service on the server that is named Proxy , this must correspond to the name of the proxy service in our server cache configuration file. Deploy the Client The simplest way to run the Extend client in Kubernetes is as a Job . The client just connects to a cache and does a put , then exits, so a Job is ideal for this type of process. The example contains yaml to create a Job manifests/minimal-job.yaml that looks like this: <markup lang=\"yaml\" title=\"manifests/minimal-job.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: extend-client spec: template: spec: restartPolicy: Never containers: - name: client image: simple-extend-client:1.0.0 env: - name: COHERENCE_CACHE_CONFIG value: minimal-client-cache-config.xml - name: COHERENCE_WKA value: storage-wka - name: COHERENCE_CLUSTER value: storage To be able to run the client we need to set in three pieces of information. The name of the cache configuration file. We set this using the COHERENCE_CACHE_CONFIG environment variable, and set the value to minimal-client-cache-config.xml , which is the configuration file we&#8217;re using in this example. The client needs to be able to discover the storage Pods to connect to. Just like the server cluster uses well known addresses to discover a cluster, the client can do the same. We set the COHERENCE_WKA environment variable to the name of the WKA service created for the server when we deployed it above, in this case it is storage-wka . Finally, we set the name of the Coherence cluster the client will connect to. When we deployed the server we did not specify a name, so the default cluster name will be the same as the Coherence resource name, in this case storage . So we set the COHERENCE_CLUSTER environment variable to storage . The client Job can be deployed into the default namespace in Kubernetes with the following command: <markup lang=\"bash\" >kubectl apply -f manifests/minimal-job.yaml The Jobs deployed can then be listed <markup lang=\"bash\" >kubectl get job Which should display something like this: <markup lang=\"bash\" >NAME COMPLETIONS DURATION AGE extend-client 1/1 4s 5s The Job above completed very quickly, which we would expect as it is just doing a trivial put to a cache. We can list the Pods created for the Job and then look at the log from the client. All Pods associated to a Job have a label in the form job-name: &lt;name-of-job&gt; , so in our case the label will be job-name: extend-client . We can use this with kubectl to list Pods associated to the Job . If the Job ran successfully there should be only one Pod . If the Job failed and has a restart policy, or was restarted by Kubernetes for other reasons there could be multiple Pods . In this case we expect a single successful Pod . <markup lang=\"bash\" >kubectl get pod -l job-name=extend-client <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE extend-client-k7wfq 0/1 Completed 0 4m24s If we look at the log for the Pod we should see the last line printed to System.out by the client: <markup lang=\"bash\" >kubectl logs extend-client-k7wfq The last line of the log will be something like this: <markup lang=\"bash\" >Put key=key-1 value=0.9332279895860512 previous=null The values will be different as we put different random values each time the client runs. The previous value was null in this case as we have not run any other client with this cluster. If we re-ran the client Job the previous value would be displayed as the cache on the server now has data in it. Clean-Up We have shown a simple Extend client running in Kubernetes, connecting to a Coherence cluster using the NameService. We can now delete the Job using kubectl . <markup lang=\"bash\" >kubectl delete job extend-client We can also delete the server. <markup lang=\"bash\" >kubectl delete -f manifests/default-server.yaml ",
            "title": "Minimal Extend Client Configuration"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " In the first example we deployed the client to the same namespace as the server. If we wanted to deploy the client to a different namespace we would need to ensure the fully qualified name of the WKA service is used when setting the COHERENCE_WKA environment variable. The Coherence cluster is deployed into the default namespace so the fully qualified WKA service name is storage-wka.default.svc . <markup lang=\"yaml\" title=\"manifests/minimal-job.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: extend-client spec: template: spec: restartPolicy: Never containers: - name: client image: simple-extend-client:1.0.0 env: - name: COHERENCE_CACHE_CONFIG value: minimal-client-cache-config.xml - name: COHERENCE_WKA value: storage-wka.default.svc - name: COHERENCE_CLUSTER value: storage We can deploy this client Job into a different namespace than the cluster is deployed into: <markup lang=\"bash\" >kubectl create ns coherence-test kubectl apply -f manifests/minimal-other-namespace-job.yaml -n coherence-test We should see the Job complete successfully. ",
            "title": "Deploy the Client to a Different Namespace"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " If the Extend client is going to run inside Kubernetes then we have a number of choices for configuration. In this section we are going to use the simplest way to configure Extend in Coherence, which is to use the Coherence NameService. In this configuration we do not need to specify any ports, the Extend proxy in the server cluster will bind to an ephemeral port. The Extend client will then use the Coherence NameService to find the addresses and ports that the Extend proxy is listening on. Proxy Server Configuration The default cache configuration file, built into coherence.jar configures an Extend proxy that binds to an ephemeral port. The proxy-scheme configuration looks like this: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;autostart system-property=\"coherence.proxy.enabled\"&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; That is all that is required in a cache configuration file to create a proxy service that will bind to an ephemeral port. The proxy is enabled by default, but could be disabled by setting the system property coherence.proxy.enabled to false. Deploy the Server To run the NameService examples below the server needs to be deployed. The example includes a manifests/ directory containing Kubernetes yaml files used by the example. For the NameService examples below the server will use the default cache configuration file from coherence.jar which has the Proxy service configured above. The yaml to deploy the server cluster is in the manifests/default-server.yaml file. <markup lang=\"yaml\" title=\"manifests/default-server.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: image: simple-coherence:1.0.0 replicas: 3 coherence: cacheConfig: coherence-cache-config.xml The yaml above will deploy a three member cluster configured to use the default coherence-cache-config.xml configuration file. There are no additional ports exposed in the configuration. The Extend proxy will be listening on an ephemeral port, so we have no idea what that port will be. We can deploy the server into the default namespace in kubernetes with the following command: <markup lang=\"bash\" >kubectl apply -f manifests/default-server.yaml We can list the resources created by the Operator. <markup lang=\"bash\" >kubectl get all Which should display something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/storage-0 1/1 Running 0 81s pod/storage-1 1/1 Running 0 81s pod/storage-2 1/1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/storage-sts ClusterIP None &lt;none&gt; 7/TCP 81s service/storage-wka ClusterIP None &lt;none&gt; 7/TCP 81s NAME READY AGE statefulset.apps/storage 3/3 81s We can see that the Operator has created a StatefulSet , with three Pods and there are two Services . The storage-sts service is the headless service required for the StatefulSet . The storage-wka service is the headless service that Coherence will use for well known address cluster discovery. Minimal Extend Client Configuration The configuration required for the Extend client is equally minimal. The example source code includes a configuration file named src/main/resources/minimal-client-cache-config.xml that can be used to connect to the proxy configured above. <markup lang=\"xml\" title=\"src/main/resources/minimal-client-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteService&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; There is a single cache-mapping that maps all cache names to the scheme named remote . The remote-scheme is named remote . The remote-scheme has a service name of RemoteService . The remote service will connect to a proxy service on the server that is named Proxy , this must correspond to the name of the proxy service in our server cache configuration file. Deploy the Client The simplest way to run the Extend client in Kubernetes is as a Job . The client just connects to a cache and does a put , then exits, so a Job is ideal for this type of process. The example contains yaml to create a Job manifests/minimal-job.yaml that looks like this: <markup lang=\"yaml\" title=\"manifests/minimal-job.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: extend-client spec: template: spec: restartPolicy: Never containers: - name: client image: simple-extend-client:1.0.0 env: - name: COHERENCE_CACHE_CONFIG value: minimal-client-cache-config.xml - name: COHERENCE_WKA value: storage-wka - name: COHERENCE_CLUSTER value: storage To be able to run the client we need to set in three pieces of information. The name of the cache configuration file. We set this using the COHERENCE_CACHE_CONFIG environment variable, and set the value to minimal-client-cache-config.xml , which is the configuration file we&#8217;re using in this example. The client needs to be able to discover the storage Pods to connect to. Just like the server cluster uses well known addresses to discover a cluster, the client can do the same. We set the COHERENCE_WKA environment variable to the name of the WKA service created for the server when we deployed it above, in this case it is storage-wka . Finally, we set the name of the Coherence cluster the client will connect to. When we deployed the server we did not specify a name, so the default cluster name will be the same as the Coherence resource name, in this case storage . So we set the COHERENCE_CLUSTER environment variable to storage . The client Job can be deployed into the default namespace in Kubernetes with the following command: <markup lang=\"bash\" >kubectl apply -f manifests/minimal-job.yaml The Jobs deployed can then be listed <markup lang=\"bash\" >kubectl get job Which should display something like this: <markup lang=\"bash\" >NAME COMPLETIONS DURATION AGE extend-client 1/1 4s 5s The Job above completed very quickly, which we would expect as it is just doing a trivial put to a cache. We can list the Pods created for the Job and then look at the log from the client. All Pods associated to a Job have a label in the form job-name: &lt;name-of-job&gt; , so in our case the label will be job-name: extend-client . We can use this with kubectl to list Pods associated to the Job . If the Job ran successfully there should be only one Pod . If the Job failed and has a restart policy, or was restarted by Kubernetes for other reasons there could be multiple Pods . In this case we expect a single successful Pod . <markup lang=\"bash\" >kubectl get pod -l job-name=extend-client <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE extend-client-k7wfq 0/1 Completed 0 4m24s If we look at the log for the Pod we should see the last line printed to System.out by the client: <markup lang=\"bash\" >kubectl logs extend-client-k7wfq The last line of the log will be something like this: <markup lang=\"bash\" >Put key=key-1 value=0.9332279895860512 previous=null The values will be different as we put different random values each time the client runs. The previous value was null in this case as we have not run any other client with this cluster. If we re-ran the client Job the previous value would be displayed as the cache on the server now has data in it. Clean-Up We have shown a simple Extend client running in Kubernetes, connecting to a Coherence cluster using the NameService. We can now delete the Job using kubectl . <markup lang=\"bash\" >kubectl delete job extend-client We can also delete the server. <markup lang=\"bash\" >kubectl delete -f manifests/default-server.yaml Deploy the Client to a Different Namespace In the first example we deployed the client to the same namespace as the server. If we wanted to deploy the client to a different namespace we would need to ensure the fully qualified name of the WKA service is used when setting the COHERENCE_WKA environment variable. The Coherence cluster is deployed into the default namespace so the fully qualified WKA service name is storage-wka.default.svc . <markup lang=\"yaml\" title=\"manifests/minimal-job.yaml\" >apiVersion: batch/v1 kind: Job metadata: name: extend-client spec: template: spec: restartPolicy: Never containers: - name: client image: simple-extend-client:1.0.0 env: - name: COHERENCE_CACHE_CONFIG value: minimal-client-cache-config.xml - name: COHERENCE_WKA value: storage-wka.default.svc - name: COHERENCE_CLUSTER value: storage We can deploy this client Job into a different namespace than the cluster is deployed into: <markup lang=\"bash\" >kubectl create ns coherence-test kubectl apply -f manifests/minimal-other-namespace-job.yaml -n coherence-test We should see the Job complete successfully. ",
            "title": "Extend Inside Kubernetes Using the Coherence NameService"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The Extend proxy service on the server must be configured to have a fixed port, so there is a little more configuration than previously. The example server image contains a Coherence configuration file named test-cache-config.xml , which contains an Extend proxy configured to bind to all host addresses ( 0.0.0.0 ) on port 20000. <markup lang=\"xml\" title=\"test-cache-config.xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;local-address&gt; &lt;!-- The proxy will listen on all local addresses --&gt; &lt;address&gt;0.0.0.0&lt;/address&gt; &lt;port&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; ",
            "title": "Proxy Server Configuration"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The example contains a yaml file that can be used to deploy a Coherence server with the fixed proxy address, as shown above. <markup lang=\"yaml\" title=\"manifests/fixed-port-server.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: image: simple-coherence:1.0.0 replicas: 3 coherence: cacheConfig: test-cache-config.xml ports: - name: extend port: 20000 The yaml above will deploy a three member cluster configured to use the default test-cache-config.xml configuration file and expose the Extend port via a service. The server can be deployed with the following command. <markup lang=\"bash\" >kubectl apply -f manifests/fixed-port-server.yaml The resources created by the Coherence Operator can be listed: <markup lang=\"bash\" >kubectl get all <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/storage-0 1/1 Running 0 61s pod/storage-1 1/1 Running 0 61s pod/storage-2 1/1 Running 0 61s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/storage-extend ClusterIP 10.101.99.24 &lt;none&gt; 20000/TCP 61s service/storage-sts ClusterIP None &lt;none&gt; 7/TCP 61s service/storage-wka ClusterIP None &lt;none&gt; 7/TCP 61s NAME READY AGE statefulset.apps/storage 3/3 61s As well as the Pods and Services created in the previous example, there is now a Service named storage-extend , which exposes the Extend port. ",
            "title": "Deploy the Server"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " An external client needs to be configured with a remote scheme that connects to a known address and port. The example contains a cache configuration file named src/main/resources/fixed-address-cache-config.xml that has this configuration. <markup lang=\"xml\" title=\"src/main/resources/fixed-address-cache-config.xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;!-- the 127.0.0.1 loop back address will only work in local dev testing --&gt; &lt;address system-property=\"coherence.extend.address\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When the client runs using the configuration above it will attempt to connect to an Extend proxy on 127.0.0.1:20000 . The address to connect to can be overridden by setting the coherence.extend.address system property. The port to connect to can be overridden by setting the coherence.extend.port system property. ",
            "title": "Configure the Extend Client"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " This example assumes that you are running Kubernetes on a development machine, for example with KinD , of Minikube or in Docker, etc. In this case the Service created is of type ClusterIP , so it is not actually exposed outside of Kubernetes as most development Kubernetes clusters do not support services of type LoadBalancer . This means that to test the external client we will need to use port forwarding. In a console start the port forwarder using kubectl as follows <markup lang=\"bash\" >kubectl port-forward svc/storage-extend 20000:20000 The example client can not connect to the Extend proxy via the host machine on port 20000 . The simplest way to run the Extend client locally is to use either Maven or Gradle. The Maven pom.xml file uses the Maven Exec plugin to run the client. The Gradle build.gradle file configures a run task to execute the client. With Maven: <markup lang=\"bash\" >./mvnw compile exec:java With Gradle: <markup lang=\"bash\" >./gradlew runClient Both of the above commands run successfully and the final line of output should be the line printed by the client showing the result of the put. <markup lang=\"bash\" >Put key=key-1 value=0.5274436018741687 previous=null Clean-up We can now delete the server. <markup lang=\"bash\" >kubectl delete -f manifests/fixed-port-server.yaml ",
            "title": "Run the Extend Client"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The NameService example above will only work if the client is running inside the same Kubernetes cluster as the server. When the client uses the Coherence NameService to look up the addresses of the Extend proxy service, the cluster only knows its internal IP addresses. If a client external to Kubernetes tried to use the NameService the addresses returned would be unreachable, as they are internal to the Kubernetes cluster. To connect external Extend clients, the proxy must be bound to known ports and those ports exposed to the client via some form of service or ingress. Proxy Server Configuration The Extend proxy service on the server must be configured to have a fixed port, so there is a little more configuration than previously. The example server image contains a Coherence configuration file named test-cache-config.xml , which contains an Extend proxy configured to bind to all host addresses ( 0.0.0.0 ) on port 20000. <markup lang=\"xml\" title=\"test-cache-config.xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;local-address&gt; &lt;!-- The proxy will listen on all local addresses --&gt; &lt;address&gt;0.0.0.0&lt;/address&gt; &lt;port&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; Deploy the Server The example contains a yaml file that can be used to deploy a Coherence server with the fixed proxy address, as shown above. <markup lang=\"yaml\" title=\"manifests/fixed-port-server.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: image: simple-coherence:1.0.0 replicas: 3 coherence: cacheConfig: test-cache-config.xml ports: - name: extend port: 20000 The yaml above will deploy a three member cluster configured to use the default test-cache-config.xml configuration file and expose the Extend port via a service. The server can be deployed with the following command. <markup lang=\"bash\" >kubectl apply -f manifests/fixed-port-server.yaml The resources created by the Coherence Operator can be listed: <markup lang=\"bash\" >kubectl get all <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE pod/storage-0 1/1 Running 0 61s pod/storage-1 1/1 Running 0 61s pod/storage-2 1/1 Running 0 61s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/storage-extend ClusterIP 10.101.99.24 &lt;none&gt; 20000/TCP 61s service/storage-sts ClusterIP None &lt;none&gt; 7/TCP 61s service/storage-wka ClusterIP None &lt;none&gt; 7/TCP 61s NAME READY AGE statefulset.apps/storage 3/3 61s As well as the Pods and Services created in the previous example, there is now a Service named storage-extend , which exposes the Extend port. Configure the Extend Client An external client needs to be configured with a remote scheme that connects to a known address and port. The example contains a cache configuration file named src/main/resources/fixed-address-cache-config.xml that has this configuration. <markup lang=\"xml\" title=\"src/main/resources/fixed-address-cache-config.xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;!-- the 127.0.0.1 loop back address will only work in local dev testing --&gt; &lt;address system-property=\"coherence.extend.address\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When the client runs using the configuration above it will attempt to connect to an Extend proxy on 127.0.0.1:20000 . The address to connect to can be overridden by setting the coherence.extend.address system property. The port to connect to can be overridden by setting the coherence.extend.port system property. Run the Extend Client This example assumes that you are running Kubernetes on a development machine, for example with KinD , of Minikube or in Docker, etc. In this case the Service created is of type ClusterIP , so it is not actually exposed outside of Kubernetes as most development Kubernetes clusters do not support services of type LoadBalancer . This means that to test the external client we will need to use port forwarding. In a console start the port forwarder using kubectl as follows <markup lang=\"bash\" >kubectl port-forward svc/storage-extend 20000:20000 The example client can not connect to the Extend proxy via the host machine on port 20000 . The simplest way to run the Extend client locally is to use either Maven or Gradle. The Maven pom.xml file uses the Maven Exec plugin to run the client. The Gradle build.gradle file configures a run task to execute the client. With Maven: <markup lang=\"bash\" >./mvnw compile exec:java With Gradle: <markup lang=\"bash\" >./gradlew runClient Both of the above commands run successfully and the final line of output should be the line printed by the client showing the result of the put. <markup lang=\"bash\" >Put key=key-1 value=0.5274436018741687 previous=null Clean-up We can now delete the server. <markup lang=\"bash\" >kubectl delete -f manifests/fixed-port-server.yaml ",
            "title": "Extend Clients External to Kubernetes"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The example server configuration used for connecting external clients can also be used for internal Extend clients, which is useful for use-cases where some clients are inside Kubernetes and some outside. An Extend client running inside Kubernetes then has the choice of using the NameService configuration from the first example, or using the fixed address and port configuration of the second example. If an internal Extend client is configured to use a fixed address then the host name of the proxy can be set to the service used to expose the server&#8217;s extend port. For example, if the client&#8217;s cache configuration file contains a remote scheme like the external example above: <markup lang=\"xml\" title=\"src/main/resources/fixed-address-cache-config.xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;!-- the 127.0.0.1 loopback address will only work in local dev testing --&gt; &lt;address system-property=\"coherence.extend.address\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; The client would be run with the coherence.extend.address system property, (or COHERENCE_EXTEND_ADDRESS environment variable) set to the fully qualified name of the Extend service, in the case of our example server running in the default namespace, this would be -Dcoherence.extend.address=storage-extend.default.svc ",
            "title": "Mixing Internal and External Extend Clients"
        },
        {
            "location": "/examples/025_extend_client/README",
            "text": " The example above used port-forward to connect the external Extend client to the cluster. This showed how to configure the client and server but is not how a real world application would work. In a real deployment the server would typically be deployed with the Extend service behind a load balancer or some other form of ingress, such as Istio. The Extend client would then be configured to connect to the external ingress address and port. Some ingress, such as Istio, can also be configured to add TLS security, which Extend will work with. Tip There is an open source project named MetalLB that can easily be deployed into development environment Kubernetes clusters and provides support for load balancer services. This is a simple way to test and try out load balancers in development Kubernetes. If MetalLB was installed (or your cluster supports LoadBalancer services) the yaml for deploying the cluster can be altered to make the Extend service a load balancer. <markup lang=\"yaml\" title=\"manifests/fixed-port-lb-server.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: image: simple-coherence:1.0.0 replicas: 3 coherence: cacheConfig: test-cache-config.xml ports: - name: extend port: 20000 service: type: LoadBalancer This can be deployed using: <markup lang=\"bash\" >kubectl apply -f manifests/fixed-port-lb-server.yaml Now if we look at the Extend service, we see it is a load balancer <markup lang=\"bash\" >kubectl get svc storage-extend <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE storage-extend LoadBalancer 10.110.84.229 127.0.0.240 20000:30710/TCP 2m20s Exactly how you connect to the MetalLB load balancer, and on which address, varies depending on where your Kubernetes cluster is running. ",
            "title": "External Client in the Real World"
        },
        {
            "location": "/examples/README",
            "text": " There are a number of examples which show you how to build and deploy applications for the Coherence Operator. Tip The complete source code for the examples is in the Coherence Operator GitHub repository. Simple Coherence Image using JIB Building a simple Coherence server image with JIB using Maven or Gradle. Simple Coherence Image using a Dockerfile Building a simple Coherence image with a Dockerfile, that works out of the box with the Operator. Hello World Deploying the most basic Coherence cluster using the Operator. Coherence*Extend Clients An example demonstrating various ways to configure and use Coherence*Extend with Kubernetes. Deployment This example shows how to deploy Coherence applications using the Coherence Operator. TLS Securing Coherence clusters using TLS. Network Policies An example covering the use of Kubernetes NetworkPolicy rules with the Operator and Coherence clusters. Federation This is a simple Coherence federation example. The federation feature requires Coherence Grid Edition. Autoscaling Scaling Coherence clusters using the horizontal Pod Autoscaler. Helm Manage Coherence resources using Helm. Istio Istio Support Coherence Demo App Deploying the Coherence demo application. ",
            "title": "Examples Overview"
        },
        {
            "location": "/docs/applications/030_deploy_application",
            "text": " To specify the image to use set the image field in the Coherence spec to the name of the image. For example if there was an application image called catalogue:1.0.0 it can be specified like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 The catalogue:1.0.0 will be used in the coherence container in the deployment&#8217;s Pods. The example above would assume that the catalogue:1.0.0 has a JVM on the PATH and all the required .jar files, or Java classes, in the default classpath locations used by the Operator. ",
            "title": "Specify the Image to Use"
        },
        {
            "location": "/docs/applications/030_deploy_application",
            "text": " If your image needs to be pulled from a private registry you may need to provide image pull secrets for this. For example, supposing the application image is repo.acme.com/catalogue:1.0.0 and that repo.acme.com is a private registry; we might a Secret to the k8s namespace named repo-acme-com-secrets . We can then specify that these secrets are used in the Coherence resource by setting the imagePullSecrets fields. The imagePullSecrets field is a list of secret names, the same format as that used when specifying secrets for a Pod spec. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: repo.acme.com/catalogue:1.0.0 imagePullSecrets: - name: repo-acme-com-secrets The repo.acme.com/catalogue:1.0.0 image will be used for the application image The Secret named repo-acme-com-secrets will be used to pull images. Multiple secrets can be specified in the case where different images used by different containers are pulled from different registries. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: repo.acme.com/catalogue:1.0.0 imagePullSecrets: - name: repo-acme-com-secrets - name: oracle-container-registry-secrets The example above has two image pull secrets, repo-acme-com-secrets and oracle-container-registry-secrets ",
            "title": "Image Pull Secrets"
        },
        {
            "location": "/docs/applications/030_deploy_application",
            "text": " Additional configuration can be added to specify other application settings, these include: setting the classpath specifying the application main specifying application arguments specifying the working directory ",
            "title": "More Application Configuration"
        },
        {
            "location": "/docs/applications/030_deploy_application",
            "text": " Once a custom application image has been built (as described in Build Custom Application Images ) a Coherence resource can be configured to use that image. Specify the Image to Use To specify the image to use set the image field in the Coherence spec to the name of the image. For example if there was an application image called catalogue:1.0.0 it can be specified like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 The catalogue:1.0.0 will be used in the coherence container in the deployment&#8217;s Pods. The example above would assume that the catalogue:1.0.0 has a JVM on the PATH and all the required .jar files, or Java classes, in the default classpath locations used by the Operator. Image Pull Secrets If your image needs to be pulled from a private registry you may need to provide image pull secrets for this. For example, supposing the application image is repo.acme.com/catalogue:1.0.0 and that repo.acme.com is a private registry; we might a Secret to the k8s namespace named repo-acme-com-secrets . We can then specify that these secrets are used in the Coherence resource by setting the imagePullSecrets fields. The imagePullSecrets field is a list of secret names, the same format as that used when specifying secrets for a Pod spec. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: repo.acme.com/catalogue:1.0.0 imagePullSecrets: - name: repo-acme-com-secrets The repo.acme.com/catalogue:1.0.0 image will be used for the application image The Secret named repo-acme-com-secrets will be used to pull images. Multiple secrets can be specified in the case where different images used by different containers are pulled from different registries. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: repo.acme.com/catalogue:1.0.0 imagePullSecrets: - name: repo-acme-com-secrets - name: oracle-container-registry-secrets The example above has two image pull secrets, repo-acme-com-secrets and oracle-container-registry-secrets More Application Configuration Additional configuration can be added to specify other application settings, these include: setting the classpath specifying the application main specifying application arguments specifying the working directory ",
            "title": "Deploy Coherence Application Images"
        },
        {
            "location": "/docs/jvm/030_jvm_args",
            "text": " The Coherence Operator will add the following JVM arguments by default: <markup >-Dcoherence.cluster=&lt;cluster-name&gt; -Dcoherence.role=&lt;role&gt; -Dcoherence.wka=&lt;deployment-name&gt;-wka.svc -Dcoherence.cacheconfig=coherence-cache-config.xml -Dcoherence.k8s.operator.health.port=6676 -Dcoherence.management.http.port=30000 -Dcoherence.metrics.http.port=9612 -Dcoherence.distributed.persistence-mode=on-demand -Dcoherence.override=k8s-coherence-override.xml -Dcoherence.ttl=0 -XX:+UseG1GC -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XshowSettings:all -XX:+UseContainerSupport -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -XX:HeapDumpPath=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/heap-dumps/&lt;member&gt;-&lt;pod-uid&gt;.hprof -XX:ErrorFile=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/hs-err-&lt;member&gt;-&lt;pod-uid&gt;.log -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics Some arguments and system properties above can be overridden or changed by setting values in the Coherence CDR spec. ",
            "title": "Default Arguments"
        },
        {
            "location": "/docs/jvm/030_jvm_args",
            "text": " The Coherence CRD allows any arbitrary JVM arguments to be passed to the JVM in the coherence container by using the jvm.args field of the CRD spec. Any valid system property or JVM argument can be added to the jvm.args list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: args: - \"-Dcoherence.pof.config=storage-pof-config.xml\" - \"-Dcoherence.tracing.ratio=0.1\" - \"-agentpath:/yourkit/bin/linux-x86-64/libyjpagent.so\" In this example the args list adds two System properties coherence.pof.config=storage-pof-config.xml and coherence.tracing.ratio=0.1 and also adds the YourKit profiling agent. When the Operator builds the command line to use when starting Coherence Pods, any arguments added to the jvm.args field will be added after all the arguments added by the Operator from other configuration fields. This means that arguments such as system properties added to jvm.args will override any added by the Operator. For example <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: cacheConfig: storage-config.xml jvm: args: - \"-Dcoherence.cache.config=test-config.xml\" Setting the coherence.cacheConfig field will make the operator add -Dcoherence.cache.config=storage-config.xml to the command line. Adding -Dcoherence.cache.config=test-config.xml to the jvm.args field will make the Operator add -Dcoherence.cache.config=test-config.xml to the end of the JVM arguments in the command line. When duplicate system properties are present on a command line, the last one wins so in the example above the cache configuration used would be test-config.xml . Default Arguments The Coherence Operator will add the following JVM arguments by default: <markup >-Dcoherence.cluster=&lt;cluster-name&gt; -Dcoherence.role=&lt;role&gt; -Dcoherence.wka=&lt;deployment-name&gt;-wka.svc -Dcoherence.cacheconfig=coherence-cache-config.xml -Dcoherence.k8s.operator.health.port=6676 -Dcoherence.management.http.port=30000 -Dcoherence.metrics.http.port=9612 -Dcoherence.distributed.persistence-mode=on-demand -Dcoherence.override=k8s-coherence-override.xml -Dcoherence.ttl=0 -XX:+UseG1GC -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XshowSettings:all -XX:+UseContainerSupport -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -XX:HeapDumpPath=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/heap-dumps/&lt;member&gt;-&lt;pod-uid&gt;.hprof -XX:ErrorFile=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/hs-err-&lt;member&gt;-&lt;pod-uid&gt;.log -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics Some arguments and system properties above can be overridden or changed by setting values in the Coherence CDR spec. ",
            "title": "Adding Arbitrary JVM Arguments"
        },
        {
            "location": "/docs/jvm/030_jvm_args",
            "text": " The Operator supports environment variable expansion in JVM arguments. The runner in the Coherence container will replace ${var} or $var in the JVM arguments with the corresponding environment variable name. For example a JVM argument of \"-Dmy.host.name=${HOSTNAME}\" when run on a Pod with a host name of COH-1 will resolve to \"-Dmy.host.name=COH-1\" . Any environment variable that is present when the Coherence container starts can be used, this would include variables created as part of the image and variables specified in the Coherence yaml. ",
            "title": "Environment Variable Expansion"
        },
        {
            "location": "/docs/coherence/060_log_level",
            "text": " Logging granularity in Coherence is controlled by a log level, that is a number between one and nine, where the higher the number the more debug logging is produced. The Coherence CRD has a field spec.coherence.logLevel that allows the log level to be configured by setting the coherence.log.level system property. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: logLevel: 9 The Coherence spec sets the log level to 9, effectively passing -Dcoherence.log.level=9 to the Coherence JVM&#8217;s command line. ",
            "title": "Set the Coherence Log Level"
        },
        {
            "location": "/docs/other/080_add_containers",
            "text": " To add a container to the Pods specify the container in the sideCars list in the Coherence CRD spec. See the Logging Documentation for a bigger example of adding a side-car container. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" An additional container named fluentd has been added to the CRD spec. The containers will added to the sideCars will be added to the Pods exactly as configured. Any configuration that is valid in a Kubernetes Container Spec may be added to an entry in sideCars ",
            "title": "Add a Container"
        },
        {
            "location": "/docs/other/080_add_containers",
            "text": " Just like normal containers above, additional init-containers can also be added to the Pods . To add an init-container to the Pods specify the container in the initContainers list in the Coherence CRD spec. As with containers, for init-containers any configuration that is valid in a Kubernetes Container Spec may be added to an entry in initContainers For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: initContainers: - name: setup image: \"app-setup:1.0.0\" An additional init-container named setup has been added to the CRD spec with an image named app-setup:1.0.0 . ",
            "title": "Add an Init-Container"
        },
        {
            "location": "/docs/other/080_add_containers",
            "text": " Additional containers and init-containers can easily be added to a Coherence resource Pod. There are two types of container that can be added, init-containers and normal containers. An example use case for this would be to add something like a Fluentd side-car container to ship logs to Elasticsearch. A note about Volumes: The Operator created a number of volumes and volume mounts by default. These default volume mounts will be added to all containers in the Pod including containers added as described here. Any additional volumes and volume mounts added to the Coherence resource spec will also be added all containers. Add a Container To add a container to the Pods specify the container in the sideCars list in the Coherence CRD spec. See the Logging Documentation for a bigger example of adding a side-car container. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" An additional container named fluentd has been added to the CRD spec. The containers will added to the sideCars will be added to the Pods exactly as configured. Any configuration that is valid in a Kubernetes Container Spec may be added to an entry in sideCars Add an Init-Container Just like normal containers above, additional init-containers can also be added to the Pods . To add an init-container to the Pods specify the container in the initContainers list in the Coherence CRD spec. As with containers, for init-containers any configuration that is valid in a Kubernetes Container Spec may be added to an entry in initContainers For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: initContainers: - name: setup image: \"app-setup:1.0.0\" An additional init-container named setup has been added to the CRD spec with an image named app-setup:1.0.0 . ",
            "title": "Configure Additional Containers"
        },
        {
            "location": "/docs/applications/010_overview",
            "text": " Build Custom Application Images Building custom Coherence application images for use with the Coherence Operator. Deploy Custom Application Images Deploying custom application images using the Coherence Operator. ",
            "title": "Building and Deploying Applications"
        },
        {
            "location": "/docs/applications/010_overview",
            "text": " There are many settings in a Coherence resource that control the behaviour of Coherence, the JVM and the application code. Some of the application specific settings are shown below: Setting the Classpath Setting a custom classpath for the application. Setting a Main Class Setting a custom main class to run. Setting Application Arguments Setting arguments to pass to the main class. Working Directory Setting the application&#8217;s working directory. ",
            "title": "Configuring Applications"
        },
        {
            "location": "/docs/applications/010_overview",
            "text": " A typical Coherence deployment contains custom application code that runs with Coherence. To run custom application code in a Coherence resource that code needs to be packaged into an image that the deployment will use. Building and Deploying Applications Build Custom Application Images Building custom Coherence application images for use with the Coherence Operator. Deploy Custom Application Images Deploying custom application images using the Coherence Operator. Configuring Applications There are many settings in a Coherence resource that control the behaviour of Coherence, the JVM and the application code. Some of the application specific settings are shown below: Setting the Classpath Setting a custom classpath for the application. Setting a Main Class Setting a custom main class to run. Setting Application Arguments Setting arguments to pass to the main class. Working Directory Setting the application&#8217;s working directory. ",
            "title": "Overview"
        },
        {
            "location": "/docs/other/050_configmap_volumes",
            "text": " Additional Volumes and VolumeMounts from ConfigMaps can easily be added to a Coherence resource. ",
            "title": "preambule"
        },
        {
            "location": "/docs/other/050_configmap_volumes",
            "text": " To add a ConfigMap as an additional volume to the Pods of a Coherence deployment add entries to the configMapVolumes list in the CRD spec. Each entry in the list has a mandatory name and mountPath field, all other fields are optional. The name field is the name of the ConfigMap to mount and is also used as the volume name. The mountPath field is the path in the container to mount the volume to. Additional volumes added in this way (either ConfigMaps shown here, or Secrets or plain Volumes ) will be added to all containers in the Pod . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: configMapVolumes: - name: storage-config mountPath: /home/coherence/config The ConfigMap named storage-config will be mounted to the Pod as an additional Volume named storage-config The ConfigMap will be mounted at /home/coherence/config in the containers. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config configMap: name: storage-config As already stated, if the Coherence resource has additional containers the ConfigMap will be mounted in all of them. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: sideCars: - name: fluentd image: \"fluent/fluentd:v1.3.3\" configMapVolumes: - name: storage-config mountPath: /home/coherence/config In this example the storage-config ConfigMap will be mounted as a Volume and mounted to both the coherence container and the fluentd container. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: storage-config mountPath: /home/coherence/config - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3\" volumeMounts: - name: storage-config mountPath: /home/coherence/config volumes: - name: storage-config configMap: name: storage-config ",
            "title": "Add ConfigMap Volumes"
        },
        {
            "location": "/docs/scaling/010_overview",
            "text": " The Coherence Operator provides the ability to safely scale up and down a Coherence deployment. A Coherence deployment is backed by a StatefulSet , which can easily be scaled using existing Kubernetes features. The problem with directly scaling down the StatefulSet is that Kubernetes will immediately kill the required number of Pods . This is obviously very bad for Coherence as killing multiple storage enabled members would almost certainly cause data loss. The Coherence Operator supports scaling by applying the scaling update directly to Coherence deployment rather than to the underlying StatefulSet . There are two methods to scale a Coherence deployment: Update the replicas field in the Coherence CRD spec. Use the kubectl scale command When either of these methods is used the Operator will detect that a change to the size of the deployment is required and ensure that the change will be applied safely. The logical steps the Operator will perform are: Detect desired replicas is different to current replicas Check the cluster is StatusHA - i.e. no cache services are endangered. If any service is not StatusHA requeue the scale request (go back to step one). If scaling up, add the required number of members. If scaling down, scale down by one member and requeue the request (go back to step one). What these steps ensure is that the deployment will not be resized unless the cluster is in a safe state. When scaling down only a single member will be removed at a time, ensuring that the cluster is in a safe state before removing the next member. The Operator will only apply safe scaling functionality to deployments that are storage enabled. If a deployment is storage disabled then it can be scaled up or down by the required number of members in one step as there is no fear of data loss in a storage disabled member. ",
            "title": "Scale Coherence Deployments"
        },
        {
            "location": "/docs/scaling/010_overview",
            "text": " The Coherence CRD spec has a field scaling.policy that can be used to override the default scaling behaviour. The scaling policy has three possible values: Value Description ParallelUpSafeDown This is the default scaling policy. With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ) and when scaling down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the cluster have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when adding members, but offers safe, albeit slower, scaling down as Pods are removed one by one. Parallel With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ). With this policy no StatusHA check is performed either when scaling up or when scaling down. This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for deployments that are storage disabled. Safe With this policy when scaling up and down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the deployment have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy is slower to start, scale up and scale down. Both the ParallelUpSafeDown and Safe policies will ensure no data loss when scaling a deployment. The policy can be set as shown below: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: policy: Safe This deployment will scale both up and down with StatusHA checks. ",
            "title": "Scaling Policy"
        },
        {
            "location": "/docs/scaling/010_overview",
            "text": " An HTTP get probe works the same way as a Kubernetes liveness http request The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: httpGet: port: 8080 path: /statusha This deployment will check the status of the services by performing a http GET on http://&lt;pod-ip&gt;:8080/statusha . If the response is 200 the check will pass, any other response the check is assumed to be false. ",
            "title": "Using a HTTP Get Probe"
        },
        {
            "location": "/docs/scaling/010_overview",
            "text": " A TCP probe works the same way as a Kubernetes TCP liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: tcpSocket: port: 7000 This deployment will check the status of the services by connecting to the socket on port 7000 . ",
            "title": "Using a TCP Probe"
        },
        {
            "location": "/docs/scaling/010_overview",
            "text": " An exec probe works the same way as a Kubernetes Exec liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: exec: command: - /bin/ah - safe.sh This deployment will check the status of the services by running the sh safe.sh command in the Pod . ",
            "title": "Using an Exec Command Probe"
        },
        {
            "location": "/docs/scaling/010_overview",
            "text": " The StatusHA check performed by the Operator uses a http endpoint that the Operator runs on a well-known port in the Coherence JVM. This endpoint performs a simple check to verify that none of the partitioned cache services known about by Coherence have an endangered status. If an application has a different concept of what \"safe\" means it can implement a different method to check the status during scaling. The operator supports different types of safety check probes, these are exactly the same as those supported by Kubernetes for readiness and liveness probes. The scaling.probe section of the Coherence CRD allows different types of probe to be configured. Using a HTTP Get Probe An HTTP get probe works the same way as a Kubernetes liveness http request The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: httpGet: port: 8080 path: /statusha This deployment will check the status of the services by performing a http GET on http://&lt;pod-ip&gt;:8080/statusha . If the response is 200 the check will pass, any other response the check is assumed to be false. Using a TCP Probe A TCP probe works the same way as a Kubernetes TCP liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: tcpSocket: port: 7000 This deployment will check the status of the services by connecting to the socket on port 7000 . Using an Exec Command Probe An exec probe works the same way as a Kubernetes Exec liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: exec: command: - /bin/ah - safe.sh This deployment will check the status of the services by running the sh safe.sh command in the Pod . ",
            "title": "Scaling StatusHA Probe"
        },
        {
            "location": "/docs/scaling/010_overview",
            "text": " The Coherence CRD has a number of fields that control the behaviour of scaling. Scaling Policy The Coherence CRD spec has a field scaling.policy that can be used to override the default scaling behaviour. The scaling policy has three possible values: Value Description ParallelUpSafeDown This is the default scaling policy. With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ) and when scaling down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the cluster have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when adding members, but offers safe, albeit slower, scaling down as Pods are removed one by one. Parallel With this policy when scaling up Pods are added in parallel (the same as using the Parallel podManagementPolicy in a StatefulSet ). With this policy no StatusHA check is performed either when scaling up or when scaling down. This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for deployments that are storage disabled. Safe With this policy when scaling up and down Pods are removed one at a time (the same as the OrderedReady podManagementPolicy for a StatefulSet). When scaling down a check is done to ensure that the members of the deployment have a safe StatusHA value before a Pod is removed (i.e. none of the Coherence cache services have an endangered status). This policy is slower to start, scale up and scale down. Both the ParallelUpSafeDown and Safe policies will ensure no data loss when scaling a deployment. The policy can be set as shown below: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: policy: Safe This deployment will scale both up and down with StatusHA checks. Scaling StatusHA Probe The StatusHA check performed by the Operator uses a http endpoint that the Operator runs on a well-known port in the Coherence JVM. This endpoint performs a simple check to verify that none of the partitioned cache services known about by Coherence have an endangered status. If an application has a different concept of what \"safe\" means it can implement a different method to check the status during scaling. The operator supports different types of safety check probes, these are exactly the same as those supported by Kubernetes for readiness and liveness probes. The scaling.probe section of the Coherence CRD allows different types of probe to be configured. Using a HTTP Get Probe An HTTP get probe works the same way as a Kubernetes liveness http request The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: httpGet: port: 8080 path: /statusha This deployment will check the status of the services by performing a http GET on http://&lt;pod-ip&gt;:8080/statusha . If the response is 200 the check will pass, any other response the check is assumed to be false. Using a TCP Probe A TCP probe works the same way as a Kubernetes TCP liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: tcpSocket: port: 7000 This deployment will check the status of the services by connecting to the socket on port 7000 . Using an Exec Command Probe An exec probe works the same way as a Kubernetes Exec liveness probe The probe can be configured as follows <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: scaling: probe: exec: command: - /bin/ah - safe.sh This deployment will check the status of the services by running the sh safe.sh command in the Pod . ",
            "title": "Controlling Safe Scaling"
        },
        {
            "location": "/docs/coherence/010_overview",
            "text": " The Coherence resource has a number of fields to configure the behaviour of Coherence , these fields are in the spec.coherence section of the CRD. ",
            "title": "preambule"
        },
        {
            "location": "/docs/coherence/010_overview",
            "text": " The Coherence CRD has specific fields to configure the most common Coherence settings. Any other settings can be configured by adding system properties to the JVM Settings . The following Coherence features can be directly specified in the Coherence spec. Cluster Name Cache Configuration File Operational Configuration File (aka, the override file) Storage Enabled or disabled deployments Log Level Well Known Addressing and cluster discovery Persistence Management over REST Metrics The Coherence settings in the Coherence CRD spec typically set system property values that will be passed through to the Coherence JVM command line, which in turn configure Coherence. This is the same behaviour that would occur when running Coherence outside of containers. Whether these system properties actually apply or not depends on the application code. For example, it is simple to override the Coherence operational configuration file in a jar file deployed as part of an application&#8217;s image in such a way that will cause all the normal Coherence system properties to be ignored. If that is done then the Coherence settings discussed in this documentation will not apply. For example, adding a tangosol-coherence-override.xml file to a jar on the application&#8217;s classpath that contains an overridden &lt;configurable-cache-factory-config&gt; section with a hard coded cache configuration file name would mean that the Coherence CRD spec.coherence.cacheConfig field, that sets the coherence.cacheconfig system property, would be ignored. It is, therefore, entirely at the application developer&#8217;s discretion whether they use the fields of the Coherence CRD to configure Coherence, or they put those settings into configuration files, either hard coded into jar files or picked up at runtime from files mapped from Kubernetes volumes, config maps, secrets, etc. ",
            "title": "Configuring Coherence"
        },
        {
            "location": "/docs/jvm/090_container_limits",
            "text": " The JVM can be configured to respect container limits set, for example cpu and memory limits. This can be important if container limits have been set for the container in the resources section as a JVM that does not respect these limits can cause the Pod to be killed. This is done by adding the -XX:+UseContainerSupport JVM option. It is possible to control this using the jvm.useContainerLimits field in the Coherence CRD spec. If the field is not set, the operator adds the -XX:+UseContainerSupport option by default. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: useContainerLimits: false The useContainerLimits field is set to false, so the -XX:+UseContainerSupport will not be passed to the JVM. See the Resource Limits documentation on how to specify resource limits for the Coherence container. ",
            "title": "Respect Container Resource Limits"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " This example shows how to use the Kubernetes Horizontal Pod Autoscaler to scale Coherence clusters. Tip The complete source code for this example is in the Coherence Operator GitHub repository. ",
            "title": "Kubernetes Horizontal Pod autoscaler Example"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " There is a lot of good documentation on the HPA, particularly the Kubernetes documentation . The HPA uses metrics, which it obtains from one of the Kubernetes metrics APIs. Many cloud providers and custom Kubernetes installations have metrics features that may be able to expose those metrics to the custom/metrics.k8s.io API. It is possible to even do everything yourself and build a custom REST endpoint that serves custom metrics to the HPA. Those alternatives are beyond the scope of this example though so to keep things simple we will use Prometheus. The diagram below shows, at a high level, how this works. Prometheus will obtain metrics from the Coherence Pod&#8217;s metrics endpoints. The Prometheus Adapter exposes certain configured metrics polled from Prometheus as custom Kubernetes metrics. The HPA is configured to poll the custom metrics and use those to scale the Coherence resource (which will in turn cause the Coherence Operator to scale the StatefulSet ). ",
            "title": "How Does the Horizontal Pod autoscaler Work"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " This example will show how to configure the HPA to scale Coherence clusters based on heap usage metrics. As Coherence stores data in memory, monitoring heap usage and using it to scale seems a sensible approach. The Coherence CRD supports the scale sub-resource, which means that the Kubernetes HPA can be used to scale a Coherence deployment. In this example we are going to use heap usage as the metric - or to be more specific the amount of heap in use after the last garbage collection. This is an important point, plain heap usage is a poor metric to use for scaling decisions because the heap may be very full at a given point in time, but most of that memory may be garbage so scaling on the plain heap usage figure may cause the cluster to scale up needlessly as a milli-second later a GC could run, and the heap use shrinks down to acceptable levels. The problem is that there is no single metric in a JVM that gives heap usage after garbage collection. Coherence has some metrics that report this value, but they are taken from the MemoryPool MBeans and this is not reliable for scaling. For example, if the JVM is using the G1 collector the G1 Old Gen memory pool value for heap use after garbage collection will be zero unless a full GC has run. It is quite possible to almost fill the heap without running a full GC so this figure could remain zero or be wildly inaccurate. A more reliable way to work out the heap usage is to obtain the values for the different heap memory pools from the Garbage Collector MBeans. There could be multiple of these MBeans with different names depending on which collector has been configured for the JVM. The Garbage Collector Mbeans have a LastGCcInfo attribute, which is a composite attribute containing information about the last garbage collection that ran on this collector. One of the attributes is the endTime , which we can use to determine which collector&#8217;s LastGCcInfo is the most recent. Once we have this we can obtain the memoryUsageAfterGc attribute for the last gc, which is a map of memory pool name to heap use data after the GC. We can use this to then sum up the usages for the different heap memory pools. The Java code in this example contains a simple MBean class HeapUsage and corresponding MBean interface HeapUsageMBean that obtain heap use metrics in the way detailed above. There is also a configuration file custom-mbeans.xml that Coherence will use to automatically add the custom MBean to Coherence management and metrics. There is Coherence documentation on how to add custom metrics and how to register custom MBeans . The custom heap use MBean will be added with an ObjectName of Coherence:type=HeapUsage,nodeId=1 where nodeId will change to match the Coherence member id for the specific JVM. There will be one heap usage MBean for each cluster member. The Coherence metrics framework will expose the custom metrics with metric names made up from the MBean domain name, type, and the attribute name. The MBean has attribute names Used and PercentageUsed , so the metric names will be: Coherence.HeapUsage.Used Coherence.HeapUsage.PercentageUsed These metrics will be scoped as application metrics, as opposed to Coherence standard metrics that are vendor scoped. This means that in Prometheus the names will be converted to: application:coherence_heap_usage_used application:coherence_heap_usage_percentage_used The metrics will have corresponding tags to identify which cluster member ( Pod ) they relate to. ",
            "title": "Autoscaling Coherence Clusters"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " To build the examples, you first need to clone the Operator GitHub repository to your development machine. <markup lang=\"bash\" >git clone https://github.com/oracle/coherence-operator cd coherence-operator/examples ",
            "title": "Clone the Coherence Operator Repository:"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " If building inside a corporate proxy (or any machine that requires http and https proxies to be configured) then the build will require the MAVEN_OPTS environment variable to be properly set, for example: <markup lang=\"bash\" >export MAVEN_OPTS=\"-Dhttps.proxyHost=host -Dhttps.proxyPort=80 -Dhttp.proxyHost=host -Dhttp.proxyPort=80\" replacing host with the required proxy hostname and 80 with the proxy&#8217;s port. ",
            "title": "Corporate Proxies"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " Java 11+ JDK either OpenJDK or Oracle JDK Docker version 17.03+. kubectl version v1.13.0+ . Access to a Kubernetes v1.14.0+ cluster. Helm version 3.2.4+ Building the project requires Maven version 3.6.0+. The commands below use the Maven Wrapper to run the commands, which will install Maven if it is not already on the development machine. If you already have a suitable version of Maven installed feel free to replace the use of ./mvnw in the examples with your normal Maven command (typically just mvn ). Corporate Proxies If building inside a corporate proxy (or any machine that requires http and https proxies to be configured) then the build will require the MAVEN_OPTS environment variable to be properly set, for example: <markup lang=\"bash\" >export MAVEN_OPTS=\"-Dhttps.proxyHost=host -Dhttps.proxyPort=80 -Dhttp.proxyHost=host -Dhttp.proxyPort=80\" replacing host with the required proxy hostname and 80 with the proxy&#8217;s port. ",
            "title": "Prerequisites"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " The autoscaler example uses the JIB Maven plugin to build the example image. To build the image run the following command from the examples/autoscaler directory: <markup lang=\"bash\" >./mvnw package jib:dockerBuild The build will produce various example images, for the autoscaler example we will be using the autoscaler-example:latest image. ",
            "title": "Build Instructions"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " Prerequisites Java 11+ JDK either OpenJDK or Oracle JDK Docker version 17.03+. kubectl version v1.13.0+ . Access to a Kubernetes v1.14.0+ cluster. Helm version 3.2.4+ Building the project requires Maven version 3.6.0+. The commands below use the Maven Wrapper to run the commands, which will install Maven if it is not already on the development machine. If you already have a suitable version of Maven installed feel free to replace the use of ./mvnw in the examples with your normal Maven command (typically just mvn ). Corporate Proxies If building inside a corporate proxy (or any machine that requires http and https proxies to be configured) then the build will require the MAVEN_OPTS environment variable to be properly set, for example: <markup lang=\"bash\" >export MAVEN_OPTS=\"-Dhttps.proxyHost=host -Dhttps.proxyPort=80 -Dhttp.proxyHost=host -Dhttp.proxyPort=80\" replacing host with the required proxy hostname and 80 with the proxy&#8217;s port. Build Instructions The autoscaler example uses the JIB Maven plugin to build the example image. To build the image run the following command from the examples/autoscaler directory: <markup lang=\"bash\" >./mvnw package jib:dockerBuild The build will produce various example images, for the autoscaler example we will be using the autoscaler-example:latest image. ",
            "title": "Build the Examples"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " Clone the Coherence Operator Repository: To build the examples, you first need to clone the Operator GitHub repository to your development machine. <markup lang=\"bash\" >git clone https://github.com/oracle/coherence-operator cd coherence-operator/examples Build the Examples Prerequisites Java 11+ JDK either OpenJDK or Oracle JDK Docker version 17.03+. kubectl version v1.13.0+ . Access to a Kubernetes v1.14.0+ cluster. Helm version 3.2.4+ Building the project requires Maven version 3.6.0+. The commands below use the Maven Wrapper to run the commands, which will install Maven if it is not already on the development machine. If you already have a suitable version of Maven installed feel free to replace the use of ./mvnw in the examples with your normal Maven command (typically just mvn ). Corporate Proxies If building inside a corporate proxy (or any machine that requires http and https proxies to be configured) then the build will require the MAVEN_OPTS environment variable to be properly set, for example: <markup lang=\"bash\" >export MAVEN_OPTS=\"-Dhttps.proxyHost=host -Dhttps.proxyPort=80 -Dhttp.proxyHost=host -Dhttp.proxyPort=80\" replacing host with the required proxy hostname and 80 with the proxy&#8217;s port. Build Instructions The autoscaler example uses the JIB Maven plugin to build the example image. To build the image run the following command from the examples/autoscaler directory: <markup lang=\"bash\" >./mvnw package jib:dockerBuild The build will produce various example images, for the autoscaler example we will be using the autoscaler-example:latest image. ",
            "title": "Building the Example"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " First install the Coherence Operator, TBD&#8230;&#8203; ",
            "title": "Install the Coherence Operator"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " The Metrics endpoint will be exposed on port 9612 on each Pod , so it is possible to query the metrics endpoints for the custom heap metrics. The simplest way to test the metrics is to use the kubectl port-forward command and curl . In one terminal session start the port forwarder to the first Pod , test-cluster-0 : <markup lang=\"bash\" >kubectl port-forward pod/test-cluster-0 9612:9612 metrics from Pod , test-cluster-0 can be queried on http://127.0.0.1:9612/metrics In a second terminal we can use curl to query the metrics. The Coherence metrics endpoint serves metrics in two formats, plain text compatible with Prometheus and JSON. If the required content type has not been specified in the curl command it could be either that is returned. To specify a content type set the accepted type in the header, for example --header \"Accept: text/plain\" or --header \"Accept: application/json\" . This command will retrieve metrics from test-cluster-0 in the same format that Prometheus would. <markup lang=\"bash\" >curl -s --header \"Accept: text/plain\" -X GET http://127.0.0.1:9612/metrics This will return quite a lot of metrics, somewhere in that output is the custom application metrics for heap usage. The simplest way to isolate them would be to use grep , for example: <markup lang=\"bash\" >curl -s --header \"Accept: text/plain\" -X GET http://127.0.0.1:9612/metrics | grep application which should show something like: <markup lang=\"bash\" >application:coherence_heap_usage_percentage_used{cluster=\"test-cluster\", machine=\"docker-desktop\", member=\"test-cluster-0\", node_id=\"2\", role=\"test-cluster\", site=\"test-cluster-sts.operator-test.svc\"} 3.09 application:coherence_heap_usage_used{cluster=\"test-cluster\", machine=\"docker-desktop\", member=\"test-cluster-0\", node_id=\"2\", role=\"test-cluster\", site=\"test-cluster-sts.operator-test.svc\"} 16177976 The first metric application:coherence_heap_usage_percentage_used shows the heap was 3.09% full after the last gc. The second metric application:coherence_heap_usage_used shows that the in-use heap after the last gc was 16177976 bytes, or around 16 MB. The port forwarder can be changed to connect to the second Pod test-cluster-1 , and the same curl command will retrieve metrics from the second Pod , which should show different heap use values. ",
            "title": "Test the Custom Heap Metrics"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " With the Coherence Operator running we can now install a simple Coherence cluster. An example of the yaml required is below: <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: image: autoscaler-example:latest imagePullPolicy: IfNotPresent replicas: 2 coherence: metrics: enabled: true jvm: memory: heapSize: 500m ports: - name: metrics serviceMonitor: enabled: true - name: extend port: 20000 The image used for the application will be the autoscaler-example:latest image we built above. The deployment will initially have 2 replicas. Coherence metrics must be enabled to publish the metrics we require for scaling. In this example the JVM heap has been fixed to 500m , which is quite small but this means we do not need to add a lot of data to cause excessive heap usage when we run the example. The metrics port must also be exposed on a Service . A Prometheus ServiceMonitor must also be enabled for the metrics service so that Prometheus can find the Coherence Pods and poll metrics from them. This example also exposes a Coherence Extend port so that test data can easily be loaded into the caches. The autoscaler example includes a suitable yaml file named cluster.yaml in the manifests/ directory that can be used to create a Coherence deployment. <markup lang=\"bash\" >kubectl create -f manifests/cluster.yaml The Pods that are part of the Coherence cluster can be listed with kubectl . All the Pods have a label coherenceCluster set by the Coherence Operator to match the name of the Coherence resource that they belong to, which makes it easier to list Pods for a specific deployment using kubectl : <markup lang=\"bash\" >kubectl get pod -l coherenceCluster=test-cluster In a short time the Pods should both be ready. <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE test-cluster-0 1/1 Running 0 2m52s test-cluster-1 1/1 Running 0 2m52s Test the Custom Heap Metrics The Metrics endpoint will be exposed on port 9612 on each Pod , so it is possible to query the metrics endpoints for the custom heap metrics. The simplest way to test the metrics is to use the kubectl port-forward command and curl . In one terminal session start the port forwarder to the first Pod , test-cluster-0 : <markup lang=\"bash\" >kubectl port-forward pod/test-cluster-0 9612:9612 metrics from Pod , test-cluster-0 can be queried on http://127.0.0.1:9612/metrics In a second terminal we can use curl to query the metrics. The Coherence metrics endpoint serves metrics in two formats, plain text compatible with Prometheus and JSON. If the required content type has not been specified in the curl command it could be either that is returned. To specify a content type set the accepted type in the header, for example --header \"Accept: text/plain\" or --header \"Accept: application/json\" . This command will retrieve metrics from test-cluster-0 in the same format that Prometheus would. <markup lang=\"bash\" >curl -s --header \"Accept: text/plain\" -X GET http://127.0.0.1:9612/metrics This will return quite a lot of metrics, somewhere in that output is the custom application metrics for heap usage. The simplest way to isolate them would be to use grep , for example: <markup lang=\"bash\" >curl -s --header \"Accept: text/plain\" -X GET http://127.0.0.1:9612/metrics | grep application which should show something like: <markup lang=\"bash\" >application:coherence_heap_usage_percentage_used{cluster=\"test-cluster\", machine=\"docker-desktop\", member=\"test-cluster-0\", node_id=\"2\", role=\"test-cluster\", site=\"test-cluster-sts.operator-test.svc\"} 3.09 application:coherence_heap_usage_used{cluster=\"test-cluster\", machine=\"docker-desktop\", member=\"test-cluster-0\", node_id=\"2\", role=\"test-cluster\", site=\"test-cluster-sts.operator-test.svc\"} 16177976 The first metric application:coherence_heap_usage_percentage_used shows the heap was 3.09% full after the last gc. The second metric application:coherence_heap_usage_used shows that the in-use heap after the last gc was 16177976 bytes, or around 16 MB. The port forwarder can be changed to connect to the second Pod test-cluster-1 , and the same curl command will retrieve metrics from the second Pod , which should show different heap use values. ",
            "title": "Install Coherence cluster"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " Make sure the stable helm repository has been added to Helm if it isn&#8217;t already present in your local Helm repositories. <markup lang=\"bash\" >helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Make sure the local Helm repository is up to date. <markup lang=\"bash\" >helm repo update ",
            "title": "Setup the Helm Repo"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " If you are using a Kubernetes cluster with RBAC enabled then the rules required by Prometheus need to be added. The autoscale example contains a yaml file with the required RBAC rules in it in the manifests/ directory. The manifests/prometheus-rbac.yaml uses a namespace coherence-example which may need to be changed if you are installing into a different namespace. The following commands use sed to replace coherence-example with default and pipe the result to kubectl to create the RBAC rules in the default Kubernetes namespace. <markup lang=\"bash\" >sed \"s/coherence-example/default/g\" manifests/prometheus-rbac.yaml | kubectl create -f - ",
            "title": "Configure Prometheus RBAC"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " The Prometheus Operator can now be installed using Helm. The autoscaler example contains a simple values files that can be used when installing the chart in the manifests/ directory. <markup lang=\"bash\" >helm install --atomic --version 8.13.9 --wait \\ --set prometheus.service.type=NodePort \\ --values manifests/prometheus-values.yaml prometheus stable/prometheus-operator The --wait parameter makes Helm block until all the installed resources are ready. The command above sets the prometheus.service.type value to NodePort so that the Prometheus UI will be exposed on a port on the Kubernetes node. This is particularly useful when testing with a local Kubernetes cluster, such as in Docker on a laptop because the UI can be reached on localhost at that port. The default node port is 30090 , this can be changed by setting a different port, e.g: --set prometheus.service.nodePort=9090 . Assuming the default port of 30090 is used the UI can be reached on http://127.0.0.1:30090 . After Prometheus has started up and is scraping metrics we should be able to see our custom metrics in the UI. Type the metric name application:coherence_heap_usage_percentage_used in the expression box and click Execute and Prometheus should show two values for the metric, one for each Pod . Prometheus is scraping many more Coherence metrics that can also be queried in the UI. ",
            "title": "Install the Prometheus Operator"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " The simplest way to install Prometheus as part of an example or demo is to use the Prometheus Operator , which can be installed using a Helm chart. Setup the Helm Repo Make sure the stable helm repository has been added to Helm if it isn&#8217;t already present in your local Helm repositories. <markup lang=\"bash\" >helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Make sure the local Helm repository is up to date. <markup lang=\"bash\" >helm repo update Configure Prometheus RBAC If you are using a Kubernetes cluster with RBAC enabled then the rules required by Prometheus need to be added. The autoscale example contains a yaml file with the required RBAC rules in it in the manifests/ directory. The manifests/prometheus-rbac.yaml uses a namespace coherence-example which may need to be changed if you are installing into a different namespace. The following commands use sed to replace coherence-example with default and pipe the result to kubectl to create the RBAC rules in the default Kubernetes namespace. <markup lang=\"bash\" >sed \"s/coherence-example/default/g\" manifests/prometheus-rbac.yaml | kubectl create -f - Install the Prometheus Operator The Prometheus Operator can now be installed using Helm. The autoscaler example contains a simple values files that can be used when installing the chart in the manifests/ directory. <markup lang=\"bash\" >helm install --atomic --version 8.13.9 --wait \\ --set prometheus.service.type=NodePort \\ --values manifests/prometheus-values.yaml prometheus stable/prometheus-operator The --wait parameter makes Helm block until all the installed resources are ready. The command above sets the prometheus.service.type value to NodePort so that the Prometheus UI will be exposed on a port on the Kubernetes node. This is particularly useful when testing with a local Kubernetes cluster, such as in Docker on a laptop because the UI can be reached on localhost at that port. The default node port is 30090 , this can be changed by setting a different port, e.g: --set prometheus.service.nodePort=9090 . Assuming the default port of 30090 is used the UI can be reached on http://127.0.0.1:30090 . After Prometheus has started up and is scraping metrics we should be able to see our custom metrics in the UI. Type the metric name application:coherence_heap_usage_percentage_used in the expression box and click Execute and Prometheus should show two values for the metric, one for each Pod . Prometheus is scraping many more Coherence metrics that can also be queried in the UI. ",
            "title": "Install Prometheus"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " Now the Prometheus adapter is running we can query metrics from the custom/metrics.k8s.io API using kubectl raw API access. This is the same API that the HPA will use to obtain metrics. If a Coherence cluster had been installed into the default namespace, then metrics could be fetched for all Pods in that specific namespace, for example to obtain the heap_memory_usage_after_gc_pct metric: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct The * after pods/ tells the adapter to fetch metrics for all Pods in the namespace. To fetch the metric for pods in another namespace change the default part of the URL to the namespace name. If you have the jq utility installed that formats json then piping the output to jq will make it prettier. <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct | jq We could fetch a metric for a specific Pod in the default namespace, for example a Pod named test-cluster-1 as follows: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/test-cluster-1/heap_memory_usage_after_gc_pct which might display something like: <markup lang=\"json\" >{ \"kind\": \"MetricValueList\", \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\", \"metadata\": { \"selfLink\": \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/coherence-test/pods/test-cluster-1/heap_memory_usage_after_gc_pct\" }, \"items\": [ { \"describedObject\": { \"kind\": \"Pod\", \"namespace\": \"operator-test\", \"name\": \"test-cluster-1\", \"apiVersion\": \"/v1\" }, \"metricName\": \"heap_memory_usage_after_gc_pct\", \"timestamp\": \"2020-09-02T12:12:01Z\", \"value\": \"1300m\", \"selector\": null } ] } The format of the value field above might look a little strange. This is because it is a Kubernetes Quantity format, in this case it is 1300m where the m stand for millis. So in this case 1300 millis is 1.3% heap usage. This is to get around the poor support in yaml and json for accurate floating-point numbers. In our case for auto-scaling we are interested in the maximum heap for a specific Coherence resource. Remember in the Prometheus Adapter configuration we configured the role metric tag to map to coherence.coherence.oracle.com resources. We also configured a query that will give back the maximum heap usage value for a query. The example yaml used to deploy the Coherence resource above will create a resource named test-cluster . If we installed this into the default Kubernetes namespace then we can fetch the maximum heap use after gc for the Pods in that Coherence deployment as follows: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct which might display something like: <markup lang=\"json\" >{ \"kind\": \"MetricValueList\", \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\", \"metadata\": { \"selfLink\": \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/operator-test/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct\" }, \"items\": [ { \"describedObject\": { \"kind\": \"Coherence\", \"namespace\": \"operator-test\", \"name\": \"test-cluster\", \"apiVersion\": \"coherence.oracle.com/v1\" }, \"metricName\": \"heap_memory_usage_after_gc_max_pct\", \"timestamp\": \"2020-09-02T12:21:02Z\", \"value\": \"3300m\", \"selector\": null } ] } ",
            "title": "Query Custom Metrics"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " The next step in the example is to install the Prometheus Adapter. This is a custom metrics server that published metrics using the Kubernetes custom/metrics.k8s.io API. This is required because the HPA cannot query metrics directly from Prometheus, only from standard Kubernetes metrics APIs. As with Prometheus the simplest way to install the adapter is by using the Helm chart. Before installing though we need to create the adapter configuration so that it can publish our custom metrics. The documentation for the adapter configuration is not the simplest to understand quickly. On top of that the adapter documentation shows how to configure the adapter using a ConfigMap whereas the Helm chart adds the configuration to the Helm values file. The basic format for configuring a metric in the adapter is as follows: <markup lang=\"yaml\" >- seriesQuery: 'application:coherence_heap_usage_percentage_used' resources: overrides: namespace: resource: \"namespace\" pod: resource: \"pod\" role: group: \"coherence.oracle.com\" resource: \"coherence\" name: matches: \"\" as: \"heap_memory_usage_after_gc_pct\" metricsQuery: sum(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) The seriesQuery is the name of the metric to be retrieved from Prometheus. This is the same name used when querying in the UI. The name can be qualified further with tags/labels but in our case just the metric name is sufficient. The overrides section matches metric labels to Kubernetes resources, which can be used in queries (more about this below). The metrics have a namespace label (as can be seen in the UI above) and this maps to a Kubernetes Namespace resource. The metrics have a pod label (as can be seen in the UI above) and this maps to a Kubernetes Pod resource. The metrics have a role label (as can be seen in the UI above) and this maps to a Kubernetes coherence.coherence.oracle.com resource. The name.as field gives the name of the metric in the metrics API. The metricsQuery determines how a specific metric will be fetched, in this case we are summing the values. The configuration above will create a metric in the custom/metrics.k8s.io API named heap_memory_usage_after_gc_pct. This metric can be retrieved from the API for a namespace, for a Pod or for a Coherence deployment (the coherence.coherence.oracle.com resource). This is why the metricsQuery uses sum , so that when querying for a metric at the namespace level we see the total summed up for the namespace. Summing up the metric might not be the best approach. Imagine that we want to scale when the heap after gc usage exceeds 80%. Ideally this is when any JVM heap in use after garbage collection exceeds 80%. Whilst Coherence will distribute data evenly across the cluster so that each member holds a similar amount of data and has similar heap usage, there could be an occasion where one member for whatever reason is processing extra load and exceeds 80% before other members. One way to approach this issue is instead of summing the metric value for a namespace or coherence.coherence.oracle.com resource we can fetch the maximum value. We do this by changing the metricsQuery to use max as shown below: <markup lang=\"yaml\" >- seriesQuery: 'application:coherence_heap_usage_percentage_used' resources: overrides: namespace: resource: \"namespace\" pod: resource: \"pod\" role: group: \"coherence.oracle.com\" resource: \"coherence\" name: matches: \"\" as: \"heap_memory_usage_after_gc_max_pct\" metricsQuery: max(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) This is the same configuration as previously but now the metricsQuery uses the max function, and the metric name has been changed to heap_memory_usage_after_gc_max_pct so that it is obvious it is a maximum value. We can repeat the configuration above for the application:coherence_heap_usage_used metric too so that we will end up with four metrics in the custom/metrics.k8s.io API: heap_memory_usage_after_gc_max_pct heap_memory_usage_after_gc_pct heap_memory_usage_after_gc heap_memory_usage_after_gc_max The autoscaler example has a Prometheus Adapter Helm chart values file that contains the configuration for the four metrics. This can be used to install the adapter Helm chart : In the command below the --set prometheus.url=http://prometheus-prometheus-oper-prometheus.default.svc parameter tells the adapter how to connect to Prometheus. The Prometheus Operator creates a Service named prometheus-prometheus-oper-prometheus to expose Prometheus. In this case the command assumes Prometheus is installed in the default namespace. If you installed Prometheus into a different namespace change the default part of prometheus-prometheus-oper-prometheus. default .svc to the actual namespace name. The manifests/prometheus-adapter-values.yaml contains the configurations for metrics that the adapter will publish. These work with Coherence Operator 3.1.0 and above. If using an earlier 3.0.x version the values file must first be edited to change all occurrences of resource: \"coherence\" to resource: \"coherence\" (to make the resource name singular). <markup lang=\"bash\" >helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install --atomic --wait \\ --set prometheus.url=http://prometheus-prometheus-oper-prometheus.default.svc \\ --values manifests/prometheus-adapter-values.yaml \\ prometheus-adapter prometheus-community/prometheus-adapter Query Custom Metrics Now the Prometheus adapter is running we can query metrics from the custom/metrics.k8s.io API using kubectl raw API access. This is the same API that the HPA will use to obtain metrics. If a Coherence cluster had been installed into the default namespace, then metrics could be fetched for all Pods in that specific namespace, for example to obtain the heap_memory_usage_after_gc_pct metric: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct The * after pods/ tells the adapter to fetch metrics for all Pods in the namespace. To fetch the metric for pods in another namespace change the default part of the URL to the namespace name. If you have the jq utility installed that formats json then piping the output to jq will make it prettier. <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct | jq We could fetch a metric for a specific Pod in the default namespace, for example a Pod named test-cluster-1 as follows: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/test-cluster-1/heap_memory_usage_after_gc_pct which might display something like: <markup lang=\"json\" >{ \"kind\": \"MetricValueList\", \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\", \"metadata\": { \"selfLink\": \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/coherence-test/pods/test-cluster-1/heap_memory_usage_after_gc_pct\" }, \"items\": [ { \"describedObject\": { \"kind\": \"Pod\", \"namespace\": \"operator-test\", \"name\": \"test-cluster-1\", \"apiVersion\": \"/v1\" }, \"metricName\": \"heap_memory_usage_after_gc_pct\", \"timestamp\": \"2020-09-02T12:12:01Z\", \"value\": \"1300m\", \"selector\": null } ] } The format of the value field above might look a little strange. This is because it is a Kubernetes Quantity format, in this case it is 1300m where the m stand for millis. So in this case 1300 millis is 1.3% heap usage. This is to get around the poor support in yaml and json for accurate floating-point numbers. In our case for auto-scaling we are interested in the maximum heap for a specific Coherence resource. Remember in the Prometheus Adapter configuration we configured the role metric tag to map to coherence.coherence.oracle.com resources. We also configured a query that will give back the maximum heap usage value for a query. The example yaml used to deploy the Coherence resource above will create a resource named test-cluster . If we installed this into the default Kubernetes namespace then we can fetch the maximum heap use after gc for the Pods in that Coherence deployment as follows: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct which might display something like: <markup lang=\"json\" >{ \"kind\": \"MetricValueList\", \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\", \"metadata\": { \"selfLink\": \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/operator-test/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct\" }, \"items\": [ { \"describedObject\": { \"kind\": \"Coherence\", \"namespace\": \"operator-test\", \"name\": \"test-cluster\", \"apiVersion\": \"coherence.oracle.com/v1\" }, \"metricName\": \"heap_memory_usage_after_gc_max_pct\", \"timestamp\": \"2020-09-02T12:21:02Z\", \"value\": \"3300m\", \"selector\": null } ] } ",
            "title": "Install Prometheus Adapter"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " Now that we have custom metrics in the Kubernets custom.metrics.k8s.io API, the final piece is to add the HPA configuration for the Coherence deployment that we want to scale. To configure the HPA we need to create a HorizontalPodautoscaler resource for each Coherence deployment in the same namespace as we deployed the Coherence deployment to. Below is an example HorizontalPodautoscaler resource that will scale our example Coherence deployment: <markup lang=\"yaml\" title=\"hpa.yaml\" >apiVersion: autoscaling/v2beta2 kind: HorizontalPodautoscaler metadata: name: test-cluster-hpa spec: scaleTargetRef: apiVersion: coherence.oracle.com/v1 kind: Coherence name: test-cluster minReplicas: 2 maxReplicas: 5 metrics: - type: Object object: describedObject: apiVersion: coherence.oracle.com/v1 kind: Coherence name: test-cluster metric: name: heap_memory_usage_after_gc_max_pct target: type: Value value: 80 behavior: scaleUp: stabilizationWindowSeconds: 120 scaleDown: stabilizationWindowSeconds: 120 The scaleTargetRef points to the resource that the HPA will scale. In this case it is our Coherence deployment which is named test-cluster . The apiVersion and kind fields match those in the Coherence resource. For this example, the Coherence deployment will have a minimum of 2 replicas and a maximum of 5, so the HPA will not scale up too much. The metrics section in the yaml above tells the HPA how to query our custom metric. In this case we want to query the single max usage value metric for the Coherence deployment (like we did manually when using kubectl above). To do this we add a metric with a type of Object . The describedObject section describes the resource to query, in this case kind Coherence in resource group coherence.oracle.com with the name test-cluster . The metric name to query is our custom max heap usage percentage metric heap_memory_usage_after_gc_max_pct . The target section describes the target value for the metric, in this case 80 thousand millis - which is 80%. The behavior section sets a window of 120 seconds so that the HAP will wait at least 120 seconds after scaling up or down before re-evaluating the metric. This gives Coherence enough time to scale the deployment and for the data to redistribute and gc to occur. In real life this value would need to be adjusted to work correctly on your actual cluster. The autoscaler example contains yaml to create the HorizontalPodautoscaler resource in the manifests/ directory. <markup lang=\"bash\" >kubectl create -f manifests/hpa.yaml The hpa.yaml file will create a HorizontalPodautoscaler resource named test-cluster-hpa . After waiting a minute or two for the HPA to get around to polling our new HorizontalPodautoscaler resource we can check its status. <markup lang=\"bash\" >kubectl describe horizontalpodautoscaler.autoscaling/test-cluster-hpa Which should show something like: <markup lang=\"bash\" >Name: test-cluster-hpa Namespace: operator-test Labels: &lt;none&gt; Annotations: &lt;none&gt; CreationTimestamp: Wed, 02 Sep 2020 15:58:26 +0300 Reference: Coherence/test-cluster Metrics: ( current / target ) \"heap_memory_usage_after_gc_max_pct\" on Coherence/test-cluster (target value): 3300m / 80 Min replicas: 2 Max replicas: 10 Coherence pods: 2 current / 2 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ScaleDownStabilized recent recommendations were higher than current one, applying the highest recent recommendation ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from Coherence metric heap_memory_usage_after_gc_max_pct ScalingLimited False DesiredWithinRange the desired count is within the acceptable range Events: &lt;none&gt; We can see that the HPA has successfully polled the metric and obtained a value of 3300m (so 3.3%) and has decided that it does not need to scale. ",
            "title": "Configure The Horizontal Pod autoscaler"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " The HPA is now monitoring our Coherence deployment so we can now add data to the cluster and see the HPA scale up when heap use grows. The autoscaler example Maven pom file has been configured to use the Maven exec plugin to execute a Coherence command line client that will connect over Coherence Extend to the demo cluster that we have deployed. First we need to create a port forwarder to expose the Coherence Extend port locally. Extend is bound to port 20000 in the Pods in our example. <markup lang=\"bash\" >kubectl port-forward pod/test-cluster-0 20000:20000 The command above forwards port 20000 in the Pod test-cluster-0 to the local port 20000. To start the client, run the following command in a terminal: <markup lang=\"bash\" >./mvnw exec:java -pl autoscaler/ The command above will start the console client and eventually display a Map (?): prompt. At the map prompt, first create a cache named test with the cache command, type cache test and hit enter: <markup lang=\"bash\" >Map (?): cache test There will now be a cache created in the cluster named test , and the map prompt will change to Map (test): . We can add random data to this with the bulkput command. The format of the bulkput command is: <markup lang=\"bash\" >bulkput &lt;# of iterations&gt; &lt;block size&gt; &lt;start key&gt; [&lt;batch size&gt; | all] So to add 20,000 entries of 10k bytes each starting at key 1 adding in batches of 1000 we can run the bulkput 20000 10000 1 1000 command at the map prompt: <markup lang=\"bash\" >Map (test): bulkput 20000 10000 1 1000 We can now look at the HorizontalPodautoscaler resource we create earlier with the command: <markup lang=\"bash\" >kubectl get horizontalpodautoscaler.autoscaling/test-cluster-hpa Which will display something like: <markup lang=\"bash\" >NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE test-cluster-hpa Coherence/test-cluster 43700m/80 2 10 2 41m The HPA is now saying that the value of our heap use metric is 43.7%, so we can add a bit more data. It may take a minute or two for the heap to increase and stabilise as different garbage collections happen across the Pods. We should be able to safely add another 20000 entries putting the heap above 80% and hopefully scaling our deployment. We need to change the third parameter to bulk put to 20000 otherwise the put will start again at key 1 and just overwrite the previous entries, not really adding to the heap. <markup lang=\"bash\" >Map (test): bulkput 20000 10000 20000 1000 Now run the kubectl describe command on the HorizontalPodautoscaler resource again, and we should see that it has scaled our cluster. If another 20,000 entries does not cause the heap to exceed 80% then you may need to run the bulkput command once or twice more with a smaller number of entries to push the heap over 80%. As previously mentioned, everything with HPA is slightly delayed due to the different components polling, and stabilization times. It could take a few minutes for the HPA to actually scale the cluster. <markup lang=\"bash\" >kubectl describe horizontalpodautoscaler.autoscaling/test-cluster-hpa The output of the kubectl describe command should now be something like this: <markup lang=\"bash\" >Name: test-cluster-hpa Namespace: operator-test Labels: &lt;none&gt; Annotations: &lt;none&gt; CreationTimestamp: Wed, 02 Sep 2020 15:58:26 +0300 Reference: Coherence/test-cluster Metrics: ( current / target ) \"heap_memory_usage_after_gc_max_pct\" on Coherence/test-cluster (target value): 88300m / 80 Min replicas: 2 Max replicas: 10 Coherence pods: 2 current / 3 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True SucceededRescale the HPA controller was able to update the target scale to 3 ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from Coherence metric heap_memory_usage_after_gc_max_pct ScalingLimited False DesiredWithinRange the desired count is within the acceptable range Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 1s horizontal-pod-autoscaler New size: 3; reason: Coherence metric heap_memory_usage_after_gc_max_pct above target We can see that the heap use value is now 88300m or 88.3% and the events section shows that the HPA has scaled the Coherence deployment to 3 . We can list the Pods and there should be three: <markup lang=\"bash\" >kubectl get pod -l coherenceCluster=test-cluster <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE test-cluster-0 1/1 Running 0 3h14m test-cluster-1 1/1 Running 0 3h14m test-cluster-2 1/1 Running 0 1m10s At this point Coherence will redistribute data to balance it over the three members of the cluster. It may be that it takes considerable time for this to affect the heap usage as a lot of the cache data will be in the old generation of the heap and not be immediately collected. This may then trigger another scale after the 120 second stabilization period that we configured in the HorizontalPodautoscaler . ",
            "title": "Add Data - Scale Up!"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " To clean-up after running the example just uninstall everything in the reverse order: <markup lang=\"bash\" >kubectl delete -f manifests/hpa.yaml helm delete prometheus-adapter helm delete prometheus kubectl delete -f manifests/cluster.yaml Remove the Prometheus RBAC rules, remembering to change the namespace name. <markup lang=\"bash\" >sed \"s/coherence-example/default/g\" manifests/prometheus-rbac.yaml | kubectl delete -f - Delete the Coherence deployment. <markup lang=\"bash\" >kubectl delete manifests/cluster.yaml Undeploy the Operator. TBD&#8230;&#8203; ",
            "title": "Clean-Up"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " Running the example requires a number of components to be installed. The example will use Prometheus as a custom metrics source, which requires installation of Prometheus and the Prometheus Adapter custom metrics source. To simplify the example commands none of the examples below use a Kubernetes namespace. If you wish to install the components below into a namespace other than default , then use the required kubectl and Helm namespace options. Install the Coherence Operator First install the Coherence Operator, TBD&#8230;&#8203; Install Coherence cluster With the Coherence Operator running we can now install a simple Coherence cluster. An example of the yaml required is below: <markup lang=\"yaml\" title=\"cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: image: autoscaler-example:latest imagePullPolicy: IfNotPresent replicas: 2 coherence: metrics: enabled: true jvm: memory: heapSize: 500m ports: - name: metrics serviceMonitor: enabled: true - name: extend port: 20000 The image used for the application will be the autoscaler-example:latest image we built above. The deployment will initially have 2 replicas. Coherence metrics must be enabled to publish the metrics we require for scaling. In this example the JVM heap has been fixed to 500m , which is quite small but this means we do not need to add a lot of data to cause excessive heap usage when we run the example. The metrics port must also be exposed on a Service . A Prometheus ServiceMonitor must also be enabled for the metrics service so that Prometheus can find the Coherence Pods and poll metrics from them. This example also exposes a Coherence Extend port so that test data can easily be loaded into the caches. The autoscaler example includes a suitable yaml file named cluster.yaml in the manifests/ directory that can be used to create a Coherence deployment. <markup lang=\"bash\" >kubectl create -f manifests/cluster.yaml The Pods that are part of the Coherence cluster can be listed with kubectl . All the Pods have a label coherenceCluster set by the Coherence Operator to match the name of the Coherence resource that they belong to, which makes it easier to list Pods for a specific deployment using kubectl : <markup lang=\"bash\" >kubectl get pod -l coherenceCluster=test-cluster In a short time the Pods should both be ready. <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE test-cluster-0 1/1 Running 0 2m52s test-cluster-1 1/1 Running 0 2m52s Test the Custom Heap Metrics The Metrics endpoint will be exposed on port 9612 on each Pod , so it is possible to query the metrics endpoints for the custom heap metrics. The simplest way to test the metrics is to use the kubectl port-forward command and curl . In one terminal session start the port forwarder to the first Pod , test-cluster-0 : <markup lang=\"bash\" >kubectl port-forward pod/test-cluster-0 9612:9612 metrics from Pod , test-cluster-0 can be queried on http://127.0.0.1:9612/metrics In a second terminal we can use curl to query the metrics. The Coherence metrics endpoint serves metrics in two formats, plain text compatible with Prometheus and JSON. If the required content type has not been specified in the curl command it could be either that is returned. To specify a content type set the accepted type in the header, for example --header \"Accept: text/plain\" or --header \"Accept: application/json\" . This command will retrieve metrics from test-cluster-0 in the same format that Prometheus would. <markup lang=\"bash\" >curl -s --header \"Accept: text/plain\" -X GET http://127.0.0.1:9612/metrics This will return quite a lot of metrics, somewhere in that output is the custom application metrics for heap usage. The simplest way to isolate them would be to use grep , for example: <markup lang=\"bash\" >curl -s --header \"Accept: text/plain\" -X GET http://127.0.0.1:9612/metrics | grep application which should show something like: <markup lang=\"bash\" >application:coherence_heap_usage_percentage_used{cluster=\"test-cluster\", machine=\"docker-desktop\", member=\"test-cluster-0\", node_id=\"2\", role=\"test-cluster\", site=\"test-cluster-sts.operator-test.svc\"} 3.09 application:coherence_heap_usage_used{cluster=\"test-cluster\", machine=\"docker-desktop\", member=\"test-cluster-0\", node_id=\"2\", role=\"test-cluster\", site=\"test-cluster-sts.operator-test.svc\"} 16177976 The first metric application:coherence_heap_usage_percentage_used shows the heap was 3.09% full after the last gc. The second metric application:coherence_heap_usage_used shows that the in-use heap after the last gc was 16177976 bytes, or around 16 MB. The port forwarder can be changed to connect to the second Pod test-cluster-1 , and the same curl command will retrieve metrics from the second Pod , which should show different heap use values. Install Prometheus The simplest way to install Prometheus as part of an example or demo is to use the Prometheus Operator , which can be installed using a Helm chart. Setup the Helm Repo Make sure the stable helm repository has been added to Helm if it isn&#8217;t already present in your local Helm repositories. <markup lang=\"bash\" >helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Make sure the local Helm repository is up to date. <markup lang=\"bash\" >helm repo update Configure Prometheus RBAC If you are using a Kubernetes cluster with RBAC enabled then the rules required by Prometheus need to be added. The autoscale example contains a yaml file with the required RBAC rules in it in the manifests/ directory. The manifests/prometheus-rbac.yaml uses a namespace coherence-example which may need to be changed if you are installing into a different namespace. The following commands use sed to replace coherence-example with default and pipe the result to kubectl to create the RBAC rules in the default Kubernetes namespace. <markup lang=\"bash\" >sed \"s/coherence-example/default/g\" manifests/prometheus-rbac.yaml | kubectl create -f - Install the Prometheus Operator The Prometheus Operator can now be installed using Helm. The autoscaler example contains a simple values files that can be used when installing the chart in the manifests/ directory. <markup lang=\"bash\" >helm install --atomic --version 8.13.9 --wait \\ --set prometheus.service.type=NodePort \\ --values manifests/prometheus-values.yaml prometheus stable/prometheus-operator The --wait parameter makes Helm block until all the installed resources are ready. The command above sets the prometheus.service.type value to NodePort so that the Prometheus UI will be exposed on a port on the Kubernetes node. This is particularly useful when testing with a local Kubernetes cluster, such as in Docker on a laptop because the UI can be reached on localhost at that port. The default node port is 30090 , this can be changed by setting a different port, e.g: --set prometheus.service.nodePort=9090 . Assuming the default port of 30090 is used the UI can be reached on http://127.0.0.1:30090 . After Prometheus has started up and is scraping metrics we should be able to see our custom metrics in the UI. Type the metric name application:coherence_heap_usage_percentage_used in the expression box and click Execute and Prometheus should show two values for the metric, one for each Pod . Prometheus is scraping many more Coherence metrics that can also be queried in the UI. Install Prometheus Adapter The next step in the example is to install the Prometheus Adapter. This is a custom metrics server that published metrics using the Kubernetes custom/metrics.k8s.io API. This is required because the HPA cannot query metrics directly from Prometheus, only from standard Kubernetes metrics APIs. As with Prometheus the simplest way to install the adapter is by using the Helm chart. Before installing though we need to create the adapter configuration so that it can publish our custom metrics. The documentation for the adapter configuration is not the simplest to understand quickly. On top of that the adapter documentation shows how to configure the adapter using a ConfigMap whereas the Helm chart adds the configuration to the Helm values file. The basic format for configuring a metric in the adapter is as follows: <markup lang=\"yaml\" >- seriesQuery: 'application:coherence_heap_usage_percentage_used' resources: overrides: namespace: resource: \"namespace\" pod: resource: \"pod\" role: group: \"coherence.oracle.com\" resource: \"coherence\" name: matches: \"\" as: \"heap_memory_usage_after_gc_pct\" metricsQuery: sum(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) The seriesQuery is the name of the metric to be retrieved from Prometheus. This is the same name used when querying in the UI. The name can be qualified further with tags/labels but in our case just the metric name is sufficient. The overrides section matches metric labels to Kubernetes resources, which can be used in queries (more about this below). The metrics have a namespace label (as can be seen in the UI above) and this maps to a Kubernetes Namespace resource. The metrics have a pod label (as can be seen in the UI above) and this maps to a Kubernetes Pod resource. The metrics have a role label (as can be seen in the UI above) and this maps to a Kubernetes coherence.coherence.oracle.com resource. The name.as field gives the name of the metric in the metrics API. The metricsQuery determines how a specific metric will be fetched, in this case we are summing the values. The configuration above will create a metric in the custom/metrics.k8s.io API named heap_memory_usage_after_gc_pct. This metric can be retrieved from the API for a namespace, for a Pod or for a Coherence deployment (the coherence.coherence.oracle.com resource). This is why the metricsQuery uses sum , so that when querying for a metric at the namespace level we see the total summed up for the namespace. Summing up the metric might not be the best approach. Imagine that we want to scale when the heap after gc usage exceeds 80%. Ideally this is when any JVM heap in use after garbage collection exceeds 80%. Whilst Coherence will distribute data evenly across the cluster so that each member holds a similar amount of data and has similar heap usage, there could be an occasion where one member for whatever reason is processing extra load and exceeds 80% before other members. One way to approach this issue is instead of summing the metric value for a namespace or coherence.coherence.oracle.com resource we can fetch the maximum value. We do this by changing the metricsQuery to use max as shown below: <markup lang=\"yaml\" >- seriesQuery: 'application:coherence_heap_usage_percentage_used' resources: overrides: namespace: resource: \"namespace\" pod: resource: \"pod\" role: group: \"coherence.oracle.com\" resource: \"coherence\" name: matches: \"\" as: \"heap_memory_usage_after_gc_max_pct\" metricsQuery: max(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) This is the same configuration as previously but now the metricsQuery uses the max function, and the metric name has been changed to heap_memory_usage_after_gc_max_pct so that it is obvious it is a maximum value. We can repeat the configuration above for the application:coherence_heap_usage_used metric too so that we will end up with four metrics in the custom/metrics.k8s.io API: heap_memory_usage_after_gc_max_pct heap_memory_usage_after_gc_pct heap_memory_usage_after_gc heap_memory_usage_after_gc_max The autoscaler example has a Prometheus Adapter Helm chart values file that contains the configuration for the four metrics. This can be used to install the adapter Helm chart : In the command below the --set prometheus.url=http://prometheus-prometheus-oper-prometheus.default.svc parameter tells the adapter how to connect to Prometheus. The Prometheus Operator creates a Service named prometheus-prometheus-oper-prometheus to expose Prometheus. In this case the command assumes Prometheus is installed in the default namespace. If you installed Prometheus into a different namespace change the default part of prometheus-prometheus-oper-prometheus. default .svc to the actual namespace name. The manifests/prometheus-adapter-values.yaml contains the configurations for metrics that the adapter will publish. These work with Coherence Operator 3.1.0 and above. If using an earlier 3.0.x version the values file must first be edited to change all occurrences of resource: \"coherence\" to resource: \"coherence\" (to make the resource name singular). <markup lang=\"bash\" >helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install --atomic --wait \\ --set prometheus.url=http://prometheus-prometheus-oper-prometheus.default.svc \\ --values manifests/prometheus-adapter-values.yaml \\ prometheus-adapter prometheus-community/prometheus-adapter Query Custom Metrics Now the Prometheus adapter is running we can query metrics from the custom/metrics.k8s.io API using kubectl raw API access. This is the same API that the HPA will use to obtain metrics. If a Coherence cluster had been installed into the default namespace, then metrics could be fetched for all Pods in that specific namespace, for example to obtain the heap_memory_usage_after_gc_pct metric: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct The * after pods/ tells the adapter to fetch metrics for all Pods in the namespace. To fetch the metric for pods in another namespace change the default part of the URL to the namespace name. If you have the jq utility installed that formats json then piping the output to jq will make it prettier. <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct | jq We could fetch a metric for a specific Pod in the default namespace, for example a Pod named test-cluster-1 as follows: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/test-cluster-1/heap_memory_usage_after_gc_pct which might display something like: <markup lang=\"json\" >{ \"kind\": \"MetricValueList\", \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\", \"metadata\": { \"selfLink\": \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/coherence-test/pods/test-cluster-1/heap_memory_usage_after_gc_pct\" }, \"items\": [ { \"describedObject\": { \"kind\": \"Pod\", \"namespace\": \"operator-test\", \"name\": \"test-cluster-1\", \"apiVersion\": \"/v1\" }, \"metricName\": \"heap_memory_usage_after_gc_pct\", \"timestamp\": \"2020-09-02T12:12:01Z\", \"value\": \"1300m\", \"selector\": null } ] } The format of the value field above might look a little strange. This is because it is a Kubernetes Quantity format, in this case it is 1300m where the m stand for millis. So in this case 1300 millis is 1.3% heap usage. This is to get around the poor support in yaml and json for accurate floating-point numbers. In our case for auto-scaling we are interested in the maximum heap for a specific Coherence resource. Remember in the Prometheus Adapter configuration we configured the role metric tag to map to coherence.coherence.oracle.com resources. We also configured a query that will give back the maximum heap usage value for a query. The example yaml used to deploy the Coherence resource above will create a resource named test-cluster . If we installed this into the default Kubernetes namespace then we can fetch the maximum heap use after gc for the Pods in that Coherence deployment as follows: <markup lang=\"bash\" >kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct which might display something like: <markup lang=\"json\" >{ \"kind\": \"MetricValueList\", \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\", \"metadata\": { \"selfLink\": \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/operator-test/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct\" }, \"items\": [ { \"describedObject\": { \"kind\": \"Coherence\", \"namespace\": \"operator-test\", \"name\": \"test-cluster\", \"apiVersion\": \"coherence.oracle.com/v1\" }, \"metricName\": \"heap_memory_usage_after_gc_max_pct\", \"timestamp\": \"2020-09-02T12:21:02Z\", \"value\": \"3300m\", \"selector\": null } ] } Configure The Horizontal Pod autoscaler Now that we have custom metrics in the Kubernets custom.metrics.k8s.io API, the final piece is to add the HPA configuration for the Coherence deployment that we want to scale. To configure the HPA we need to create a HorizontalPodautoscaler resource for each Coherence deployment in the same namespace as we deployed the Coherence deployment to. Below is an example HorizontalPodautoscaler resource that will scale our example Coherence deployment: <markup lang=\"yaml\" title=\"hpa.yaml\" >apiVersion: autoscaling/v2beta2 kind: HorizontalPodautoscaler metadata: name: test-cluster-hpa spec: scaleTargetRef: apiVersion: coherence.oracle.com/v1 kind: Coherence name: test-cluster minReplicas: 2 maxReplicas: 5 metrics: - type: Object object: describedObject: apiVersion: coherence.oracle.com/v1 kind: Coherence name: test-cluster metric: name: heap_memory_usage_after_gc_max_pct target: type: Value value: 80 behavior: scaleUp: stabilizationWindowSeconds: 120 scaleDown: stabilizationWindowSeconds: 120 The scaleTargetRef points to the resource that the HPA will scale. In this case it is our Coherence deployment which is named test-cluster . The apiVersion and kind fields match those in the Coherence resource. For this example, the Coherence deployment will have a minimum of 2 replicas and a maximum of 5, so the HPA will not scale up too much. The metrics section in the yaml above tells the HPA how to query our custom metric. In this case we want to query the single max usage value metric for the Coherence deployment (like we did manually when using kubectl above). To do this we add a metric with a type of Object . The describedObject section describes the resource to query, in this case kind Coherence in resource group coherence.oracle.com with the name test-cluster . The metric name to query is our custom max heap usage percentage metric heap_memory_usage_after_gc_max_pct . The target section describes the target value for the metric, in this case 80 thousand millis - which is 80%. The behavior section sets a window of 120 seconds so that the HAP will wait at least 120 seconds after scaling up or down before re-evaluating the metric. This gives Coherence enough time to scale the deployment and for the data to redistribute and gc to occur. In real life this value would need to be adjusted to work correctly on your actual cluster. The autoscaler example contains yaml to create the HorizontalPodautoscaler resource in the manifests/ directory. <markup lang=\"bash\" >kubectl create -f manifests/hpa.yaml The hpa.yaml file will create a HorizontalPodautoscaler resource named test-cluster-hpa . After waiting a minute or two for the HPA to get around to polling our new HorizontalPodautoscaler resource we can check its status. <markup lang=\"bash\" >kubectl describe horizontalpodautoscaler.autoscaling/test-cluster-hpa Which should show something like: <markup lang=\"bash\" >Name: test-cluster-hpa Namespace: operator-test Labels: &lt;none&gt; Annotations: &lt;none&gt; CreationTimestamp: Wed, 02 Sep 2020 15:58:26 +0300 Reference: Coherence/test-cluster Metrics: ( current / target ) \"heap_memory_usage_after_gc_max_pct\" on Coherence/test-cluster (target value): 3300m / 80 Min replicas: 2 Max replicas: 10 Coherence pods: 2 current / 2 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ScaleDownStabilized recent recommendations were higher than current one, applying the highest recent recommendation ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from Coherence metric heap_memory_usage_after_gc_max_pct ScalingLimited False DesiredWithinRange the desired count is within the acceptable range Events: &lt;none&gt; We can see that the HPA has successfully polled the metric and obtained a value of 3300m (so 3.3%) and has decided that it does not need to scale. Add Data - Scale Up! The HPA is now monitoring our Coherence deployment so we can now add data to the cluster and see the HPA scale up when heap use grows. The autoscaler example Maven pom file has been configured to use the Maven exec plugin to execute a Coherence command line client that will connect over Coherence Extend to the demo cluster that we have deployed. First we need to create a port forwarder to expose the Coherence Extend port locally. Extend is bound to port 20000 in the Pods in our example. <markup lang=\"bash\" >kubectl port-forward pod/test-cluster-0 20000:20000 The command above forwards port 20000 in the Pod test-cluster-0 to the local port 20000. To start the client, run the following command in a terminal: <markup lang=\"bash\" >./mvnw exec:java -pl autoscaler/ The command above will start the console client and eventually display a Map (?): prompt. At the map prompt, first create a cache named test with the cache command, type cache test and hit enter: <markup lang=\"bash\" >Map (?): cache test There will now be a cache created in the cluster named test , and the map prompt will change to Map (test): . We can add random data to this with the bulkput command. The format of the bulkput command is: <markup lang=\"bash\" >bulkput &lt;# of iterations&gt; &lt;block size&gt; &lt;start key&gt; [&lt;batch size&gt; | all] So to add 20,000 entries of 10k bytes each starting at key 1 adding in batches of 1000 we can run the bulkput 20000 10000 1 1000 command at the map prompt: <markup lang=\"bash\" >Map (test): bulkput 20000 10000 1 1000 We can now look at the HorizontalPodautoscaler resource we create earlier with the command: <markup lang=\"bash\" >kubectl get horizontalpodautoscaler.autoscaling/test-cluster-hpa Which will display something like: <markup lang=\"bash\" >NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE test-cluster-hpa Coherence/test-cluster 43700m/80 2 10 2 41m The HPA is now saying that the value of our heap use metric is 43.7%, so we can add a bit more data. It may take a minute or two for the heap to increase and stabilise as different garbage collections happen across the Pods. We should be able to safely add another 20000 entries putting the heap above 80% and hopefully scaling our deployment. We need to change the third parameter to bulk put to 20000 otherwise the put will start again at key 1 and just overwrite the previous entries, not really adding to the heap. <markup lang=\"bash\" >Map (test): bulkput 20000 10000 20000 1000 Now run the kubectl describe command on the HorizontalPodautoscaler resource again, and we should see that it has scaled our cluster. If another 20,000 entries does not cause the heap to exceed 80% then you may need to run the bulkput command once or twice more with a smaller number of entries to push the heap over 80%. As previously mentioned, everything with HPA is slightly delayed due to the different components polling, and stabilization times. It could take a few minutes for the HPA to actually scale the cluster. <markup lang=\"bash\" >kubectl describe horizontalpodautoscaler.autoscaling/test-cluster-hpa The output of the kubectl describe command should now be something like this: <markup lang=\"bash\" >Name: test-cluster-hpa Namespace: operator-test Labels: &lt;none&gt; Annotations: &lt;none&gt; CreationTimestamp: Wed, 02 Sep 2020 15:58:26 +0300 Reference: Coherence/test-cluster Metrics: ( current / target ) \"heap_memory_usage_after_gc_max_pct\" on Coherence/test-cluster (target value): 88300m / 80 Min replicas: 2 Max replicas: 10 Coherence pods: 2 current / 3 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True SucceededRescale the HPA controller was able to update the target scale to 3 ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from Coherence metric heap_memory_usage_after_gc_max_pct ScalingLimited False DesiredWithinRange the desired count is within the acceptable range Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 1s horizontal-pod-autoscaler New size: 3; reason: Coherence metric heap_memory_usage_after_gc_max_pct above target We can see that the heap use value is now 88300m or 88.3% and the events section shows that the HPA has scaled the Coherence deployment to 3 . We can list the Pods and there should be three: <markup lang=\"bash\" >kubectl get pod -l coherenceCluster=test-cluster <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE test-cluster-0 1/1 Running 0 3h14m test-cluster-1 1/1 Running 0 3h14m test-cluster-2 1/1 Running 0 1m10s At this point Coherence will redistribute data to balance it over the three members of the cluster. It may be that it takes considerable time for this to affect the heap usage as a lot of the cache data will be in the old generation of the heap and not be immediately collected. This may then trigger another scale after the 120 second stabilization period that we configured in the HorizontalPodautoscaler . Clean-Up To clean-up after running the example just uninstall everything in the reverse order: <markup lang=\"bash\" >kubectl delete -f manifests/hpa.yaml helm delete prometheus-adapter helm delete prometheus kubectl delete -f manifests/cluster.yaml Remove the Prometheus RBAC rules, remembering to change the namespace name. <markup lang=\"bash\" >sed \"s/coherence-example/default/g\" manifests/prometheus-rbac.yaml | kubectl delete -f - Delete the Coherence deployment. <markup lang=\"bash\" >kubectl delete manifests/cluster.yaml Undeploy the Operator. TBD&#8230;&#8203; ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/200_autoscaler/README",
            "text": " As we&#8217;ve shown, it is possible to use the HPA to scale a Coherence cluster based on metrics published by Coherence or custom metrics, but there are some obvious caveats due to how HPA works. There are inherent delays in the scaling process, the HPA only polls metrics periodically, which themselves have been polled by Prometheus periodically and hence there can be some delay after reaching a given heap size before the scale command actually reaches the Coherence Operator. This will be obvious when running the example below. Given a suitable configuration the HPA can be useful to scale as load increases but in no way can it guarantee that an out of memory exception will never happen. Using the HPA to scale as Coherence Pod&#8217;s heaps become filled is in no way an excuse not to do proper capacity planning and size your Coherence clusters appropriately. ",
            "title": "Conclusions"
        },
        {
            "location": "/docs/management/010_overview",
            "text": " There are a number of management and diagnostic features available with Oracle Coherence that can be used in a cluster deployed in Kubernetes. Management Over REST Coherence Management over REST feature The Coherence CLI Using the Coherence CLI in Pods VisualVM Coherence VisualVM plugin. SSL Enable SSL on the Management over REST endpoint. ",
            "title": "Overview"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " Each JVM in a Coherence cluster has an identity. This is made up of a number of values for site , rack , member , machine and node-id . The node-id is assigned by Coherence when a node joins a cluster. The other values can be assigned using system properties, or will have defaults assigned by Coherence if not set. The Coherence Operator will configure properties for these values. The member name is set to the Pod name. The machine name is set to the name of the Node that the Pod has been scheduled onto. The site name is taken from the topology.kubernetes.io/zone label on the Node that the Pod has been scheduled onto. If the topology.kubernetes.io/zone label is not set then the deprecated failure-domain.beta.kubernetes.io/zone label will be tried. If neither of these labels are set then the site will be unset, and the cache services may not reach site safe. The rack name is taken from the oci.oraclecloud.com/fault-domain label on the Node that the Pod has been scheduled onto. If the oci.oraclecloud.com/fault-domain label is not set then the site labels will be set to the same value as the site name. ",
            "title": "Member Identity"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " As well as identifying cluster members, these values are also used by the partitioned cache service to distribute data as widely (safely) as possible in the cluster. The backup owner will be as far away as possible from the primary owner. Ideally this would be on a member with a different site; failing that, a different rack, machine and finally member. ",
            "title": "Status HA Values"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " One solution to missing site and rack values is to apply the required labels to the Nodes in the k8s cluster. For example the command below labels the node in Docker dDesktop on MacOS to \"twighlight-zone\". <markup lang=\"bash\" >kubectl label node docker-desktop topology.kubernetes.io/zone=twighlight-zone ",
            "title": "Apply Node Labels"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " The site and rack values can be specified as system properties as part of the Coherence deployment yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster spec: jvm: args: - \"-Dcoherence.site=foo\" - \"-Dcoherence.rack=fbar\" In the deployment above the site name is set to \"foo\" using the coherence.site system property. The rack name is set to \"bar\" using the coherence.rack system property. ",
            "title": "Specify Site and Rack Using System Properties"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " If the Operator is installed using the Helm chart then the site and rack labels can be set using the siteLabel and rackLabel values; for example: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set siteLabel=identity/site \\ --set siteLabel=identity/rack \\ coherence-operator \\ coherence/coherence-operator In the example above the Node label used by the Operator to get the value for the site will be identity/site , and the Node label used to get the value for the rack will be identity/rack . ",
            "title": "Using Helm"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " If using kubectl or kustomize as described in the Installation Guide the additional environment variables can be applied using kustomize commands. <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal SITE_LABEL='identity/site' <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal RACK_LABEL='identity/rack' ",
            "title": "Using Kubectl or Kustomize"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " The Operator can be configured to use different labels to obtain values for the site and rack names. This will obviously apply to all Coherence deployments managed by the Operator, but is useful if the Nodes in the k8s cluster do not have the normal k8s labels. The SITE_LABEL and RACK_LABEL environment variables are used to specify different labels to use. How these environment variables are set depends on how you are installing the Operator. Using Helm If the Operator is installed using the Helm chart then the site and rack labels can be set using the siteLabel and rackLabel values; for example: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set siteLabel=identity/site \\ --set siteLabel=identity/rack \\ coherence-operator \\ coherence/coherence-operator In the example above the Node label used by the Operator to get the value for the site will be identity/site , and the Node label used to get the value for the rack will be identity/rack . Using Kubectl or Kustomize If using kubectl or kustomize as described in the Installation Guide the additional environment variables can be applied using kustomize commands. <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal SITE_LABEL='identity/site' <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal RACK_LABEL='identity/rack' ",
            "title": "Configure the Operator to Use Different Labels"
        },
        {
            "location": "/docs/coherence/021_member_identity",
            "text": " You should not usually need to change the default values applied for the member and machine names, but you may need to change the values used for the site, or rack. The labels used for the site and rack are standard k8s labels but the k8s cluster being used may not have these labels set Apply Node Labels One solution to missing site and rack values is to apply the required labels to the Nodes in the k8s cluster. For example the command below labels the node in Docker dDesktop on MacOS to \"twighlight-zone\". <markup lang=\"bash\" >kubectl label node docker-desktop topology.kubernetes.io/zone=twighlight-zone Specify Site and Rack Using System Properties The site and rack values can be specified as system properties as part of the Coherence deployment yaml. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster spec: jvm: args: - \"-Dcoherence.site=foo\" - \"-Dcoherence.rack=fbar\" In the deployment above the site name is set to \"foo\" using the coherence.site system property. The rack name is set to \"bar\" using the coherence.rack system property. Configure the Operator to Use Different Labels The Operator can be configured to use different labels to obtain values for the site and rack names. This will obviously apply to all Coherence deployments managed by the Operator, but is useful if the Nodes in the k8s cluster do not have the normal k8s labels. The SITE_LABEL and RACK_LABEL environment variables are used to specify different labels to use. How these environment variables are set depends on how you are installing the Operator. Using Helm If the Operator is installed using the Helm chart then the site and rack labels can be set using the siteLabel and rackLabel values; for example: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set siteLabel=identity/site \\ --set siteLabel=identity/rack \\ coherence-operator \\ coherence/coherence-operator In the example above the Node label used by the Operator to get the value for the site will be identity/site , and the Node label used to get the value for the rack will be identity/rack . Using Kubectl or Kustomize If using kubectl or kustomize as described in the Installation Guide the additional environment variables can be applied using kustomize commands. <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal SITE_LABEL='identity/site' <markup lang=\"bash\" >cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal RACK_LABEL='identity/rack' ",
            "title": "Changing Site and Rack Values"
        },
        {
            "location": "/docs/troubleshooting/02_heap_dump",
            "text": " Heap dumps can be very useful when debugging but generating and downloading a heap dump from a container in Kubernetes can be tricky. When you are running minimal images without an O/S or full JDK (such as the distroless images used by JIB) this becomes even more tricky. ",
            "title": "Capture Heap Dumps"
        },
        {
            "location": "/docs/troubleshooting/02_heap_dump",
            "text": " We use KinD for a lot of our CI builds and testing, enabling the EphemeralContainers feature gate in KinD is very easy. For example, this KinD configuration enables the EphemeralContainers feature gate <markup lang=\"yaml\" >kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 featureGates: EphemeralContainers: true nodes: - role: control-plane - role: worker - role: worker The EphemeralContainers feature gate is set to true ",
            "title": "Enable EphemeralContainers in KinD"
        },
        {
            "location": "/docs/troubleshooting/02_heap_dump",
            "text": " In this example we are going to use the jps and jcmd tools to generate the heap dump from an ephemeral container. For this to work the ephemeral container must be able to see the processes running in the coherence container. The Coherence CRD spec has a field named ShareProcessNamespace , which sets the corresponding field in the Coherence Pods that will be created for the deployment. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: shareProcessNamespace: true The shareProcessNamespace must be set to true . If you have some other way to trigger a heap dump to a specific location without requiring the ephemeral container to see the Coherence container processes then the technique below can still be used without setting shareProcessNamespace to true . ",
            "title": "Shared Process Namespace"
        },
        {
            "location": "/docs/troubleshooting/02_heap_dump",
            "text": " Let&#8217;s say we have a Coherence cluster deployed named test-cluster in a namespace named coherence-test . There will be a number of Pods created for this deployment, named test-cluster-0 , test-cluster-1 and so on. For this example we will obtain a heap dump from Pod test-cluster-1 . The purpose of using an ephemeral container is because the Coherence container we are running does not contain any of the tools and programs we require for debugging, e.g. jps , jcmd etc. The ephemeral container we run obviously needs to have all the required tools. You could create a custom image with what you need in it, but for this example we will use the openjdk:11 image, as it has a full JDK and other tools we need in it. You should obviously use a JDK version that matches the version in the Coherence container. We can use the kubectl debug command that can be used to create an ephemeral containers. For our purposes we cannot use this command as we will require volume mounts to share storage between the ephemeral container and the Coherence container so that the ephemeral container can see the heap dump file. Instead of the kubectl debug command we can create ephemeral containers using the kubectl --raw API. Ephemeral containers are a sub-resource of the Pod API. First obtain the current ephemeral containers sub-resource for the Pod. We do this using the kubectl get --raw command with the URL path in the format /api/v1/namespaces/&lt;namespace&gt;&gt;/pods/&lt;pod&gt;/ephemeralcontainers , where &lt;namespace&gt; is the namespace that the Pod is deployed into and &lt;pod&gt; is the name of the Pod. So in our example the command would be: <markup lang=\"bash\" >kubectl get --raw /api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers Which will output json similar to this, which we will save to a file named ec.json : <markup lang=\"json\" title=\"ec.json\" >{ \"kind\": \"EphemeralContainers\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"test-cluster-1\", \"namespace\": \"coherence-test\", \"selfLink\": \"/api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers\", \"uid\": \"731ca9a9-332f-4999-821d-adfea2e1d2d4\", \"resourceVersion\": \"24921\", \"creationTimestamp\": \"2021-03-12T10:41:35Z\" }, \"ephemeralContainers\": [] } The \"ephemeralContainers\" field is an empty array as we have not created any previous containers. We now need to edit this yaml to define the ephemeral container we want to create. The Pod created by the Operator contains an empty directory volume with a volume mount at /coherence-operator/jvm , which is where the JVM is configured to dump debug information, such as heap dumps. We will create an ephemeral container with the same mount so that the /coherence-operator/jvm directory will be shared between the Coherence container and the ephemeral container. Another thing to note is that the default entrypoint in the openjdk:11 image we are using in this example is JShell. This is obviously not what we want, so we will make sure we specify /bin/sh as the entry point as we want a command line shell. Our edited ec.json file looks like this: <markup lang=\"json\" title=\"ec.json\" >{ \"kind\": \"EphemeralContainers\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"test-cluster-1\", \"namespace\": \"coherence-test\", \"selfLink\": \"/api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers\", \"uid\": \"731ca9a9-332f-4999-821d-adfea2e1d2d4\", \"resourceVersion\": \"24921\", \"creationTimestamp\": \"2021-03-12T10:41:35Z\" }, \"ephemeralContainers\": [ { \"name\": \"debug\", \"image\": \"openjdk:11\", \"command\": [ \"bin/sh\" ], \"imagePullPolicy\": \"IfNotPresent\", \"terminationMessagePolicy\":\"File\", \"stdin\": true, \"tty\": true, \"volumeMounts\": [ { \"mountPath\": \"/coherence-operator/jvm\", \"name\": \"jvm\" } ] } ] } We add an ephemeral container named debug . The name can be anything as long as it is unique in the Pod. We specify that the image used for the container is openjdk:11 Specify /bin/sh as the container entry point so that we get a command line shell We must specify an image pull policy We want an interactive container, so we specify stdin and tty We create the same volume mount to /coherence-operator/jvm that the Coherence container has. We can now re-apply the json to add the new ephemeral container using the kubectl replace --raw command to the same URL path we used for the get command above, this time using -f ec.json to specify the json we want to replace. <markup lang=\"bash\" >kubectl replace --raw /api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers -f ec.json After executing the above command the ephemeral container should have been created, we can now attach to it. ",
            "title": "Create an Ephemeral Container"
        },
        {
            "location": "/docs/troubleshooting/02_heap_dump",
            "text": " We now have an ephemeral container named debug in the Pod test-cluster-1 . We need to attach to the container so that we can create the heap dump. <markup lang=\"bash\" >kubectl attach test-cluster-1 -c debug -it -n coherence-test The command above will attach an interactive ( -it ) session to the debug container (specified with -c debug ) in Pod test-cluster-1 , in the namespace coherence-test . Displaying something like this: <markup lang=\"bash\" >If you don't see a command prompt, try pressing enter. # ",
            "title": "Attach to the Ephemeral Container"
        },
        {
            "location": "/docs/troubleshooting/02_heap_dump",
            "text": " We can now generate the heap dump for the Coherence process using jcmd , but first we need to find its PID using jps . <markup lang=\"bash\" >jps -l Which will display something like this: <markup lang=\"bash\" >117 jdk.jcmd/sun.tools.jps.Jps 55 com.oracle.coherence.k8s.Main The main class run by the Operator is com.oracle.coherence.k8s.Main so the PID of the Coherence process is 55 . We can now use jcmd to generate the heap dump. We need to make sure that the heap dump is created in the /coherence-operator/jvm/ directory, as this is shared between both containers. <markup lang=\"bash\" >jcmd 55 GC.heap_dump /coherence-operator/jvm/heap-dump.hprof After running the command above, we will have a heap dump file that we can access from the ephemeral Pod . We have a number of choices about how to get the file out of the Pod and somewhere that we can analyze it. We could use sftp to ship it somewhere, or some tools to copy it to cloud storage or just simply use kubectl cp to copy it. Do not exit out of the ephemeral container session until you have copied the heap dump. The kubectl cp command is in the form kubectl cp &lt;namespace&gt;/&lt;pod&gt;/&lt;file&gt; &lt;local-file&gt; -c &lt;container&gt; . So to use kubectl cp we can execute a command like the following: <markup lang=\"bash\" >kubectl cp coherence-test/test-cluster-1:/coherence-operator/jvm/heap-dump.hprof \\ $(pwd)/heap-dump.hprof -c debug We will now have a file called heap-dump.hprof in the current directory. We can now exit out of the ephemeral container. ",
            "title": "Trigger the Heap Dump"
        },
        {
            "location": "/docs/troubleshooting/02_heap_dump",
            "text": " Ephemeral containers were introduced in Kubernetes v1.16 and moved to beta in v1.23. Ephemeral containers is a feature gate that must be enabled for your cluster. If you have the EphemeralContainers feature gate enabled, then obtaining a heap dump is not so difficult. Enable EphemeralContainers in KinD We use KinD for a lot of our CI builds and testing, enabling the EphemeralContainers feature gate in KinD is very easy. For example, this KinD configuration enables the EphemeralContainers feature gate <markup lang=\"yaml\" >kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 featureGates: EphemeralContainers: true nodes: - role: control-plane - role: worker - role: worker The EphemeralContainers feature gate is set to true Shared Process Namespace In this example we are going to use the jps and jcmd tools to generate the heap dump from an ephemeral container. For this to work the ephemeral container must be able to see the processes running in the coherence container. The Coherence CRD spec has a field named ShareProcessNamespace , which sets the corresponding field in the Coherence Pods that will be created for the deployment. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: shareProcessNamespace: true The shareProcessNamespace must be set to true . If you have some other way to trigger a heap dump to a specific location without requiring the ephemeral container to see the Coherence container processes then the technique below can still be used without setting shareProcessNamespace to true . Create an Ephemeral Container Let&#8217;s say we have a Coherence cluster deployed named test-cluster in a namespace named coherence-test . There will be a number of Pods created for this deployment, named test-cluster-0 , test-cluster-1 and so on. For this example we will obtain a heap dump from Pod test-cluster-1 . The purpose of using an ephemeral container is because the Coherence container we are running does not contain any of the tools and programs we require for debugging, e.g. jps , jcmd etc. The ephemeral container we run obviously needs to have all the required tools. You could create a custom image with what you need in it, but for this example we will use the openjdk:11 image, as it has a full JDK and other tools we need in it. You should obviously use a JDK version that matches the version in the Coherence container. We can use the kubectl debug command that can be used to create an ephemeral containers. For our purposes we cannot use this command as we will require volume mounts to share storage between the ephemeral container and the Coherence container so that the ephemeral container can see the heap dump file. Instead of the kubectl debug command we can create ephemeral containers using the kubectl --raw API. Ephemeral containers are a sub-resource of the Pod API. First obtain the current ephemeral containers sub-resource for the Pod. We do this using the kubectl get --raw command with the URL path in the format /api/v1/namespaces/&lt;namespace&gt;&gt;/pods/&lt;pod&gt;/ephemeralcontainers , where &lt;namespace&gt; is the namespace that the Pod is deployed into and &lt;pod&gt; is the name of the Pod. So in our example the command would be: <markup lang=\"bash\" >kubectl get --raw /api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers Which will output json similar to this, which we will save to a file named ec.json : <markup lang=\"json\" title=\"ec.json\" >{ \"kind\": \"EphemeralContainers\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"test-cluster-1\", \"namespace\": \"coherence-test\", \"selfLink\": \"/api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers\", \"uid\": \"731ca9a9-332f-4999-821d-adfea2e1d2d4\", \"resourceVersion\": \"24921\", \"creationTimestamp\": \"2021-03-12T10:41:35Z\" }, \"ephemeralContainers\": [] } The \"ephemeralContainers\" field is an empty array as we have not created any previous containers. We now need to edit this yaml to define the ephemeral container we want to create. The Pod created by the Operator contains an empty directory volume with a volume mount at /coherence-operator/jvm , which is where the JVM is configured to dump debug information, such as heap dumps. We will create an ephemeral container with the same mount so that the /coherence-operator/jvm directory will be shared between the Coherence container and the ephemeral container. Another thing to note is that the default entrypoint in the openjdk:11 image we are using in this example is JShell. This is obviously not what we want, so we will make sure we specify /bin/sh as the entry point as we want a command line shell. Our edited ec.json file looks like this: <markup lang=\"json\" title=\"ec.json\" >{ \"kind\": \"EphemeralContainers\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"test-cluster-1\", \"namespace\": \"coherence-test\", \"selfLink\": \"/api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers\", \"uid\": \"731ca9a9-332f-4999-821d-adfea2e1d2d4\", \"resourceVersion\": \"24921\", \"creationTimestamp\": \"2021-03-12T10:41:35Z\" }, \"ephemeralContainers\": [ { \"name\": \"debug\", \"image\": \"openjdk:11\", \"command\": [ \"bin/sh\" ], \"imagePullPolicy\": \"IfNotPresent\", \"terminationMessagePolicy\":\"File\", \"stdin\": true, \"tty\": true, \"volumeMounts\": [ { \"mountPath\": \"/coherence-operator/jvm\", \"name\": \"jvm\" } ] } ] } We add an ephemeral container named debug . The name can be anything as long as it is unique in the Pod. We specify that the image used for the container is openjdk:11 Specify /bin/sh as the container entry point so that we get a command line shell We must specify an image pull policy We want an interactive container, so we specify stdin and tty We create the same volume mount to /coherence-operator/jvm that the Coherence container has. We can now re-apply the json to add the new ephemeral container using the kubectl replace --raw command to the same URL path we used for the get command above, this time using -f ec.json to specify the json we want to replace. <markup lang=\"bash\" >kubectl replace --raw /api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers -f ec.json After executing the above command the ephemeral container should have been created, we can now attach to it. Attach to the Ephemeral Container We now have an ephemeral container named debug in the Pod test-cluster-1 . We need to attach to the container so that we can create the heap dump. <markup lang=\"bash\" >kubectl attach test-cluster-1 -c debug -it -n coherence-test The command above will attach an interactive ( -it ) session to the debug container (specified with -c debug ) in Pod test-cluster-1 , in the namespace coherence-test . Displaying something like this: <markup lang=\"bash\" >If you don't see a command prompt, try pressing enter. # Trigger the Heap Dump We can now generate the heap dump for the Coherence process using jcmd , but first we need to find its PID using jps . <markup lang=\"bash\" >jps -l Which will display something like this: <markup lang=\"bash\" >117 jdk.jcmd/sun.tools.jps.Jps 55 com.oracle.coherence.k8s.Main The main class run by the Operator is com.oracle.coherence.k8s.Main so the PID of the Coherence process is 55 . We can now use jcmd to generate the heap dump. We need to make sure that the heap dump is created in the /coherence-operator/jvm/ directory, as this is shared between both containers. <markup lang=\"bash\" >jcmd 55 GC.heap_dump /coherence-operator/jvm/heap-dump.hprof After running the command above, we will have a heap dump file that we can access from the ephemeral Pod . We have a number of choices about how to get the file out of the Pod and somewhere that we can analyze it. We could use sftp to ship it somewhere, or some tools to copy it to cloud storage or just simply use kubectl cp to copy it. Do not exit out of the ephemeral container session until you have copied the heap dump. The kubectl cp command is in the form kubectl cp &lt;namespace&gt;/&lt;pod&gt;/&lt;file&gt; &lt;local-file&gt; -c &lt;container&gt; . So to use kubectl cp we can execute a command like the following: <markup lang=\"bash\" >kubectl cp coherence-test/test-cluster-1:/coherence-operator/jvm/heap-dump.hprof \\ $(pwd)/heap-dump.hprof -c debug We will now have a file called heap-dump.hprof in the current directory. We can now exit out of the ephemeral container. ",
            "title": "Ephemeral Containers"
        },
        {
            "location": "/docs/ports/030_services",
            "text": " As described in the Additional Container Ports documentation, it is possible to expose additional ports on the Coherence container in the Pods of a Coherence resource. The Coherence Operator will create a Service to expose each additional port. By default, the name of the service is the combination of the Coherence resource name and the port name (this can default behaviour can be overridden as shown below in the section). The configuration of the Service can be altered using fields in the port spec&#8217;s service section. For example: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: {} This example exposes a single port named rest on port 8080 . The service section of the port spec is empty so the Operator will use its default behaviour to create a Service in the same namespace with the name test-cluster-rest . ",
            "title": "Configure Services for Ports"
        },
        {
            "location": "/docs/ports/030_services",
            "text": " Sometimes it is useful to use a different name than the default for a Service for a port, for example, when the port is exposing functionality that other applications want to consume on a fixed well know endpoint. To override the generated service name with another name the service.name field can be set. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: name: payments By setting the service.name field the Service for this port will be named payments . The service for the above example would look like this: <markup lang=\"yaml\" >apiVersion: v1 kind: Service metadata: name: payments spec: ports: - name: rest port: 8080 targetPort: rest type: ClusterIP selector: coherenceDeployment: test-cluster coherenceCluster: test-cluster coherenceRole: storage coherenceComponent: coherencePod The Service name is payments instead of test-cluster-rest ",
            "title": "Override the Service Name"
        },
        {
            "location": "/docs/ports/030_services",
            "text": " It is sometimes useful to be able to expose a service on a different port on the Service to that being used by the container. One use-case for this would be where the Coherence deployment is providing a http service where the container exposes the service on port 8080 whereas the Service can use port 80 . For example, using the same example payemnts service above: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: name: payments port: 80 The Service name will be payments The Service port will be 80 This then allows the payments service to be accessed on a simple url of http://payments ",
            "title": "Override the Service Port"
        },
        {
            "location": "/docs/ports/030_services",
            "text": " Sometimes it may be desirable to expose a port on the Coherence container but not have the Operator automatically create a Service to expose the port. For example, maybe the port is to be exposed via some other load balancer service controlled by another system. To disable automatic service creation set the service.enabled field to false . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: ports: - name: rest port: 8080 service: enabled: false With the service.enabled field set to false no Service will be created. ",
            "title": "Disable Service Creation"
        },
        {
            "location": "/docs/ports/030_services",
            "text": " The Coherence resource CRD allows many other settings to be configured on the Service . These fields are identical to the corresponding fields in the Kubernetes Service spec. See the Coherence CRD Service Spec documentation and the Kubernetes Service API reference . ",
            "title": "Other Service Configuration"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " Webhooks in Kubernetes are a cluster resource, not a namespaced scoped resource, so consequently there is typically only a single webhook installed for a given resource type. If the Coherence Operator has been installed as a cluster scoped operator then this is not a problem but if multiple Coherence Operators have been deployed then they could all attempt to install the webhooks and update or overwrite a previous configuration. This might not be an issue if all the operators deployed in a Kubernetes cluster are the same version but different versions could cause issues. This is one of the reasons that it is recommended to install a single cluster scoped Coherence Operator. ",
            "title": "Webhook Scope"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " The Coherence Operator uses Kubernetes dynamic admission control commonly known as defaulting and validating web-hooks. As the name implies, these are used to provide default values for some fields in a Coherence resource and to also validate Coherence resources on creation and update. The operator creates and configures the two web-hooks when it starts. Webhook Scope Webhooks in Kubernetes are a cluster resource, not a namespaced scoped resource, so consequently there is typically only a single webhook installed for a given resource type. If the Coherence Operator has been installed as a cluster scoped operator then this is not a problem but if multiple Coherence Operators have been deployed then they could all attempt to install the webhooks and update or overwrite a previous configuration. This might not be an issue if all the operators deployed in a Kubernetes cluster are the same version but different versions could cause issues. This is one of the reasons that it is recommended to install a single cluster scoped Coherence Operator. ",
            "title": "Operator Web-Hooks"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " This is the default option, the operator will create and manage a set of self-signed certificates. The Operator will update the Secret with its certificates and create the MutatingWebhookConfiguration and ValidatingWebhookConfiguration resources configured to use those certificates. ",
            "title": "Self-Signed Certificates"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " If installing the operator using the manifest yaml file first replace the occurrences of self-signed in the yaml file with cert-manager . For example: <markup lang=\"bash\" >curl -L https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml \\ -o coherence-operator.yaml sed -i s/self-signed/cert-manager/g coherence-operator.yaml kubectl apply -f coherence-operator.yaml Note On MacOS the sed command is slightly different for in-place replacement and requires an empty string after the -i parameter: <markup lang=\"bash\" >sed -i '' s/self-signed/cert-manager/g coherence-operator.yaml ",
            "title": "Install Using Manifest File"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " To set the certificate manager to use when installing the Helm chart, set the webhookCertType value: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=cert-manager coherence-operator \\ coherence/coherence-operator The certificate manager will be set to cert-manager ",
            "title": "Install Using Helm"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " Assuming Kubernetes Cert Manager has been installed in the Kubernetes cluster then to use it for managing the web-hook certificates, the Operator needs to be installed with the CERT_TYPE environment variable set to cert-manager . The Operator will then detect the version of Cert Manager and automatically create the required self-signed Issuer and Certificate resources. Cert Manager will detect these and create the Secret . This may cause the operator Pod to re-start until the Secret has been created. Install Using Manifest File If installing the operator using the manifest yaml file first replace the occurrences of self-signed in the yaml file with cert-manager . For example: <markup lang=\"bash\" >curl -L https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml \\ -o coherence-operator.yaml sed -i s/self-signed/cert-manager/g coherence-operator.yaml kubectl apply -f coherence-operator.yaml Note On MacOS the sed command is slightly different for in-place replacement and requires an empty string after the -i parameter: <markup lang=\"bash\" >sed -i '' s/self-signed/cert-manager/g coherence-operator.yaml Install Using Helm To set the certificate manager to use when installing the Helm chart, set the webhookCertType value: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=cert-manager coherence-operator \\ coherence/coherence-operator The certificate manager will be set to cert-manager ",
            "title": "Cert Manager (Self-Signed)"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " If installing the operator using the manifest yaml file first replace the occurrences of self-signed in the yaml file with cert-manager . For example: <markup lang=\"bash\" >curl -L https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml \\ -o coherence-operator.yaml sed -i s/self-signed/manual/g coherence-operator.yaml kubectl apply -f coherence-operator.yaml Note On MacOS the sed command is slightly different for in-place replacement and requires an empty string after the -i parameter: <markup lang=\"bash\" >sed -i '' s/self-signed/cert-manager/g coherence-operator.yaml ",
            "title": "Install Using Manifest File"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " To configure the operator to use manually managed certificates when installing the Helm chart, set the webhookCertType value. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=manual \\ coherence-operator \\ coherence/coherence-operator The certificate manager will be set to manual and the operator will expect to find a Secret named coherence-webhook-server-cert To use manually managed certificates and store the keys and certs in a different secret, set the secret name using the webhookCertSecret value. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=manual \\ --set webhookCertSecret=operator-certs \\ coherence-operator \\ coherence/coherence-operator The certificate manager will be set to manual The name of the secret is set to operator-certs The Coherence Operator will now expect to find the keys and certs in a Secret named operator-certs in the same namespace that the Operator is deployed into. ",
            "title": "Install Using Helm"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " If certificates will be managed some other way (for example by Cert Manager managing real certificates) then the CERT_TYPE environment variable should be set to manual . A Secret must exist in the namespace the operator will be installed into containing the CA certificate, certificate and key files that the operator will use to configure the web-hook. The files must exist with the names expected by the operator. The default name of the Secret expected by the operator is coherence-webhook-server-cert but this can be changed. The certificates in the Secret must be valid for the Service name that exposes the Coherence web-hook. The default format of the DNS used for the certificate CN (common name) is coherence-operator-webhook.&lt;namespace&gt;.svc where &lt;namespace&gt; is the namespace the operator is installed into. Additional names may also be configured using the different formats of Kubernetes Service DNS names. For example, if the Operator is installed into a namespace named coherence the Service DNS names would be: <markup > - coherence-operator-webhook.coherence - coherence-operator-webhook.coherence.svc - coherence-operator-webhook.coherence.svc.cluster.local An example of the format of the Secret is shown below: <markup lang=\"yaml\" title=\"sh\" >apiVersion: v1 kind: Secret metadata: name: coherence-webhook-server-cert type: Opaque data: ca.crt: ... # &lt;base64 endocde CA certificate file&gt; tls.crt: ... # &lt;base64 endocde certificate file&gt; tls.key: ... # &lt;base64 endocde private key file&gt; Warning If a Secret with the name specified in webhookCertSecret does not exist in the namespace the operator is being installed into then the operator Pod will not start as the Secret will be mounted as a volume in the operator Pod. Install Using Manifest File If installing the operator using the manifest yaml file first replace the occurrences of self-signed in the yaml file with cert-manager . For example: <markup lang=\"bash\" >curl -L https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml \\ -o coherence-operator.yaml sed -i s/self-signed/manual/g coherence-operator.yaml kubectl apply -f coherence-operator.yaml Note On MacOS the sed command is slightly different for in-place replacement and requires an empty string after the -i parameter: <markup lang=\"bash\" >sed -i '' s/self-signed/cert-manager/g coherence-operator.yaml Install Using Helm To configure the operator to use manually managed certificates when installing the Helm chart, set the webhookCertType value. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=manual \\ coherence-operator \\ coherence/coherence-operator The certificate manager will be set to manual and the operator will expect to find a Secret named coherence-webhook-server-cert To use manually managed certificates and store the keys and certs in a different secret, set the secret name using the webhookCertSecret value. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=manual \\ --set webhookCertSecret=operator-certs \\ coherence-operator \\ coherence/coherence-operator The certificate manager will be set to manual The name of the secret is set to operator-certs The Coherence Operator will now expect to find the keys and certs in a Secret named operator-certs in the same namespace that the Operator is deployed into. ",
            "title": "Manual Certificates"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " If installing using the manifest yaml files, then you need to edit the coherence-operator.yaml manifest to add a command line argument to the Operator. Update the controller-manager deployment and add an argument, edit the section that looks like this: <markup lang=\"yaml\" > args: - operator - --enable-leader-election and add the additional --enable-webhook=false argument like this: <markup lang=\"yaml\" > args: - operator - --enable-leader-election - --enable-webhook=false apiVersion: apps/v1 kind: Deployment metadata: name: controller-manager ",
            "title": "Install Using Manifest File"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " If installing the Operator using Helm, the webhooks value can be set to false in the values file or on the command line. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhooks=false \\ coherence-operator \\ coherence/coherence-operator ",
            "title": "Installing Using Helm"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " It is possible to start the Operator without it registering any web-hooks with the API server. Caution Running the Operator without web-hooks is not recommended. The admission web-hooks validate the Coherence resource yaml before it gets into the k8s cluster. Without the web-hooks, invalid yaml will be accepted by k8s and the Operator will then log errors when it tries to reconcile invalid yaml. Or worse, the Operator will create an invalid StatefulSet which will then fail to start. Install Using Manifest File If installing using the manifest yaml files, then you need to edit the coherence-operator.yaml manifest to add a command line argument to the Operator. Update the controller-manager deployment and add an argument, edit the section that looks like this: <markup lang=\"yaml\" > args: - operator - --enable-leader-election and add the additional --enable-webhook=false argument like this: <markup lang=\"yaml\" > args: - operator - --enable-leader-election - --enable-webhook=false apiVersion: apps/v1 kind: Deployment metadata: name: controller-manager Installing Using Helm If installing the Operator using Helm, the webhooks value can be set to false in the values file or on the command line. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhooks=false \\ coherence-operator \\ coherence/coherence-operator ",
            "title": "Install the Operator Without Web-Hooks"
        },
        {
            "location": "/docs/installation/07_webhooks",
            "text": " A web-hook requires certificates to be able to work in Kubernetes. By default, the operator will create and manage self-signed certificates for this purpose. These certificates are created using the Kubernetes certificate It is possible to use other certificates, either managed by the Kubernetes cert-manager or managed manually. The certificates should be stored in a Secret named coherence-webhook-server-cert in the same namespace that the operator has installed in. (although this name can be changed if required). This Secret must exist, or the operator wil fail to start. The Operator Helm chart will create this Secret when the Operator is managing its own self-signed certs, otherwise the Secret must be created manually or by an external certificate manager. Self-Signed Certificates This is the default option, the operator will create and manage a set of self-signed certificates. The Operator will update the Secret with its certificates and create the MutatingWebhookConfiguration and ValidatingWebhookConfiguration resources configured to use those certificates. Cert Manager (Self-Signed) Assuming Kubernetes Cert Manager has been installed in the Kubernetes cluster then to use it for managing the web-hook certificates, the Operator needs to be installed with the CERT_TYPE environment variable set to cert-manager . The Operator will then detect the version of Cert Manager and automatically create the required self-signed Issuer and Certificate resources. Cert Manager will detect these and create the Secret . This may cause the operator Pod to re-start until the Secret has been created. Install Using Manifest File If installing the operator using the manifest yaml file first replace the occurrences of self-signed in the yaml file with cert-manager . For example: <markup lang=\"bash\" >curl -L https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml \\ -o coherence-operator.yaml sed -i s/self-signed/cert-manager/g coherence-operator.yaml kubectl apply -f coherence-operator.yaml Note On MacOS the sed command is slightly different for in-place replacement and requires an empty string after the -i parameter: <markup lang=\"bash\" >sed -i '' s/self-signed/cert-manager/g coherence-operator.yaml Install Using Helm To set the certificate manager to use when installing the Helm chart, set the webhookCertType value: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=cert-manager coherence-operator \\ coherence/coherence-operator The certificate manager will be set to cert-manager Manual Certificates If certificates will be managed some other way (for example by Cert Manager managing real certificates) then the CERT_TYPE environment variable should be set to manual . A Secret must exist in the namespace the operator will be installed into containing the CA certificate, certificate and key files that the operator will use to configure the web-hook. The files must exist with the names expected by the operator. The default name of the Secret expected by the operator is coherence-webhook-server-cert but this can be changed. The certificates in the Secret must be valid for the Service name that exposes the Coherence web-hook. The default format of the DNS used for the certificate CN (common name) is coherence-operator-webhook.&lt;namespace&gt;.svc where &lt;namespace&gt; is the namespace the operator is installed into. Additional names may also be configured using the different formats of Kubernetes Service DNS names. For example, if the Operator is installed into a namespace named coherence the Service DNS names would be: <markup > - coherence-operator-webhook.coherence - coherence-operator-webhook.coherence.svc - coherence-operator-webhook.coherence.svc.cluster.local An example of the format of the Secret is shown below: <markup lang=\"yaml\" title=\"sh\" >apiVersion: v1 kind: Secret metadata: name: coherence-webhook-server-cert type: Opaque data: ca.crt: ... # &lt;base64 endocde CA certificate file&gt; tls.crt: ... # &lt;base64 endocde certificate file&gt; tls.key: ... # &lt;base64 endocde private key file&gt; Warning If a Secret with the name specified in webhookCertSecret does not exist in the namespace the operator is being installed into then the operator Pod will not start as the Secret will be mounted as a volume in the operator Pod. Install Using Manifest File If installing the operator using the manifest yaml file first replace the occurrences of self-signed in the yaml file with cert-manager . For example: <markup lang=\"bash\" >curl -L https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml \\ -o coherence-operator.yaml sed -i s/self-signed/manual/g coherence-operator.yaml kubectl apply -f coherence-operator.yaml Note On MacOS the sed command is slightly different for in-place replacement and requires an empty string after the -i parameter: <markup lang=\"bash\" >sed -i '' s/self-signed/cert-manager/g coherence-operator.yaml Install Using Helm To configure the operator to use manually managed certificates when installing the Helm chart, set the webhookCertType value. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=manual \\ coherence-operator \\ coherence/coherence-operator The certificate manager will be set to manual and the operator will expect to find a Secret named coherence-webhook-server-cert To use manually managed certificates and store the keys and certs in a different secret, set the secret name using the webhookCertSecret value. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhookCertType=manual \\ --set webhookCertSecret=operator-certs \\ coherence-operator \\ coherence/coherence-operator The certificate manager will be set to manual The name of the secret is set to operator-certs The Coherence Operator will now expect to find the keys and certs in a Secret named operator-certs in the same namespace that the Operator is deployed into. Install the Operator Without Web-Hooks It is possible to start the Operator without it registering any web-hooks with the API server. Caution Running the Operator without web-hooks is not recommended. The admission web-hooks validate the Coherence resource yaml before it gets into the k8s cluster. Without the web-hooks, invalid yaml will be accepted by k8s and the Operator will then log errors when it tries to reconcile invalid yaml. Or worse, the Operator will create an invalid StatefulSet which will then fail to start. Install Using Manifest File If installing using the manifest yaml files, then you need to edit the coherence-operator.yaml manifest to add a command line argument to the Operator. Update the controller-manager deployment and add an argument, edit the section that looks like this: <markup lang=\"yaml\" > args: - operator - --enable-leader-election and add the additional --enable-webhook=false argument like this: <markup lang=\"yaml\" > args: - operator - --enable-leader-election - --enable-webhook=false apiVersion: apps/v1 kind: Deployment metadata: name: controller-manager Installing Using Helm If installing the Operator using Helm, the webhooks value can be set to false in the values file or on the command line. <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --set webhooks=false \\ coherence-operator \\ coherence/coherence-operator ",
            "title": "Manage Web-Hook Certificates"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " The Coherence Operator provides detailed Grafana dashboards to provide insight into your running Coherence Clusters. ",
            "title": "preambule"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Note: The Grafana dashboards require Coherence metrics, which is available only when using Coherence version 12.2.1.4 or above. ",
            "title": "Coherence Grafana Dashboards"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Navigation Dashboards Coherence Dashboard Main Members Summary &amp; Details Dashboards Services Summary &amp; Details Dashboards Caches Summary &amp; Detail Dashboards Proxy Servers Summary &amp; Detail Dashboards Persistence Summary Dashboard Federation Summary &amp; Details Dashboards Machines Summary Dashboard HTTP Servers Summary Dashboard Elastic Data Summary Dashboard Executors Summary &amp; Details Dashboards ",
            "title": "Table of Contents"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Allows for selection of information to be displayed where there is more than one item. Cluster Name - Allows selection of the cluster to view metrics for Top N Limit - Limits the display of Top values for tables that support it Service Name, Member Name, Cache Name - These will appear on various dashboards See the Grafana Documentation for more information on Variables. ",
            "title": "Variables"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Vertical red lines on a graph to indicate a change in a key markers such as: Show Cluster Size Changes - Displays when the cluster size has changed Show Partition Transfers - Displays when partition transfers have occurred See the Grafana Documentation for more information on Annotations. ",
            "title": "Annotations"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Select Dashboard - In the top right a drop down list of dashboards is available selection Drill Through - Ability to drill through based upon service, member, node, etc. ",
            "title": "Navigation"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " The pre-loaded Coherence Dashboards provide a number of common features and navigation capabilities that appear at the top of most dashboards. Variables Allows for selection of information to be displayed where there is more than one item. Cluster Name - Allows selection of the cluster to view metrics for Top N Limit - Limits the display of Top values for tables that support it Service Name, Member Name, Cache Name - These will appear on various dashboards See the Grafana Documentation for more information on Variables. Annotations Vertical red lines on a graph to indicate a change in a key markers such as: Show Cluster Size Changes - Displays when the cluster size has changed Show Partition Transfers - Displays when partition transfers have occurred See the Grafana Documentation for more information on Annotations. Navigation Select Dashboard - In the top right a drop down list of dashboards is available selection Drill Through - Ability to drill through based upon service, member, node, etc. ",
            "title": "Navigation"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows a high-level overview of the selected Coherence cluster including metrics such as: Cluster member count, services, memory and health Top N loaded members, Top N heap usage and GC activity Service backlogs and endangered or vulnerable services Top query times, non-optimized queries Guardian recoveries and terminations ",
            "title": "1. Coherence Dashboard Main"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Members Summary"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Member Details"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows an overview of all cluster members that are enabled for metrics capture including metrics such as: Member list include heap usage Top N members for GC time and count Total GC collection count and time by Member Publisher and Receiver success rates Guardian recoveries and send queue size Members Summary Member Details ",
            "title": "2. Members Summary &amp; Details Dashboards"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Services Summary"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Service Details"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows an overview of all cluster services including metrics such as: Service members for storage and non-storage services Service task count StatusHA values as well as endangered, vulnerable and unbalanced partitions Top N services by task count and backlog Task rates, request pending counts and task and request averages Services Summary Service Details ",
            "title": "3. Services Summary &amp; Details Dashboards"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Caches Summary"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Cache Details"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows an overview of all caches including metrics such as: Cache entries, memory and index usage Cache access counts including gets, puts and removed, max query times Front cache hit and miss rates Caches Summary Cache Details ",
            "title": "4. Caches Summary &amp; Detail Dashboards"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Proxy Servers Summary"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Proxy Servers Detail"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows and overview of Proxy servers including metrics such as: Active connection count and service member count Total messages sent/ received Proxy server data rates Individual connection details abd byte backlogs Proxy Servers Summary Proxy Servers Detail ",
            "title": "5. Proxy Servers Summary &amp; Detail Dashboards"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows and overview of Persistence including metrics such as: Persistence enabled services Maximum active persistence latency Active space total usage and by service ",
            "title": "6. Persistence Summary Dashboard"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Federation Summary"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Federation Details"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows overview of Federation including metrics such as: Destination and Origins details Entries, records and bytes send and received Federation Summary Federation Details ",
            "title": "7. Federation Summary &amp; Details Dashboards"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows an overview of all machines that make up the Kubernetes cluster underlying the Coherence cluster including metrics such as: Machine processors, free swap space and physical memory Load averages ",
            "title": "8. Machines Summary Dashboard"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows an overview of all HTTP Servers running in the cluster including metrics such as: Service member count, requests, error count and average request time HTTP Request rates and response codes ",
            "title": "9. HTTP Servers Summary Dashboard"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows an overview of all HTTP Servers running in the cluster including metrics such as: RAM and Flash journal files in use RAM and Flash compactions ",
            "title": "10. Elastic Data Summary Dashboard"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Executors Summary"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " ",
            "title": "Executor Details"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " Shows an overview of all Executors running in the cluster including metrics such as: Tasks in Progress Completed and Rejected Tasks Individual Executor status Executors Summary Executor Details ",
            "title": "11. Executors Summary &amp; Details Dashboards"
        },
        {
            "location": "/docs/metrics/040_dashboards",
            "text": " 1. Coherence Dashboard Main Shows a high-level overview of the selected Coherence cluster including metrics such as: Cluster member count, services, memory and health Top N loaded members, Top N heap usage and GC activity Service backlogs and endangered or vulnerable services Top query times, non-optimized queries Guardian recoveries and terminations 2. Members Summary &amp; Details Dashboards Shows an overview of all cluster members that are enabled for metrics capture including metrics such as: Member list include heap usage Top N members for GC time and count Total GC collection count and time by Member Publisher and Receiver success rates Guardian recoveries and send queue size Members Summary Member Details 3. Services Summary &amp; Details Dashboards Shows an overview of all cluster services including metrics such as: Service members for storage and non-storage services Service task count StatusHA values as well as endangered, vulnerable and unbalanced partitions Top N services by task count and backlog Task rates, request pending counts and task and request averages Services Summary Service Details 4. Caches Summary &amp; Detail Dashboards Shows an overview of all caches including metrics such as: Cache entries, memory and index usage Cache access counts including gets, puts and removed, max query times Front cache hit and miss rates Caches Summary Cache Details 5. Proxy Servers Summary &amp; Detail Dashboards Shows and overview of Proxy servers including metrics such as: Active connection count and service member count Total messages sent/ received Proxy server data rates Individual connection details abd byte backlogs Proxy Servers Summary Proxy Servers Detail 6. Persistence Summary Dashboard Shows and overview of Persistence including metrics such as: Persistence enabled services Maximum active persistence latency Active space total usage and by service 7. Federation Summary &amp; Details Dashboards Shows overview of Federation including metrics such as: Destination and Origins details Entries, records and bytes send and received Federation Summary Federation Details 8. Machines Summary Dashboard Shows an overview of all machines that make up the Kubernetes cluster underlying the Coherence cluster including metrics such as: Machine processors, free swap space and physical memory Load averages 9. HTTP Servers Summary Dashboard Shows an overview of all HTTP Servers running in the cluster including metrics such as: Service member count, requests, error count and average request time HTTP Request rates and response codes 10. Elastic Data Summary Dashboard Shows an overview of all HTTP Servers running in the cluster including metrics such as: RAM and Flash journal files in use RAM and Flash compactions 11. Executors Summary &amp; Details Dashboards Shows an overview of all Executors running in the cluster including metrics such as: Tasks in Progress Completed and Rejected Tasks Individual Executor status Executors Summary Executor Details ",
            "title": "Dashboards"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " This example contains the most basic Helm chart possible to support managing a Coherence resource locate in the chart/ directory. The chart is actually completely generic and would support any configuration of Coherence resource. The values file contains a single value spec , which will contain the entire spec of the Coherence resource. <markup lang=\"yaml\" title=\"chart/values.yaml\" >spec: There is a single template file, as we only create a single Coherence resource. test-cluster.yaml apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: {{ .Release.Name }} namespace: {{ .Release.Namespace }} labels: {{- include \"coherence-labels\" . | indent 4 }} spec: {{- if .Values.spec }} {{ toYaml .Values.spec | indent 2 }} {{- end }} content_copy Copied The first part of the template is fairly standard for a Helm chart, we configure the resource name, namespace and add some labels. The generic nature of the chart comes from the fact that the template then just takes the whole spec value from the values file, and if it is not null converts it to yaml under the spec: section of the template. This means that any yaml that is valid in a Coherence CRD spec section can be used in a values file (or with --set arguments) when installing the chart. ",
            "title": "A Simple Generic Helm Chart"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " To start with we will do a simple dry-run install that will display the yaml Helm would have created if the install command had been real. <markup lang=\"bash\" >helm install test ./chart --dry-run The above command should result in the following output <markup >NAME: test LAST DEPLOYED: Sat Aug 28 16:30:53 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: coherence-example/templates/coherence.yaml apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: We can see at the bottom of the output the simple Coherence resource that would have been created by helm. This is a valid Coherence resource because every field in the spec section is optional. If the install had been real this would have resulted in a Coherence cluster named \"test\" with three storage enabled cluster members, as the default replica count is three. ",
            "title": "A Simple Dry Run"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " But how do we set other values in the Coherence resouce. That is simple because Helm does not validate what we enter as values we can either create a values file with anything we like under the spec secion or we can specify values using the --set Helm argument. For example, if we wanted to set the replica count to six in a Coherence resource we would need to set the spec.replicas field to 6 , and we do exactly the same in the helm chart. We could create a values file like this: <markup title=\"test-values.yaml\" >spec: replicas: 6 Which we can install with <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml Which would produce a Coherence resource like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: replicas: 6 We could have done the same thing using --set , for example: <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml --set spec.replicas=6 We can even set more deeply nested values, for example the Coherence log level is set in the spec.coherence.logLevel field of the Coherence CRD so we can use the same value in the Helm install command or values file: <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml --set spec.coherence.logLevel=9 Which would produce the following Coherence resource: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: coherence: logLevel: 9 Just like any Helm chart, whether you use --set arguments or use a values file depends on how complex the resulting yaml will be. Some fields of the Coherence CRD spec would be impractical to try to configure on the command line with --set and would be much simpler in the values file. ",
            "title": "Setting Values"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " Installing the example Helm chart is as simple as any other chart. One difference here being that the chart is not installed into a chart repository so has to be installed from the char/ directory. If you wanted to you could The following commands are all run from the examples/helm directory so that the chart location is specified as ./chart . You can run the commands from anywhere, but you would need to specify the full path to the example chart directory. A Simple Dry Run To start with we will do a simple dry-run install that will display the yaml Helm would have created if the install command had been real. <markup lang=\"bash\" >helm install test ./chart --dry-run The above command should result in the following output <markup >NAME: test LAST DEPLOYED: Sat Aug 28 16:30:53 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: coherence-example/templates/coherence.yaml apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: We can see at the bottom of the output the simple Coherence resource that would have been created by helm. This is a valid Coherence resource because every field in the spec section is optional. If the install had been real this would have resulted in a Coherence cluster named \"test\" with three storage enabled cluster members, as the default replica count is three. Setting Values But how do we set other values in the Coherence resouce. That is simple because Helm does not validate what we enter as values we can either create a values file with anything we like under the spec secion or we can specify values using the --set Helm argument. For example, if we wanted to set the replica count to six in a Coherence resource we would need to set the spec.replicas field to 6 , and we do exactly the same in the helm chart. We could create a values file like this: <markup title=\"test-values.yaml\" >spec: replicas: 6 Which we can install with <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml Which would produce a Coherence resource like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: replicas: 6 We could have done the same thing using --set , for example: <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml --set spec.replicas=6 We can even set more deeply nested values, for example the Coherence log level is set in the spec.coherence.logLevel field of the Coherence CRD so we can use the same value in the Helm install command or values file: <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml --set spec.coherence.logLevel=9 Which would produce the following Coherence resource: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: coherence: logLevel: 9 Just like any Helm chart, whether you use --set arguments or use a values file depends on how complex the resulting yaml will be. Some fields of the Coherence CRD spec would be impractical to try to configure on the command line with --set and would be much simpler in the values file. ",
            "title": "Installing the Chart"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " The Coherence Operator has two images, the Operator itself and a second image containing an executable named runner which the Operator uses to run Coherence servers in the Pods it is managing. One of the other commands that the runner can execute is a status command, which queries the Operator for the current status of a Coherence resource. If you pull the image and execute it you can see the help text for the runner CLI. The following commands will pull the Operator utils image and run it to display the help fot eh status command: <markup lang=\"bash\" >docker pull ghcr.io/oracle/coherence-operator:3.3.4 docker run -it --rm ghcr.io/oracle/coherence-operator:3.3.4 status -h By creating a K8s Job that runs the status command we can query the Operator for the status of the Coherence resource we installed from the Helm chart. Of course, we could have written something similar that used kubectl in the Job or similar to query k8s for the state of the Coherence resource, but this becomes more complex in RBAC enabled cluster. Querying the simple REST endpoint of the Coherence Operator does not require RBAC rules for the Job to execute. To run a simple status check we are only interested in the following parameters for the status command: Argument Description --operator-url The Coherence Operator URL, typically the operator&#8217;s REST service (default \"http://coherence-operator-rest.coherence.svc.local:8000\" --namespace The namespace the Coherence resource is deployed into. This will be the namespace our Helm chart was installed into. --name The name of the Coherence resource. This will be the name from the Helm chart install --timeout The maximum amount of time to wait for the Coherence resource to reach the required condition (default 5m0s) --interval The status check re-try interval (default 10s) First we can add a few additional default values to our Helm chart values file that will be sensible defaults to pass to the hook Job. <markup lang=\"yaml\" title=\"chart/values.yaml\" >spec: operator: namespace: coherence service: coherence-operator-rest port: 8000 image: ghcr.io/oracle/coherence-operator-utils:3.3.4 condition: Ready timeout: 5m interval: 10s We have added an operator section to isolate the values for the hook from the spec values used in our Coherence resource. We can now create the hook template in our Helm chart using the new values in the values file. chart/templates/hook.yaml apiVersion: batch/v1 kind: Job metadata: name: \"{{ .Release.Name }}-helm-hook\" namespace: {{ .Release.Namespace }} annotations: \"helm.sh/hook\": post-install,post-upgrade \"helm.sh/hook-delete-policy\": hook-succeeded spec: template: metadata: name: \"{{ .Release.Name }}-helm-hook\" spec: restartPolicy: Never containers: - name: post-install-job image: {{ .Values.operator.image }} command: - \"/files/runner\" - \"status\" - \"--namespace\" - {{ .Release.Namespace | quote }} - \"--name\" - {{ .Release.Name | quote }} - \"--operator-url\" - \"http://{{ .Values.operator.service | default \"coherence-operator-rest\" }}.{{ .Values.operator.namespace | default \"coherence\" }}.svc:{{ .Values.operator.port | default 8000 }}\" - \"--condition\" - {{ .Values.operator.condition | default \"Ready\" | quote }} - \"--timeout\" - {{ .Values.operator.timeout | default \"5m\" | quote }} - \"--interval\" - {{ .Values.operator.interval | default \"10s\" | quote }} content_copy Copied The annotations section is what tells Helm that this is a hook resource: <markup lang=\"yaml\" > annotations: \"helm.sh/hook\": post-install,post-upgrade \"helm.sh/hook-delete-policy\": hook-succeeded We define the hook as a post-install and post-update hook, so that it runs on both install and update of the Coherence resource. The hook job will also be deleted once it has successfully run. It will not be deleted if it fails, so we can look at the output of the failure in the Jon Pod logs. ",
            "title": "The Coherence Operator Utils Runner"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " If we repeat the Helm install command to install a Coherence resource with the hook in the chart we should see Helm wait and not complete until the Coherence resource (and by inference the StatefulSet and Pods) are all ready. <markup lang=\"bash\" >helm install test ./chart If we were installing a large Coherence cluster, or doing a Helm upgrade, which results in a rolling upgrade of the Coherence cluster, this could take a lot longer than the default timeout we used of 5 minutes. We can alter the timeout and re-try interval using --set arguments. <markup lang=\"bash\" >helm install test ./chart --set operator.timeout=20m --set operator.interval=1m In the above command the timeout is now 20 minutes and the status check will re-try every one minute. ",
            "title": "Installing with the Hook"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " Sometime we might want to install the chart and not wait for everything to be ready. We can use the Helm --no-hooks argument to skip hook execution. <markup lang=\"bash\" >helm install test ./chart --no-hooks Now the Helm install command will return as soon as the Coherence resource has been created. ",
            "title": "Skipping Hooks"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " We saw above how a custom post-install and post-update hook could be used to work aroud the restrictions of Helm&#8217;s --wait argument. Of course there are other hooks available in Helm that the method above could be used in. For example, say I had a front end application to be deployed using a Helm chart, but I did not want Helm to start the deployment until the Coherence back-end was ready, I could use the same method above in a pre-install hook. ",
            "title": "Other Helm Hooks"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " The Helm install command (and update command) have a --wait argument that tells Helm to wait until the installed resources are ready. This can be very useful if you want to ensure that everything is created and running correctly after and install or upgrade. If you read the help test for the --wait argument you will see the following: The limitation should be obvious, Helm can only wait for a sub-set of al the possible resources that you can create from a Helm chart. It has no idea how to wait for a Coherence resource to be ready. To work around this limitation we can use a Helm chart hook , mre specifically a post-install and post-upgrade hook. A hook is typically a k8s Job that Helm will execute, you create the Job spec as part of the Helm chart templates. The Coherence Operator Utils Runner The Coherence Operator has two images, the Operator itself and a second image containing an executable named runner which the Operator uses to run Coherence servers in the Pods it is managing. One of the other commands that the runner can execute is a status command, which queries the Operator for the current status of a Coherence resource. If you pull the image and execute it you can see the help text for the runner CLI. The following commands will pull the Operator utils image and run it to display the help fot eh status command: <markup lang=\"bash\" >docker pull ghcr.io/oracle/coherence-operator:3.3.4 docker run -it --rm ghcr.io/oracle/coherence-operator:3.3.4 status -h By creating a K8s Job that runs the status command we can query the Operator for the status of the Coherence resource we installed from the Helm chart. Of course, we could have written something similar that used kubectl in the Job or similar to query k8s for the state of the Coherence resource, but this becomes more complex in RBAC enabled cluster. Querying the simple REST endpoint of the Coherence Operator does not require RBAC rules for the Job to execute. To run a simple status check we are only interested in the following parameters for the status command: Argument Description --operator-url The Coherence Operator URL, typically the operator&#8217;s REST service (default \"http://coherence-operator-rest.coherence.svc.local:8000\" --namespace The namespace the Coherence resource is deployed into. This will be the namespace our Helm chart was installed into. --name The name of the Coherence resource. This will be the name from the Helm chart install --timeout The maximum amount of time to wait for the Coherence resource to reach the required condition (default 5m0s) --interval The status check re-try interval (default 10s) First we can add a few additional default values to our Helm chart values file that will be sensible defaults to pass to the hook Job. <markup lang=\"yaml\" title=\"chart/values.yaml\" >spec: operator: namespace: coherence service: coherence-operator-rest port: 8000 image: ghcr.io/oracle/coherence-operator-utils:3.3.4 condition: Ready timeout: 5m interval: 10s We have added an operator section to isolate the values for the hook from the spec values used in our Coherence resource. We can now create the hook template in our Helm chart using the new values in the values file. chart/templates/hook.yaml apiVersion: batch/v1 kind: Job metadata: name: \"{{ .Release.Name }}-helm-hook\" namespace: {{ .Release.Namespace }} annotations: \"helm.sh/hook\": post-install,post-upgrade \"helm.sh/hook-delete-policy\": hook-succeeded spec: template: metadata: name: \"{{ .Release.Name }}-helm-hook\" spec: restartPolicy: Never containers: - name: post-install-job image: {{ .Values.operator.image }} command: - \"/files/runner\" - \"status\" - \"--namespace\" - {{ .Release.Namespace | quote }} - \"--name\" - {{ .Release.Name | quote }} - \"--operator-url\" - \"http://{{ .Values.operator.service | default \"coherence-operator-rest\" }}.{{ .Values.operator.namespace | default \"coherence\" }}.svc:{{ .Values.operator.port | default 8000 }}\" - \"--condition\" - {{ .Values.operator.condition | default \"Ready\" | quote }} - \"--timeout\" - {{ .Values.operator.timeout | default \"5m\" | quote }} - \"--interval\" - {{ .Values.operator.interval | default \"10s\" | quote }} content_copy Copied The annotations section is what tells Helm that this is a hook resource: <markup lang=\"yaml\" > annotations: \"helm.sh/hook\": post-install,post-upgrade \"helm.sh/hook-delete-policy\": hook-succeeded We define the hook as a post-install and post-update hook, so that it runs on both install and update of the Coherence resource. The hook job will also be deleted once it has successfully run. It will not be deleted if it fails, so we can look at the output of the failure in the Jon Pod logs. Installing with the Hook If we repeat the Helm install command to install a Coherence resource with the hook in the chart we should see Helm wait and not complete until the Coherence resource (and by inference the StatefulSet and Pods) are all ready. <markup lang=\"bash\" >helm install test ./chart If we were installing a large Coherence cluster, or doing a Helm upgrade, which results in a rolling upgrade of the Coherence cluster, this could take a lot longer than the default timeout we used of 5 minutes. We can alter the timeout and re-try interval using --set arguments. <markup lang=\"bash\" >helm install test ./chart --set operator.timeout=20m --set operator.interval=1m In the above command the timeout is now 20 minutes and the status check will re-try every one minute. Skipping Hooks Sometime we might want to install the chart and not wait for everything to be ready. We can use the Helm --no-hooks argument to skip hook execution. <markup lang=\"bash\" >helm install test ./chart --no-hooks Now the Helm install command will return as soon as the Coherence resource has been created. Other Helm Hooks We saw above how a custom post-install and post-update hook could be used to work aroud the restrictions of Helm&#8217;s --wait argument. Of course there are other hooks available in Helm that the method above could be used in. For example, say I had a front end application to be deployed using a Helm chart, but I did not want Helm to start the deployment until the Coherence back-end was ready, I could use the same method above in a pre-install hook. ",
            "title": "Helm Wait - Waiting for the Install to Complete"
        },
        {
            "location": "/examples/300_helm/README",
            "text": " Occasionally there is a requirement to manage Coherence resources using Helm instead of Kubernetes tools such as kubectl . There is no Helm chart for a Coherence resource as it is a single resource and any Helm chart and values file would need to replicate the entire Coherence CRD if it was to be of generic enough use for everyone. For this reason, anyone wanting to manage Coherence resource using Helm will need to create their own chart, which can then be specific to their needs. This example shows some ways that Helm can be used to manage Coherence resources. Tip The complete source code for this example is in the Coherence Operator GitHub repository. A Simple Generic Helm Chart This example contains the most basic Helm chart possible to support managing a Coherence resource locate in the chart/ directory. The chart is actually completely generic and would support any configuration of Coherence resource. The values file contains a single value spec , which will contain the entire spec of the Coherence resource. <markup lang=\"yaml\" title=\"chart/values.yaml\" >spec: There is a single template file, as we only create a single Coherence resource. test-cluster.yaml apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: {{ .Release.Name }} namespace: {{ .Release.Namespace }} labels: {{- include \"coherence-labels\" . | indent 4 }} spec: {{- if .Values.spec }} {{ toYaml .Values.spec | indent 2 }} {{- end }} content_copy Copied The first part of the template is fairly standard for a Helm chart, we configure the resource name, namespace and add some labels. The generic nature of the chart comes from the fact that the template then just takes the whole spec value from the values file, and if it is not null converts it to yaml under the spec: section of the template. This means that any yaml that is valid in a Coherence CRD spec section can be used in a values file (or with --set arguments) when installing the chart. Installing the Chart Installing the example Helm chart is as simple as any other chart. One difference here being that the chart is not installed into a chart repository so has to be installed from the char/ directory. If you wanted to you could The following commands are all run from the examples/helm directory so that the chart location is specified as ./chart . You can run the commands from anywhere, but you would need to specify the full path to the example chart directory. A Simple Dry Run To start with we will do a simple dry-run install that will display the yaml Helm would have created if the install command had been real. <markup lang=\"bash\" >helm install test ./chart --dry-run The above command should result in the following output <markup >NAME: test LAST DEPLOYED: Sat Aug 28 16:30:53 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: coherence-example/templates/coherence.yaml apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: We can see at the bottom of the output the simple Coherence resource that would have been created by helm. This is a valid Coherence resource because every field in the spec section is optional. If the install had been real this would have resulted in a Coherence cluster named \"test\" with three storage enabled cluster members, as the default replica count is three. Setting Values But how do we set other values in the Coherence resouce. That is simple because Helm does not validate what we enter as values we can either create a values file with anything we like under the spec secion or we can specify values using the --set Helm argument. For example, if we wanted to set the replica count to six in a Coherence resource we would need to set the spec.replicas field to 6 , and we do exactly the same in the helm chart. We could create a values file like this: <markup title=\"test-values.yaml\" >spec: replicas: 6 Which we can install with <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml Which would produce a Coherence resource like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: replicas: 6 We could have done the same thing using --set , for example: <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml --set spec.replicas=6 We can even set more deeply nested values, for example the Coherence log level is set in the spec.coherence.logLevel field of the Coherence CRD so we can use the same value in the Helm install command or values file: <markup lang=\"bash\" >helm install test ./chart -f test-values.yaml --set spec.coherence.logLevel=9 Which would produce the following Coherence resource: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test namespace: default labels: app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: test app.kubernetes.io/version: \"1.0.0\" spec: coherence: logLevel: 9 Just like any Helm chart, whether you use --set arguments or use a values file depends on how complex the resulting yaml will be. Some fields of the Coherence CRD spec would be impractical to try to configure on the command line with --set and would be much simpler in the values file. Helm Wait - Waiting for the Install to Complete The Helm install command (and update command) have a --wait argument that tells Helm to wait until the installed resources are ready. This can be very useful if you want to ensure that everything is created and running correctly after and install or upgrade. If you read the help test for the --wait argument you will see the following: The limitation should be obvious, Helm can only wait for a sub-set of al the possible resources that you can create from a Helm chart. It has no idea how to wait for a Coherence resource to be ready. To work around this limitation we can use a Helm chart hook , mre specifically a post-install and post-upgrade hook. A hook is typically a k8s Job that Helm will execute, you create the Job spec as part of the Helm chart templates. The Coherence Operator Utils Runner The Coherence Operator has two images, the Operator itself and a second image containing an executable named runner which the Operator uses to run Coherence servers in the Pods it is managing. One of the other commands that the runner can execute is a status command, which queries the Operator for the current status of a Coherence resource. If you pull the image and execute it you can see the help text for the runner CLI. The following commands will pull the Operator utils image and run it to display the help fot eh status command: <markup lang=\"bash\" >docker pull ghcr.io/oracle/coherence-operator:3.3.4 docker run -it --rm ghcr.io/oracle/coherence-operator:3.3.4 status -h By creating a K8s Job that runs the status command we can query the Operator for the status of the Coherence resource we installed from the Helm chart. Of course, we could have written something similar that used kubectl in the Job or similar to query k8s for the state of the Coherence resource, but this becomes more complex in RBAC enabled cluster. Querying the simple REST endpoint of the Coherence Operator does not require RBAC rules for the Job to execute. To run a simple status check we are only interested in the following parameters for the status command: Argument Description --operator-url The Coherence Operator URL, typically the operator&#8217;s REST service (default \"http://coherence-operator-rest.coherence.svc.local:8000\" --namespace The namespace the Coherence resource is deployed into. This will be the namespace our Helm chart was installed into. --name The name of the Coherence resource. This will be the name from the Helm chart install --timeout The maximum amount of time to wait for the Coherence resource to reach the required condition (default 5m0s) --interval The status check re-try interval (default 10s) First we can add a few additional default values to our Helm chart values file that will be sensible defaults to pass to the hook Job. <markup lang=\"yaml\" title=\"chart/values.yaml\" >spec: operator: namespace: coherence service: coherence-operator-rest port: 8000 image: ghcr.io/oracle/coherence-operator-utils:3.3.4 condition: Ready timeout: 5m interval: 10s We have added an operator section to isolate the values for the hook from the spec values used in our Coherence resource. We can now create the hook template in our Helm chart using the new values in the values file. chart/templates/hook.yaml apiVersion: batch/v1 kind: Job metadata: name: \"{{ .Release.Name }}-helm-hook\" namespace: {{ .Release.Namespace }} annotations: \"helm.sh/hook\": post-install,post-upgrade \"helm.sh/hook-delete-policy\": hook-succeeded spec: template: metadata: name: \"{{ .Release.Name }}-helm-hook\" spec: restartPolicy: Never containers: - name: post-install-job image: {{ .Values.operator.image }} command: - \"/files/runner\" - \"status\" - \"--namespace\" - {{ .Release.Namespace | quote }} - \"--name\" - {{ .Release.Name | quote }} - \"--operator-url\" - \"http://{{ .Values.operator.service | default \"coherence-operator-rest\" }}.{{ .Values.operator.namespace | default \"coherence\" }}.svc:{{ .Values.operator.port | default 8000 }}\" - \"--condition\" - {{ .Values.operator.condition | default \"Ready\" | quote }} - \"--timeout\" - {{ .Values.operator.timeout | default \"5m\" | quote }} - \"--interval\" - {{ .Values.operator.interval | default \"10s\" | quote }} content_copy Copied The annotations section is what tells Helm that this is a hook resource: <markup lang=\"yaml\" > annotations: \"helm.sh/hook\": post-install,post-upgrade \"helm.sh/hook-delete-policy\": hook-succeeded We define the hook as a post-install and post-update hook, so that it runs on both install and update of the Coherence resource. The hook job will also be deleted once it has successfully run. It will not be deleted if it fails, so we can look at the output of the failure in the Jon Pod logs. Installing with the Hook If we repeat the Helm install command to install a Coherence resource with the hook in the chart we should see Helm wait and not complete until the Coherence resource (and by inference the StatefulSet and Pods) are all ready. <markup lang=\"bash\" >helm install test ./chart If we were installing a large Coherence cluster, or doing a Helm upgrade, which results in a rolling upgrade of the Coherence cluster, this could take a lot longer than the default timeout we used of 5 minutes. We can alter the timeout and re-try interval using --set arguments. <markup lang=\"bash\" >helm install test ./chart --set operator.timeout=20m --set operator.interval=1m In the above command the timeout is now 20 minutes and the status check will re-try every one minute. Skipping Hooks Sometime we might want to install the chart and not wait for everything to be ready. We can use the Helm --no-hooks argument to skip hook execution. <markup lang=\"bash\" >helm install test ./chart --no-hooks Now the Helm install command will return as soon as the Coherence resource has been created. Other Helm Hooks We saw above how a custom post-install and post-update hook could be used to work aroud the restrictions of Helm&#8217;s --wait argument. Of course there are other hooks available in Helm that the method above could be used in. For example, say I had a front end application to be deployed using a Helm chart, but I did not want Helm to start the deployment until the Coherence back-end was ready, I could use the same method above in a pre-install hook. ",
            "title": "Manage Coherence Resources using Helm"
        },
        {
            "location": "/examples/no-operator/README",
            "text": " There are some common prerequisites used by all the examples. The Server Image These examples use the image built in the Build a Coherence Server Image example. The image is nothing more than a cache configuration file that has an Extend proxy along with Coherence metrics and management over REST. We will use this image in the various examples we cover here. When we run the image it will start a simple storage enabled Coherence server. The Test Client In the test-client/ directory is a simple Maven project that we will use to run a simple Extend client. Network Policies When running in Kubernetes cluster where NetworkPolicy rules are applied there are certain ingress and egress policies required to allow Coherence to work. These are covered in the Network Policies Example ",
            "title": "Prerequisites"
        },
        {
            "location": "/examples/no-operator/README",
            "text": " Although this project is all about the Coherence Kubernetes Operator, there are occasions where using an Operator is not possible. For example, some corporate or cloud security policies ban the use of CRDs, or have very restrictive RBAC policies that ultimately make it impossible to run Operators that uses their own CRDs or require cluster roles (or even just namespace roles). These example shows how to run a Coherence clusters in Kubernetes manually. Obviously the features of the Operator such as safe scaling, safe rolling upgrades, etc. will not be available. Note We really recommend that you try and use the Coherence Operator for managing Coherence clusters in Kubernetes. It is possible to run the Operator with fewer RBAC permissions, for example without ClusterRoles and only using Roles restricted to a single namespace. The Operator can also run without installing its web-hooks. Ultimately though it requires the CRD to be installed, which could be done manually instead of allowing the Operator to install it. If you really cannot change the minds of those dictating policies that mean you cannot use the Operator then these examples may be useful. Tip The complete source code for the examples is in the Coherence Operator GitHub repository. Prerequisites There are some common prerequisites used by all the examples. The Server Image These examples use the image built in the Build a Coherence Server Image example. The image is nothing more than a cache configuration file that has an Extend proxy along with Coherence metrics and management over REST. We will use this image in the various examples we cover here. When we run the image it will start a simple storage enabled Coherence server. The Test Client In the test-client/ directory is a simple Maven project that we will use to run a simple Extend client. Network Policies When running in Kubernetes cluster where NetworkPolicy rules are applied there are certain ingress and egress policies required to allow Coherence to work. These are covered in the Network Policies Example ",
            "title": "Coherence in Kubernetes Without the Operator"
        },
        {
            "location": "/examples/no-operator/README",
            "text": " Simple Server Run a simple Coherence storage enabled cluster as a StatefulSet and connect an Extend client to it. Simple Server with Metrics Expands the simple storage enabled server to expose metrics that can be scraped by Prometheus. Securing Extend with TLS Expands the simple storage enabled server to secure Extend using TLS. Running Coherence with Istio Expands the simple storage enabled server to secure Extend using TLS. ",
            "title": "The Examples"
        },
        {
            "location": "/docs/other/070_add_volumes",
            "text": " Volumes and volume mappings can easily be added to a Coherence resource Pod to allow application code deployed in the Pods to access additional storage. Volumes are added by adding configuration to the volumes list in the Coherence CRD spec. The configuration of the volume can be any valid yaml that would be used when adding a Volume to a Pod spec. Volume mounts are added by adding configuration to the volumeMounts list in the Coherence CRD spec. The configuration of the volume mount can be any valid yaml that would be used when adding a volume mount to a container in a Pod spec. Additional volumes added in this way will be added to all containers in the Pod . <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: volumes: - name: data-volume nfs: path: /shared/data server: nfs-server volumeMounts: - name: data-volume mountPath: /data An additional Volume named data-volume has been added (in this case the volume is an NFS volume). An additional volume mount has been added tthat will mount the data-volume at the /data mount point. The yaml above would result in a Pod spec similar to the following (a lot of the Pod spec has been omitted to just show the relevant volume information): <markup lang=\"yaml\" >apiVersion: v1 kind: Pod metadata: name: storage-0 spec: containers: - name: coherence volumeMounts: - name: data-volume mountPath: /data volumes: - name: data-volume nfs: path: /shared/data server: nfs-server ",
            "title": "Add Pod Volumes"
        },
        {
            "location": "/docs/coherence/030_cache_config",
            "text": " The name of the Coherence cache configuration file that the Coherence processes in a Coherence resource will use can be set with the spec.coherence.cacheConfig field. By setting this field the coherence.cacheconfig system property will be set in the Coherence JVM. When the spec.coherence.cacheConfig is blank or not specified, Coherence use its default behaviour to find the cache configuration file to use. Typically, this is to use the first occurrence of coherence-cache-config.xml that is found on the classpath (consult the Coherence documentation for an explanation of the default behaviour). To set a specific cache configuration file to use set the spec.coherence.cacheConfig field, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: coherence: cacheConfig: storage-cache-config.xml The spec.coherence.cacheConfig field has been set to storage-cache-config.xml which will effectively pass -Dcoherence.cacheconfig=storage-cache-config.xml to the JVM command line. ",
            "title": "Set the Cache Configuration File Name"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " Istio is a \"Service Mesh\" so the clue to how Istio works in Kubernetes is in the name, it relies on the configuration of Kubernetes Services. This means that any ports than need to be accessed in Pods, including those using in \"Pod to Pod\" communication must be exposed via a Service. Usually a Pod can reach any port on another Pod even if it is not exposed in the container spec, but this is not the case when using Istio as only ports exposed by the Envoy proxy are allowed. For Coherence cluster membership, this means the cluster port and the local port must be exposed on a Service. To do this the local port must be configured to be a fixed port instead of the default ephemeral port. The Coherence Operator uses the default cluster port of 7574 and there is no reason to ever change this. The Coherence Operator always configures a fixed port for the local port so this works with Istio out of the box. In addition, the Operator uses the health check port to determine the status of a cluster, so this needs to be exposed so that the Operator can reach Coherence Pods. The Coherence localhost property can be set to the name of the Pod. This is easily done using the container environment variables, which the Operator does automatically. Coherence clusters are run as a StatefulSet in Kubernetes. This means that the Pods are configured with a host name and a subdomain based on the name of the StatefulSet headless service name, and it is this name that should be used to access Pods. For example for a Coherence resource named storage the Operator will create a StatefulSet named storgage with a headless service named storage-sts . Each Pod in a StatefulSet is numbered with a fixed identity, so the first Pod in this cluster will be storage-0 . The Pod has a number of DNS names that it is reachable with, but the fully qualified name using the headless service will be storage-0.storage-sts or storage-0.storage-sts.&lt;namespace&gt;.svc`. By default, the Operator will expose all the ports configured for the Coherence resource on the StatefulSet headless service. This allows Coherence Extend and gRPC clients to use this service name as the WKA address when using the Coherence NameService to lookup endpoints (see the client example below). ",
            "title": "How Does Coherence Work with Istio?"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " For this example we make Istio run in \"strict\" mode so that it will not allow any traffic between Pods outside the Envoy proxy. If other modes are used, such as permissive, then Istio allows Pod to Pod communication so a cluster may appear to work in permissive mode, when it would not in strict mode. To set Istio to strict mode create the following yaml file. <markup lang=\"yaml\" title=\"istio-strict.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"default\" spec: mtls: mode: STRICT Install this yaml into the Istio system namespace with the following command: <markup lang=\"bash\" >kubectl -n istio-system apply istio-strict.yaml ",
            "title": "Enable Istio Strict Mode"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " The instructions assume that you are using a Kubernetes cluster with Istio installed and configured already. Enable Istio Strict Mode For this example we make Istio run in \"strict\" mode so that it will not allow any traffic between Pods outside the Envoy proxy. If other modes are used, such as permissive, then Istio allows Pod to Pod communication so a cluster may appear to work in permissive mode, when it would not in strict mode. To set Istio to strict mode create the following yaml file. <markup lang=\"yaml\" title=\"istio-strict.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"default\" spec: mtls: mode: STRICT Install this yaml into the Istio system namespace with the following command: <markup lang=\"bash\" >kubectl -n istio-system apply istio-strict.yaml ",
            "title": "Prerequisites"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " The Coherence Operator uses an admissions web-hook, which Kubernetes will call to validate Coherence resources. This web-hook binds to port 9443 in the Operator Pods and is already configured to use TLS as is standard for Kubernetes admissions web-hooks. If this port is routed through the Envoy proxy Kubernetes will be unable to access the web-hook. The Operator yaml manifests and Helm chart already add the traffic.sidecar.istio.io/excludeInboundPorts annotation to the Operator Pods. This should exclude the web-hook port from being Istio. Another way to do this is to add a PeerAuthentication resource to the Operator namespace. Before installing the Operator , create the following PeerAuthentication yaml. <markup lang=\"yaml\" title=\"istio-operator.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"coherence-operator\" spec: selector: matchLabels: app.kubernetes.io/name: coherence-operator app.kubernetes.io/instance: coherence-operator-manager app.kubernetes.io/component: manager mtls: mode: STRICT portLevelMtls: 9443: mode: PERMISSIVE Then install this PeerAuthentication resource into the same namespace that the Operator will be installed into. For example, if the Operator will be in the coherence namespace: <markup lang=\"bash\" >kubectl -n coherence apply istio-operator.yaml You can then install the operator using your preferred method in the Operator Installation Guide . After installed operator, use the following command to confirm the operator is running: <markup lang=\"bash\" >kubectl get pods -n coherence NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-7d76f9f475-q2vwv 2/2 Running 1 17h The output should show 2/2 in READY column, meaning there are 2 containers running in the Operator pod. One is Coherence Operator and the other is Envoy Proxy. If we use the Istio Kiali addon to visualize Istio we can see the Operator in the list of applications We can also see on the detailed view, that the Operator talks to the Kubernetes API server ",
            "title": "Exclude the Operator Web-Hook from the Envoy Proxy"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " To use Coherence operator with Istio, you can deploy the operator into a namespace which has Istio automatic sidecar injection enabled. Before installing the operator, create the namespace in which you want to run the Coherence operator and label it for automatic injection. <markup lang=\"bash\" >kubectl create namespace coherence kubectl label namespace coherence istio-injection=enabled Istio Sidecar AutoInjection is done automatically when you label the coherence namespace with istio-injection. Exclude the Operator Web-Hook from the Envoy Proxy The Coherence Operator uses an admissions web-hook, which Kubernetes will call to validate Coherence resources. This web-hook binds to port 9443 in the Operator Pods and is already configured to use TLS as is standard for Kubernetes admissions web-hooks. If this port is routed through the Envoy proxy Kubernetes will be unable to access the web-hook. The Operator yaml manifests and Helm chart already add the traffic.sidecar.istio.io/excludeInboundPorts annotation to the Operator Pods. This should exclude the web-hook port from being Istio. Another way to do this is to add a PeerAuthentication resource to the Operator namespace. Before installing the Operator , create the following PeerAuthentication yaml. <markup lang=\"yaml\" title=\"istio-operator.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"coherence-operator\" spec: selector: matchLabels: app.kubernetes.io/name: coherence-operator app.kubernetes.io/instance: coherence-operator-manager app.kubernetes.io/component: manager mtls: mode: STRICT portLevelMtls: 9443: mode: PERMISSIVE Then install this PeerAuthentication resource into the same namespace that the Operator will be installed into. For example, if the Operator will be in the coherence namespace: <markup lang=\"bash\" >kubectl -n coherence apply istio-operator.yaml You can then install the operator using your preferred method in the Operator Installation Guide . After installed operator, use the following command to confirm the operator is running: <markup lang=\"bash\" >kubectl get pods -n coherence NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-7d76f9f475-q2vwv 2/2 Running 1 17h The output should show 2/2 in READY column, meaning there are 2 containers running in the Operator pod. One is Coherence Operator and the other is Envoy Proxy. If we use the Istio Kiali addon to visualize Istio we can see the Operator in the list of applications We can also see on the detailed view, that the Operator talks to the Kubernetes API server ",
            "title": "Using the Coherence operator with Istio"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " You can configure a cluster to run with Istio automatic sidecar injection enabled. Before creating the cluster, create the namespace in which the cluster will run and label it for automatic injection. <markup lang=\"bash\" >kubectl create namespace coherence-example kubectl label namespace coherence-example istio-injection=enabled Now create a Coherence resource as normal, there is no additional configuration required to work in Istio. For example using the yaml below to create a three member cluster with management and metrics enabled: <markup lang=\"yaml\" title=\"storage.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 image: ghcr.io/oracle/coherence-ce:22.06.7 labels: app: storage version: 1.0.0 coherence: management: enabled: true metrics: enabled: true ports: - name: management - name: metrics - name: extend port: 20000 appProtocol: tcp - name: grpc-proxy port: 1408 appProtocol: grpc Istio prefers applications to have an app label Istio prefers applications to have a version label The Coherence Pods will expose ports for Management over REST, metrics, a Coherence*Extend proxy and a gRPC proxy The Operator will set the appProtocol for the management and metrics ports to http , but the Extend port must be set manually to tcp so that Istio knows what sort of traffic is being used by that port The gRPC port&#8217;s appProtocol field is set to grpc Using the Kiali console, we can now see two applications, the Coherence Operator in the \"coherence\" namespace and the \"storage\" application in the \"coherence-example\" namespace. If we look at the graph view we can see all the traffic between the different parts of the system We can see the Kubernetes API server accessing the Operator web-hook to validate the yaml We can see tge storage pods (the box marked \"storage 1.0.0\") communicate with each other via the storage-sts service to from a Coherence cluster We can see the storage pods communicate with the Operator REST service to request their Coherence site and rack labels We can see the Operator ping the storage pods health endpoints via the storage-sts service All of this traffic is using mTLS controlled by Istio ",
            "title": "Creating a Coherence cluster with Istio"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " Coherence Extend clients and gRPC clients running inside the cluster will also work with Istio. For this example the clients will be run in the coherence-client namespace, so it needs to be created and labelled so that Istio injection works in that namespace. <markup lang=\"bash\" >kubectl create namespace coherence-client kubectl label namespace coherence-client istio-injection=enabled To simulate a client application a CoherenceJob resource will be used with different configurations for the different types of client. The simplest way to configure a Coherence extend client in a cache configuration file is a default configuration similar to that shown below. No ports or addresses need to be configured. Coherence will use the JVM&#8217;s configured cluster name and well know addresses to locate to look up the Extend endpoints using the Coherence NameService. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;/remote-cache-scheme&gt; We can configure a CoherenceJob to run an Extend client with this configuration as shown below: <markup lang=\"yaml\" title=\"extend-client.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceJob metadata: name: client spec: image: ghcr.io/oracle/coherence-ce:22.06.7 restartPolicy: Never cluster: storage coherence: wka: addresses: - \"storage-sts.coherence-example.svc\" application: type: operator args: - sleep - \"300s\" env: - name: COHERENCE_CLIENT value: \"remote\" - name: COHERENCE_PROFILE value: \"thin\" The client will use the CE image published on GitHub, which will use the default cache configuration file from Coherence jar. The cluster name must be set to the cluster name of the cluster started above, in this case storage The WKA address needs to be set to the DNS name of the headless service for the storage cluster created above. As this Job is running in a different name space this is the fully qualified name &lt;service-name&gt;.&lt;namespace&gt;.svc which is storage-sts.coherence-example.svc Instead of running a normal command this Job will run the Operator&#8217;s sleep command and sleep for 300s (300 seconds). The COHERENCE_CLIENT environment variable value of remote sets the Coherence cache configuration to be an Extend client using the NameService The COHERENCE_PROFILE environment variable value of thin sets the Coherence cache configuration not to use a Near Cache. The yaml above can be deployed into Kubernetes: <markup lang=\"bash\" >kubectl -n coherence-client apply -f extend-client.yaml <markup lang=\"bash\" >$ kubectl -n coherence-client get pod NAME READY STATUS RESTARTS AGE client-qgnw5 2/2 Running 0 80s The Pod is now running but not doing anything, just sleeping. If we look at the Kiali dashboard we can see the client application started and communicated wth the Operator. We can use this sleeping Pod to exec into and run commands. In this case we will create a Coherence QueryPlus client and run some CohQL commands. The command below will exec into the sleeping Pod. <markup lang=\"bash\" >kubectl -n coherence-client exec -it client-qgnw5 -- /coherence-operator/utils/runner queryplus A QueryPlus client will be started and eventually display the CohQL&gt; prompt. <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; A simple command to try is just creating a cache, so at the prompt type the command create cache test which will create a cache named test . If all is configured correctly this client will connect to the cluster over Extend and create the cache called test and return to the CohQL prompt. <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; create cache test We can also try selecting data from the cache using the CohQL query select * from test (which will return nothing as the cache is empty). <markup lang=\"bash\" >CohQL&gt; select * from test Results CohQL&gt; If we now look at the Kiali dashboard we can see that the client has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. To exit from the CohQL&gt; prompt type the bye command. Coherence Extend clients can connect to the cluster also using Istio to provide mTLS support. Coherence clusters work with mTLS and Coherence clients can also support TLS through the Istio Gateway with TLS termination to connect to Coherence cluster running inside kubernetes. For example, you can apply the following Istio Gateway and Virtual Service in the namespace of the Coherence cluster. Before applying the gateway, create a secret for the credential from the certificate and key (e.g. server.crt and server.key) to be used by the Gateway: ",
            "title": "Coherence Clients Running in Kubernetes"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " Coherence clients running outside the Kubernetes can be configured to connect to a Coherence cluster inside Kubernetes using any of the ingress or gateway features of Istio and Kubernetes. All the different ways to do this are beyond the scope of this simple example as there are many and they depend on the versions of Istio and Kubernetes being used. When connecting Coherence Extend or gRPC clients from outside Kubernetes, the Coherence NameService cannot be used by clients to look up the endpoints. The clients must be configured with fixed endpoints using the hostnames and ports of the configured ingress or gateway services. ",
            "title": "Coherence Clients Running Outside Kubernetes"
        },
        {
            "location": "/examples/400_Istio/README",
            "text": " You can run the Coherence cluster and manage them using the Coherence Operator alongside Istio . Coherence clusters managed with the Coherence Operator 3.3.4 and later work with Istio 1.9.1 and later out of the box. Coherence caches can be accessed from outside the Coherence cluster via Coherence*Extend, REST, and other supported Coherence clients. Using Coherence clusters with Istio does not require the Coherence Operator to also be using Istio (and vice-versa) . The Coherence Operator can manage Coherence clusters independent of whether those clusters are using Istio or not. Although Coherence itself can be configured to use TLS, when using Istio Coherence cluster members and clients can just use the default socket configurations and Istio will control and route all the traffic over mTLS. Tip Coherence clusters can be manually configured to work with Istio, even if not using the Operator. See the Istio example in the No Operator Examples How Does Coherence Work with Istio? Istio is a \"Service Mesh\" so the clue to how Istio works in Kubernetes is in the name, it relies on the configuration of Kubernetes Services. This means that any ports than need to be accessed in Pods, including those using in \"Pod to Pod\" communication must be exposed via a Service. Usually a Pod can reach any port on another Pod even if it is not exposed in the container spec, but this is not the case when using Istio as only ports exposed by the Envoy proxy are allowed. For Coherence cluster membership, this means the cluster port and the local port must be exposed on a Service. To do this the local port must be configured to be a fixed port instead of the default ephemeral port. The Coherence Operator uses the default cluster port of 7574 and there is no reason to ever change this. The Coherence Operator always configures a fixed port for the local port so this works with Istio out of the box. In addition, the Operator uses the health check port to determine the status of a cluster, so this needs to be exposed so that the Operator can reach Coherence Pods. The Coherence localhost property can be set to the name of the Pod. This is easily done using the container environment variables, which the Operator does automatically. Coherence clusters are run as a StatefulSet in Kubernetes. This means that the Pods are configured with a host name and a subdomain based on the name of the StatefulSet headless service name, and it is this name that should be used to access Pods. For example for a Coherence resource named storage the Operator will create a StatefulSet named storgage with a headless service named storage-sts . Each Pod in a StatefulSet is numbered with a fixed identity, so the first Pod in this cluster will be storage-0 . The Pod has a number of DNS names that it is reachable with, but the fully qualified name using the headless service will be storage-0.storage-sts or storage-0.storage-sts.&lt;namespace&gt;.svc`. By default, the Operator will expose all the ports configured for the Coherence resource on the StatefulSet headless service. This allows Coherence Extend and gRPC clients to use this service name as the WKA address when using the Coherence NameService to lookup endpoints (see the client example below). Prerequisites The instructions assume that you are using a Kubernetes cluster with Istio installed and configured already. Enable Istio Strict Mode For this example we make Istio run in \"strict\" mode so that it will not allow any traffic between Pods outside the Envoy proxy. If other modes are used, such as permissive, then Istio allows Pod to Pod communication so a cluster may appear to work in permissive mode, when it would not in strict mode. To set Istio to strict mode create the following yaml file. <markup lang=\"yaml\" title=\"istio-strict.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"default\" spec: mtls: mode: STRICT Install this yaml into the Istio system namespace with the following command: <markup lang=\"bash\" >kubectl -n istio-system apply istio-strict.yaml Using the Coherence operator with Istio To use Coherence operator with Istio, you can deploy the operator into a namespace which has Istio automatic sidecar injection enabled. Before installing the operator, create the namespace in which you want to run the Coherence operator and label it for automatic injection. <markup lang=\"bash\" >kubectl create namespace coherence kubectl label namespace coherence istio-injection=enabled Istio Sidecar AutoInjection is done automatically when you label the coherence namespace with istio-injection. Exclude the Operator Web-Hook from the Envoy Proxy The Coherence Operator uses an admissions web-hook, which Kubernetes will call to validate Coherence resources. This web-hook binds to port 9443 in the Operator Pods and is already configured to use TLS as is standard for Kubernetes admissions web-hooks. If this port is routed through the Envoy proxy Kubernetes will be unable to access the web-hook. The Operator yaml manifests and Helm chart already add the traffic.sidecar.istio.io/excludeInboundPorts annotation to the Operator Pods. This should exclude the web-hook port from being Istio. Another way to do this is to add a PeerAuthentication resource to the Operator namespace. Before installing the Operator , create the following PeerAuthentication yaml. <markup lang=\"yaml\" title=\"istio-operator.yaml\" >apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \"coherence-operator\" spec: selector: matchLabels: app.kubernetes.io/name: coherence-operator app.kubernetes.io/instance: coherence-operator-manager app.kubernetes.io/component: manager mtls: mode: STRICT portLevelMtls: 9443: mode: PERMISSIVE Then install this PeerAuthentication resource into the same namespace that the Operator will be installed into. For example, if the Operator will be in the coherence namespace: <markup lang=\"bash\" >kubectl -n coherence apply istio-operator.yaml You can then install the operator using your preferred method in the Operator Installation Guide . After installed operator, use the following command to confirm the operator is running: <markup lang=\"bash\" >kubectl get pods -n coherence NAME READY STATUS RESTARTS AGE coherence-operator-controller-manager-7d76f9f475-q2vwv 2/2 Running 1 17h The output should show 2/2 in READY column, meaning there are 2 containers running in the Operator pod. One is Coherence Operator and the other is Envoy Proxy. If we use the Istio Kiali addon to visualize Istio we can see the Operator in the list of applications We can also see on the detailed view, that the Operator talks to the Kubernetes API server Creating a Coherence cluster with Istio You can configure a cluster to run with Istio automatic sidecar injection enabled. Before creating the cluster, create the namespace in which the cluster will run and label it for automatic injection. <markup lang=\"bash\" >kubectl create namespace coherence-example kubectl label namespace coherence-example istio-injection=enabled Now create a Coherence resource as normal, there is no additional configuration required to work in Istio. For example using the yaml below to create a three member cluster with management and metrics enabled: <markup lang=\"yaml\" title=\"storage.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: replicas: 3 image: ghcr.io/oracle/coherence-ce:22.06.7 labels: app: storage version: 1.0.0 coherence: management: enabled: true metrics: enabled: true ports: - name: management - name: metrics - name: extend port: 20000 appProtocol: tcp - name: grpc-proxy port: 1408 appProtocol: grpc Istio prefers applications to have an app label Istio prefers applications to have a version label The Coherence Pods will expose ports for Management over REST, metrics, a Coherence*Extend proxy and a gRPC proxy The Operator will set the appProtocol for the management and metrics ports to http , but the Extend port must be set manually to tcp so that Istio knows what sort of traffic is being used by that port The gRPC port&#8217;s appProtocol field is set to grpc Using the Kiali console, we can now see two applications, the Coherence Operator in the \"coherence\" namespace and the \"storage\" application in the \"coherence-example\" namespace. If we look at the graph view we can see all the traffic between the different parts of the system We can see the Kubernetes API server accessing the Operator web-hook to validate the yaml We can see tge storage pods (the box marked \"storage 1.0.0\") communicate with each other via the storage-sts service to from a Coherence cluster We can see the storage pods communicate with the Operator REST service to request their Coherence site and rack labels We can see the Operator ping the storage pods health endpoints via the storage-sts service All of this traffic is using mTLS controlled by Istio Coherence Clients Running in Kubernetes Coherence Extend clients and gRPC clients running inside the cluster will also work with Istio. For this example the clients will be run in the coherence-client namespace, so it needs to be created and labelled so that Istio injection works in that namespace. <markup lang=\"bash\" >kubectl create namespace coherence-client kubectl label namespace coherence-client istio-injection=enabled To simulate a client application a CoherenceJob resource will be used with different configurations for the different types of client. The simplest way to configure a Coherence extend client in a cache configuration file is a default configuration similar to that shown below. No ports or addresses need to be configured. Coherence will use the JVM&#8217;s configured cluster name and well know addresses to locate to look up the Extend endpoints using the Coherence NameService. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;/remote-cache-scheme&gt; We can configure a CoherenceJob to run an Extend client with this configuration as shown below: <markup lang=\"yaml\" title=\"extend-client.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceJob metadata: name: client spec: image: ghcr.io/oracle/coherence-ce:22.06.7 restartPolicy: Never cluster: storage coherence: wka: addresses: - \"storage-sts.coherence-example.svc\" application: type: operator args: - sleep - \"300s\" env: - name: COHERENCE_CLIENT value: \"remote\" - name: COHERENCE_PROFILE value: \"thin\" The client will use the CE image published on GitHub, which will use the default cache configuration file from Coherence jar. The cluster name must be set to the cluster name of the cluster started above, in this case storage The WKA address needs to be set to the DNS name of the headless service for the storage cluster created above. As this Job is running in a different name space this is the fully qualified name &lt;service-name&gt;.&lt;namespace&gt;.svc which is storage-sts.coherence-example.svc Instead of running a normal command this Job will run the Operator&#8217;s sleep command and sleep for 300s (300 seconds). The COHERENCE_CLIENT environment variable value of remote sets the Coherence cache configuration to be an Extend client using the NameService The COHERENCE_PROFILE environment variable value of thin sets the Coherence cache configuration not to use a Near Cache. The yaml above can be deployed into Kubernetes: <markup lang=\"bash\" >kubectl -n coherence-client apply -f extend-client.yaml <markup lang=\"bash\" >$ kubectl -n coherence-client get pod NAME READY STATUS RESTARTS AGE client-qgnw5 2/2 Running 0 80s The Pod is now running but not doing anything, just sleeping. If we look at the Kiali dashboard we can see the client application started and communicated wth the Operator. We can use this sleeping Pod to exec into and run commands. In this case we will create a Coherence QueryPlus client and run some CohQL commands. The command below will exec into the sleeping Pod. <markup lang=\"bash\" >kubectl -n coherence-client exec -it client-qgnw5 -- /coherence-operator/utils/runner queryplus A QueryPlus client will be started and eventually display the CohQL&gt; prompt. <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; A simple command to try is just creating a cache, so at the prompt type the command create cache test which will create a cache named test . If all is configured correctly this client will connect to the cluster over Extend and create the cache called test and return to the CohQL prompt. <markup lang=\"bash\" >Coherence Command Line Tool CohQL&gt; create cache test We can also try selecting data from the cache using the CohQL query select * from test (which will return nothing as the cache is empty). <markup lang=\"bash\" >CohQL&gt; select * from test Results CohQL&gt; If we now look at the Kiali dashboard we can see that the client has communicated with the storage cluster. All of this communication was using mTLS but without configuring Coherence to use TLS. To exit from the CohQL&gt; prompt type the bye command. Coherence Extend clients can connect to the cluster also using Istio to provide mTLS support. Coherence clusters work with mTLS and Coherence clients can also support TLS through the Istio Gateway with TLS termination to connect to Coherence cluster running inside kubernetes. For example, you can apply the following Istio Gateway and Virtual Service in the namespace of the Coherence cluster. Before applying the gateway, create a secret for the credential from the certificate and key (e.g. server.crt and server.key) to be used by the Gateway: Coherence Clients Running Outside Kubernetes Coherence clients running outside the Kubernetes can be configured to connect to a Coherence cluster inside Kubernetes using any of the ingress or gateway features of Istio and Kubernetes. All the different ways to do this are beyond the scope of this simple example as there are many and they depend on the versions of Istio and Kubernetes being used. When connecting Coherence Extend or gRPC clients from outside Kubernetes, the Coherence NameService cannot be used by clients to look up the endpoints. The clients must be configured with fixed endpoints using the hostnames and ports of the configured ingress or gateway services. ",
            "title": "Using Coherence with Istio"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " This guide is a simple set of steps to install the Coherence Operator and then use that to install a simple Coherence cluster. ",
            "title": "preambule"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Ensure that the Coherence Operator prerequisites are available. ",
            "title": "Prerequisites"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Add the Coherence Operator Helm repo to your local Helm. <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update To avoid confusion, the URL https://oracle.github.io/coherence-operator/charts is a Helm repo, it is not a web site you open in a browser. You may think we shouldn&#8217;t have to say this, but you&#8217;d be surprised. ",
            "title": "Add the Coherence Operator Helm repository"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": "<markup lang=\"bash\" title=\"helm v3 install command\" >helm install \\ --namespace &lt;namespace&gt; \\ &lt;release-name&gt; \\ coherence/coherence-operator e.g. if the Kubernetes namespace is coherence-test the command would be: <markup lang=\"bash\" title=\"helm v3 install command\" >helm install --namespace coherence-test operator coherence/coherence-operator or with Helm v2 <markup lang=\"bash\" >helm install --namespace coherence-test --name operator coherence/coherence-operator See the full install guide for more details. ",
            "title": "Install the Coherence Operator Helm chart"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Alternatively you can install the Operator using the Helm chart. Add the Coherence Operator Helm repository Add the Coherence Operator Helm repo to your local Helm. <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update To avoid confusion, the URL https://oracle.github.io/coherence-operator/charts is a Helm repo, it is not a web site you open in a browser. You may think we shouldn&#8217;t have to say this, but you&#8217;d be surprised. Install the Coherence Operator Helm chart <markup lang=\"bash\" title=\"helm v3 install command\" >helm install \\ --namespace &lt;namespace&gt; \\ &lt;release-name&gt; \\ coherence/coherence-operator e.g. if the Kubernetes namespace is coherence-test the command would be: <markup lang=\"bash\" title=\"helm v3 install command\" >helm install --namespace coherence-test operator coherence/coherence-operator or with Helm v2 <markup lang=\"bash\" >helm install --namespace coherence-test --name operator coherence/coherence-operator See the full install guide for more details. ",
            "title": "Alternatively Install Using Helm"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " If you want the default Coherence Operator installation then the simplest solution is use kubectl to apply the manifests from the Operator release. <markup lang=\"bash\" >kubectl apply -f https://github.com/oracle/coherence-operator/releases/download/v3.3.4/coherence-operator.yaml This will create a namespace called coherence and install the Operator into it along with all the required ClusterRole and RoleBinding resources. The coherence namespace can be changed by downloading and editing the yaml file. Because the coherence-operator.yaml manifest also creates the namespace, the corresponding kubectl delete command will remove the namespace and everything deployed to it ! If you do not want this behaviour you should edit the coherence-operator.yaml to remove the namespace section from the start of the file. Alternatively Install Using Helm Alternatively you can install the Operator using the Helm chart. Add the Coherence Operator Helm repository Add the Coherence Operator Helm repo to your local Helm. <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update To avoid confusion, the URL https://oracle.github.io/coherence-operator/charts is a Helm repo, it is not a web site you open in a browser. You may think we shouldn&#8217;t have to say this, but you&#8217;d be surprised. Install the Coherence Operator Helm chart <markup lang=\"bash\" title=\"helm v3 install command\" >helm install \\ --namespace &lt;namespace&gt; \\ &lt;release-name&gt; \\ coherence/coherence-operator e.g. if the Kubernetes namespace is coherence-test the command would be: <markup lang=\"bash\" title=\"helm v3 install command\" >helm install --namespace coherence-test operator coherence/coherence-operator or with Helm v2 <markup lang=\"bash\" >helm install --namespace coherence-test --name operator coherence/coherence-operator See the full install guide for more details. ",
            "title": "1. Install the Coherence Operator"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The minimal required yaml to create a Coherence resource is shown below. <markup lang=\"yaml\" title=\"my-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-deployment The only required field is metadata.name which will be used as the Coherence cluster name, in this case my-deployment <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; apply -f my-deployment.yaml Use the same namespace that the operator was installed into, e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test create -f my-deployment.yaml ",
            "title": "2.1 Install a Coherence resource using the minimal required configuration."
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " After installing the my-deployment.yaml above here should be a single Coherence resource named my-deployment in the Coherence Operator namespace. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coherence or alternatively using the Coherence CRD a short name of coh <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coh e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test get coherence <markup lang=\"bash\" >NAME AGE coherence.coherence.oracle.com/my-deployment 19s ",
            "title": "2.2 List the Coherence Resources"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The Coherence Operator applies a coherenceDeployment label to all Pods so this label can be used with the kubectl command to find Pods for a CoherenceCoherence resource. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceDeployment=my-deployment e.g. if the namespace is coherence the command would be: kubectl -n coherence get pod -l coherenceDeployment=my-deployment <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE my-deployment-0 1/1 Running 0 2m58s my-deployment-1 1/1 Running 0 2m58s my-deployment-2 1/1 Running 0 2m58s ",
            "title": "2.3 List all of the Pods for the Coherence resource."
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The Coherence Operator applies a coherenceCluster label to all Pods , so this label can be used with the kubectl command to find all Pods for a Coherence cluster, which will be made up of multiple Coherence resources. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster e.g. If there is a cluster named my-cluster made up of two Coherence resources in the namespace coherence-test , one named storage and one named front-end then the kubectl command to list all Pods for the cluster would be: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=my-cluster The result of which might look something like this <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE storage-0 1/1 Running 0 2m58s storage-1 1/1 Running 0 2m58s storage-2 1/1 Running 0 2m58s front-end-0 1/1 Running 0 2m58s front-end-1 1/1 Running 0 2m58s front-end-2 1/1 Running 0 2m58s ",
            "title": "2.3 List all the Pods for the Coherence cluster."
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Ensure that the Coherence images can be pulled by the Kubernetes cluster, see Obtain Coherence Images . By default, a Coherence resource will use the OSS Coherence CE image from Docker Hub. If a different image is to be used the image name will need to be specified in the Coherence yaml, see Setting the Application Image for documentation on how to specify a different images to use. 2.1 Install a Coherence resource using the minimal required configuration. The minimal required yaml to create a Coherence resource is shown below. <markup lang=\"yaml\" title=\"my-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-deployment The only required field is metadata.name which will be used as the Coherence cluster name, in this case my-deployment <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; apply -f my-deployment.yaml Use the same namespace that the operator was installed into, e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test create -f my-deployment.yaml 2.2 List the Coherence Resources After installing the my-deployment.yaml above here should be a single Coherence resource named my-deployment in the Coherence Operator namespace. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coherence or alternatively using the Coherence CRD a short name of coh <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get coh e.g. if the namespace is coherence-test the command would be kubectl -n coherence-test get coherence <markup lang=\"bash\" >NAME AGE coherence.coherence.oracle.com/my-deployment 19s 2.3 List all of the Pods for the Coherence resource. The Coherence Operator applies a coherenceDeployment label to all Pods so this label can be used with the kubectl command to find Pods for a CoherenceCoherence resource. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceDeployment=my-deployment e.g. if the namespace is coherence the command would be: kubectl -n coherence get pod -l coherenceDeployment=my-deployment <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE my-deployment-0 1/1 Running 0 2m58s my-deployment-1 1/1 Running 0 2m58s my-deployment-2 1/1 Running 0 2m58s 2.3 List all the Pods for the Coherence cluster. The Coherence Operator applies a coherenceCluster label to all Pods , so this label can be used with the kubectl command to find all Pods for a Coherence cluster, which will be made up of multiple Coherence resources. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster e.g. If there is a cluster named my-cluster made up of two Coherence resources in the namespace coherence-test , one named storage and one named front-end then the kubectl command to list all Pods for the cluster would be: <markup lang=\"bash\" >kubectl -n coherence-test get pod -l coherenceCluster=my-cluster The result of which might look something like this <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE storage-0 1/1 Running 0 2m58s storage-1 1/1 Running 0 2m58s storage-2 1/1 Running 0 2m58s front-end-0 1/1 Running 0 2m58s front-end-1 1/1 Running 0 2m58s front-end-2 1/1 Running 0 2m58s ",
            "title": "2. Install a Coherence Deployment"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Using the kubectl scale command a specific Coherence resource can be scaled up or down. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; scale coherence/my-deployment --replicas=6 e.g. if the namespace is coherence-test the command would be: kubectl -n coherence scale coherence/my-deployment --replicas=6 ",
            "title": "3.1 Use kubectl to Scale Up"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " 3.1 Use kubectl to Scale Up Using the kubectl scale command a specific Coherence resource can be scaled up or down. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; scale coherence/my-deployment --replicas=6 e.g. if the namespace is coherence-test the command would be: kubectl -n coherence scale coherence/my-deployment --replicas=6 ",
            "title": "3. Scale the Coherence Cluster"
        },
        {
            "location": "/docs/metrics/050_ssl",
            "text": " It is possible to configure metrics endpoint to use SSL to secure the communication between server and client. The SSL configuration is in the coherence.metrics.ssl section of the CRD spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-cluster spec: coherence: metrics: enabled: true ssl: enabled: true keyStore: metrics-keys.jks keyStoreType: JKS keyStorePasswordFile: store-pass.txt keyPasswordFile: key-pass.txt keyStoreProvider: keyStoreAlgorithm: SunX509 trustStore: metrics-trust.jks trustStoreType: JKS trustStorePasswordFile: trust-pass.txt trustStoreProvider: trustStoreAlgorithm: SunX509 requireClientCert: true secrets: metrics-secret The enabled field when set to true enables SSL for metrics or when set to false disables SSL The keyStore field sets the name of the Java key store file that should be used to obtain the server&#8217;s key The optional keyStoreType field sets the type of the key store file, the default value is JKS The optional keyStorePasswordFile sets the name of the text file containing the key store password The optional keyPasswordFile sets the name of the text file containing the password of the key in the key store The optional keyStoreProvider sets the provider name for the key store The optional keyStoreAlgorithm sets the algorithm name for the key store, the default value is SunX509 The trustStore field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key The optional trustStoreType field sets the type of the trust store file, the default value is JKS The optional trustStorePasswordFile sets the name of the text file containing the trust store password The optional trustStoreProvider sets the provider name for the trust store The optional trustStoreAlgorithm sets the algorithm name for the trust store, the default value is SunX509 The optional requireClientCert field if set to true enables two-way SSL where the client must also provide a valid certificate The optional secrets field sets the name of the Kubernetes Secret to use to obtain the key store, truct store and password files from. The various files and keystores referred to in the configuration above can be any location accessible in the image used by the coherence container in the deployment&#8217;s Pods . Typically, for things such as SSL keys and certs, these would be provided by obtained from Secrets loaded as additional Pod Volumes . See Add Secrets Volumes for the documentation on how to specify secrets as additional volumes. ",
            "title": "SSL with Metrics"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " Install the Operator Setting Up Cert-Manager Create the SelfSigned Issuer Create the CA Certificate Create the CA issuer Create the Coherence Keys, Certs and KeyStores Create the Server Keystore Password Secret Create the Server Certificate Create the Client Certificate Securing Coherence Clusters Build the Example Images Configure a Socket Provider Secure Cluster Membership Secure Extend Secure gRPC ",
            "title": "What the Example will Cover"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " To run the examples below, you will need to have installed the Coherence Operator, do this using whatever method you prefer from the Installation Guide ",
            "title": "Install the Operator"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " This is used to generate a root CA for use with the CA Issuer. Here we are using a ClusterIssuer so that we can use a single self-signed issuer across all namespaces. We could have instead created an Issuer in a single namespace. <markup lang=\"yaml\" title=\"manifests/selfsigned-issuer.yaml\" >apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-issuer spec: selfSigned: {} Create the ClusterIssuer with the following command. As this is a ClusterIssuer , is does not require a namespace. <markup lang=\"bash\" >kubectl apply -f manifests/selfsigned-issuer.yaml We can list the ClusterIssuers in the cluster: <markup lang=\"bash\" >kubectl get clusterissuer We should see that the selfsigned-issuer is present and is ready. <markup lang=\"bash\" >NAME READY AGE selfsigned-issuer True 14m ",
            "title": "Create the SelfSigned Issuer"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " In this example we will use self-signed certs as this makes everything easy to get going. Cert Manager has a number of ways to configure real certificates for production use. Assuming that you&#8217;ve installed Cert Manager using one of the methods in its Install Guide we can proceed to created all of the required resources. Create the SelfSigned Issuer This is used to generate a root CA for use with the CA Issuer. Here we are using a ClusterIssuer so that we can use a single self-signed issuer across all namespaces. We could have instead created an Issuer in a single namespace. <markup lang=\"yaml\" title=\"manifests/selfsigned-issuer.yaml\" >apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-issuer spec: selfSigned: {} Create the ClusterIssuer with the following command. As this is a ClusterIssuer , is does not require a namespace. <markup lang=\"bash\" >kubectl apply -f manifests/selfsigned-issuer.yaml We can list the ClusterIssuers in the cluster: <markup lang=\"bash\" >kubectl get clusterissuer We should see that the selfsigned-issuer is present and is ready. <markup lang=\"bash\" >NAME READY AGE selfsigned-issuer True 14m ",
            "title": "Setting Up Cert-Manager"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " Were going to create an internal CA that will be used to sign our certificate requests for the Coherence server and clients that we will run later. Both the server and client will use the CA to validate a connection. To create the CA issuer, first create a self-signed CA certificate. <markup lang=\"yaml\" title=\"manifests/ca-cert.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate spec: issuerRef: name: selfsigned-issuer kind: ClusterIssuer group: cert-manager.io secretName: ca-cert duration: 2880h # 120d renewBefore: 360h # 15d commonName: Cert Admin isCA: true privateKey: size: 2048 usages: - digital signature - key encipherment The certificate will use the selfsigned-issuer cluster issuer we created above. There will be a secret named ca-cert created containing the key and certificate Note that the isCA field is set to true in the body of the spec. The CA issuer that we will create later will also be a ClusterIssuer , so in order for the issuer to find the Certificate above we will create the certificate in the cert-manager namespace, which is where Cert Manager is running. <markup lang=\"bash\" >kubectl -n cert-manager apply -f manifests/ca-cert.yaml We can see that the certificate was created and should be ready: <markup lang=\"bash\" >kubectl -n cert-manager get certificate <markup lang=\"bash\" >NAME READY SECRET AGE ca-certificate True ca-cert 12m There will also be a secret named ca-secret created in the cert-manager namespace. The Secret will contain the certificate and signing key, this will be created when the CA certificate is deployed, and the CA issuer will reference that secret. ",
            "title": "Create the CA Certificate"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " As with the self-signed issuer above, we will create a ClusterIssuer for the CA issuer. <markup lang=\"bash\" title=\"manifests/ca-cert.yaml\" >apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: ca-issuer spec: ca: secretName: ca-cert The ca-issuer will use the ca-cert secret created by the ca-certificate Certificate we created above. Create the CA issuer with the following command. As this is a ClusterIssuer , is does not require a namespace. <markup lang=\"bash\" >kubectl apply -f manifests/ca-issuer.yaml You can then check that the issuer have been successfully configured by checking the status. <markup lang=\"bash\" >kubectl get clusterissuer We should see that both ClusterIssuers we created are present and is ready. <markup lang=\"bash\" >NAME READY AGE ca-issuer True 22m selfsigned-issuer True 31m ",
            "title": "Create the CA issuer."
        },
        {
            "location": "/examples/090_tls/README",
            "text": " We will run the Coherence cluster in a namespace called coherence-test , so we will first create this: <markup lang=\"bash\" >kubectl create ns coherence-test ",
            "title": "Create a Namespace"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " The keystore will be secured with a password. We will create this password in a Secret so that Cert-Manager can find and use it. The simplest way to create this secret is with kubectl: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic \\ server-keystore-secret --from-literal=password-key=[your-password] &#8230;&#8203;replacing [your-password] with the actual password you want to use. Resulting in a Secret similar to this: <markup lang=\"bash\" title=\"manifests/ca-cert.yaml\" >apiVersion: v1 kind: Secret metadata: name: server-keystore-secret data: password-key: \"cGFzc3dvcmQ=\" In this example the password used is password ",
            "title": "Create the Server Keystore Password Secret"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " We can now create the server certificate and keystore. <markup lang=\"yaml\" title=\"manifests/server-keystore.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: server-keystore spec: issuerRef: name: ca-issuer kind: ClusterIssuer group: cert-manager.io secretName: coherence-server-certs keystores: jks: create: true passwordSecretRef: key: password-key name: server-keystore-secret duration: 2160h # 90d renewBefore: 360h # 15d privateKey: size: 2048 algorithm: RSA encoding: PKCS1 usages: - digital signature - key encipherment - client auth - server auth commonName: Coherence Certs The issuer will the ClusterIssuer named ca-issuer that we created above. The keys, certs and keystores will be created in a secret named coherence-server-certs The keystore password secret is the Secret named server-keystore-secret we created above We can create the certificate in the coherence-test namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/server-keystore.yaml If we list the certificate in the coherence-test namespace we should see the new certificate and that it is ready. <markup lang=\"bash\" >kubectl -n coherence-test get certificate <markup lang=\"bash\" >NAME READY SECRET AGE server-keystore True coherence-server-certs 4s If we list the secrets in the coherence-test namespace we should see both the password secret and the keystore secret: <markup lang=\"bash\" >kubectl -n coherence-test get secret <markup lang=\"bash\" >NAME TYPE DATA AGE coherence-server-certs kubernetes.io/tls 5 117s server-keystore-secret Opaque 1 2m9s ",
            "title": "Create the Server Certificate"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " We can create the certificates and keystores for the client in exactly the same way we did for the server. Create a password secret for the client keystore: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic \\ client-keystore-secret --from-literal=password-key=[your-password] Create the client certificate and keystore. <markup lang=\"yaml\" title=\"manifests/client-keystore.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: client-keystore spec: issuerRef: name: ca-issuer kind: ClusterIssuer group: cert-manager.io secretName: coherence-client-certs keystores: jks: create: true passwordSecretRef: key: password-key name: client-keystore-secret duration: 2160h # 90d renewBefore: 360h # 15d privateKey: size: 2048 algorithm: RSA encoding: PKCS1 usages: - digital signature - key encipherment - client auth commonName: Coherence Certs The issuer is the same cluster-wide ca-issuer that we used for the server. The keys, certs and keystores will be created in a secret named coherence-client-certs The keystore password secret is the Secret named client-keystore-secret we created above <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client-keystore.yaml If we list the certificate in the coherence-test namespace we should see the new client certificate and that it is ready. <markup lang=\"bash\" >kubectl -n coherence-test get certificate <markup >NAME READY SECRET AGE client-keystore True coherence-client-certs 12s server-keystore True coherence-server-certs 2m13s ",
            "title": "Create the Client Certificate"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " As the Coherence server, and client in this example, are Java applications they will require Java keystores to hold the certificates. We can use Cert-Manager to create these for us. Create a Namespace We will run the Coherence cluster in a namespace called coherence-test , so we will first create this: <markup lang=\"bash\" >kubectl create ns coherence-test Create the Server Keystore Password Secret The keystore will be secured with a password. We will create this password in a Secret so that Cert-Manager can find and use it. The simplest way to create this secret is with kubectl: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic \\ server-keystore-secret --from-literal=password-key=[your-password] &#8230;&#8203;replacing [your-password] with the actual password you want to use. Resulting in a Secret similar to this: <markup lang=\"bash\" title=\"manifests/ca-cert.yaml\" >apiVersion: v1 kind: Secret metadata: name: server-keystore-secret data: password-key: \"cGFzc3dvcmQ=\" In this example the password used is password Create the Server Certificate We can now create the server certificate and keystore. <markup lang=\"yaml\" title=\"manifests/server-keystore.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: server-keystore spec: issuerRef: name: ca-issuer kind: ClusterIssuer group: cert-manager.io secretName: coherence-server-certs keystores: jks: create: true passwordSecretRef: key: password-key name: server-keystore-secret duration: 2160h # 90d renewBefore: 360h # 15d privateKey: size: 2048 algorithm: RSA encoding: PKCS1 usages: - digital signature - key encipherment - client auth - server auth commonName: Coherence Certs The issuer will the ClusterIssuer named ca-issuer that we created above. The keys, certs and keystores will be created in a secret named coherence-server-certs The keystore password secret is the Secret named server-keystore-secret we created above We can create the certificate in the coherence-test namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/server-keystore.yaml If we list the certificate in the coherence-test namespace we should see the new certificate and that it is ready. <markup lang=\"bash\" >kubectl -n coherence-test get certificate <markup lang=\"bash\" >NAME READY SECRET AGE server-keystore True coherence-server-certs 4s If we list the secrets in the coherence-test namespace we should see both the password secret and the keystore secret: <markup lang=\"bash\" >kubectl -n coherence-test get secret <markup lang=\"bash\" >NAME TYPE DATA AGE coherence-server-certs kubernetes.io/tls 5 117s server-keystore-secret Opaque 1 2m9s Create the Client Certificate We can create the certificates and keystores for the client in exactly the same way we did for the server. Create a password secret for the client keystore: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic \\ client-keystore-secret --from-literal=password-key=[your-password] Create the client certificate and keystore. <markup lang=\"yaml\" title=\"manifests/client-keystore.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: client-keystore spec: issuerRef: name: ca-issuer kind: ClusterIssuer group: cert-manager.io secretName: coherence-client-certs keystores: jks: create: true passwordSecretRef: key: password-key name: client-keystore-secret duration: 2160h # 90d renewBefore: 360h # 15d privateKey: size: 2048 algorithm: RSA encoding: PKCS1 usages: - digital signature - key encipherment - client auth commonName: Coherence Certs The issuer is the same cluster-wide ca-issuer that we used for the server. The keys, certs and keystores will be created in a secret named coherence-client-certs The keystore password secret is the Secret named client-keystore-secret we created above <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client-keystore.yaml If we list the certificate in the coherence-test namespace we should see the new client certificate and that it is ready. <markup lang=\"bash\" >kubectl -n coherence-test get certificate <markup >NAME READY SECRET AGE client-keystore True coherence-client-certs 12s server-keystore True coherence-server-certs 2m13s ",
            "title": "Create the Coherence Keys, Certs and KeyStores"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " This example is going to show how to use TLS (or SSL) to secure communication between different parts of a Coherence cluster and applications. This is quite a long guide as there are a number of things that can be secured wth TLS. This example shows how to secure various parts of Coherence clusters using TLS. Tip The complete source code for this example is in the Coherence Operator GitHub repository. In this example we are going to use Cert Manager to manage the keys and certs for our Coherence server and clients. Cert Manage makes managing certificates in Kubernetes very simple, but it isn&#8217;t the only solution. Although securing clusters with TLS is a common request, if running in a secure isolated Kubernetes cluster, you need to weigh up the pros and cons regarding the performance impact TLS will give over the additional security. Using Cert Manager we will ultimately end up with four k8s Secrets : A Secret containing the server keys, certs, keystore and truststore A Secret containing a single file containing the server keystore, truststore and key password A Secret containing the client keys, certs, keystore and truststore A Secret containing a single file containing the client keystore, truststore and key password If you do not want to use Cert Manager to try this example then a long as you have a way to create the required Secrets containing the keys and passwords above then you can skip to the section on Securing Coherence . What the Example will Cover Install the Operator Setting Up Cert-Manager Create the SelfSigned Issuer Create the CA Certificate Create the CA issuer Create the Coherence Keys, Certs and KeyStores Create the Server Keystore Password Secret Create the Server Certificate Create the Client Certificate Securing Coherence Clusters Build the Example Images Configure a Socket Provider Secure Cluster Membership Secure Extend Secure gRPC Install the Operator To run the examples below, you will need to have installed the Coherence Operator, do this using whatever method you prefer from the Installation Guide Setting Up Cert-Manager In this example we will use self-signed certs as this makes everything easy to get going. Cert Manager has a number of ways to configure real certificates for production use. Assuming that you&#8217;ve installed Cert Manager using one of the methods in its Install Guide we can proceed to created all of the required resources. Create the SelfSigned Issuer This is used to generate a root CA for use with the CA Issuer. Here we are using a ClusterIssuer so that we can use a single self-signed issuer across all namespaces. We could have instead created an Issuer in a single namespace. <markup lang=\"yaml\" title=\"manifests/selfsigned-issuer.yaml\" >apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-issuer spec: selfSigned: {} Create the ClusterIssuer with the following command. As this is a ClusterIssuer , is does not require a namespace. <markup lang=\"bash\" >kubectl apply -f manifests/selfsigned-issuer.yaml We can list the ClusterIssuers in the cluster: <markup lang=\"bash\" >kubectl get clusterissuer We should see that the selfsigned-issuer is present and is ready. <markup lang=\"bash\" >NAME READY AGE selfsigned-issuer True 14m Create the CA Certificate Were going to create an internal CA that will be used to sign our certificate requests for the Coherence server and clients that we will run later. Both the server and client will use the CA to validate a connection. To create the CA issuer, first create a self-signed CA certificate. <markup lang=\"yaml\" title=\"manifests/ca-cert.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate spec: issuerRef: name: selfsigned-issuer kind: ClusterIssuer group: cert-manager.io secretName: ca-cert duration: 2880h # 120d renewBefore: 360h # 15d commonName: Cert Admin isCA: true privateKey: size: 2048 usages: - digital signature - key encipherment The certificate will use the selfsigned-issuer cluster issuer we created above. There will be a secret named ca-cert created containing the key and certificate Note that the isCA field is set to true in the body of the spec. The CA issuer that we will create later will also be a ClusterIssuer , so in order for the issuer to find the Certificate above we will create the certificate in the cert-manager namespace, which is where Cert Manager is running. <markup lang=\"bash\" >kubectl -n cert-manager apply -f manifests/ca-cert.yaml We can see that the certificate was created and should be ready: <markup lang=\"bash\" >kubectl -n cert-manager get certificate <markup lang=\"bash\" >NAME READY SECRET AGE ca-certificate True ca-cert 12m There will also be a secret named ca-secret created in the cert-manager namespace. The Secret will contain the certificate and signing key, this will be created when the CA certificate is deployed, and the CA issuer will reference that secret. Create the CA issuer. As with the self-signed issuer above, we will create a ClusterIssuer for the CA issuer. <markup lang=\"bash\" title=\"manifests/ca-cert.yaml\" >apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: ca-issuer spec: ca: secretName: ca-cert The ca-issuer will use the ca-cert secret created by the ca-certificate Certificate we created above. Create the CA issuer with the following command. As this is a ClusterIssuer , is does not require a namespace. <markup lang=\"bash\" >kubectl apply -f manifests/ca-issuer.yaml You can then check that the issuer have been successfully configured by checking the status. <markup lang=\"bash\" >kubectl get clusterissuer We should see that both ClusterIssuers we created are present and is ready. <markup lang=\"bash\" >NAME READY AGE ca-issuer True 22m selfsigned-issuer True 31m Create the Coherence Keys, Certs and KeyStores As the Coherence server, and client in this example, are Java applications they will require Java keystores to hold the certificates. We can use Cert-Manager to create these for us. Create a Namespace We will run the Coherence cluster in a namespace called coherence-test , so we will first create this: <markup lang=\"bash\" >kubectl create ns coherence-test Create the Server Keystore Password Secret The keystore will be secured with a password. We will create this password in a Secret so that Cert-Manager can find and use it. The simplest way to create this secret is with kubectl: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic \\ server-keystore-secret --from-literal=password-key=[your-password] &#8230;&#8203;replacing [your-password] with the actual password you want to use. Resulting in a Secret similar to this: <markup lang=\"bash\" title=\"manifests/ca-cert.yaml\" >apiVersion: v1 kind: Secret metadata: name: server-keystore-secret data: password-key: \"cGFzc3dvcmQ=\" In this example the password used is password Create the Server Certificate We can now create the server certificate and keystore. <markup lang=\"yaml\" title=\"manifests/server-keystore.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: server-keystore spec: issuerRef: name: ca-issuer kind: ClusterIssuer group: cert-manager.io secretName: coherence-server-certs keystores: jks: create: true passwordSecretRef: key: password-key name: server-keystore-secret duration: 2160h # 90d renewBefore: 360h # 15d privateKey: size: 2048 algorithm: RSA encoding: PKCS1 usages: - digital signature - key encipherment - client auth - server auth commonName: Coherence Certs The issuer will the ClusterIssuer named ca-issuer that we created above. The keys, certs and keystores will be created in a secret named coherence-server-certs The keystore password secret is the Secret named server-keystore-secret we created above We can create the certificate in the coherence-test namespace with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/server-keystore.yaml If we list the certificate in the coherence-test namespace we should see the new certificate and that it is ready. <markup lang=\"bash\" >kubectl -n coherence-test get certificate <markup lang=\"bash\" >NAME READY SECRET AGE server-keystore True coherence-server-certs 4s If we list the secrets in the coherence-test namespace we should see both the password secret and the keystore secret: <markup lang=\"bash\" >kubectl -n coherence-test get secret <markup lang=\"bash\" >NAME TYPE DATA AGE coherence-server-certs kubernetes.io/tls 5 117s server-keystore-secret Opaque 1 2m9s Create the Client Certificate We can create the certificates and keystores for the client in exactly the same way we did for the server. Create a password secret for the client keystore: <markup lang=\"bash\" >kubectl -n coherence-test create secret generic \\ client-keystore-secret --from-literal=password-key=[your-password] Create the client certificate and keystore. <markup lang=\"yaml\" title=\"manifests/client-keystore.yaml\" >apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: client-keystore spec: issuerRef: name: ca-issuer kind: ClusterIssuer group: cert-manager.io secretName: coherence-client-certs keystores: jks: create: true passwordSecretRef: key: password-key name: client-keystore-secret duration: 2160h # 90d renewBefore: 360h # 15d privateKey: size: 2048 algorithm: RSA encoding: PKCS1 usages: - digital signature - key encipherment - client auth commonName: Coherence Certs The issuer is the same cluster-wide ca-issuer that we used for the server. The keys, certs and keystores will be created in a secret named coherence-client-certs The keystore password secret is the Secret named client-keystore-secret we created above <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client-keystore.yaml If we list the certificate in the coherence-test namespace we should see the new client certificate and that it is ready. <markup lang=\"bash\" >kubectl -n coherence-test get certificate <markup >NAME READY SECRET AGE client-keystore True coherence-client-certs 12s server-keystore True coherence-server-certs 2m13s ",
            "title": "Secure Coherence with TLS"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " This example includes a Maven project that will build a Coherence server and client images with configuration files that allow us to easily demonstrate TLS. To build the images run the following command: <markup lang=\"bash\" >./mvnw clean package jib:dockerBuild This will produce two images: tls-example-server:1.0.0 tls-example-client:1.0.0 These images can run secure or insecure depending on various system properties passed in at runtime. ",
            "title": "Build the Test Images"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " When configuring Coherence to use TLS, we need to configure a socket provider that Coherence can use to create secure socket. We then tell Coherence to use this provider in various places, such as Extend connections, cluster member TCMP connections etc. This configuration is typically done by adding the provider configuration to the Coherence operational configuration override file. The Coherence documentation has a lot of details on configuring socket providers in the section on Using SSL Secure Communication Below is an example that we will use on the server cluster members <markup lang=\"xml\" title=\"src/main/resources/tls-coherence-override.xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.tls.keystore\"/&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.keystore.password\"&gt;/empty.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.key.password\"&gt;/empty.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.tls.truststore\"/&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.truststore.password\"&gt;/empty.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; &lt;/cluster-config&gt; &lt;/coherence&gt; The file above has a number of key parts. We must give the provider a name so that we can refer to it in other configuration. This is done by setting the id attribute of the &lt;socket-provider&gt; element. In this case we name the provider \"tls\" in &lt;socket-provider id=\"tls\"&gt; . We set the &lt;protocol&gt; element to TLS to tell Coherence that this is a TLS socket. We need to set the keystore URL. If we always used a common location, we could hard code it in the configuration. In this case we will configure the &lt;keystore&gt;&lt;url&gt; element to be injected from a system property which we will configure at runtime &lt;url system-property=\"coherence.tls.keystore\"/&gt; . We obviously do not want hard-coded passwords in our configuration. In this example we will use a password provider, which is a class implementing the com.tangosol.net.PasswordProvider interface, that can provide the password by reading file. In this case the file will be the one from the password secret created above that we will mount into the container. <markup lang=\"xml\" title=\"src/main/resources/server-cache-config.xml\" >&lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.keystore.password\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; In the snippet above the password file location will be passed in using the coherence.tls.keystore.password system property. We declare another password provider for the private key password. We then declare the configuration for the truststore, which follows the same pattern as the keystore. The configuration above is included in both of the example images that we built above. ",
            "title": "Configure a Socket Provider"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " By this point, you should have installed the Operator and have the four Secrets required, either created by Cert Manager, or manually. Now we can secure Coherence clusters. Build the Test Images This example includes a Maven project that will build a Coherence server and client images with configuration files that allow us to easily demonstrate TLS. To build the images run the following command: <markup lang=\"bash\" >./mvnw clean package jib:dockerBuild This will produce two images: tls-example-server:1.0.0 tls-example-client:1.0.0 These images can run secure or insecure depending on various system properties passed in at runtime. Configure a Socket Provider When configuring Coherence to use TLS, we need to configure a socket provider that Coherence can use to create secure socket. We then tell Coherence to use this provider in various places, such as Extend connections, cluster member TCMP connections etc. This configuration is typically done by adding the provider configuration to the Coherence operational configuration override file. The Coherence documentation has a lot of details on configuring socket providers in the section on Using SSL Secure Communication Below is an example that we will use on the server cluster members <markup lang=\"xml\" title=\"src/main/resources/tls-coherence-override.xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.tls.keystore\"/&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.keystore.password\"&gt;/empty.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.key.password\"&gt;/empty.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;key-store&gt; &lt;url system-property=\"coherence.tls.truststore\"/&gt; &lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.truststore.password\"&gt;/empty.txt&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; &lt;/cluster-config&gt; &lt;/coherence&gt; The file above has a number of key parts. We must give the provider a name so that we can refer to it in other configuration. This is done by setting the id attribute of the &lt;socket-provider&gt; element. In this case we name the provider \"tls\" in &lt;socket-provider id=\"tls\"&gt; . We set the &lt;protocol&gt; element to TLS to tell Coherence that this is a TLS socket. We need to set the keystore URL. If we always used a common location, we could hard code it in the configuration. In this case we will configure the &lt;keystore&gt;&lt;url&gt; element to be injected from a system property which we will configure at runtime &lt;url system-property=\"coherence.tls.keystore\"/&gt; . We obviously do not want hard-coded passwords in our configuration. In this example we will use a password provider, which is a class implementing the com.tangosol.net.PasswordProvider interface, that can provide the password by reading file. In this case the file will be the one from the password secret created above that we will mount into the container. <markup lang=\"xml\" title=\"src/main/resources/server-cache-config.xml\" >&lt;password-provider&gt; &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;String&lt;/param-type&gt; &lt;param-value system-property=\"coherence.tls.keystore.password\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/password-provider&gt; In the snippet above the password file location will be passed in using the coherence.tls.keystore.password system property. We declare another password provider for the private key password. We then declare the configuration for the truststore, which follows the same pattern as the keystore. The configuration above is included in both of the example images that we built above. ",
            "title": "Securing Coherence"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " Remember that we exposed a number of ports in our Coherence cluster, one of these was REST management on port 30000 . We can use this along with curl to enquire about the cluster state. We need to use kubectl to forward a local port to one of the Coherence Pods. Open another terminal session and run the following command: <markup lang=\"bash\" >kubectl -n coherence-test port-forward tls-cluster-0 30000:30000 This will forward port 30000 on the local machine (e.g. your dev laptop) to the tls-cluster-0 Pod. We can now obtain the cluster state from the REST endpoint with the following command: <markup lang=\"bash\" >curl -X GET http://127.0.0.1:30000/management/coherence/cluster or if you have the jq utility we can pretty print the json output: <markup lang=\"bash\" >curl -X GET http://127.0.0.1:30000/management/coherence/cluster | jq We will see json something like this: <markup lang=\"json\" >{ \"links\": [ ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2021-03-07T12:27:20.193Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"22.06\", \"running\": true, \"clusterName\": \"test-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2021-03-07 12:24:32.982, Address=10.244.1.6:38271, MachineId=17483, Location=site:zone-two,rack:two,machine:operator-worker2,process:33,member:tls-cluster-1, Role=tls-cluster)\", \"Member(Id=2, Timestamp=2021-03-07 12:24:36.572, Address=10.244.2.5:36139, MachineId=21703, Location=site:zone-one,rack:one,machine:operator-worker,process:35,member:tls-cluster-0, Role=tls-cluster)\", \"Member(Id=3, Timestamp=2021-03-07 12:24:36.822, Address=10.244.1.7:40357, MachineId=17483, Location=site:zone-two,rack:two,machine:operator-worker2,process:34,member:tls-cluster-2, Role=tls-cluster)\" ], \"type\": \"Cluster\" } We can see that the cluster size is three. The member list shows details of the three Pods in the cluster ",
            "title": "Port Forward to the REST Management Port"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " To demonstrate that the cluster is secure we can start another cluster with yaml that does not enable TLS. <markup lang=\"yaml\" title=\"manifests/coherence-cluster-no-tls.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: no-tls-cluster spec: replicas: 3 image: tls-example-server:1.0.0 cluster: test-cluster coherence: cacheConfig: server-cache-config.xml ports: - name: extend port: 20000 - name: grpc port: 1408 - name: management port: 30000 - name: metrics port: 9612 This Coherence resource uses the same server image as the secure cluster This Coherence resource also uses the same cluster name as the secure cluster, test-cluster , so it should attempt to join with the secure cluster. If the existing cluster is not secure, we will end up with a cluster of six members. Install the yaml above into the coherence-test namespace: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/coherence-cluster-no-tls.yaml If we list the Pods in the coherence-test namespace then after a minute or so there should be three ready Pods. <markup lang=\"bash\" >kubectl -n coherence-test get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE tls-cluster-0 1/1 Running 0 15m tls-cluster-1 1/1 Running 0 15m tls-cluster-2 1/1 Running 0 15m no-tls-cluster-0 1/1 Running 0 78s no-tls-cluster-1 1/1 Running 0 78s no-tls-cluster-2 1/1 Running 0 78s There are six pods running, but they have not formed a six member cluster. If we re-run the curl command to query the REST management endpoint of the secure cluster we will see that the cluster size is still three: <markup lang=\"bash\" >curl -X GET http://127.0.0.1:30000/management/coherence/cluster -s | jq '.clusterSize' What happens is that the non-TLS members have effectively formed their own cluster of three members, but have not been able to form a cluster with the TLS enabled members. ",
            "title": "Start Non-TLS Cluster Members"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " After trying the example, remove both clusters with the corresponding kubectl delete commands so that they do not interfere with the next example. <markup lang=\"bash\" >kubectl -n coherence-test delete -f manifests/coherence-cluster-no-tls.yaml kubectl -n coherence-test delete -f manifests/coherence-cluster.yaml ",
            "title": "Cleanup"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " To enable TLS for an Extend proxy, we can just specify the name of the socket provider that we want to use in the &lt;proxy-scheme&gt; in the server&#8217;s cache configuration file. The snippet of configuration below is taken from the server-cache-config.xml file in the example source. <markup lang=\"xml\" title=\"src/main/resources/server-cache-config.xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;local-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;0.0.0.0&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;load-balancer&gt;client&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; The &lt;socket-provider&gt; element is empty by default, but is configured to be set from the system property named coherence.extend.socket.provider . This means that by default, Extend will run without TLS. If we start the server with the system property set to \"tls\", the name of our socket provider, then the proxy will use TLS. The Extend proxy will bind to all local addresses. The Extend proxy service will bind to port 20000. We add the additional coherence.extend.socket.provider system property to the spec.jvm.args section of the Coherence resource yaml we will use to deploy the server. The yaml below is identical to the yaml we used above to secure TCMP, but with the addition of the coherence.extend.socket.provider property. <markup lang=\"yaml\" title=\"coherence-cluster-extend.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: tls-cluster spec: replicas: 3 image: tls-example-server:1.0.0 cluster: test-cluster coherence: cacheConfig: server-cache-config.xml overrideConfig: tls-coherence-override.xml jvm: args: - -Dcoherence.socketprovider=tls - -Dcoherence.extend.socket.provider=tls - -Dcoherence.tls.keystore=file:/coherence/certs/keystore.jks - -Dcoherence.tls.keystore.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.key.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.truststore=file:/coherence/certs/truststore.jks - -Dcoherence.tls.truststore.password=file:/coherence/certs/credentials/password-key secretVolumes: - mountPath: coherence/certs name: coherence-server-certs - mountPath: coherence/certs/credentials name: server-keystore-secret ports: - name: extend port: 20000 - name: grpc port: 1408 The -Dcoherence.extend.socket.provider=tls has been added to enable TLS for the Extend proxy. Installing the yaml above will give us a Coherence cluster that uses TLS for both TCMP inter-cluster communication and for Extend connections. ",
            "title": "Secure the Proxy"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " We can install the Coherence cluster defined in the yaml above using kubectl : <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/coherence-cluster-extend.yaml After a minute or two the three Pods should be ready, which can be confirmed with kubectl . Because the yaml above declares a port named extend on port 20000 , the Coherence Operator will create a k8s Service to expose this port. The service name will be the Coherence resource name suffixed with the port name, so in this case tls-cluster-extend . As a Service in k8s can be looked up by DNS, we can use this service name as the host name for the client to connect to. ",
            "title": "Install the Cluster"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " Just like the server, we can include a socket provider configuration in the override file and configure the name of the socket provider that the client should use in the client&#8217;s cache configuration file. The socket provider configuration is identical to that shown already above (with the different FileBasedPasswordProvider class name). The Extend client code used in the src/main/java/com/oracle/coherence/examples/k8s/client/Main.java file in this example just starts a Coherence client, then obtains a NamedMap , and in a very long loop just puts data into the map, logging out the keys added. This is very trivial but allows us to see that the client is connected and working (or not). The snippet of xml below is from the client&#8217;s cache configuration file. <markup lang=\"xml\" title=\"src/main/resources/client-cache-config.xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; The &lt;socket-provider&gt; element is empty by default, but is configured to be set from the system property named coherence.extend.socket.provider . This means that by default, the Extend client will connect without TLS. If we start the client with the system property set to \"tls\", the name of our socket provider, then the client will use TLS. By default, the Extend client will connect loopback, on 127.0.0.1 but this can be overridden by setting the coherence.extend.address system property. We will use this when we deploy the client to specify the name of the Service that is used to expose the server&#8217;s Extend port. The Extend client will connect to port 20000. Although this can be overridden with a system property, port 20000 is also the default port used by the server, so there is no need to override it. ",
            "title": "Configure the Extend Client"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " As a demonstration we can first start a non-TLS client and see what happens. We can create a simple Pod that will run the client image using the yaml below. One of the features of newer Coherence CE versions is that configuration set via system properties prefixed with coherence. can also be set with corresponding environment variable names. The convention used for the environment variable name is to convert the system property name to uppercase and convert \".\" characters to \"_\", so setting the cache configuration file with the coherence.cacheconfig system property can be done using the COHERENCE_CACHECONFIG environment variable. This makes it simple to set Coherence configuration properties in a Pod yaml using environment variables instead of having to build a custom Java command line. <markup lang=\"yaml\" title=\"manifests/client-no-tls.yaml\" >apiVersion: v1 kind: Pod metadata: name: client spec: containers: - name: client image: tls-example-client:1.0.0 env: - name: COHERENCE_CACHECONFIG value: client-cache-config.xml - name: COHERENCE_EXTEND_ADDRESS value: tls-cluster-extend The client will use the client-cache-config.xml cache configuration file. The COHERENCE_EXTEND_ADDRESS is set to tls-cluster-extend , which is the name of the service exposing the server&#8217;s Extend port and which will be injected into the client&#8217;s cache configuration file, as explained above. We can run the client Pod with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client-no-tls.yaml If we look at the Pods now in the coherence-test namespace we will see the client running: <markup lang=\"bash\" >$ kubectl -n coherence-test get pod <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE client 1/1 Running 0 3s tls-cluster-0 1/1 Running 0 2m8s tls-cluster-1 1/1 Running 0 2m8s tls-cluster-2 1/1 Running 0 2m8s If we look at the log of the client Pod though we will see a stack trace with the cause: <markup lang=\"bash\" >kubectl -n coherence-test logs client <markup >2021-03-07 12:53:13.481/1.992 Oracle Coherence CE 22.06 &lt;Error&gt; (thread=main, member=n/a): Error while starting service \"Proxy\": com.tangosol.net.messaging.ConnectionException: could not establish a connection to one of the following addresses: [] This tells us that the client failed to connect to the cluster, because the client is not using TLS. We can remove the non-TLS client: <markup >kubectl -n coherence-test delete -f manifests/client-no-tls.yaml ",
            "title": "Start an Insecure Client"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " We can now modify the client yaml to run the client with TLS enabled. The client image already contains the tls-coherence-override.xml file with the configuration for the TLS socket provider. We need to set the relevant environment variables to inject the location of the keystores and tell Coherence to use the \"tls\" socket provider for the Extend connection. <markup lang=\"yaml\" title=\"manifests/client.yaml\" >apiVersion: v1 kind: Pod metadata: name: client spec: containers: - name: client image: tls-example-client:1.0.0 env: - name: COHERENCE_CACHECONFIG value: client-cache-config.xml - name: COHERENCE_EXTEND_ADDRESS value: tls-cluster-extend - name: COHERENCE_OVERRIDE value: tls-coherence-override.xml - name: COHERENCE_EXTEND_SOCKET_PROVIDER value: tls - name: COHERENCE_TLS_KEYSTORE value: file:/coherence/certs/keystore.jks - name: COHERENCE_TLS_KEYSTORE_PASSWORD value: /coherence/certs/credentials/password-key - name: COHERENCE_TLS_KEY_PASSWORD value: /coherence/certs/credentials/password-key - name: COHERENCE_TLS_TRUSTSTORE value: file:/coherence/certs/truststore.jks - name: COHERENCE_TLS_TRUSTSTORE_PASSWORD value: /coherence/certs/credentials/password-key volumeMounts: - name: coherence-client-certs mountPath: coherence/certs - name: keystore-credentials mountPath: coherence/certs/credentials volumes: - name: coherence-client-certs secret: defaultMode: 420 secretName: coherence-client-certs - name: keystore-credentials secret: defaultMode: 420 secretName: client-keystore-secret The yaml is identical to the non-TLS client with the addition of the environment variables to configure TLS. We create volume mount points to map the Secret volumes containing the keystores and password to directories in the container We mount the Secrets as volumes We can run the client Pod with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client.yaml If we now look at the client&#8217;s logs: <markup lang=\"bash\" >kubectl -n coherence-test logs client The end of the log should show the messages from the client as it puts each entry into a NamedMap . <markup >Put 0 Put 1 Put 2 Put 3 Put 4 Put 5 So now we have a TLS secured Extend proxy and client. We can remove the client and test cluster: <markup lang=\"bash\" >kubectl -n coherence-test delete -f manifests/client.yaml kubectl -n coherence-test delete -f manifests/coherence-cluster-extend.yaml ",
            "title": "Start a TLS Enabled Client"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " A common connection type to secure are client connections into the cluster from Coherence Extend clients. The Coherence documentation contains details on Using SSL to Secure Extend Client Communication for more in-depth details. As with securing TCMP, we can specify a socket provider in the Extend proxy configuration in the server&#8217;s cache configuration file and also in the remote scheme in the client&#8217;s cache configuration. In this example we will use exactly the same TLS socket provider configuration that we created above. The only difference being the name of the PasswordProvider class used by the client. At the time of writing this, Coherence does not include an implementation of PasswordProvider that reads from a file. The Coherence Operator injects one into the classpath of the server, but our simple client is not managed by the Operator. We have added a simple FileBasedPasswordProvider class to the client code in this example. Secure the Proxy To enable TLS for an Extend proxy, we can just specify the name of the socket provider that we want to use in the &lt;proxy-scheme&gt; in the server&#8217;s cache configuration file. The snippet of configuration below is taken from the server-cache-config.xml file in the example source. <markup lang=\"xml\" title=\"src/main/resources/server-cache-config.xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;local-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;0.0.0.0&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;load-balancer&gt;client&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; The &lt;socket-provider&gt; element is empty by default, but is configured to be set from the system property named coherence.extend.socket.provider . This means that by default, Extend will run without TLS. If we start the server with the system property set to \"tls\", the name of our socket provider, then the proxy will use TLS. The Extend proxy will bind to all local addresses. The Extend proxy service will bind to port 20000. We add the additional coherence.extend.socket.provider system property to the spec.jvm.args section of the Coherence resource yaml we will use to deploy the server. The yaml below is identical to the yaml we used above to secure TCMP, but with the addition of the coherence.extend.socket.provider property. <markup lang=\"yaml\" title=\"coherence-cluster-extend.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: tls-cluster spec: replicas: 3 image: tls-example-server:1.0.0 cluster: test-cluster coherence: cacheConfig: server-cache-config.xml overrideConfig: tls-coherence-override.xml jvm: args: - -Dcoherence.socketprovider=tls - -Dcoherence.extend.socket.provider=tls - -Dcoherence.tls.keystore=file:/coherence/certs/keystore.jks - -Dcoherence.tls.keystore.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.key.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.truststore=file:/coherence/certs/truststore.jks - -Dcoherence.tls.truststore.password=file:/coherence/certs/credentials/password-key secretVolumes: - mountPath: coherence/certs name: coherence-server-certs - mountPath: coherence/certs/credentials name: server-keystore-secret ports: - name: extend port: 20000 - name: grpc port: 1408 The -Dcoherence.extend.socket.provider=tls has been added to enable TLS for the Extend proxy. Installing the yaml above will give us a Coherence cluster that uses TLS for both TCMP inter-cluster communication and for Extend connections. Install the Cluster We can install the Coherence cluster defined in the yaml above using kubectl : <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/coherence-cluster-extend.yaml After a minute or two the three Pods should be ready, which can be confirmed with kubectl . Because the yaml above declares a port named extend on port 20000 , the Coherence Operator will create a k8s Service to expose this port. The service name will be the Coherence resource name suffixed with the port name, so in this case tls-cluster-extend . As a Service in k8s can be looked up by DNS, we can use this service name as the host name for the client to connect to. Configure the Extend Client Just like the server, we can include a socket provider configuration in the override file and configure the name of the socket provider that the client should use in the client&#8217;s cache configuration file. The socket provider configuration is identical to that shown already above (with the different FileBasedPasswordProvider class name). The Extend client code used in the src/main/java/com/oracle/coherence/examples/k8s/client/Main.java file in this example just starts a Coherence client, then obtains a NamedMap , and in a very long loop just puts data into the map, logging out the keys added. This is very trivial but allows us to see that the client is connected and working (or not). The snippet of xml below is from the client&#8217;s cache configuration file. <markup lang=\"xml\" title=\"src/main/resources/client-cache-config.xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; The &lt;socket-provider&gt; element is empty by default, but is configured to be set from the system property named coherence.extend.socket.provider . This means that by default, the Extend client will connect without TLS. If we start the client with the system property set to \"tls\", the name of our socket provider, then the client will use TLS. By default, the Extend client will connect loopback, on 127.0.0.1 but this can be overridden by setting the coherence.extend.address system property. We will use this when we deploy the client to specify the name of the Service that is used to expose the server&#8217;s Extend port. The Extend client will connect to port 20000. Although this can be overridden with a system property, port 20000 is also the default port used by the server, so there is no need to override it. Start an Insecure Client As a demonstration we can first start a non-TLS client and see what happens. We can create a simple Pod that will run the client image using the yaml below. One of the features of newer Coherence CE versions is that configuration set via system properties prefixed with coherence. can also be set with corresponding environment variable names. The convention used for the environment variable name is to convert the system property name to uppercase and convert \".\" characters to \"_\", so setting the cache configuration file with the coherence.cacheconfig system property can be done using the COHERENCE_CACHECONFIG environment variable. This makes it simple to set Coherence configuration properties in a Pod yaml using environment variables instead of having to build a custom Java command line. <markup lang=\"yaml\" title=\"manifests/client-no-tls.yaml\" >apiVersion: v1 kind: Pod metadata: name: client spec: containers: - name: client image: tls-example-client:1.0.0 env: - name: COHERENCE_CACHECONFIG value: client-cache-config.xml - name: COHERENCE_EXTEND_ADDRESS value: tls-cluster-extend The client will use the client-cache-config.xml cache configuration file. The COHERENCE_EXTEND_ADDRESS is set to tls-cluster-extend , which is the name of the service exposing the server&#8217;s Extend port and which will be injected into the client&#8217;s cache configuration file, as explained above. We can run the client Pod with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client-no-tls.yaml If we look at the Pods now in the coherence-test namespace we will see the client running: <markup lang=\"bash\" >$ kubectl -n coherence-test get pod <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE client 1/1 Running 0 3s tls-cluster-0 1/1 Running 0 2m8s tls-cluster-1 1/1 Running 0 2m8s tls-cluster-2 1/1 Running 0 2m8s If we look at the log of the client Pod though we will see a stack trace with the cause: <markup lang=\"bash\" >kubectl -n coherence-test logs client <markup >2021-03-07 12:53:13.481/1.992 Oracle Coherence CE 22.06 &lt;Error&gt; (thread=main, member=n/a): Error while starting service \"Proxy\": com.tangosol.net.messaging.ConnectionException: could not establish a connection to one of the following addresses: [] This tells us that the client failed to connect to the cluster, because the client is not using TLS. We can remove the non-TLS client: <markup >kubectl -n coherence-test delete -f manifests/client-no-tls.yaml Start a TLS Enabled Client We can now modify the client yaml to run the client with TLS enabled. The client image already contains the tls-coherence-override.xml file with the configuration for the TLS socket provider. We need to set the relevant environment variables to inject the location of the keystores and tell Coherence to use the \"tls\" socket provider for the Extend connection. <markup lang=\"yaml\" title=\"manifests/client.yaml\" >apiVersion: v1 kind: Pod metadata: name: client spec: containers: - name: client image: tls-example-client:1.0.0 env: - name: COHERENCE_CACHECONFIG value: client-cache-config.xml - name: COHERENCE_EXTEND_ADDRESS value: tls-cluster-extend - name: COHERENCE_OVERRIDE value: tls-coherence-override.xml - name: COHERENCE_EXTEND_SOCKET_PROVIDER value: tls - name: COHERENCE_TLS_KEYSTORE value: file:/coherence/certs/keystore.jks - name: COHERENCE_TLS_KEYSTORE_PASSWORD value: /coherence/certs/credentials/password-key - name: COHERENCE_TLS_KEY_PASSWORD value: /coherence/certs/credentials/password-key - name: COHERENCE_TLS_TRUSTSTORE value: file:/coherence/certs/truststore.jks - name: COHERENCE_TLS_TRUSTSTORE_PASSWORD value: /coherence/certs/credentials/password-key volumeMounts: - name: coherence-client-certs mountPath: coherence/certs - name: keystore-credentials mountPath: coherence/certs/credentials volumes: - name: coherence-client-certs secret: defaultMode: 420 secretName: coherence-client-certs - name: keystore-credentials secret: defaultMode: 420 secretName: client-keystore-secret The yaml is identical to the non-TLS client with the addition of the environment variables to configure TLS. We create volume mount points to map the Secret volumes containing the keystores and password to directories in the container We mount the Secrets as volumes We can run the client Pod with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client.yaml If we now look at the client&#8217;s logs: <markup lang=\"bash\" >kubectl -n coherence-test logs client The end of the log should show the messages from the client as it puts each entry into a NamedMap . <markup >Put 0 Put 1 Put 2 Put 3 Put 4 Put 5 So now we have a TLS secured Extend proxy and client. We can remove the client and test cluster: <markup lang=\"bash\" >kubectl -n coherence-test delete -f manifests/client.yaml kubectl -n coherence-test delete -f manifests/coherence-cluster-extend.yaml ",
            "title": "Secure Extend Connections"
        },
        {
            "location": "/examples/090_tls/README",
            "text": " Now we have a \"tls\" socket provider we can use it to secure Coherence. The Coherence documentation has a section on Securing Coherence TCMP with TLS . Securing communication between cluster members is very simple, we just set the coherence.socketprovider system property to the name of the socket provider we want to use. In our case this will be the \"tls\" provider we configured above, so we would use -Dcoherence.socketprovider=tls The yaml below is a Coherence resource that will cause the Operator to create a three member Coherence cluster. <markup lang=\"yaml\" title=\"manifests/coherence-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: tls-cluster spec: replicas: 3 image: tls-example-server:1.0.0 cluster: test-cluster coherence: overrideConfig: tls-coherence-override.xml cacheConfig: server-cache-config.xml jvm: args: - -Dcoherence.socketprovider=tls - -Dcoherence.tls.keystore=file:/coherence/certs/keystore.jks - -Dcoherence.tls.keystore.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.key.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.truststore=file:/coherence/certs/truststore.jks - -Dcoherence.tls.truststore.password=file:/coherence/certs/credentials/password-key secretVolumes: - mountPath: coherence/certs name: coherence-server-certs - mountPath: coherence/certs/credentials name: server-keystore-secret ports: - name: extend port: 20000 - name: grpc port: 1408 - name: management port: 30000 - name: metrics port: 9612 The image name is the server image built from this example project We specify a cluster name because we want to be able to demonstrate other Coherence deployments can or cannot join this cluster, so their yaml files will use this same cluster name. We set the Coherence override file to the file containing the \"tls\" socket provider configuration. We use a custom cache configuration file that has an Extend proxy that we can secure later. We set the coherence.socketprovider system property to use the \"tls\" provider, we also set a number of other properties that will set the locations of the keystores and password files to map to the secret volume mounts. We mount the certificate and password secrets to volumes We expose some ports for clients which we will use later, and for management, so we can enquire on the cluster state using REST. Install the yaml above into the coherence-test namespace: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/coherence-cluster.yaml If we list the Pods in the coherence-test namespace then after a minute or so there should be three ready Pods. <markup lang=\"bash\" >kubectl -n coherence-test get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE tls-cluster-0 1/1 Running 0 88s tls-cluster-1 1/1 Running 0 88s tls-cluster-2 1/1 Running 0 88s Port Forward to the REST Management Port Remember that we exposed a number of ports in our Coherence cluster, one of these was REST management on port 30000 . We can use this along with curl to enquire about the cluster state. We need to use kubectl to forward a local port to one of the Coherence Pods. Open another terminal session and run the following command: <markup lang=\"bash\" >kubectl -n coherence-test port-forward tls-cluster-0 30000:30000 This will forward port 30000 on the local machine (e.g. your dev laptop) to the tls-cluster-0 Pod. We can now obtain the cluster state from the REST endpoint with the following command: <markup lang=\"bash\" >curl -X GET http://127.0.0.1:30000/management/coherence/cluster or if you have the jq utility we can pretty print the json output: <markup lang=\"bash\" >curl -X GET http://127.0.0.1:30000/management/coherence/cluster | jq We will see json something like this: <markup lang=\"json\" >{ \"links\": [ ], \"clusterSize\": 3, \"membersDeparted\": [], \"memberIds\": [ 1, 2, 3 ], \"oldestMemberId\": 1, \"refreshTime\": \"2021-03-07T12:27:20.193Z\", \"licenseMode\": \"Development\", \"localMemberId\": 1, \"version\": \"22.06\", \"running\": true, \"clusterName\": \"test-cluster\", \"membersDepartureCount\": 0, \"members\": [ \"Member(Id=1, Timestamp=2021-03-07 12:24:32.982, Address=10.244.1.6:38271, MachineId=17483, Location=site:zone-two,rack:two,machine:operator-worker2,process:33,member:tls-cluster-1, Role=tls-cluster)\", \"Member(Id=2, Timestamp=2021-03-07 12:24:36.572, Address=10.244.2.5:36139, MachineId=21703, Location=site:zone-one,rack:one,machine:operator-worker,process:35,member:tls-cluster-0, Role=tls-cluster)\", \"Member(Id=3, Timestamp=2021-03-07 12:24:36.822, Address=10.244.1.7:40357, MachineId=17483, Location=site:zone-two,rack:two,machine:operator-worker2,process:34,member:tls-cluster-2, Role=tls-cluster)\" ], \"type\": \"Cluster\" } We can see that the cluster size is three. The member list shows details of the three Pods in the cluster Start Non-TLS Cluster Members To demonstrate that the cluster is secure we can start another cluster with yaml that does not enable TLS. <markup lang=\"yaml\" title=\"manifests/coherence-cluster-no-tls.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: no-tls-cluster spec: replicas: 3 image: tls-example-server:1.0.0 cluster: test-cluster coherence: cacheConfig: server-cache-config.xml ports: - name: extend port: 20000 - name: grpc port: 1408 - name: management port: 30000 - name: metrics port: 9612 This Coherence resource uses the same server image as the secure cluster This Coherence resource also uses the same cluster name as the secure cluster, test-cluster , so it should attempt to join with the secure cluster. If the existing cluster is not secure, we will end up with a cluster of six members. Install the yaml above into the coherence-test namespace: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/coherence-cluster-no-tls.yaml If we list the Pods in the coherence-test namespace then after a minute or so there should be three ready Pods. <markup lang=\"bash\" >kubectl -n coherence-test get pods <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE tls-cluster-0 1/1 Running 0 15m tls-cluster-1 1/1 Running 0 15m tls-cluster-2 1/1 Running 0 15m no-tls-cluster-0 1/1 Running 0 78s no-tls-cluster-1 1/1 Running 0 78s no-tls-cluster-2 1/1 Running 0 78s There are six pods running, but they have not formed a six member cluster. If we re-run the curl command to query the REST management endpoint of the secure cluster we will see that the cluster size is still three: <markup lang=\"bash\" >curl -X GET http://127.0.0.1:30000/management/coherence/cluster -s | jq '.clusterSize' What happens is that the non-TLS members have effectively formed their own cluster of three members, but have not been able to form a cluster with the TLS enabled members. Cleanup After trying the example, remove both clusters with the corresponding kubectl delete commands so that they do not interfere with the next example. <markup lang=\"bash\" >kubectl -n coherence-test delete -f manifests/coherence-cluster-no-tls.yaml kubectl -n coherence-test delete -f manifests/coherence-cluster.yaml Secure Extend Connections A common connection type to secure are client connections into the cluster from Coherence Extend clients. The Coherence documentation contains details on Using SSL to Secure Extend Client Communication for more in-depth details. As with securing TCMP, we can specify a socket provider in the Extend proxy configuration in the server&#8217;s cache configuration file and also in the remote scheme in the client&#8217;s cache configuration. In this example we will use exactly the same TLS socket provider configuration that we created above. The only difference being the name of the PasswordProvider class used by the client. At the time of writing this, Coherence does not include an implementation of PasswordProvider that reads from a file. The Coherence Operator injects one into the classpath of the server, but our simple client is not managed by the Operator. We have added a simple FileBasedPasswordProvider class to the client code in this example. Secure the Proxy To enable TLS for an Extend proxy, we can just specify the name of the socket provider that we want to use in the &lt;proxy-scheme&gt; in the server&#8217;s cache configuration file. The snippet of configuration below is taken from the server-cache-config.xml file in the example source. <markup lang=\"xml\" title=\"src/main/resources/server-cache-config.xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;local-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;0.0.0.0&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;load-balancer&gt;client&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; The &lt;socket-provider&gt; element is empty by default, but is configured to be set from the system property named coherence.extend.socket.provider . This means that by default, Extend will run without TLS. If we start the server with the system property set to \"tls\", the name of our socket provider, then the proxy will use TLS. The Extend proxy will bind to all local addresses. The Extend proxy service will bind to port 20000. We add the additional coherence.extend.socket.provider system property to the spec.jvm.args section of the Coherence resource yaml we will use to deploy the server. The yaml below is identical to the yaml we used above to secure TCMP, but with the addition of the coherence.extend.socket.provider property. <markup lang=\"yaml\" title=\"coherence-cluster-extend.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: tls-cluster spec: replicas: 3 image: tls-example-server:1.0.0 cluster: test-cluster coherence: cacheConfig: server-cache-config.xml overrideConfig: tls-coherence-override.xml jvm: args: - -Dcoherence.socketprovider=tls - -Dcoherence.extend.socket.provider=tls - -Dcoherence.tls.keystore=file:/coherence/certs/keystore.jks - -Dcoherence.tls.keystore.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.key.password=file:/coherence/certs/credentials/password-key - -Dcoherence.tls.truststore=file:/coherence/certs/truststore.jks - -Dcoherence.tls.truststore.password=file:/coherence/certs/credentials/password-key secretVolumes: - mountPath: coherence/certs name: coherence-server-certs - mountPath: coherence/certs/credentials name: server-keystore-secret ports: - name: extend port: 20000 - name: grpc port: 1408 The -Dcoherence.extend.socket.provider=tls has been added to enable TLS for the Extend proxy. Installing the yaml above will give us a Coherence cluster that uses TLS for both TCMP inter-cluster communication and for Extend connections. Install the Cluster We can install the Coherence cluster defined in the yaml above using kubectl : <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/coherence-cluster-extend.yaml After a minute or two the three Pods should be ready, which can be confirmed with kubectl . Because the yaml above declares a port named extend on port 20000 , the Coherence Operator will create a k8s Service to expose this port. The service name will be the Coherence resource name suffixed with the port name, so in this case tls-cluster-extend . As a Service in k8s can be looked up by DNS, we can use this service name as the host name for the client to connect to. Configure the Extend Client Just like the server, we can include a socket provider configuration in the override file and configure the name of the socket provider that the client should use in the client&#8217;s cache configuration file. The socket provider configuration is identical to that shown already above (with the different FileBasedPasswordProvider class name). The Extend client code used in the src/main/java/com/oracle/coherence/examples/k8s/client/Main.java file in this example just starts a Coherence client, then obtains a NamedMap , and in a very long loop just puts data into the map, logging out the keys added. This is very trivial but allows us to see that the client is connected and working (or not). The snippet of xml below is from the client&#8217;s cache configuration file. <markup lang=\"xml\" title=\"src/main/resources/client-cache-config.xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"coherence.extend.socket.provider\"/&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"coherence.extend.port\"&gt;20000&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; The &lt;socket-provider&gt; element is empty by default, but is configured to be set from the system property named coherence.extend.socket.provider . This means that by default, the Extend client will connect without TLS. If we start the client with the system property set to \"tls\", the name of our socket provider, then the client will use TLS. By default, the Extend client will connect loopback, on 127.0.0.1 but this can be overridden by setting the coherence.extend.address system property. We will use this when we deploy the client to specify the name of the Service that is used to expose the server&#8217;s Extend port. The Extend client will connect to port 20000. Although this can be overridden with a system property, port 20000 is also the default port used by the server, so there is no need to override it. Start an Insecure Client As a demonstration we can first start a non-TLS client and see what happens. We can create a simple Pod that will run the client image using the yaml below. One of the features of newer Coherence CE versions is that configuration set via system properties prefixed with coherence. can also be set with corresponding environment variable names. The convention used for the environment variable name is to convert the system property name to uppercase and convert \".\" characters to \"_\", so setting the cache configuration file with the coherence.cacheconfig system property can be done using the COHERENCE_CACHECONFIG environment variable. This makes it simple to set Coherence configuration properties in a Pod yaml using environment variables instead of having to build a custom Java command line. <markup lang=\"yaml\" title=\"manifests/client-no-tls.yaml\" >apiVersion: v1 kind: Pod metadata: name: client spec: containers: - name: client image: tls-example-client:1.0.0 env: - name: COHERENCE_CACHECONFIG value: client-cache-config.xml - name: COHERENCE_EXTEND_ADDRESS value: tls-cluster-extend The client will use the client-cache-config.xml cache configuration file. The COHERENCE_EXTEND_ADDRESS is set to tls-cluster-extend , which is the name of the service exposing the server&#8217;s Extend port and which will be injected into the client&#8217;s cache configuration file, as explained above. We can run the client Pod with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client-no-tls.yaml If we look at the Pods now in the coherence-test namespace we will see the client running: <markup lang=\"bash\" >$ kubectl -n coherence-test get pod <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE client 1/1 Running 0 3s tls-cluster-0 1/1 Running 0 2m8s tls-cluster-1 1/1 Running 0 2m8s tls-cluster-2 1/1 Running 0 2m8s If we look at the log of the client Pod though we will see a stack trace with the cause: <markup lang=\"bash\" >kubectl -n coherence-test logs client <markup >2021-03-07 12:53:13.481/1.992 Oracle Coherence CE 22.06 &lt;Error&gt; (thread=main, member=n/a): Error while starting service \"Proxy\": com.tangosol.net.messaging.ConnectionException: could not establish a connection to one of the following addresses: [] This tells us that the client failed to connect to the cluster, because the client is not using TLS. We can remove the non-TLS client: <markup >kubectl -n coherence-test delete -f manifests/client-no-tls.yaml Start a TLS Enabled Client We can now modify the client yaml to run the client with TLS enabled. The client image already contains the tls-coherence-override.xml file with the configuration for the TLS socket provider. We need to set the relevant environment variables to inject the location of the keystores and tell Coherence to use the \"tls\" socket provider for the Extend connection. <markup lang=\"yaml\" title=\"manifests/client.yaml\" >apiVersion: v1 kind: Pod metadata: name: client spec: containers: - name: client image: tls-example-client:1.0.0 env: - name: COHERENCE_CACHECONFIG value: client-cache-config.xml - name: COHERENCE_EXTEND_ADDRESS value: tls-cluster-extend - name: COHERENCE_OVERRIDE value: tls-coherence-override.xml - name: COHERENCE_EXTEND_SOCKET_PROVIDER value: tls - name: COHERENCE_TLS_KEYSTORE value: file:/coherence/certs/keystore.jks - name: COHERENCE_TLS_KEYSTORE_PASSWORD value: /coherence/certs/credentials/password-key - name: COHERENCE_TLS_KEY_PASSWORD value: /coherence/certs/credentials/password-key - name: COHERENCE_TLS_TRUSTSTORE value: file:/coherence/certs/truststore.jks - name: COHERENCE_TLS_TRUSTSTORE_PASSWORD value: /coherence/certs/credentials/password-key volumeMounts: - name: coherence-client-certs mountPath: coherence/certs - name: keystore-credentials mountPath: coherence/certs/credentials volumes: - name: coherence-client-certs secret: defaultMode: 420 secretName: coherence-client-certs - name: keystore-credentials secret: defaultMode: 420 secretName: client-keystore-secret The yaml is identical to the non-TLS client with the addition of the environment variables to configure TLS. We create volume mount points to map the Secret volumes containing the keystores and password to directories in the container We mount the Secrets as volumes We can run the client Pod with the following command: <markup lang=\"bash\" >kubectl -n coherence-test apply -f manifests/client.yaml If we now look at the client&#8217;s logs: <markup lang=\"bash\" >kubectl -n coherence-test logs client The end of the log should show the messages from the client as it puts each entry into a NamedMap . <markup >Put 0 Put 1 Put 2 Put 3 Put 4 Put 5 So now we have a TLS secured Extend proxy and client. We can remove the client and test cluster: <markup lang=\"bash\" >kubectl -n coherence-test delete -f manifests/client.yaml kubectl -n coherence-test delete -f manifests/coherence-cluster-extend.yaml ",
            "title": "Secure Cluster Membership"
        },
        {
            "location": "/docs/management/100_tmb_test",
            "text": " Coherence provides utilities that can be used to test network performance, which obviously has a big impact on a distributed system such as Coherence. The documentation for these utilities can be found in the official Coherence Documentation . Whilst generally these tests would be run on server hardware, with more and more Coherence deployments moving into the cloud and into Kubernetes these tests can also be performed in Pods to measure inter-Pod network performance. This test can be used to see the impact of running Pods across different zones, or on different types of Kubernetes networks, with different Pod resource settings, etc. ",
            "title": "Coherence Network Testing"
        },
        {
            "location": "/docs/management/100_tmb_test",
            "text": " Create a yaml file that will create the Service and Pod for the listener: <markup lang=\"yaml\" title=\"message-bus-listener.yaml\" >apiVersion: v1 kind: Service metadata: name: message-bus-listener spec: selector: app: message-bus-listener ports: - protocol: TCP port: 8000 targetPort: mbus --- apiVersion: v1 kind: Pod metadata: name: message-bus-listener labels: app: message-bus-listener spec: restartPolicy: Never containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06 ports: - name: mbus containerPort: 8000 protocol: TCP command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 This example uses a Coherence CE image, but any image with coherence.jar in it could be used. The command line that the container will execute is exactly the same as that for the listener process in the Coherence Documentation . Start the listener Pod : <markup lang=\"bash\" >kubectl create -f message-bus-listener.yaml Retrieving the logs for the listener Pod the messages should show that the Pod has started: <markup lang=\"bash\" >kubectl logs pod/message-bus-listener OPEN event for tmb://message-bus-listener:8000 ",
            "title": "Run the Listener Pod"
        },
        {
            "location": "/docs/management/100_tmb_test",
            "text": "<markup lang=\"yaml\" title=\"message-bus-sender.yaml\" >apiVersion: v1 kind: Pod metadata: name: message-bus-sender labels: app: message-bus-sender spec: restartPolicy: Never containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06 command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 - -peer - tmb://message-bus-listener:8000 Again, the command line is the same as that for the sender process in the Coherence Documentation . The peer address uses the Service name message-bus-listener from the sender yaml . Start the sender Pod : <markup lang=\"bash\" >kubectl create -f message-bus-sender.yaml Retrieving the logs for the sender Pod the messages should show that the Pod has started and show the test results: <markup lang=\"bash\" >kubectl logs pod/message-bus-sender OPEN event for tmb://message-bus-sender:8000 CONNECT event for tmb://message-bus-listener:8000 on tmb://message-bus-sender:8000 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 Note Don&#8217;t forget to stop the Pods after obtaining the results: <markup lang=\"bash\" >kubectl delete -f message-bus-sender.yaml kubectl delete -f message-bus-listener.yaml ",
            "title": "Run the Sender Pod"
        },
        {
            "location": "/docs/management/100_tmb_test",
            "text": " In the example above the Pods will be scheduled wherever Kubernetes decides to put them. This could have a big impact on the test result for different test runs. For example in a Kubernetes cluster that spans zones and data centres, if the two Pods get scheduled in different data centres this will have worse results than if the two Pods get scheduled onto the same node. To get consistent results add node selectors, taints, tolerations etc, as covered in the Kubernetes assign Pods to Nodes documentation. ",
            "title": "Run Pods on Specific Nodes"
        },
        {
            "location": "/docs/management/100_tmb_test",
            "text": " The message bus test can easily be run using Pods in Kubernetes. Using the example from the Coherence documentation there will need to be two Pods , a listener and a sender. This example will create a Service for the listener so that the sender Pod can use the Service name to resolve the listener Pod address. Run the Listener Pod Create a yaml file that will create the Service and Pod for the listener: <markup lang=\"yaml\" title=\"message-bus-listener.yaml\" >apiVersion: v1 kind: Service metadata: name: message-bus-listener spec: selector: app: message-bus-listener ports: - protocol: TCP port: 8000 targetPort: mbus --- apiVersion: v1 kind: Pod metadata: name: message-bus-listener labels: app: message-bus-listener spec: restartPolicy: Never containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06 ports: - name: mbus containerPort: 8000 protocol: TCP command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 This example uses a Coherence CE image, but any image with coherence.jar in it could be used. The command line that the container will execute is exactly the same as that for the listener process in the Coherence Documentation . Start the listener Pod : <markup lang=\"bash\" >kubectl create -f message-bus-listener.yaml Retrieving the logs for the listener Pod the messages should show that the Pod has started: <markup lang=\"bash\" >kubectl logs pod/message-bus-listener OPEN event for tmb://message-bus-listener:8000 Run the Sender Pod <markup lang=\"yaml\" title=\"message-bus-sender.yaml\" >apiVersion: v1 kind: Pod metadata: name: message-bus-sender labels: app: message-bus-sender spec: restartPolicy: Never containers: - name: coherence image: ghcr.io/oracle/coherence-ce:22.06 command: - java - -cp - /u01/oracle/oracle_home/coherence/lib/coherence.jar - com.oracle.common.net.exabus.util.MessageBusTest - -bind - tmb://0.0.0.0:8000 - -peer - tmb://message-bus-listener:8000 Again, the command line is the same as that for the sender process in the Coherence Documentation . The peer address uses the Service name message-bus-listener from the sender yaml . Start the sender Pod : <markup lang=\"bash\" >kubectl create -f message-bus-sender.yaml Retrieving the logs for the sender Pod the messages should show that the Pod has started and show the test results: <markup lang=\"bash\" >kubectl logs pod/message-bus-sender OPEN event for tmb://message-bus-sender:8000 CONNECT event for tmb://message-bus-listener:8000 on tmb://message-bus-sender:8000 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 now: throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0 Note Don&#8217;t forget to stop the Pods after obtaining the results: <markup lang=\"bash\" >kubectl delete -f message-bus-sender.yaml kubectl delete -f message-bus-listener.yaml Run Pods on Specific Nodes In the example above the Pods will be scheduled wherever Kubernetes decides to put them. This could have a big impact on the test result for different test runs. For example in a Kubernetes cluster that spans zones and data centres, if the two Pods get scheduled in different data centres this will have worse results than if the two Pods get scheduled onto the same node. To get consistent results add node selectors, taints, tolerations etc, as covered in the Kubernetes assign Pods to Nodes documentation. ",
            "title": "Run the Message Bus Test in Pods"
        },
        {
            "location": "/docs/logging/020_logging",
            "text": " Whilst any valid Java util logging configuration file may be used, the Coherence Operator injects a default logging configuration file into the coherence container that can be used to configure the logger to write logs to files under the /logs directory. The log files will have the name coherence-%g.log , where %g is the log file generation created as logs get rotated. This file will be injected into the container at /coherence-operator/utils/logging/logging.properties and will look something like this: <markup >com.oracle.coherence.handlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler com.oracle.coherence.level=FINEST java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.ConsoleHandler.level=FINEST java.util.logging.FileHandler.pattern=/logs/coherence-%g.log java.util.logging.FileHandler.limit=10485760 java.util.logging.FileHandler.count=50 java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format=%5$s%6$s%n To configure Cohrence and the logger some system properties need to be added to the jvm.args field of the Coherence CRD spec: For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" Coherence has been configured to use the Java util logging. The Coherence logger name has been set to com.oracle.coherence , which matches the logging configuration file. The Java util logging configuration file is set to the file injected by the Operator. ",
            "title": "Operator Provided Logging Configuration File"
        },
        {
            "location": "/docs/logging/020_logging",
            "text": " The logging configuration above configures Coherence to write logs to the /logs directory. For this location to be accessible to both the coherence container and to the fluentd container it needs to be created as a Volume in the Pod and mounted to both containers. As this Volume can be ephemeral and is typically not required to live longer than the Pod the simplest type of Volume to use is an emptyDir volume source. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs An additional empty-dir Volume named logs has been added to the Coherence spec. The logs volume will be mounted at /logs in all containers in the Pod . ",
            "title": "Log Files Volume"
        },
        {
            "location": "/docs/logging/020_logging",
            "text": " Coherence will log to the console by default so to be able to ship logs to Elasticsearch it needs to be configured to write to log files. One way to do this is to add a Java Util Logging configuration file and then to configure Coherence to use the JDK logger. In the jvm.args section of the Coherence CRD the system properties should be added to set the configuration file used by Java util logging and to configure Coherence logging. See the Coherence Logging Config documentation for more details. There are alternative ways to configure the Java util logger besides using a configuration file, just as there are alternative logging frameworks that Coherence can be configured to use to produce log files. This example is going to use Java util logging as that is the simplest to demonstrate without requiring any additional logging libraries. Operator Provided Logging Configuration File Whilst any valid Java util logging configuration file may be used, the Coherence Operator injects a default logging configuration file into the coherence container that can be used to configure the logger to write logs to files under the /logs directory. The log files will have the name coherence-%g.log , where %g is the log file generation created as logs get rotated. This file will be injected into the container at /coherence-operator/utils/logging/logging.properties and will look something like this: <markup >com.oracle.coherence.handlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler com.oracle.coherence.level=FINEST java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.ConsoleHandler.level=FINEST java.util.logging.FileHandler.pattern=/logs/coherence-%g.log java.util.logging.FileHandler.limit=10485760 java.util.logging.FileHandler.count=50 java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format=%5$s%6$s%n To configure Cohrence and the logger some system properties need to be added to the jvm.args field of the Coherence CRD spec: For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" Coherence has been configured to use the Java util logging. The Coherence logger name has been set to com.oracle.coherence , which matches the logging configuration file. The Java util logging configuration file is set to the file injected by the Operator. Log Files Volume The logging configuration above configures Coherence to write logs to the /logs directory. For this location to be accessible to both the coherence container and to the fluentd container it needs to be created as a Volume in the Pod and mounted to both containers. As this Volume can be ephemeral and is typically not required to live longer than the Pod the simplest type of Volume to use is an emptyDir volume source. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs An additional empty-dir Volume named logs has been added to the Coherence spec. The logs volume will be mounted at /logs in all containers in the Pod . ",
            "title": "Configure Coherence to Log to Files"
        },
        {
            "location": "/docs/logging/020_logging",
            "text": " The ConfigMap used to provide the Fluentd configuration might look something like this: <markup lang=\"yaml\" >apiVersion: v1 kind: ConfigMap metadata: name: efk-config labels: component: coherence-efk-config data: fluentd-coherence.conf: | # Ignore fluentd messages &lt;match fluent.**&gt; @type null &lt;/match&gt; # Coherence Logs &lt;source&gt; @type tail path /logs/coherence-*.log pos_file /tmp/cohrence.log.pos read_from_head true tag coherence-cluster multiline_flush_interval 20s &lt;parse&gt; @type multiline format_firstline /^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3}/ format1 /^(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3})\\/(?&lt;uptime&gt;[0-9\\.]+) (?&lt;product&gt;.+) &lt;(?&lt;level&gt;[^\\s]+)&gt; \\(thread=(?&lt;thread&gt;.+), member=(?&lt;member&gt;.+)\\):[\\S\\s](?&lt;log&gt;.*)/ &lt;/parse&gt; &lt;/source&gt; &lt;filter coherence-cluster&gt; @type record_transformer &lt;record&gt; cluster \"#{ENV['COH_CLUSTER_NAME']}\" role \"#{ENV['COH_ROLE']}\" host \"#{ENV['HOSTNAME']}\" pod-uid \"#{ENV['COH_POD_UID']}\" &lt;/record&gt; &lt;/filter&gt; &lt;match coherence-cluster&gt; @type elasticsearch hosts \"http://elasticsearch-master:9200\" logstash_format true logstash_prefix coherence-cluster &lt;/match&gt; The name of the ConfigMap is efk-config to match the name specified in the Coherence CRD spec. The source section is configured to match log files with the name /logs/coherence-*.log , which is the name that Coherence logging has been configured to use. The pattern in the source section is a Fluentd pattern that matches the standard Coherence log message format. A filter section will add additional fields to the log message. These come from the environment variables that the Operator will inject into all containers in the Pod. In this case the Coherence cluster name, the Coherence role name, the Pod host name and Pod UID. The final section tells Fluentd how to ship the logs to Elasticsearch, in this case to the endpoint http://elasticsearch-master:9200 There are many ways to configure Fluentd, the example above is just one way and is in fact taken from one of the Operator&#8217;s functional tests. With the efk-config ConfigMap created in the same namespace as the Coherence resource the Coherence logs from the containers will now be shipped to Elasticsearch. ",
            "title": "The Fluentd Configuration File"
        },
        {
            "location": "/docs/logging/020_logging",
            "text": " With Coherence configured to write to log files, and those log files visible to other containers in the Pod the Fluentd side-car container can be added. The example yaml below shows a Coherence resource with the additional Fluentd container added. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs sideCars: - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.14-debian-elasticsearch7-1\" args: - \"-c\" - \"/etc/fluent.conf\" env: - name: \"FLUENTD_CONF\" value: \"fluentd-coherence.conf\" - name: \"FLUENT_ELASTICSEARCH_SED_DISABLE\" value: \"true\" configMapVolumes: - name: \"efk-config\" mountPath: \"/fluentd/etc/fluentd-coherence.conf\" subPath: \"fluentd-coherence.conf\" The fluentd container has been added to the sideCars list. This will create another container in the Pod exactly as configured. The FLUENTD_CONF environment variable has been set to the name of the configuration file that Fluentd should use. The standard Fluentd behaviour is to locate this file in the /fluentd/etc/ directory. The FLUENT_ELASTICSEARCH_SED_DISABLE environment variable has been set to work around a known issue here . An additional volume has been added from a ConfigMap named efk-config , that contains the Fluentd configuration to use. This will be mounted to the fluentd container at /fluentd/etc/fluentd-coherence.conf , which corresponds to the name of the file set in the FLUENTD_CONF environment variable. There is no need to add a /logs volume mount to the fluentd container. The operator will mount the logs Volume to all containers in the Pod . In the example above the Fluentd configuration has been provided from a ConfigMap . It could just as easily have come from a Secret or some other external Volume mount, or it could have been baked into the Fluentd image to be used. The Fluentd Configuration File The ConfigMap used to provide the Fluentd configuration might look something like this: <markup lang=\"yaml\" >apiVersion: v1 kind: ConfigMap metadata: name: efk-config labels: component: coherence-efk-config data: fluentd-coherence.conf: | # Ignore fluentd messages &lt;match fluent.**&gt; @type null &lt;/match&gt; # Coherence Logs &lt;source&gt; @type tail path /logs/coherence-*.log pos_file /tmp/cohrence.log.pos read_from_head true tag coherence-cluster multiline_flush_interval 20s &lt;parse&gt; @type multiline format_firstline /^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3}/ format1 /^(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3})\\/(?&lt;uptime&gt;[0-9\\.]+) (?&lt;product&gt;.+) &lt;(?&lt;level&gt;[^\\s]+)&gt; \\(thread=(?&lt;thread&gt;.+), member=(?&lt;member&gt;.+)\\):[\\S\\s](?&lt;log&gt;.*)/ &lt;/parse&gt; &lt;/source&gt; &lt;filter coherence-cluster&gt; @type record_transformer &lt;record&gt; cluster \"#{ENV['COH_CLUSTER_NAME']}\" role \"#{ENV['COH_ROLE']}\" host \"#{ENV['HOSTNAME']}\" pod-uid \"#{ENV['COH_POD_UID']}\" &lt;/record&gt; &lt;/filter&gt; &lt;match coherence-cluster&gt; @type elasticsearch hosts \"http://elasticsearch-master:9200\" logstash_format true logstash_prefix coherence-cluster &lt;/match&gt; The name of the ConfigMap is efk-config to match the name specified in the Coherence CRD spec. The source section is configured to match log files with the name /logs/coherence-*.log , which is the name that Coherence logging has been configured to use. The pattern in the source section is a Fluentd pattern that matches the standard Coherence log message format. A filter section will add additional fields to the log message. These come from the environment variables that the Operator will inject into all containers in the Pod. In this case the Coherence cluster name, the Coherence role name, the Pod host name and Pod UID. The final section tells Fluentd how to ship the logs to Elasticsearch, in this case to the endpoint http://elasticsearch-master:9200 There are many ways to configure Fluentd, the example above is just one way and is in fact taken from one of the Operator&#8217;s functional tests. With the efk-config ConfigMap created in the same namespace as the Coherence resource the Coherence logs from the containers will now be shipped to Elasticsearch. ",
            "title": "Add the Fluentd Side-Car"
        },
        {
            "location": "/docs/logging/020_logging",
            "text": " There are many ways to capture container logs in Kubernetes, one possibility that this guide will cover is using a Fluentd side-car container to ship log files to Elasticsearch. This is a common pattern and one the the Coherence CRD makes simple by allowing easy injection of additional containers. This guide is going to assume that the default logging related configurations provided by the operator will be used. For example, Coherence will be configured to use Java util logging for logs, and the default logging configuration file will be used. Whilst these things are not pre-requisites for shipping logs to Elasticsearch they are required to make the examples below work. To be able to send Coherence logs to Elasticsearch there are some steps that must be completed: Configure Coherence to log to files Add a Volume and VolumeMount to be used for log files Add the Fluentd side-car container Configure Coherence to Log to Files Coherence will log to the console by default so to be able to ship logs to Elasticsearch it needs to be configured to write to log files. One way to do this is to add a Java Util Logging configuration file and then to configure Coherence to use the JDK logger. In the jvm.args section of the Coherence CRD the system properties should be added to set the configuration file used by Java util logging and to configure Coherence logging. See the Coherence Logging Config documentation for more details. There are alternative ways to configure the Java util logger besides using a configuration file, just as there are alternative logging frameworks that Coherence can be configured to use to produce log files. This example is going to use Java util logging as that is the simplest to demonstrate without requiring any additional logging libraries. Operator Provided Logging Configuration File Whilst any valid Java util logging configuration file may be used, the Coherence Operator injects a default logging configuration file into the coherence container that can be used to configure the logger to write logs to files under the /logs directory. The log files will have the name coherence-%g.log , where %g is the log file generation created as logs get rotated. This file will be injected into the container at /coherence-operator/utils/logging/logging.properties and will look something like this: <markup >com.oracle.coherence.handlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler com.oracle.coherence.level=FINEST java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.ConsoleHandler.level=FINEST java.util.logging.FileHandler.pattern=/logs/coherence-%g.log java.util.logging.FileHandler.limit=10485760 java.util.logging.FileHandler.count=50 java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format=%5$s%6$s%n To configure Cohrence and the logger some system properties need to be added to the jvm.args field of the Coherence CRD spec: For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" Coherence has been configured to use the Java util logging. The Coherence logger name has been set to com.oracle.coherence , which matches the logging configuration file. The Java util logging configuration file is set to the file injected by the Operator. Log Files Volume The logging configuration above configures Coherence to write logs to the /logs directory. For this location to be accessible to both the coherence container and to the fluentd container it needs to be created as a Volume in the Pod and mounted to both containers. As this Volume can be ephemeral and is typically not required to live longer than the Pod the simplest type of Volume to use is an emptyDir volume source. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs An additional empty-dir Volume named logs has been added to the Coherence spec. The logs volume will be mounted at /logs in all containers in the Pod . Add the Fluentd Side-Car With Coherence configured to write to log files, and those log files visible to other containers in the Pod the Fluentd side-car container can be added. The example yaml below shows a Coherence resource with the additional Fluentd container added. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: cluster-one spec: jvm: args: - \"-Dcoherence.log=jdk\" - \"-Dcoherence.log.logger=com.oracle.coherence\" - \"-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties\" volumes: - name: logs emptyDir: {} volumeMounts: - name: logs mountPath: /logs sideCars: - name: fluentd image: \"fluent/fluentd-kubernetes-daemonset:v1.14-debian-elasticsearch7-1\" args: - \"-c\" - \"/etc/fluent.conf\" env: - name: \"FLUENTD_CONF\" value: \"fluentd-coherence.conf\" - name: \"FLUENT_ELASTICSEARCH_SED_DISABLE\" value: \"true\" configMapVolumes: - name: \"efk-config\" mountPath: \"/fluentd/etc/fluentd-coherence.conf\" subPath: \"fluentd-coherence.conf\" The fluentd container has been added to the sideCars list. This will create another container in the Pod exactly as configured. The FLUENTD_CONF environment variable has been set to the name of the configuration file that Fluentd should use. The standard Fluentd behaviour is to locate this file in the /fluentd/etc/ directory. The FLUENT_ELASTICSEARCH_SED_DISABLE environment variable has been set to work around a known issue here . An additional volume has been added from a ConfigMap named efk-config , that contains the Fluentd configuration to use. This will be mounted to the fluentd container at /fluentd/etc/fluentd-coherence.conf , which corresponds to the name of the file set in the FLUENTD_CONF environment variable. There is no need to add a /logs volume mount to the fluentd container. The operator will mount the logs Volume to all containers in the Pod . In the example above the Fluentd configuration has been provided from a ConfigMap . It could just as easily have come from a Secret or some other external Volume mount, or it could have been baked into the Fluentd image to be used. The Fluentd Configuration File The ConfigMap used to provide the Fluentd configuration might look something like this: <markup lang=\"yaml\" >apiVersion: v1 kind: ConfigMap metadata: name: efk-config labels: component: coherence-efk-config data: fluentd-coherence.conf: | # Ignore fluentd messages &lt;match fluent.**&gt; @type null &lt;/match&gt; # Coherence Logs &lt;source&gt; @type tail path /logs/coherence-*.log pos_file /tmp/cohrence.log.pos read_from_head true tag coherence-cluster multiline_flush_interval 20s &lt;parse&gt; @type multiline format_firstline /^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3}/ format1 /^(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3})\\/(?&lt;uptime&gt;[0-9\\.]+) (?&lt;product&gt;.+) &lt;(?&lt;level&gt;[^\\s]+)&gt; \\(thread=(?&lt;thread&gt;.+), member=(?&lt;member&gt;.+)\\):[\\S\\s](?&lt;log&gt;.*)/ &lt;/parse&gt; &lt;/source&gt; &lt;filter coherence-cluster&gt; @type record_transformer &lt;record&gt; cluster \"#{ENV['COH_CLUSTER_NAME']}\" role \"#{ENV['COH_ROLE']}\" host \"#{ENV['HOSTNAME']}\" pod-uid \"#{ENV['COH_POD_UID']}\" &lt;/record&gt; &lt;/filter&gt; &lt;match coherence-cluster&gt; @type elasticsearch hosts \"http://elasticsearch-master:9200\" logstash_format true logstash_prefix coherence-cluster &lt;/match&gt; The name of the ConfigMap is efk-config to match the name specified in the Coherence CRD spec. The source section is configured to match log files with the name /logs/coherence-*.log , which is the name that Coherence logging has been configured to use. The pattern in the source section is a Fluentd pattern that matches the standard Coherence log message format. A filter section will add additional fields to the log message. These come from the environment variables that the Operator will inject into all containers in the Pod. In this case the Coherence cluster name, the Coherence role name, the Pod host name and Pod UID. The final section tells Fluentd how to ship the logs to Elasticsearch, in this case to the endpoint http://elasticsearch-master:9200 There are many ways to configure Fluentd, the example above is just one way and is in fact taken from one of the Operator&#8217;s functional tests. With the efk-config ConfigMap created in the same namespace as the Coherence resource the Coherence logs from the containers will now be shipped to Elasticsearch. ",
            "title": "Log Capture with Fluentd"
        },
        {
            "location": "/docs/ports/010_overview",
            "text": " Adding Ports Adding additional container ports to the Coherence container. Expose Ports via Services Configuring Services used to expose ports. Prometheus ServiceMonitors Adding Prometheus ServiceMonitors to expose ports to be scraped for metrics. ",
            "title": "Guides to Adding and Exposing Ports"
        },
        {
            "location": "/docs/ports/010_overview",
            "text": " Almost every application deployed into a Kubernetes cluster needs to communicate with other processes to provide services to other processes or consume services to other processes. This is achieved by exposing ports on containers in Pods and optionally exposing those same ports using Services and ingress. The Coherence CRD spec makes it simple to add ports to the Coherence container and configure Services to expose those ports. Each additional port configured is exposed via its own Service . If the configuration of Services for ports provided by the Coherence CRD spec is not sufficient or cannot provide the required Service configuration then it is always possible to just create your own Services in Kubernetes. Guides to Adding and Exposing Ports Adding Ports Adding additional container ports to the Coherence container. Expose Ports via Services Configuring Services used to expose ports. Prometheus ServiceMonitors Adding Prometheus ServiceMonitors to expose ports to be scraped for metrics. ",
            "title": "Overview"
        },
        {
            "location": "/docs/jvm/070_debugger",
            "text": " One scenario for debugging is for the Coherence JVM to open a port and listen for a debugger connection request. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true port: 5005 suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.port field specifies the port the JVM will listen on for a debugger connection. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 The address=*:5005 value comes from the jvm.debug.port field The suspend=n value comes from the jvm.debug.suspend field If the jvm.debug.port is not specified the default value used by the Operator will be 5005 . ",
            "title": "Listening for a Debugger Connection"
        },
        {
            "location": "/docs/jvm/070_debugger",
            "text": " Another scenario for debugging is for the Coherence JVM to connect out to a listening debugger. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true attach: \"10.10.100.2:5000\" suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.attach field specifies the address of the debugger that the JVM will connect to. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=n,address=10.10.100.2:5000,suspend=n,timeout=10000 ",
            "title": "Attaching to a Debugger Connection"
        },
        {
            "location": "/docs/jvm/070_debugger",
            "text": " Occasionally it is useful to be able to connect a debugger to a JVM, and the Coherence CRD spec has fields to configure the Coherence container&#8217;s JVM to work with a debugger. The fields in the CRD will ultimately result in arguments being passed to the JVM and could have been added as plain JVM arguments, but having specific fields in the CRD makes it simpler to configure and the intention more obvious. The fields to control debug settings of the JVM are in the jvm.debug section of the CRD spec. Listening for a Debugger Connection One scenario for debugging is for the Coherence JVM to open a port and listen for a debugger connection request. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true port: 5005 suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.port field specifies the port the JVM will listen on for a debugger connection. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 The address=*:5005 value comes from the jvm.debug.port field The suspend=n value comes from the jvm.debug.suspend field If the jvm.debug.port is not specified the default value used by the Operator will be 5005 . Attaching to a Debugger Connection Another scenario for debugging is for the Coherence JVM to connect out to a listening debugger. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: jvm: debug: enabled: true attach: \"10.10.100.2:5000\" suspend: false The jvm.debug.enabled flag is set to true to enable debug mode. The jvm.debug.attach field specifies the address of the debugger that the JVM will connect to. The jvm.debug.suspend flag is set to false so that the JVM will start without waiting for a debugger to connect. The example above results in the following arguments being passed to the JVM: <markup >-agentlib:jdwp=transport=dt_socket,server=n,address=10.10.100.2:5000,suspend=n,timeout=10000 ",
            "title": "Debugger Configuration"
        },
        {
            "location": "/docs/applications/060_application_working_dir",
            "text": " When running a custom application there may be a requirement to run in a specific working directory. The working directory can be specified in the application.workingDir field in the Coherence spec. For example, a deployment uses a custom image catalogue:1.0.0 that requires a custom main class called com.acme.Catalogue , and that class takes additional arguments. In this example we&#8217;ll use two fictitious arguments such as a name and a language for the catalogue. the Coherence resource would look like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: image: catalogue:1.0.0 application: workingDir: \"/apps/catalogue\" main: \"com.acme.Catalogue\" The catalogue:1.0.0 image will be used. The Java command will be executed in the /apps/catalogue working directory. The Java main class executed will be com.acme.Catalogue The example would be equivalent to the Coherence container running: <markup lang=\"bash\" >$ cd /apps/catalogue $ java com.acme.Catalogue ",
            "title": "Set the Working Directory"
        },
        {
            "location": "/docs/other/045_security_context",
            "text": " To specify security settings for a Pod, include the securityContext field in the Coherence resource specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod. Here is a configuration file for a Pod that has a securityContext: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 ",
            "title": "Setting the Pod Security Context"
        },
        {
            "location": "/docs/other/045_security_context",
            "text": " To specify security settings for the Coherence container within the Pods, include the containerSecurityContext field in the Container manifest. The containerSecurityContext field is a SecurityContext object. Security settings that you specify in the containerSecurityContext field apply only to the individual Coherence container and the Operator init-container, and they override settings made at the Pod level in the securityContext field when there is overlap. Container settings do not affect the Pod&#8217;s Volumes. Here is the configuration file for a Coherence resource that has both the Pod and the container security context: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 containerSecurityContext: runAsUser: 2000 allowPrivilegeEscalation: false capabilities: add: [\"NET_ADMIN\", \"SYS_TIME\"] ",
            "title": "Setting the Coherence Container Security Context"
        },
        {
            "location": "/docs/other/045_security_context",
            "text": " Kubernetes allows you to configure a Security Context for both Pods and Containers. The Coherence CRD exposes both of these to allow you to set the security context configuration for the Coherence Pods and for the Coherence containers withing the Pods. For more details see the Kubernetes Security Context documentation. Setting the Pod Security Context To specify security settings for a Pod, include the securityContext field in the Coherence resource specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod. Here is a configuration file for a Pod that has a securityContext: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 Setting the Coherence Container Security Context To specify security settings for the Coherence container within the Pods, include the containerSecurityContext field in the Container manifest. The containerSecurityContext field is a SecurityContext object. Security settings that you specify in the containerSecurityContext field apply only to the individual Coherence container and the Operator init-container, and they override settings made at the Pod level in the securityContext field when there is overlap. Container settings do not affect the Pod&#8217;s Volumes. Here is the configuration file for a Coherence resource that has both the Pod and the container security context: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 containerSecurityContext: runAsUser: 2000 allowPrivilegeEscalation: false capabilities: add: [\"NET_ADMIN\", \"SYS_TIME\"] ",
            "title": "Pod &amp; Container SecurityContext"
        },
        {
            "location": "/docs/about/05_upgrade",
            "text": " Version 3 of the Coherence Operator is very different to version 2. There is only a single CRD named Coherence instead of the three CRDs used by v2, and the operator no longer uses Helm internally to install the Kubernetes resources. In terms of usage and concepts, the biggest change is that there are no longer clusters and roles. The Coherence CRD represents what would previously in v2 have been a role. A Coherence cluster that is made up of multiple roles will just require multiple Coherence resources deploying to Kubernetes. The simplification of the operator, and consequently the better reliability, far outweigh any advantage of being able to put multiple roles in a single yaml file. If this is desire just put multiple Coherence resource definitions in a single yaml file with the --- separator. For example: In Operator v2 a cluster may have been defined with two roles, storage and proxy like this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: storage replicas: 3 - role: proxy replicas: 2 In Operator v3 this needs to be two separate`Coherence` resources. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster-storage spec: - role: storage replicas: 3 cluster: my-cluster --- apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: my-cluster-proxy spec: - role: proxy replicas: 2 cluster: my-cluster To make both Coherence resources part of the same cluster the cluster field must now be set in both resources to the same value, in this case my-cluster . ",
            "title": "Upgrading from Operator v2"
        },
        {
            "location": "/docs/about/05_upgrade",
            "text": " In Operator v2 there were multiple images defined, one for Coherence and one used to provide application artifacts. Because of the application changes described only a single image now needs to be specified in the image field of the CRD spec. See the Applications section of the doecumentation for more details. ",
            "title": "Images"
        },
        {
            "location": "/docs/about/05_upgrade",
            "text": " Coherence applications in Operator v2 worked by application resources (jar files etc) being provided in an image that was loaded as an init-container in the Pod , and the application artifacts copied to the classpath of the Coherence container. In version 3 of the Operator there is only one image required that should contain all of the resources required for the application, including Coherence jar. This gives the application developer much more control over how the image is built and what resources it contains, as well as making it more obvious what is going to be run when the container starts. Images In Operator v2 there were multiple images defined, one for Coherence and one used to provide application artifacts. Because of the application changes described only a single image now needs to be specified in the image field of the CRD spec. See the Applications section of the doecumentation for more details. ",
            "title": "Applications"
        },
        {
            "location": "/docs/about/05_upgrade",
            "text": " A lot of the fields in the Coherence CRD are the same as when defining a role in version 2. Whilst a number of new fields and features have been added in version 3, a handful of fields have moved, and a small number, that no longer made sense, have been removed. The Coherence Spec page documents the full Coherence CRD, so it is simple to locate where a field might have moved to. ",
            "title": "CRD Differences"
        },
        {
            "location": "/docs/about/05_upgrade",
            "text": " Version 3 of the operator no longer has fields to configure a Fluentd side-car container. There are a lot of different ways to configure Fluentd and making the Operator accomodate all of these was becoming too much of a head-ache to do in a backwards compatible way. If a Fluentd side-car is required it can just be added to the Coherence resource spec as an additional container, so there is no limitation on the Fluentd configuration. See the Logging documentation for more examples. ",
            "title": "Logging and Fluentd"
        },
        {
            "location": "/docs/about/05_upgrade",
            "text": " Version 3 of the Operator no longer comes with the option to install Prometheus and/or Elasticsearch. This feature was only ever intended to make it easier to demo features that required Prometheus and Elasticsearch and keeping this up to date was a headache nobody needed. Both Prometheus and Elasticsearch have operators of their own which make installing them simple and importing the dashboards provided by the Coherence Operator simple too. ",
            "title": "Prometheus and Elasticsearch"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " A reference guide to the Coherence Operator CRD types. ",
            "title": "preambule"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " Action ActionJob CoherenceResourceSpec ApplicationSpec CloudNativeBuildPackSpec CoherenceSpec CoherenceTracingSpec CoherenceWKASpec ConfigMapVolumeSpec GlobalSpec ImageSpec JVMSpec JvmDebugSpec JvmGarbageCollectorSpec JvmMemorySpec JvmOutOfMemorySpec LocalObjectReference NamedPortSpec NetworkSpec PersistenceSpec PersistentStorageSpec PersistentVolumeClaim PersistentVolumeClaimObjectMeta PodDNSConfig PortSpecWithSSL Probe ProbeHandler ReadinessProbeSpec Resource Resources SSLSpec ScalingSpec SecretVolumeSpec ServiceMonitorSpec ServiceSpec StartQuorum StartQuorumStatus Coherence CoherenceList CoherenceResourceStatus CoherenceStatefulSetResourceSpec ",
            "title": "Table of Contents"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " Action is an action to execute when the StatefulSet becomes ready. Field Description Type Required name Action name string false probe This is the spec of some sort of probe to fire when the Coherence resource becomes ready &#42; Probe false job or this is the spec of a Job to create when the Coherence resource becomes ready &#42; ActionJob false Back to TOC ",
            "title": "Action"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " Field Description Type Required spec Spec will be used to create a Job, the name is the Coherence deployment name + \"-\" + the action name The Job will be fire and forget, we do not monitor it in the Operator. We set its owner to be the Coherence resource, so it gets deleted when the Coherence resource is deleted. batchv1.JobSpec true labels Labels are the extra labels to add to the Job. map[string]string false annotations Annotations to add to the Job. map[string]string false Back to TOC ",
            "title": "ActionJob"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CoherenceResourceSpec defines the specification of a Coherence resource. A Coherence resource is typically one or more Pods that perform the same functionality, for example storage members. Field Description Type Required image The name of the image. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; https://pkg.go.dev/k8s.io/api/core/v1#PullPolicy false imagePullSecrets ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec. If specified, these secrets will be passed to individual puller implementations for them to use. For example, in the case of docker, only DockerConfig type secrets are honored. More info: https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod [] LocalObjectReference false replicas The desired number of cluster members of this deployment. This is a pointer to distinguish between explicit zero and not specified. If not specified a default value of 3 will be used. This field cannot be negative. &#42;int32 false role The name of the role that this deployment represents in a Coherence cluster. This value will be used to set the Coherence role property for all members of this role string false appLabel An optional app label to apply to resources created for this deployment. This is useful for example to apply an app label for use by Istio. This field follows standard Kubernetes label syntax. &#42;string false versionLabel An optional version label to apply to resources created for this deployment. This is useful for example to apply a version label for use by Istio. This field follows standard Kubernetes label syntax. &#42;string false coherence The optional settings specific to Coherence functionality. &#42; CoherenceSpec false application The optional application specific settings. &#42; ApplicationSpec false jvm The JVM specific options &#42; JVMSpec false ports Ports specifies additional port mappings for the Pod and additional Services for those ports. [] NamedPortSpec false startQuorum StartQuorum controls the start-up order of this Coherence resource in relation to other Coherence resources. [] StartQuorum false env Env is additional environment variable mappings that will be passed to the Coherence container in the Pod. To specify extra variables add them as name value pairs the same as they would be added to a Pod containers spec. [] corev1.EnvVar false labels The extra labels to add to the all the Pods in this deployment. Labels here will add to or override those defined for the cluster. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations are free-form yaml that will be added to the Coherence cluster member Pods as annotations. Any annotations should be placed BELOW this \"annotations:\" key, for example: annotations: foo.io/one: \"value1\" + foo.io/two: \"value2\" + see: Kubernetes Annotations map[string]string false initContainers List of additional initialization containers to add to the deployment&#8217;s Pod. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ [] corev1.Container false sideCars List of additional side-car containers to add to the deployment&#8217;s Pod. [] corev1.Container false configMapVolumes A list of ConfigMaps to add as volumes. Each entry in the list will be added as a ConfigMap Volume to the deployment&#8217;s Pods and as a VolumeMount to all the containers and init-containers in the Pod. see: Add ConfigMap Volumes [] ConfigMapVolumeSpec false secretVolumes A list of Secrets to add as volumes. Each entry in the list will be added as a Secret Volume to the deployment&#8217;s Pods and as a VolumeMount to all the containers and init-containers in the Pod. see: Add Secret Volumes [] SecretVolumeSpec false volumes Volumes defines extra volume mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumes section of a Pod definition + as described in https://kubernetes.io/docs/concepts/storage/volumes/ [] corev1.Volume false volumeMounts VolumeMounts defines extra volume mounts to map to the additional volumes or PVCs declared above in store.volumes and store.volumeClaimTemplates [] corev1.VolumeMount false healthPort The port that the health check endpoint will bind to. &#42;int32 false readinessProbe The readiness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false livenessProbe The liveness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false startupProbe The startup probe config to be used for the Pods in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ &#42; ReadinessProbeSpec false readinessGates ReadinessGates defines a list of additional conditions that the kubelet evaluates for Pod readiness. See: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate [] corev1.PodReadinessGate false resources Resources is the optional resource requests and limits for the containers ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ + The Coherence operator does not apply any default resources. &#42; corev1.ResourceRequirements false affinity Affinity controls Pod scheduling preferences. ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity &#42; corev1.Affinity false nodeSelector NodeSelector is the Node labels for pod assignment ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector map[string]string false tolerations Tolerations for nodes that have taints on them. Useful if you want to dedicate nodes to just run the coherence container + For example: tolerations: + - key: \"key\" + operator: \"Equal\" + value: \"value\" + effect: \"NoSchedule\" + ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] corev1.Toleration false securityContext SecurityContext is the PodSecurityContext that will be added to all the Pods in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ &#42; corev1.PodSecurityContext false containerSecurityContext ContainerSecurityContext is the SecurityContext that will be added to the Coherence container in each Pod in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ &#42; corev1.SecurityContext false shareProcessNamespace Share a single process namespace between all the containers in a pod. When this is set containers will be able to view and signal processes from other containers in the same pod, and the first process in each container will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be set. Optional: Default to false. &#42;bool false hostIPC Use the host&#8217;s ipc namespace. Optional: Default to false. &#42;bool false network Configure various networks and DNS settings for Pods in this role. &#42; NetworkSpec false coherenceUtils The configuration for the Coherence operator image name &#42; ImageSpec false serviceAccountName The name to use for the service account to use when RBAC is enabled The role bindings must already have been created as this chart does not create them it just sets the serviceAccountName value in the Pod spec. string false automountServiceAccountToken Whether to auto-mount the Kubernetes API credentials for a service account &#42;bool false operatorRequestTimeout The timeout to apply to REST requests made back to the Operator from Coherence Pods. These requests are typically to obtain site and rack information for the Pod. &#42;int32 false activeDeadlineSeconds ActiveDeadlineSeconds is the optional duration in seconds the pod may be active on the node relative to StartTime before the system will actively try to mark it failed and kill associated containers. Value must be a positive integer. &#42;int64 false enableServiceLinks EnableServiceLinks indicates whether information about services should be injected into pod&#8217;s environment variables, matching the syntax of Docker links. Optional: Defaults to true. &#42;bool false preemptionPolicy PreemptionPolicy is the Policy for preempting pods with lower priority. One of Never, PreemptLowerPriority. Defaults to PreemptLowerPriority if unset. &#42; https://pkg.go.dev/k8s.io/api/core/v1#PreemptionPolicy false priorityClassName PriorityClassName, if specified, indicates the pod&#8217;s priority. \"system-node-critical\" and \"system-cluster-critical\" are two special keywords which indicate the highest priorities with the former being the highest priority. Any other name must be defined by creating a PriorityClass object with that name. If not specified, the pod priority will be default or zero if there is no default. &#42;string false restartPolicy Restart policy for all containers within the pod. One of Always, OnFailure, Never. Default to Always. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy &#42; https://pkg.go.dev/k8s.io/api/core/v1#RestartPolicy false runtimeClassName RuntimeClassName refers to a RuntimeClass object in the node.k8s.io group, which should be used to run this pod. If no RuntimeClass resource matches the named class, the pod will not be run. If unset or empty, the \"legacy\" RuntimeClass will be used, which is an implicit class with an empty definition that uses the default runtime handler. More info: https://git.k8s.io/enhancements/keps/sig-node/585-runtime-class &#42;string false schedulerName If specified, the pod will be dispatched by specified scheduler. If not specified, the pod will be dispatched by default scheduler. &#42;string false topologySpreadConstraints TopologySpreadConstraints describes how a group of pods ought to spread across topology domains. Scheduler will schedule pods in a way which abides by the constraints. All topologySpreadConstraints are ANDed. [] corev1.TopologySpreadConstraint false rackLabel RackLabel is an optional Node label to use for the value of the Coherence member&#8217;s rack name. The default labels to use are determined by the Operator. &#42;string false siteLabel SiteLabel is an optional Node label to use for the value of the Coherence member&#8217;s site name The default labels to use are determined by the Operator. &#42;string false Back to TOC ",
            "title": "CoherenceResourceSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ApplicationSpec is the specification of the application deployed into the Coherence. Field Description Type Required type The application type to execute. This field would be set if using the Coherence Graal image and running a none-Java application. For example if the application was a Node application this field would be set to \"node\". The default is to run a plain Java application. &#42;string false main Class is the Coherence container main class. The default value is com.tangosol.net.DefaultCacheServer. If the application type is non-Java this would be the name of the corresponding language specific runnable, for example if the application type is \"node\" the main may be a Javascript file. &#42;string false args Args is the optional arguments to pass to the main class. []string false workingDir The application folder in the custom artifacts Docker image containing application artifacts. This will effectively become the working directory of the Coherence container. If not set the application directory default value is \"/app\". &#42;string false cloudNativeBuildPack Optional settings that may be configured if using a Cloud Native Buildpack Image. For example an image build with the Spring Boot Maven/Gradle plugin. See: https://github.com/paketo-buildpacks/spring-boot and https://buildpacks.io/ &#42; CloudNativeBuildPackSpec false springBootFatJar SpringBootFatJar is the full path name to the Spring Boot fat jar if the application image has been built by just adding a Spring Boot fat jar to the image. If this field is set then the application will be run by executing this jar. For example, if this field is \"/app/libs/foo.jar\" the command line will be \"java -jar app/libs/foo.jar\" &#42;string false Back to TOC ",
            "title": "ApplicationSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CloudNativeBuildPackSpec is the configuration when using a Cloud Native Buildpack Image. For example an image build with the Spring Boot Maven/Gradle plugin. See: https://github.com/paketo-buildpacks/spring-boot and https://buildpacks.io/ Field Description Type Required enabled Enable or disable buildpack detection. The operator will automatically detect Cloud Native Buildpack images but if this auto-detection fails to work correctly for a specific image then this field can be set to true to signify that the image is a buildpack image or false to signify that it is not a buildpack image. &#42;bool false launcher &#160; &#42;string false Back to TOC ",
            "title": "CloudNativeBuildPackSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CoherenceSpec is the section of the CRD configures settings specific to Coherence. see: Coherence Configuration Field Description Type Required cacheConfig CacheConfig is the name of the cache configuration file to use see: Configure Cache Config File &#42;string false overrideConfig OverrideConfig is name of the Coherence operational configuration override file, the default is tangosol-coherence-override.xml see: Configure Operational Config File &#42;string false storageEnabled A boolean flag indicating whether members of this deployment are storage enabled. This value will set the corresponding coherence.distributed.localstorage System property. If not specified the default value is true. This flag is also used to configure the ScalingPolicy value if a value is not specified. If the StorageEnabled field is not specified or is true the scaling will be safe, if StorageEnabled is set to false scaling will be parallel. see: Configure Storage Enabled &#42;bool false persistence Persistence values configure the on-disc data persistence settings. The bool Enabled enables or disabled on disc persistence of data. see: Configure Persistence &#42; PersistenceSpec false logLevel The Coherence log level, default being 5 (info level). see: Configure Coherence log level &#42;int32 false management Management configures Coherence management over REST Note: Coherence management over REST will is available in Coherence version &gt;= 12.2.1.4. see: Management &amp; Diagnostics &#42; PortSpecWithSSL false metrics Metrics configures Coherence metrics publishing Note: Coherence metrics publishing will is available in Coherence version &gt;= 12.2.1.4. see: Metrics &#42; PortSpecWithSSL false tracing Tracing is used to configure Coherence distributed tracing functionality. &#42; CoherenceTracingSpec false allowEndangeredForStatusHA AllowEndangeredForStatusHA is a list of Coherence partitioned cache service names that are allowed to be in an endangered state when testing for StatusHA. Instances where a StatusHA check is performed include the readiness probe and when scaling a deployment. This field would not typically be used except in cases where a cache service is configured with a backup count greater than zero but it does not matter if caches in those services loose data due to member departure. Normally, such cache services would have a backup count of zero, which would automatically excluded them from the StatusHA check. []string false excludeFromWKA Exclude members of this deployment from being part of the cluster&#8217;s WKA list. see: Well Known Addressing &#42;bool false wka Specify an existing Coherence deployment to be used for WKA. If an existing deployment is to be used for WKA the ExcludeFromWKA is implicitly set to true. see: Well Known Addressing &#42; CoherenceWKASpec false skipVersionCheck Certain features rely on a version check prior to starting the server, e.g. metrics requires &gt;= 12.2.1.4. The version check relies on the ability of the start script to find coherence.jar but if due to how the image has been built this check is failing then setting this flag to true will skip version checking and assume that the latest coherence.jar is being used. &#42;bool false enableIpMonitor Enables the Coherence IP Monitor feature. The Operator disables the IP Monitor by default. &#42;bool false localPort LocalPort sets the Coherence unicast port. When manually configuring unicast ports, a single port is specified and the second port is automatically selected. If either of the ports are not available, then the default behavior is to select the next available port. For example, if port 9000 is configured for the first port (port1) and it is not available, then the next available port is automatically selected. The second port (port2) is automatically opened and defaults to the next available port after port1 (port1 + 1 if available). &#42;int32 false localPortAdjust LocalPortAdjust sets the Coherence unicast port adjust value. To specify a range of unicast ports from which ports are selected, include a port value that represents the upper limit of the port range. &#42; https://pkg.go.dev/k8s.io/apimachinery/pkg/util/intstr#IntOrString false Back to TOC ",
            "title": "CoherenceSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CoherenceTracingSpec configures Coherence tracing. Field Description Type Required ratio Ratio is the tracing sampling-ratio, which controls the likelihood of a tracing span being collected. For instance, a value of 1.0 will result in all tracing spans being collected, while a value of 0.1 will result in roughly 1 out of every 10 tracing spans being collected. A value of 0 indicates that tracing spans should only be collected if they are already in the context of another tracing span. With such a configuration, Coherence will not initiate tracing on its own, and it is up to the application to start an outer tracing span, from which Coherence will then collect inner tracing spans. A value of -1 disables tracing completely. The Coherence default is -1 if not overridden. For values other than -1, numbers between 0 and 1 are acceptable. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. &#42;resource.Quantity false Back to TOC ",
            "title": "CoherenceTracingSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CoherenceWKASpec configures Coherence well-known-addressing to use an existing Coherence deployment for WKA. Field Description Type Required deployment The name of the existing Coherence deployment to use for WKA. string true namespace The optional namespace of the existing Coherence deployment to use for WKA if different from this deployment&#8217;s namespace. string false addresses A list of addresses to be used for WKA. If this field is set, the WKA property for the Coherence cluster will be set using this value and the other WKA fields will be ignored. []string false Back to TOC ",
            "title": "CoherenceWKASpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ConfigMapVolumeSpec represents a ConfigMap that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add ConfigMap Volumes Field Description Type Required name The name of the ConfigMap to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the ConfigMap name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \"\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; https://pkg.go.dev/k8s.io/api/core/v1#MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \"\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the ConfigMap or its keys must be defined &#42;bool false Back to TOC ",
            "title": "ConfigMapVolumeSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " GlobalSpec is attributes that will be applied to all resources managed by the Operator. Field Description Type Required labels Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ map[string]string false Back to TOC ",
            "title": "GlobalSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ImageSpec defines the settings for a Docker image Field Description Type Required image The image name. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; https://pkg.go.dev/k8s.io/api/core/v1#PullPolicy false Back to TOC ",
            "title": "ImageSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " JVMSpec is the JVM configuration. Field Description Type Required classpath Classpath specifies additional items to add to the classpath of the JVM. []string false args Args specifies the options (System properties, -XX: args etc) to pass to the JVM. []string false debug The settings for enabling debug mode in the JVM. &#42; JvmDebugSpec false useContainerLimits If set to true Adds the -XX:+UseContainerSupport JVM option to ensure that the JVM respects any container resource limits. The default value is true &#42;bool false gc Set JVM garbage collector options. &#42; JvmGarbageCollectorSpec false diagnosticsVolume DiagnosticsVolume is the volume to write JVM diagnostic information to, for example heap dumps, JFRs etc. &#42; https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#volume-v1-core false memory Configure the JVM memory options. &#42; JvmMemorySpec false useJibClasspath A flag indicating whether to automatically add the default classpath for images created by the JIB tool https://github.com/GoogleContainerTools/jib If true then the /app/lib/* /app/classes and /app/resources entries are added to the JVM classpath. The default value fif not specified is true. &#42;bool false Back to TOC ",
            "title": "JVMSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " JvmDebugSpec the JVM Debug specific configuration. Field Description Type Required enabled Enabled is a flag to enable or disable running the JVM in debug mode. Default is disabled. &#42;bool false suspend A boolean true if the target VM is to be suspended immediately before the main class is loaded; false otherwise. The default value is false. &#42;bool false attach Attach specifies the address of the debugger that the JVM should attempt to connect back to instead of listening on a port. &#42;string false port The port that the debugger will listen on; the default is 5005. &#42;int32 false Back to TOC ",
            "title": "JvmDebugSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " JvmGarbageCollectorSpec is options for managing the JVM garbage collector. Field Description Type Required collector The name of the JVM garbage collector to use. G1 - adds the -XX:+UseG1GC option CMS - adds the -XX:+UseConcMarkSweepGC option Parallel - adds the -XX:+UseParallelGC Default - use the JVMs default collector The field value is case insensitive If not set G1 is used. If set to a value other than those above then the default collector for the JVM will be used. &#42;string false args Args specifies the GC options to pass to the JVM. []string false logging Enable the following GC logging args -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime Default is true &#42;bool false Back to TOC ",
            "title": "JvmGarbageCollectorSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " JvmMemorySpec is options for managing the JVM memory. Field Description Type Required heapSize HeapSize sets both the initial and max heap size values for the JVM. This will set both the -XX:InitialHeapSize and -XX:MaxHeapSize JVM options to the same value (the equivalent of setting -Xms and -Xmx to the same value). The format should be the same as that used when specifying these JVM options. If not set the JVM defaults are used. &#42;string false initialHeapSize InitialHeapSize sets the initial heap size value for the JVM. This will set the -XX:InitialHeapSize JVM option (the equivalent of setting -Xms). The format should be the same as that used when specifying this JVM options. NOTE: If the HeapSize field is set it will override this field. &#42;string false maxHeapSize MaxHeapSize sets the maximum heap size value for the JVM. This will set the -XX:MaxHeapSize JVM option (the equivalent of setting -Xmx). The format should be the same as that used when specifying this JVM options. NOTE: If the HeapSize field is set it will override this field. &#42;string false maxRAM Sets the JVM option -XX:MaxRAM=N which sets the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). &#42;string false percentage Percentage sets the initial and maximum and minimum heap percentage sizes to the same value, This will set the -XX:InitialRAMPercentage -XX:MinRAMPercentage and -XX:MaxRAMPercentage JVM options to the same value. The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps to the -XX:InitialRAMPercentage -XX:MinRAMPercentage and -XX:MaxRAMPercentage JVM options and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false initialRAMPercentage Set initial heap size as a percentage of total memory. The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set. Valid values are decimal numbers between 0 and 100. NOTE: If the Percentage field is set it will override this field. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps to the -XX:InitialRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false maxRAMPercentage Set maximum heap size as a percentage of total memory. The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set. Valid values are decimal numbers between 0 and 100. NOTE: If the Percentage field is set it will override this field. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps to the -XX:MaxRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false minRAMPercentage Set the minimal JVM Heap size as a percentage of the total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:MinRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false stackSize StackSize is the stack size value to pass to the JVM. The format should be the same as that used for Java&#8217;s -Xss JVM option. If not set the JVM defaults are used. &#42;string false metaspaceSize MetaspaceSize is the min/max metaspace size to pass to the JVM. This sets the -XX:MetaspaceSize and -XX:MaxMetaspaceSize=size JVM options. If not set the JVM defaults are used. &#42;string false directMemorySize DirectMemorySize sets the maximum total size (in bytes) of the New I/O (the java.nio package) direct-buffer allocations. This value sets the -XX:MaxDirectMemorySize JVM option. If not set the JVM defaults are used. &#42;string false nativeMemoryTracking Adds the -XX:NativeMemoryTracking=mode JVM options where mode is on of \"off\", \"summary\" or \"detail\", the default is \"summary\" If not set to \"off\" also add -XX:+PrintNMTStatistics &#42;string false onOutOfMemory Configure the JVM behaviour when an OutOfMemoryError occurs. &#42; JvmOutOfMemorySpec false Back to TOC ",
            "title": "JvmMemorySpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " JvmOutOfMemorySpec is options for managing the JVM behaviour when an OutOfMemoryError occurs. Field Description Type Required exit If set to true the JVM will exit when an OOM error occurs. Default is true &#42;bool false heapDump If set to true adds the -XX:+HeapDumpOnOutOfMemoryError JVM option to cause a heap dump to be created when an OOM error occurs. Default is true &#42;bool false Back to TOC ",
            "title": "JvmOutOfMemorySpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Field Description Type Required name Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names string true Back to TOC ",
            "title": "LocalObjectReference"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " NamedPortSpec defines a named port for a Coherence component Field Description Type Required name Name specifies the name of the port. string true port Port specifies the port used. int32 false protocol Protocol for container port. Must be UDP or TCP. Defaults to \"TCP\" &#42; https://pkg.go.dev/k8s.io/api/core/v1#Protocol false appProtocol The application protocol for this port. This field follows standard Kubernetes label syntax. Un-prefixed names are reserved for IANA standard service names (as per RFC-6335 and http://www.iana.org/assignments/service-names ). Non-standard protocols should use prefixed names such as mycompany.com/my-custom-protocol. &#42;string false nodePort The port on each node on which this service is exposed when type=NodePort or LoadBalancer. Usually assigned by the system. If specified, it will be allocated to the service if unused or else creation of the service will fail. If set, this field must be in the range 30000 - 32767 inclusive. Default is to auto-allocate a port if the ServiceType of this Service requires one. More info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport &#42;int32 false hostPort Number of port to expose on the host. If specified, this must be a valid port number, 0 &lt; x &lt; 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. &#42;int32 false hostIP What host IP to bind the external port to. &#42;string false service Service configures the Kubernetes Service used to expose the port. &#42; ServiceSpec false serviceMonitor The specification of a Prometheus ServiceMonitor resource that will be created for the Service being exposed for this port. &#42; ServiceMonitorSpec false exposeOnSts ExposeOnSTS is a flag to indicate that this port should also be exposed on the StatefulSetHeadless service. This is useful in cases where a service mesh such as Istio is being used and ports such as the Extend or gRPC ports are accessed via the StatefulSet service. The default is true so all additional ports are exposed on the StatefulSet headless service. &#42;bool false Back to TOC ",
            "title": "NamedPortSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " NetworkSpec configures various networking and DNS settings for Pods in a deployment. Field Description Type Required dnsConfig Specifies the DNS parameters of a pod. Parameters specified here will be merged to the generated DNS configuration based on DNSPolicy. &#42; PodDNSConfig false dnsPolicy Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. &#42; https://pkg.go.dev/k8s.io/api/core/v1#DNSPolicy false hostAliases HostAliases is an optional list of hosts and IPs that will be injected into the pod&#8217;s hosts file if specified. This is only valid for non-hostNetwork pods. [] corev1.HostAlias false hostNetwork Host networking requested for this pod. Use the host&#8217;s network namespace. If this option is set, the ports that will be used must be specified. Default to false. &#42;bool false hostname Specifies the hostname of the Pod If not specified, the pod&#8217;s hostname will be set to a system-defined value. &#42;string false setHostnameAsFQDN SetHostnameAsFQDN if true the pod&#8217;s hostname will be configured as the pod&#8217;s FQDN, rather than the leaf name (the default). In Linux containers, this means setting the FQDN in the hostname field of the kernel (the nodename field of struct utsname). In Windows containers, this means setting the registry value of hostname for the registry key HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\CurrentControlSet\\\\Services\\\\Tcpip\\\\Parameters to FQDN. If a pod does not have FQDN, this has no effect. Default to false. &#42;bool false subdomain Subdomain, if specified, the fully qualified Pod hostname will be \"&lt;hostname&gt;.&lt;subdomain&gt;.&lt;pod namespace&gt;.svc.&lt;cluster domain&gt;\". If not specified, the pod will not have a domain name at all. &#42;string false Back to TOC ",
            "title": "NetworkSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " PersistenceSpec is the spec for Coherence persistence. Field Description Type Required mode The persistence mode to use. Valid choices are \"on-demand\", \"active\", \"active-async\". This field will set the coherence.distributed.persistence-mode System property to \"default-\" + Mode. &#42;string false persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#volume-v1-core false snapshots Snapshot values configure the on-disc persistence data snapshot (backup) settings. These settings enable a different location for persistence snapshot data. If not set then snapshot files will be written to the same volume configured for persistence data in the Persistence section. &#42; PersistentStorageSpec false Back to TOC ",
            "title": "PersistenceSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " PersistentStorageSpec defines the persistence settings for the Coherence Field Description Type Required persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#volume-v1-core false Back to TOC ",
            "title": "PersistentStorageSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " PersistentVolumeClaim is a request for and claim to a persistent volume Field Description Type Required metadata Standard object&#8217;s metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata PersistentVolumeClaimObjectMeta false spec Spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims corev1.PersistentVolumeClaimSpec false Back to TOC ",
            "title": "PersistentVolumeClaim"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " PersistentVolumeClaimObjectMeta is metadata for the PersistentVolumeClaim. Field Description Type Required name Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/ string false labels Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ map[string]string false Back to TOC ",
            "title": "PersistentVolumeClaimObjectMeta"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Field Description Type Required nameservers A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. []string false searches A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed. []string false options A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. [] corev1.PodDNSConfigOption false Back to TOC ",
            "title": "PodDNSConfig"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " PortSpecWithSSL defines a port with SSL settings for a Coherence component Field Description Type Required enabled Enable or disable flag. &#42;bool false port The port to bind to. &#42;int32 false ssl SSL configures SSL settings for a Coherence component &#42; SSLSpec false Back to TOC ",
            "title": "PortSpecWithSSL"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " Probe is the handler that will be used to determine how to communicate with a Coherence deployment for operations like StatusHA checking and service suspension. StatusHA checking is primarily used during scaling of a deployment, a deployment must be in a safe Phase HA state before scaling takes place. If StatusHA handler is disabled for a deployment (by specifically setting Enabled to false then no check will take place and a deployment will be assumed to be safe). Field Description Type Required timeoutSeconds Number of seconds after which the handler times out (only applies to http and tcp handlers). Defaults to 1 second. Minimum value is 1. &#42;int false Back to TOC ",
            "title": "Probe"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ProbeHandler is the definition of a probe handler. Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false Back to TOC ",
            "title": "ProbeHandler"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ReadinessProbeSpec defines the settings for the Coherence Pod readiness probe Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false timeoutSeconds Number of seconds after which the probe times out. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false periodSeconds How often (in seconds) to perform the probe. &#42;int32 false successThreshold Minimum consecutive successes for the probe to be considered successful after having failed. &#42;int32 false failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. &#42;int32 false Back to TOC ",
            "title": "ReadinessProbeSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " Resource is a structure holding a resource to be managed Field Description Type Required kind &#160; ResourceType true name &#160; string true spec &#160; client.Object true Back to TOC ",
            "title": "Resource"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " Resources is a cloolection of resources to be managed. Field Description Type Required version &#160; int32 true items &#160; [] Resource true Back to TOC ",
            "title": "Resources"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " SSLSpec defines the SSL settings for a Coherence component over REST endpoint. Field Description Type Required enabled Enabled is a boolean flag indicating whether enables or disables SSL on the Coherence management over REST endpoint, the default is false (disabled). &#42;bool false secrets Secrets is the name of the k8s secret containing the Java key stores and password files. The secret should be in the same namespace as the Coherence resource. + This value MUST be provided if SSL is enabled on the Coherence management over REST endpoint. &#42;string false keyStore Keystore is the name of the Java key store file in the k8s secret to use as the SSL keystore when configuring component over REST to use SSL. &#42;string false keyStorePasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the keystore password when configuring component over REST to use SSL. &#42;string false keyPasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the key password when configuring component over REST to use SSL. &#42;string false keyStoreAlgorithm KeyStoreAlgorithm is the name of the keystore algorithm for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509 &#42;string false keyStoreProvider KeyStoreProvider is the name of the keystore provider for the keystore in the k8s secret used when configuring component over REST to use SSL. &#42;string false keyStoreType KeyStoreType is the name of the Java keystore type for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false trustStore TrustStore is the name of the Java trust store file in the k8s secret to use as the SSL trust store when configuring component over REST to use SSL. &#42;string false trustStorePasswordFile TrustStorePasswordFile is the name of the file in the k8s secret containing the trust store password when configuring component over REST to use SSL. &#42;string false trustStoreAlgorithm TrustStoreAlgorithm is the name of the keystore algorithm for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509. &#42;string false trustStoreProvider TrustStoreProvider is the name of the keystore provider for the trust store in the k8s secret used when configuring component over REST to use SSL. &#42;string false trustStoreType TrustStoreType is the name of the Java keystore type for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false requireClientCert RequireClientCert is a boolean flag indicating whether the client certificate will be authenticated by the server (two-way SSL) when configuring component over REST to use SSL. + If not set the default is false &#42;bool false Back to TOC ",
            "title": "SSLSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ScalingSpec is the configuration to control safe scaling. Field Description Type Required policy ScalingPolicy describes how the replicas of the deployment will be scaled. The default if not specified is based upon the value of the StorageEnabled field. If StorageEnabled field is not specified or is true the default scaling will be safe, if StorageEnabled is set to false the default scaling will be parallel. &#42;ScalingPolicy false probe The probe to use to determine whether a deployment is Phase HA. If not set the default handler will be used. In most use-cases the default handler would suffice but in advanced use-cases where the application code has a different concept of Phase HA to just checking Coherence services then a different handler may be specified. &#42; Probe false Back to TOC ",
            "title": "ScalingSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " SecretVolumeSpec represents a Secret that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add Secret Volumes Field Description Type Required name The name of the Secret to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the Secret name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \"\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; https://pkg.go.dev/k8s.io/api/core/v1#MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \"\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the Secret or its keys must be defined &#42;bool false Back to TOC ",
            "title": "SecretVolumeSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ServiceMonitorSpec the ServiceMonitor spec for a port service. Field Description Type Required enabled Enabled is a flag to enable or disable creation of a Prometheus ServiceMonitor for a port. If Prometheus ServiceMonitor CR is not installed no ServiceMonitor then even if this flag is true no ServiceMonitor will be created. &#42;bool false labels Additional labels to add to the ServiceMonitor. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false jobLabel The label to use to retrieve the job name from. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec string false targetLabels TargetLabels transfers labels on the Kubernetes Service onto the target. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec []string false podTargetLabels PodTargetLabels transfers labels on the Kubernetes Pod onto the target. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec []string false sampleLimit SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec &#42;uint64 false path HTTP path to scrape for metrics. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint string false scheme HTTP scheme to use for scraping. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint string false params Optional HTTP URL parameters See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint map[string][]string false interval Interval at which metrics should be scraped See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint monitoringv1.Duration false scrapeTimeout Timeout after which the scrape is ended See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint monitoringv1.Duration false tlsConfig TLS configuration to use when scraping the endpoint See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;monitoringv1.TLSConfig false bearerTokenFile File to read bearer token for scraping targets. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint string false bearerTokenSecret Secret to mount to read bearer token for scraping targets. The secret needs to be in the same namespace as the service monitor and accessible by the Prometheus Operator. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42; corev1.SecretKeySelector false honorLabels HonorLabels chooses the metric labels on collisions with target labels. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint bool false honorTimestamps HonorTimestamps controls whether Prometheus respects the timestamps present in scraped data. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;bool false basicAuth BasicAuth allow an endpoint to authenticate over basic authentication More info: https://prometheus.io/docs/operating/configuration/#endpoints See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;monitoringv1.BasicAuth false metricRelabelings MetricRelabelings to apply to samples before ingestion. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint []&#42;monitoringv1.RelabelConfig false relabelings Relabelings to apply to samples before scraping. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint []&#42;monitoringv1.RelabelConfig false proxyURL ProxyURL eg http://proxyserver:2195 Directs scrapes to proxy through this endpoint. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;string false Back to TOC ",
            "title": "ServiceMonitorSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " ServiceSpec defines the settings for a Service Field Description Type Required enabled Enabled controls whether to create the service yaml or not &#42;bool false name An optional name to use to override the generated service name. &#42;string false portName An optional name to use to override the port name. &#42;string false port The service port value &#42;int32 false type Kind is the K8s service type (typically ClusterIP or LoadBalancer) The default is \"ClusterIP\". &#42; https://pkg.go.dev/k8s.io/api/core/v1#ServiceType false externalIPs externalIPs is a list of IP addresses for which nodes in the cluster will also accept traffic for this service. These IPs are not managed by Kubernetes. The user is responsible for ensuring that traffic arrives at a node with this IP. A common example is external load-balancers that are not part of the Kubernetes system. []string false clusterIP clusterIP is the IP address of the service and is usually assigned randomly by the master. If an address is specified manually and is not in use by others, it will be allocated to the service; otherwise, creation of the service will fail. This field can not be changed through updates. Valid values are \"None\", empty string (\"\"), or a valid IP address. \"None\" can be specified for headless services when proxying is not required. Only applies to types ClusterIP, NodePort, and LoadBalancer. Ignored if type is ExternalName. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42;string false clusterIPs ClusterIPs is a list of IP addresses assigned to this service, and are usually assigned randomly. If an address is specified manually, is in-range (as per system configuration), and is not in use, it will be allocated to the service; otherwise creation of the service will fail. This field may not be changed through updates unless the type field is also being changed to ExternalName (which requires this field to be empty) or the type field is being changed from ExternalName (in which case this field may optionally be specified, as describe above). Valid values are \"None\", empty string (\"\"), or a valid IP address. Setting this to \"None\" makes a \"headless service\" (no virtual IP), which is useful when direct endpoint connections are preferred and proxying is not required. Only applies to types ClusterIP, NodePort, and LoadBalancer. If this field is specified when creating a Service of type ExternalName, creation will fail. This field will be wiped when updating a Service to type ExternalName. If this field is not specified, it will be initialized from the clusterIP field. If this field is specified, clients must ensure that clusterIPs[0] and clusterIP have the same value. Unless the \"IPv6DualStack\" feature gate is enabled, this field is limited to one value, which must be the same as the clusterIP field. If the feature gate is enabled, this field may hold a maximum of two entries (dual-stack IPs, in either order). These IPs must correspond to the values of the ipFamilies field. Both clusterIPs and ipFamilies are governed by the ipFamilyPolicy field. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies []string false loadBalancerIP LoadBalancerIP is the IP address of the load balancer &#42;string false labels The extra labels to add to the service. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations is free form yaml that will be added to the service annotations map[string]string false sessionAffinity Supports \"ClientIP\" and \"None\". Used to maintain session affinity. Enable client IP based session affinity. Must be ClientIP or None. Defaults to None. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42; https://pkg.go.dev/k8s.io/api/core/v1#ServiceAffinity false loadBalancerSourceRanges If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature.\" []string false externalName externalName is the external reference that kubedns or equivalent will return as a CNAME record for this service. No proxying will be involved. Must be a valid RFC-1123 hostname ( https://tools.ietf.org/html/rfc1123 ) and requires Kind to be ExternalName. &#42;string false externalTrafficPolicy externalTrafficPolicy denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints. \"Local\" preserves the client source IP and avoids a second hop for LoadBalancer and Nodeport type services, but risks potentially imbalanced traffic spreading. \"Cluster\" obscures the client source IP and may cause a second hop to another node, but should have good overall load-spreading. &#42; https://pkg.go.dev/k8s.io/api/core/v1#ServiceExternalTrafficPolicyType false healthCheckNodePort healthCheckNodePort specifies the healthcheck nodePort for the service. If not specified, HealthCheckNodePort is created by the service api backend with the allocated nodePort. Will use user-specified nodePort value if specified by the client. Only effects when Kind is set to LoadBalancer and ExternalTrafficPolicy is set to Local. &#42;int32 false publishNotReadyAddresses publishNotReadyAddresses, when set to true, indicates that DNS implementations must publish the notReadyAddresses of subsets for the Endpoints associated with the Service. The default value is false. The primary use case for setting this field is to use a StatefulSet&#8217;s Headless Service to propagate SRV records for its Pods without respect to their readiness for purpose of peer discovery. &#42;bool false sessionAffinityConfig sessionAffinityConfig contains the configurations of session affinity. &#42; corev1.SessionAffinityConfig false ipFamilies IPFamilies is a list of IP families (e.g. IPv4, IPv6) assigned to this service, and is gated by the \"IPv6DualStack\" feature gate. This field is usually assigned automatically based on cluster configuration and the ipFamilyPolicy field. If this field is specified manually, the requested family is available in the cluster, and ipFamilyPolicy allows it, it will be used; otherwise creation of the service will fail. This field is conditionally mutable: it allows for adding or removing a secondary IP family, but it does not allow changing the primary IP family of the Service. Valid values are \"IPv4\" and \"IPv6\". This field only applies to Services of types ClusterIP, NodePort, and LoadBalancer, and does apply to \"headless\" services. This field will be wiped when updating a Service to type ExternalName. This field may hold a maximum of two entries (dual-stack families, in either order). These families must correspond to the values of the clusterIPs field, if specified. Both clusterIPs and ipFamilies are governed by the ipFamilyPolicy field. [] https://pkg.go.dev/k8s.io/api/core/v1#IPFamily false ipFamilyPolicy IPFamilyPolicy represents the dual-stack-ness requested or required by this Service, and is gated by the \"IPv6DualStack\" feature gate. If there is no value provided, then this field will be set to SingleStack. Services can be \"SingleStack\" (a single IP family), \"PreferDualStack\" (two IP families on dual-stack configured clusters or a single IP family on single-stack clusters), or \"RequireDualStack\" (two IP families on dual-stack configured clusters, otherwise fail). The ipFamilies and clusterIPs fields depend on the value of this field. This field will be wiped when updating a service to type ExternalName. &#42; https://pkg.go.dev/k8s.io/api/core/v1#IPFamilyPolicyType false allocateLoadBalancerNodePorts allocateLoadBalancerNodePorts defines if NodePorts will be automatically allocated for services with type LoadBalancer. Default is \"true\". It may be set to \"false\" if the cluster load-balancer does not rely on NodePorts. allocateLoadBalancerNodePorts may only be set for services with type LoadBalancer and will be cleared if the type is changed to any other type. This field is alpha-level and is only honored by servers that enable the ServiceLBNodePortControl feature. &#42;bool false Back to TOC ",
            "title": "ServiceSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " StartQuorum defines the order that deployments will be started in a Coherence cluster made up of multiple deployments. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false Back to TOC ",
            "title": "StartQuorum"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " StartQuorumStatus tracks the state of a deployment&#8217;s start quorums. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false ready Whether this quorum&#8217;s condition has been met bool true Back to TOC ",
            "title": "StartQuorumStatus"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " Coherence is the top level schema for the Coherence API and custom resource definition (CRD). Field Description Type Required metadata &#160; metav1.ObjectMeta false spec &#160; CoherenceStatefulSetResourceSpec false status &#160; CoherenceResourceStatus false Back to TOC ",
            "title": "Coherence"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CoherenceList is a list of Coherence resources. Field Description Type Required metadata &#160; metav1.ListMeta false items &#160; [] Coherence true Back to TOC ",
            "title": "CoherenceList"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CoherenceResourceStatus defines the observed state of Coherence resource. Field Description Type Required phase The phase of a Coherence resource is a simple, high-level summary of where the Coherence resource is in its lifecycle. The conditions array, the reason and message fields, and the individual container status arrays contain more detail about the pod&#8217;s status. There are eight possible phase values: Initialized: The deployment has been accepted by the Kubernetes system. Created: The deployments secondary resources, (e.g. the StatefulSet, Services etc.) have been created. Ready: The StatefulSet for the deployment has the correct number of replicas and ready replicas. Waiting: The deployment&#8217;s start quorum conditions have not yet been met. Scaling: The number of replicas in the deployment is being scaled up or down. RollingUpgrade: The StatefulSet is performing a rolling upgrade. Stopped: The replica count has been set to zero. Completed: The Coherence resource is running a Job and the Job has completed. Failed: An error occurred reconciling the deployment and its secondary resources. ConditionType false coherenceCluster The name of the Coherence cluster that this deployment is part of. string false type The type of the Coherence resource. CoherenceType false replicas Replicas is the desired number of members in the Coherence deployment represented by the Coherence resource. int32 true currentReplicas CurrentReplicas is the current number of members in the Coherence deployment represented by the Coherence resource. int32 true readyReplicas ReadyReplicas is the number of members in the Coherence deployment represented by the Coherence resource that are in the ready state. int32 true active When the Coherence resource is running a Job, the number of pending and running pods. int32 false succeeded When the Coherence resource is running a Job, the number of pods which reached phase Succeeded. int32 false failed When the Coherence resource is running a Job, the number of pods which reached phase Failed. int32 false role The effective role name for this deployment. This will come from the Spec.Role field if set otherwise the deployment name will be used for the role name string false selector label query over deployments that should match the replicas count. This is same as the label selector but in the string format to avoid introspection by clients. The string will be in the same format as the query-param syntax. More info about label selectors: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ string false conditions The status conditions. Conditions false hash Hash is the hash of the latest applied Coherence spec string false actionsExecuted ActionsExecuted tracks whether actions were executed bool false jobProbes &#160; []CoherenceJobProbeStatus false Back to TOC ",
            "title": "CoherenceResourceStatus"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " CoherenceStatefulSetResourceSpec defines the specification of a Coherence resource. A Coherence resource is typically one or more Pods that perform the same functionality, for example storage members. Field Description Type Required cluster The optional name of the Coherence cluster that this Coherence resource belongs to. If this value is set the Pods controlled by this Coherence resource will form a cluster with other Pods controlled by Coherence resources with the same cluster name. If not set the Coherence resource&#8217;s name will be used as the cluster name. &#42;string false statefulSetAnnotations StatefulSetAnnotations are free-form yaml that will be added to the Coherence cluster StatefulSet as annotations. Any annotations should be placed BELOW this \"annotations:\" key, for example: The default behaviour is to copy all annotations from the Coherence resource to the StatefulSet , specifying any annotations in the StatefulSetAnnotations will override this behaviour and only include the StatefulSetAnnotations . annotations: foo.io/one: \"value1\" + foo.io/two: \"value2\" + see: Kubernetes Annotations map[string]string false volumeClaimTemplates VolumeClaimTemplates defines extra PVC mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumeClaimTemplates section of a StatefulSet spec as described in https://kubernetes.io/docs/concepts/storage/persistent-volumes/ Every claim in this list must have at least one matching (by name) volumeMount in one container in the template. A claim in this list takes precedence over any volumes in the template, with the same name. [] PersistentVolumeClaim false scaling The configuration to control safe scaling. &#42; ScalingSpec false suspendProbe The configuration of the probe used to signal that services must be suspended before a deployment is stopped. &#42; Probe false suspendServicesOnShutdown A flag controlling whether storage enabled cache services in this deployment will be suspended before the deployment is shutdown or scaled to zero. The action of suspending storage enabled services when the whole deployment is being stopped ensures that cache services with persistence enabled will shut down cleanly without the possibility of Coherence trying to recover and re-balance partitions as Pods are stopped. The default value if not specified is true. &#42;bool false resumeServicesOnStartup ResumeServicesOnStartup allows the Operator to resume suspended Coherence services when the Coherence container is started. This only applies to storage enabled distributed cache services. This ensures that services that are suspended due to the shutdown of a storage tier, but those services are still running (albeit suspended) in other storage disabled deployments, will be resumed when storage comes back. Note that starting Pods with suspended partitioned cache services may stop the Pod reaching the ready state. The default value if not specified is true. &#42;bool false autoResumeServices AutoResumeServices is a map of Coherence service names to allow more fine-grained control over which services may be auto-resumed by the operator when a Coherence Pod starts. The key to the map is the name of the Coherence service. This should be the fully qualified name if scoped services are being used in Coherence. The value is a bool, set to true to allow the service to be auto-resumed or false to not allow the service to be auto-resumed. Adding service names to this list will override any value set in ResumeServicesOnStartup , so if the ResumeServicesOnStartup field is false but there are service names in the AutoResumeServices , mapped to true , those services will still be resumed. Note that starting Pods with suspended partitioned cache services may stop the Pod reaching the ready state. map[string]bool false suspendServiceTimeout SuspendServiceTimeout sets the number of seconds to wait for the service suspend call to return (the default is 60 seconds) &#42;int false haBeforeUpdate Whether to perform a StatusHA test on the cluster before performing an update or deletion. This field can be set to \"false\" to force through an update even when a Coherence deployment is in an unstable state. The default is true, to always check for StatusHA before updating a Coherence deployment. &#42;bool false allowUnsafeDelete AllowUnsafeDelete controls whether the Operator will add a finalizer to the Coherence resource so that it can intercept deletion of the resource and initiate a controlled shutdown of the Coherence cluster. The default value is false . The primary use for setting this flag to true is in CI/CD environments so that cleanup jobs can delete a whole namespace without requiring the Operator to have removed finalizers from any Coherence resources deployed into that namespace. It is not recommended to set this flag to true in a production environment, especially when using Coherence persistence features. &#42;bool false actions Actions to execute once all the Pods are ready after an initial deployment [] Action false envFrom List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. [] corev1.EnvFromSource false global Global contains attributes that will be applied to all resources managed by the Coherence Operator. &#42; GlobalSpec false initResources InitResources is the optional resource requests and limits for the init-container that the Operator adds to the Pod. ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ + The Coherence operator does not apply any default resources. &#42; corev1.ResourceRequirements false Back to TOC ",
            "title": "CoherenceStatefulSetResourceSpec"
        },
        {
            "location": "/docs/about/04_coherence_spec",
            "text": " This is a reference for the Coherence Operator API types. These are all the types and fields that are used in the Coherence CRD. This document was generated from comments in the Go structs in the pkg/api/ directory. Table of Contents Action ActionJob CoherenceResourceSpec ApplicationSpec CloudNativeBuildPackSpec CoherenceSpec CoherenceTracingSpec CoherenceWKASpec ConfigMapVolumeSpec GlobalSpec ImageSpec JVMSpec JvmDebugSpec JvmGarbageCollectorSpec JvmMemorySpec JvmOutOfMemorySpec LocalObjectReference NamedPortSpec NetworkSpec PersistenceSpec PersistentStorageSpec PersistentVolumeClaim PersistentVolumeClaimObjectMeta PodDNSConfig PortSpecWithSSL Probe ProbeHandler ReadinessProbeSpec Resource Resources SSLSpec ScalingSpec SecretVolumeSpec ServiceMonitorSpec ServiceSpec StartQuorum StartQuorumStatus Coherence CoherenceList CoherenceResourceStatus CoherenceStatefulSetResourceSpec Action Action is an action to execute when the StatefulSet becomes ready. Field Description Type Required name Action name string false probe This is the spec of some sort of probe to fire when the Coherence resource becomes ready &#42; Probe false job or this is the spec of a Job to create when the Coherence resource becomes ready &#42; ActionJob false Back to TOC ActionJob Field Description Type Required spec Spec will be used to create a Job, the name is the Coherence deployment name + \"-\" + the action name The Job will be fire and forget, we do not monitor it in the Operator. We set its owner to be the Coherence resource, so it gets deleted when the Coherence resource is deleted. batchv1.JobSpec true labels Labels are the extra labels to add to the Job. map[string]string false annotations Annotations to add to the Job. map[string]string false Back to TOC CoherenceResourceSpec CoherenceResourceSpec defines the specification of a Coherence resource. A Coherence resource is typically one or more Pods that perform the same functionality, for example storage members. Field Description Type Required image The name of the image. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; https://pkg.go.dev/k8s.io/api/core/v1#PullPolicy false imagePullSecrets ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec. If specified, these secrets will be passed to individual puller implementations for them to use. For example, in the case of docker, only DockerConfig type secrets are honored. More info: https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod [] LocalObjectReference false replicas The desired number of cluster members of this deployment. This is a pointer to distinguish between explicit zero and not specified. If not specified a default value of 3 will be used. This field cannot be negative. &#42;int32 false role The name of the role that this deployment represents in a Coherence cluster. This value will be used to set the Coherence role property for all members of this role string false appLabel An optional app label to apply to resources created for this deployment. This is useful for example to apply an app label for use by Istio. This field follows standard Kubernetes label syntax. &#42;string false versionLabel An optional version label to apply to resources created for this deployment. This is useful for example to apply a version label for use by Istio. This field follows standard Kubernetes label syntax. &#42;string false coherence The optional settings specific to Coherence functionality. &#42; CoherenceSpec false application The optional application specific settings. &#42; ApplicationSpec false jvm The JVM specific options &#42; JVMSpec false ports Ports specifies additional port mappings for the Pod and additional Services for those ports. [] NamedPortSpec false startQuorum StartQuorum controls the start-up order of this Coherence resource in relation to other Coherence resources. [] StartQuorum false env Env is additional environment variable mappings that will be passed to the Coherence container in the Pod. To specify extra variables add them as name value pairs the same as they would be added to a Pod containers spec. [] corev1.EnvVar false labels The extra labels to add to the all the Pods in this deployment. Labels here will add to or override those defined for the cluster. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations are free-form yaml that will be added to the Coherence cluster member Pods as annotations. Any annotations should be placed BELOW this \"annotations:\" key, for example: annotations: foo.io/one: \"value1\" + foo.io/two: \"value2\" + see: Kubernetes Annotations map[string]string false initContainers List of additional initialization containers to add to the deployment&#8217;s Pod. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ [] corev1.Container false sideCars List of additional side-car containers to add to the deployment&#8217;s Pod. [] corev1.Container false configMapVolumes A list of ConfigMaps to add as volumes. Each entry in the list will be added as a ConfigMap Volume to the deployment&#8217;s Pods and as a VolumeMount to all the containers and init-containers in the Pod. see: Add ConfigMap Volumes [] ConfigMapVolumeSpec false secretVolumes A list of Secrets to add as volumes. Each entry in the list will be added as a Secret Volume to the deployment&#8217;s Pods and as a VolumeMount to all the containers and init-containers in the Pod. see: Add Secret Volumes [] SecretVolumeSpec false volumes Volumes defines extra volume mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumes section of a Pod definition + as described in https://kubernetes.io/docs/concepts/storage/volumes/ [] corev1.Volume false volumeMounts VolumeMounts defines extra volume mounts to map to the additional volumes or PVCs declared above in store.volumes and store.volumeClaimTemplates [] corev1.VolumeMount false healthPort The port that the health check endpoint will bind to. &#42;int32 false readinessProbe The readiness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false livenessProbe The liveness probe config to be used for the Pods in this deployment. ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ &#42; ReadinessProbeSpec false startupProbe The startup probe config to be used for the Pods in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ &#42; ReadinessProbeSpec false readinessGates ReadinessGates defines a list of additional conditions that the kubelet evaluates for Pod readiness. See: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate [] corev1.PodReadinessGate false resources Resources is the optional resource requests and limits for the containers ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ + The Coherence operator does not apply any default resources. &#42; corev1.ResourceRequirements false affinity Affinity controls Pod scheduling preferences. ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity &#42; corev1.Affinity false nodeSelector NodeSelector is the Node labels for pod assignment ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector map[string]string false tolerations Tolerations for nodes that have taints on them. Useful if you want to dedicate nodes to just run the coherence container + For example: tolerations: + - key: \"key\" + operator: \"Equal\" + value: \"value\" + effect: \"NoSchedule\" + ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] corev1.Toleration false securityContext SecurityContext is the PodSecurityContext that will be added to all the Pods in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ &#42; corev1.PodSecurityContext false containerSecurityContext ContainerSecurityContext is the SecurityContext that will be added to the Coherence container in each Pod in this deployment. See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ &#42; corev1.SecurityContext false shareProcessNamespace Share a single process namespace between all the containers in a pod. When this is set containers will be able to view and signal processes from other containers in the same pod, and the first process in each container will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be set. Optional: Default to false. &#42;bool false hostIPC Use the host&#8217;s ipc namespace. Optional: Default to false. &#42;bool false network Configure various networks and DNS settings for Pods in this role. &#42; NetworkSpec false coherenceUtils The configuration for the Coherence operator image name &#42; ImageSpec false serviceAccountName The name to use for the service account to use when RBAC is enabled The role bindings must already have been created as this chart does not create them it just sets the serviceAccountName value in the Pod spec. string false automountServiceAccountToken Whether to auto-mount the Kubernetes API credentials for a service account &#42;bool false operatorRequestTimeout The timeout to apply to REST requests made back to the Operator from Coherence Pods. These requests are typically to obtain site and rack information for the Pod. &#42;int32 false activeDeadlineSeconds ActiveDeadlineSeconds is the optional duration in seconds the pod may be active on the node relative to StartTime before the system will actively try to mark it failed and kill associated containers. Value must be a positive integer. &#42;int64 false enableServiceLinks EnableServiceLinks indicates whether information about services should be injected into pod&#8217;s environment variables, matching the syntax of Docker links. Optional: Defaults to true. &#42;bool false preemptionPolicy PreemptionPolicy is the Policy for preempting pods with lower priority. One of Never, PreemptLowerPriority. Defaults to PreemptLowerPriority if unset. &#42; https://pkg.go.dev/k8s.io/api/core/v1#PreemptionPolicy false priorityClassName PriorityClassName, if specified, indicates the pod&#8217;s priority. \"system-node-critical\" and \"system-cluster-critical\" are two special keywords which indicate the highest priorities with the former being the highest priority. Any other name must be defined by creating a PriorityClass object with that name. If not specified, the pod priority will be default or zero if there is no default. &#42;string false restartPolicy Restart policy for all containers within the pod. One of Always, OnFailure, Never. Default to Always. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy &#42; https://pkg.go.dev/k8s.io/api/core/v1#RestartPolicy false runtimeClassName RuntimeClassName refers to a RuntimeClass object in the node.k8s.io group, which should be used to run this pod. If no RuntimeClass resource matches the named class, the pod will not be run. If unset or empty, the \"legacy\" RuntimeClass will be used, which is an implicit class with an empty definition that uses the default runtime handler. More info: https://git.k8s.io/enhancements/keps/sig-node/585-runtime-class &#42;string false schedulerName If specified, the pod will be dispatched by specified scheduler. If not specified, the pod will be dispatched by default scheduler. &#42;string false topologySpreadConstraints TopologySpreadConstraints describes how a group of pods ought to spread across topology domains. Scheduler will schedule pods in a way which abides by the constraints. All topologySpreadConstraints are ANDed. [] corev1.TopologySpreadConstraint false rackLabel RackLabel is an optional Node label to use for the value of the Coherence member&#8217;s rack name. The default labels to use are determined by the Operator. &#42;string false siteLabel SiteLabel is an optional Node label to use for the value of the Coherence member&#8217;s site name The default labels to use are determined by the Operator. &#42;string false Back to TOC ApplicationSpec ApplicationSpec is the specification of the application deployed into the Coherence. Field Description Type Required type The application type to execute. This field would be set if using the Coherence Graal image and running a none-Java application. For example if the application was a Node application this field would be set to \"node\". The default is to run a plain Java application. &#42;string false main Class is the Coherence container main class. The default value is com.tangosol.net.DefaultCacheServer. If the application type is non-Java this would be the name of the corresponding language specific runnable, for example if the application type is \"node\" the main may be a Javascript file. &#42;string false args Args is the optional arguments to pass to the main class. []string false workingDir The application folder in the custom artifacts Docker image containing application artifacts. This will effectively become the working directory of the Coherence container. If not set the application directory default value is \"/app\". &#42;string false cloudNativeBuildPack Optional settings that may be configured if using a Cloud Native Buildpack Image. For example an image build with the Spring Boot Maven/Gradle plugin. See: https://github.com/paketo-buildpacks/spring-boot and https://buildpacks.io/ &#42; CloudNativeBuildPackSpec false springBootFatJar SpringBootFatJar is the full path name to the Spring Boot fat jar if the application image has been built by just adding a Spring Boot fat jar to the image. If this field is set then the application will be run by executing this jar. For example, if this field is \"/app/libs/foo.jar\" the command line will be \"java -jar app/libs/foo.jar\" &#42;string false Back to TOC CloudNativeBuildPackSpec CloudNativeBuildPackSpec is the configuration when using a Cloud Native Buildpack Image. For example an image build with the Spring Boot Maven/Gradle plugin. See: https://github.com/paketo-buildpacks/spring-boot and https://buildpacks.io/ Field Description Type Required enabled Enable or disable buildpack detection. The operator will automatically detect Cloud Native Buildpack images but if this auto-detection fails to work correctly for a specific image then this field can be set to true to signify that the image is a buildpack image or false to signify that it is not a buildpack image. &#42;bool false launcher &#160; &#42;string false Back to TOC CoherenceSpec CoherenceSpec is the section of the CRD configures settings specific to Coherence. see: Coherence Configuration Field Description Type Required cacheConfig CacheConfig is the name of the cache configuration file to use see: Configure Cache Config File &#42;string false overrideConfig OverrideConfig is name of the Coherence operational configuration override file, the default is tangosol-coherence-override.xml see: Configure Operational Config File &#42;string false storageEnabled A boolean flag indicating whether members of this deployment are storage enabled. This value will set the corresponding coherence.distributed.localstorage System property. If not specified the default value is true. This flag is also used to configure the ScalingPolicy value if a value is not specified. If the StorageEnabled field is not specified or is true the scaling will be safe, if StorageEnabled is set to false scaling will be parallel. see: Configure Storage Enabled &#42;bool false persistence Persistence values configure the on-disc data persistence settings. The bool Enabled enables or disabled on disc persistence of data. see: Configure Persistence &#42; PersistenceSpec false logLevel The Coherence log level, default being 5 (info level). see: Configure Coherence log level &#42;int32 false management Management configures Coherence management over REST Note: Coherence management over REST will is available in Coherence version &gt;= 12.2.1.4. see: Management &amp; Diagnostics &#42; PortSpecWithSSL false metrics Metrics configures Coherence metrics publishing Note: Coherence metrics publishing will is available in Coherence version &gt;= 12.2.1.4. see: Metrics &#42; PortSpecWithSSL false tracing Tracing is used to configure Coherence distributed tracing functionality. &#42; CoherenceTracingSpec false allowEndangeredForStatusHA AllowEndangeredForStatusHA is a list of Coherence partitioned cache service names that are allowed to be in an endangered state when testing for StatusHA. Instances where a StatusHA check is performed include the readiness probe and when scaling a deployment. This field would not typically be used except in cases where a cache service is configured with a backup count greater than zero but it does not matter if caches in those services loose data due to member departure. Normally, such cache services would have a backup count of zero, which would automatically excluded them from the StatusHA check. []string false excludeFromWKA Exclude members of this deployment from being part of the cluster&#8217;s WKA list. see: Well Known Addressing &#42;bool false wka Specify an existing Coherence deployment to be used for WKA. If an existing deployment is to be used for WKA the ExcludeFromWKA is implicitly set to true. see: Well Known Addressing &#42; CoherenceWKASpec false skipVersionCheck Certain features rely on a version check prior to starting the server, e.g. metrics requires &gt;= 12.2.1.4. The version check relies on the ability of the start script to find coherence.jar but if due to how the image has been built this check is failing then setting this flag to true will skip version checking and assume that the latest coherence.jar is being used. &#42;bool false enableIpMonitor Enables the Coherence IP Monitor feature. The Operator disables the IP Monitor by default. &#42;bool false localPort LocalPort sets the Coherence unicast port. When manually configuring unicast ports, a single port is specified and the second port is automatically selected. If either of the ports are not available, then the default behavior is to select the next available port. For example, if port 9000 is configured for the first port (port1) and it is not available, then the next available port is automatically selected. The second port (port2) is automatically opened and defaults to the next available port after port1 (port1 + 1 if available). &#42;int32 false localPortAdjust LocalPortAdjust sets the Coherence unicast port adjust value. To specify a range of unicast ports from which ports are selected, include a port value that represents the upper limit of the port range. &#42; https://pkg.go.dev/k8s.io/apimachinery/pkg/util/intstr#IntOrString false Back to TOC CoherenceTracingSpec CoherenceTracingSpec configures Coherence tracing. Field Description Type Required ratio Ratio is the tracing sampling-ratio, which controls the likelihood of a tracing span being collected. For instance, a value of 1.0 will result in all tracing spans being collected, while a value of 0.1 will result in roughly 1 out of every 10 tracing spans being collected. A value of 0 indicates that tracing spans should only be collected if they are already in the context of another tracing span. With such a configuration, Coherence will not initiate tracing on its own, and it is up to the application to start an outer tracing span, from which Coherence will then collect inner tracing spans. A value of -1 disables tracing completely. The Coherence default is -1 if not overridden. For values other than -1, numbers between 0 and 1 are acceptable. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. &#42;resource.Quantity false Back to TOC CoherenceWKASpec CoherenceWKASpec configures Coherence well-known-addressing to use an existing Coherence deployment for WKA. Field Description Type Required deployment The name of the existing Coherence deployment to use for WKA. string true namespace The optional namespace of the existing Coherence deployment to use for WKA if different from this deployment&#8217;s namespace. string false addresses A list of addresses to be used for WKA. If this field is set, the WKA property for the Coherence cluster will be set using this value and the other WKA fields will be ignored. []string false Back to TOC ConfigMapVolumeSpec ConfigMapVolumeSpec represents a ConfigMap that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add ConfigMap Volumes Field Description Type Required name The name of the ConfigMap to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the ConfigMap name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \"\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; https://pkg.go.dev/k8s.io/api/core/v1#MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \"\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the ConfigMap or its keys must be defined &#42;bool false Back to TOC GlobalSpec GlobalSpec is attributes that will be applied to all resources managed by the Operator. Field Description Type Required labels Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ map[string]string false Back to TOC ImageSpec ImageSpec defines the settings for a Docker image Field Description Type Required image The image name. More info: https://kubernetes.io/docs/concepts/containers/images &#42;string false imagePullPolicy Image pull policy. One of Always, Never, IfNotPresent. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images &#42; https://pkg.go.dev/k8s.io/api/core/v1#PullPolicy false Back to TOC JVMSpec JVMSpec is the JVM configuration. Field Description Type Required classpath Classpath specifies additional items to add to the classpath of the JVM. []string false args Args specifies the options (System properties, -XX: args etc) to pass to the JVM. []string false debug The settings for enabling debug mode in the JVM. &#42; JvmDebugSpec false useContainerLimits If set to true Adds the -XX:+UseContainerSupport JVM option to ensure that the JVM respects any container resource limits. The default value is true &#42;bool false gc Set JVM garbage collector options. &#42; JvmGarbageCollectorSpec false diagnosticsVolume DiagnosticsVolume is the volume to write JVM diagnostic information to, for example heap dumps, JFRs etc. &#42; https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#volume-v1-core false memory Configure the JVM memory options. &#42; JvmMemorySpec false useJibClasspath A flag indicating whether to automatically add the default classpath for images created by the JIB tool https://github.com/GoogleContainerTools/jib If true then the /app/lib/* /app/classes and /app/resources entries are added to the JVM classpath. The default value fif not specified is true. &#42;bool false Back to TOC JvmDebugSpec JvmDebugSpec the JVM Debug specific configuration. Field Description Type Required enabled Enabled is a flag to enable or disable running the JVM in debug mode. Default is disabled. &#42;bool false suspend A boolean true if the target VM is to be suspended immediately before the main class is loaded; false otherwise. The default value is false. &#42;bool false attach Attach specifies the address of the debugger that the JVM should attempt to connect back to instead of listening on a port. &#42;string false port The port that the debugger will listen on; the default is 5005. &#42;int32 false Back to TOC JvmGarbageCollectorSpec JvmGarbageCollectorSpec is options for managing the JVM garbage collector. Field Description Type Required collector The name of the JVM garbage collector to use. G1 - adds the -XX:+UseG1GC option CMS - adds the -XX:+UseConcMarkSweepGC option Parallel - adds the -XX:+UseParallelGC Default - use the JVMs default collector The field value is case insensitive If not set G1 is used. If set to a value other than those above then the default collector for the JVM will be used. &#42;string false args Args specifies the GC options to pass to the JVM. []string false logging Enable the following GC logging args -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime Default is true &#42;bool false Back to TOC JvmMemorySpec JvmMemorySpec is options for managing the JVM memory. Field Description Type Required heapSize HeapSize sets both the initial and max heap size values for the JVM. This will set both the -XX:InitialHeapSize and -XX:MaxHeapSize JVM options to the same value (the equivalent of setting -Xms and -Xmx to the same value). The format should be the same as that used when specifying these JVM options. If not set the JVM defaults are used. &#42;string false initialHeapSize InitialHeapSize sets the initial heap size value for the JVM. This will set the -XX:InitialHeapSize JVM option (the equivalent of setting -Xms). The format should be the same as that used when specifying this JVM options. NOTE: If the HeapSize field is set it will override this field. &#42;string false maxHeapSize MaxHeapSize sets the maximum heap size value for the JVM. This will set the -XX:MaxHeapSize JVM option (the equivalent of setting -Xmx). The format should be the same as that used when specifying this JVM options. NOTE: If the HeapSize field is set it will override this field. &#42;string false maxRAM Sets the JVM option -XX:MaxRAM=N which sets the maximum amount of memory used by the JVM to n , where n is expressed in terms of megabytes (for example, 100m ) or gigabytes (for example 2g ). &#42;string false percentage Percentage sets the initial and maximum and minimum heap percentage sizes to the same value, This will set the -XX:InitialRAMPercentage -XX:MinRAMPercentage and -XX:MaxRAMPercentage JVM options to the same value. The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps to the -XX:InitialRAMPercentage -XX:MinRAMPercentage and -XX:MaxRAMPercentage JVM options and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false initialRAMPercentage Set initial heap size as a percentage of total memory. The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set. Valid values are decimal numbers between 0 and 100. NOTE: If the Percentage field is set it will override this field. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps to the -XX:InitialRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false maxRAMPercentage Set maximum heap size as a percentage of total memory. The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set. Valid values are decimal numbers between 0 and 100. NOTE: If the Percentage field is set it will override this field. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps to the -XX:MaxRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false minRAMPercentage Set the minimal JVM Heap size as a percentage of the total memory. This option will be ignored if HeapSize is set. Valid values are decimal numbers between 0 and 100. NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity for the different formats of number that may be entered. NOTE: This field maps the the -XX:MinRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8). &#42;resource.Quantity false stackSize StackSize is the stack size value to pass to the JVM. The format should be the same as that used for Java&#8217;s -Xss JVM option. If not set the JVM defaults are used. &#42;string false metaspaceSize MetaspaceSize is the min/max metaspace size to pass to the JVM. This sets the -XX:MetaspaceSize and -XX:MaxMetaspaceSize=size JVM options. If not set the JVM defaults are used. &#42;string false directMemorySize DirectMemorySize sets the maximum total size (in bytes) of the New I/O (the java.nio package) direct-buffer allocations. This value sets the -XX:MaxDirectMemorySize JVM option. If not set the JVM defaults are used. &#42;string false nativeMemoryTracking Adds the -XX:NativeMemoryTracking=mode JVM options where mode is on of \"off\", \"summary\" or \"detail\", the default is \"summary\" If not set to \"off\" also add -XX:+PrintNMTStatistics &#42;string false onOutOfMemory Configure the JVM behaviour when an OutOfMemoryError occurs. &#42; JvmOutOfMemorySpec false Back to TOC JvmOutOfMemorySpec JvmOutOfMemorySpec is options for managing the JVM behaviour when an OutOfMemoryError occurs. Field Description Type Required exit If set to true the JVM will exit when an OOM error occurs. Default is true &#42;bool false heapDump If set to true adds the -XX:+HeapDumpOnOutOfMemoryError JVM option to cause a heap dump to be created when an OOM error occurs. Default is true &#42;bool false Back to TOC LocalObjectReference LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Field Description Type Required name Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names string true Back to TOC NamedPortSpec NamedPortSpec defines a named port for a Coherence component Field Description Type Required name Name specifies the name of the port. string true port Port specifies the port used. int32 false protocol Protocol for container port. Must be UDP or TCP. Defaults to \"TCP\" &#42; https://pkg.go.dev/k8s.io/api/core/v1#Protocol false appProtocol The application protocol for this port. This field follows standard Kubernetes label syntax. Un-prefixed names are reserved for IANA standard service names (as per RFC-6335 and http://www.iana.org/assignments/service-names ). Non-standard protocols should use prefixed names such as mycompany.com/my-custom-protocol. &#42;string false nodePort The port on each node on which this service is exposed when type=NodePort or LoadBalancer. Usually assigned by the system. If specified, it will be allocated to the service if unused or else creation of the service will fail. If set, this field must be in the range 30000 - 32767 inclusive. Default is to auto-allocate a port if the ServiceType of this Service requires one. More info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport &#42;int32 false hostPort Number of port to expose on the host. If specified, this must be a valid port number, 0 &lt; x &lt; 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. &#42;int32 false hostIP What host IP to bind the external port to. &#42;string false service Service configures the Kubernetes Service used to expose the port. &#42; ServiceSpec false serviceMonitor The specification of a Prometheus ServiceMonitor resource that will be created for the Service being exposed for this port. &#42; ServiceMonitorSpec false exposeOnSts ExposeOnSTS is a flag to indicate that this port should also be exposed on the StatefulSetHeadless service. This is useful in cases where a service mesh such as Istio is being used and ports such as the Extend or gRPC ports are accessed via the StatefulSet service. The default is true so all additional ports are exposed on the StatefulSet headless service. &#42;bool false Back to TOC NetworkSpec NetworkSpec configures various networking and DNS settings for Pods in a deployment. Field Description Type Required dnsConfig Specifies the DNS parameters of a pod. Parameters specified here will be merged to the generated DNS configuration based on DNSPolicy. &#42; PodDNSConfig false dnsPolicy Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. &#42; https://pkg.go.dev/k8s.io/api/core/v1#DNSPolicy false hostAliases HostAliases is an optional list of hosts and IPs that will be injected into the pod&#8217;s hosts file if specified. This is only valid for non-hostNetwork pods. [] corev1.HostAlias false hostNetwork Host networking requested for this pod. Use the host&#8217;s network namespace. If this option is set, the ports that will be used must be specified. Default to false. &#42;bool false hostname Specifies the hostname of the Pod If not specified, the pod&#8217;s hostname will be set to a system-defined value. &#42;string false setHostnameAsFQDN SetHostnameAsFQDN if true the pod&#8217;s hostname will be configured as the pod&#8217;s FQDN, rather than the leaf name (the default). In Linux containers, this means setting the FQDN in the hostname field of the kernel (the nodename field of struct utsname). In Windows containers, this means setting the registry value of hostname for the registry key HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\CurrentControlSet\\\\Services\\\\Tcpip\\\\Parameters to FQDN. If a pod does not have FQDN, this has no effect. Default to false. &#42;bool false subdomain Subdomain, if specified, the fully qualified Pod hostname will be \"&lt;hostname&gt;.&lt;subdomain&gt;.&lt;pod namespace&gt;.svc.&lt;cluster domain&gt;\". If not specified, the pod will not have a domain name at all. &#42;string false Back to TOC PersistenceSpec PersistenceSpec is the spec for Coherence persistence. Field Description Type Required mode The persistence mode to use. Valid choices are \"on-demand\", \"active\", \"active-async\". This field will set the coherence.distributed.persistence-mode System property to \"default-\" + Mode. &#42;string false persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#volume-v1-core false snapshots Snapshot values configure the on-disc persistence data snapshot (backup) settings. These settings enable a different location for persistence snapshot data. If not set then snapshot files will be written to the same volume configured for persistence data in the Persistence section. &#42; PersistentStorageSpec false Back to TOC PersistentStorageSpec PersistentStorageSpec defines the persistence settings for the Coherence Field Description Type Required persistentVolumeClaim PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data. &#42; corev1.PersistentVolumeClaimSpec false volume Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here. &#42; https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#volume-v1-core false Back to TOC PersistentVolumeClaim PersistentVolumeClaim is a request for and claim to a persistent volume Field Description Type Required metadata Standard object&#8217;s metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata PersistentVolumeClaimObjectMeta false spec Spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims corev1.PersistentVolumeClaimSpec false Back to TOC PersistentVolumeClaimObjectMeta PersistentVolumeClaimObjectMeta is metadata for the PersistentVolumeClaim. Field Description Type Required name Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/ string false labels Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ map[string]string false Back to TOC PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Field Description Type Required nameservers A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. []string false searches A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed. []string false options A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. [] corev1.PodDNSConfigOption false Back to TOC PortSpecWithSSL PortSpecWithSSL defines a port with SSL settings for a Coherence component Field Description Type Required enabled Enable or disable flag. &#42;bool false port The port to bind to. &#42;int32 false ssl SSL configures SSL settings for a Coherence component &#42; SSLSpec false Back to TOC Probe Probe is the handler that will be used to determine how to communicate with a Coherence deployment for operations like StatusHA checking and service suspension. StatusHA checking is primarily used during scaling of a deployment, a deployment must be in a safe Phase HA state before scaling takes place. If StatusHA handler is disabled for a deployment (by specifically setting Enabled to false then no check will take place and a deployment will be assumed to be safe). Field Description Type Required timeoutSeconds Number of seconds after which the handler times out (only applies to http and tcp handlers). Defaults to 1 second. Minimum value is 1. &#42;int false Back to TOC ProbeHandler ProbeHandler is the definition of a probe handler. Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false Back to TOC ReadinessProbeSpec ReadinessProbeSpec defines the settings for the Coherence Pod readiness probe Field Description Type Required exec One and only one of the following should be specified. Exec specifies the action to take. &#42; corev1.ExecAction false httpGet HTTPGet specifies the http request to perform. &#42; corev1.HTTPGetAction false tcpSocket TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported &#42; corev1.TCPSocketAction false initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false timeoutSeconds Number of seconds after which the probe times out. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes &#42;int32 false periodSeconds How often (in seconds) to perform the probe. &#42;int32 false successThreshold Minimum consecutive successes for the probe to be considered successful after having failed. &#42;int32 false failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. &#42;int32 false Back to TOC Resource Resource is a structure holding a resource to be managed Field Description Type Required kind &#160; ResourceType true name &#160; string true spec &#160; client.Object true Back to TOC Resources Resources is a cloolection of resources to be managed. Field Description Type Required version &#160; int32 true items &#160; [] Resource true Back to TOC SSLSpec SSLSpec defines the SSL settings for a Coherence component over REST endpoint. Field Description Type Required enabled Enabled is a boolean flag indicating whether enables or disables SSL on the Coherence management over REST endpoint, the default is false (disabled). &#42;bool false secrets Secrets is the name of the k8s secret containing the Java key stores and password files. The secret should be in the same namespace as the Coherence resource. + This value MUST be provided if SSL is enabled on the Coherence management over REST endpoint. &#42;string false keyStore Keystore is the name of the Java key store file in the k8s secret to use as the SSL keystore when configuring component over REST to use SSL. &#42;string false keyStorePasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the keystore password when configuring component over REST to use SSL. &#42;string false keyPasswordFile KeyStorePasswordFile is the name of the file in the k8s secret containing the key password when configuring component over REST to use SSL. &#42;string false keyStoreAlgorithm KeyStoreAlgorithm is the name of the keystore algorithm for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509 &#42;string false keyStoreProvider KeyStoreProvider is the name of the keystore provider for the keystore in the k8s secret used when configuring component over REST to use SSL. &#42;string false keyStoreType KeyStoreType is the name of the Java keystore type for the keystore in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false trustStore TrustStore is the name of the Java trust store file in the k8s secret to use as the SSL trust store when configuring component over REST to use SSL. &#42;string false trustStorePasswordFile TrustStorePasswordFile is the name of the file in the k8s secret containing the trust store password when configuring component over REST to use SSL. &#42;string false trustStoreAlgorithm TrustStoreAlgorithm is the name of the keystore algorithm for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is SunX509. &#42;string false trustStoreProvider TrustStoreProvider is the name of the keystore provider for the trust store in the k8s secret used when configuring component over REST to use SSL. &#42;string false trustStoreType TrustStoreType is the name of the Java keystore type for the trust store in the k8s secret used when configuring component over REST to use SSL. If not set the default is JKS. &#42;string false requireClientCert RequireClientCert is a boolean flag indicating whether the client certificate will be authenticated by the server (two-way SSL) when configuring component over REST to use SSL. + If not set the default is false &#42;bool false Back to TOC ScalingSpec ScalingSpec is the configuration to control safe scaling. Field Description Type Required policy ScalingPolicy describes how the replicas of the deployment will be scaled. The default if not specified is based upon the value of the StorageEnabled field. If StorageEnabled field is not specified or is true the default scaling will be safe, if StorageEnabled is set to false the default scaling will be parallel. &#42;ScalingPolicy false probe The probe to use to determine whether a deployment is Phase HA. If not set the default handler will be used. In most use-cases the default handler would suffice but in advanced use-cases where the application code has a different concept of Phase HA to just checking Coherence services then a different handler may be specified. &#42; Probe false Back to TOC SecretVolumeSpec SecretVolumeSpec represents a Secret that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers. see: Add Secret Volumes Field Description Type Required name The name of the Secret to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set. string true mountPath Path within the container at which the volume should be mounted. Must not contain ':'. string true volumeName The optional name to use for the Volume added to the Pod. If not set, the Secret name will be used as the VolumeName. string false readOnly Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. bool false subPath Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \"\" (volume&#8217;s root). string false mountPropagation mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. &#42; https://pkg.go.dev/k8s.io/api/core/v1#MountPropagationMode false subPathExpr Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \"\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive. string false items If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. [] corev1.KeyToPath false defaultMode Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. &#42;int32 false optional Specify whether the Secret or its keys must be defined &#42;bool false Back to TOC ServiceMonitorSpec ServiceMonitorSpec the ServiceMonitor spec for a port service. Field Description Type Required enabled Enabled is a flag to enable or disable creation of a Prometheus ServiceMonitor for a port. If Prometheus ServiceMonitor CR is not installed no ServiceMonitor then even if this flag is true no ServiceMonitor will be created. &#42;bool false labels Additional labels to add to the ServiceMonitor. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false jobLabel The label to use to retrieve the job name from. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec string false targetLabels TargetLabels transfers labels on the Kubernetes Service onto the target. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec []string false podTargetLabels PodTargetLabels transfers labels on the Kubernetes Pod onto the target. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec []string false sampleLimit SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec &#42;uint64 false path HTTP path to scrape for metrics. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint string false scheme HTTP scheme to use for scraping. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint string false params Optional HTTP URL parameters See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint map[string][]string false interval Interval at which metrics should be scraped See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint monitoringv1.Duration false scrapeTimeout Timeout after which the scrape is ended See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint monitoringv1.Duration false tlsConfig TLS configuration to use when scraping the endpoint See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;monitoringv1.TLSConfig false bearerTokenFile File to read bearer token for scraping targets. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint string false bearerTokenSecret Secret to mount to read bearer token for scraping targets. The secret needs to be in the same namespace as the service monitor and accessible by the Prometheus Operator. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42; corev1.SecretKeySelector false honorLabels HonorLabels chooses the metric labels on collisions with target labels. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint bool false honorTimestamps HonorTimestamps controls whether Prometheus respects the timestamps present in scraped data. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;bool false basicAuth BasicAuth allow an endpoint to authenticate over basic authentication More info: https://prometheus.io/docs/operating/configuration/#endpoints See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;monitoringv1.BasicAuth false metricRelabelings MetricRelabelings to apply to samples before ingestion. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint []&#42;monitoringv1.RelabelConfig false relabelings Relabelings to apply to samples before scraping. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint []&#42;monitoringv1.RelabelConfig false proxyURL ProxyURL eg http://proxyserver:2195 Directs scrapes to proxy through this endpoint. See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint &#42;string false Back to TOC ServiceSpec ServiceSpec defines the settings for a Service Field Description Type Required enabled Enabled controls whether to create the service yaml or not &#42;bool false name An optional name to use to override the generated service name. &#42;string false portName An optional name to use to override the port name. &#42;string false port The service port value &#42;int32 false type Kind is the K8s service type (typically ClusterIP or LoadBalancer) The default is \"ClusterIP\". &#42; https://pkg.go.dev/k8s.io/api/core/v1#ServiceType false externalIPs externalIPs is a list of IP addresses for which nodes in the cluster will also accept traffic for this service. These IPs are not managed by Kubernetes. The user is responsible for ensuring that traffic arrives at a node with this IP. A common example is external load-balancers that are not part of the Kubernetes system. []string false clusterIP clusterIP is the IP address of the service and is usually assigned randomly by the master. If an address is specified manually and is not in use by others, it will be allocated to the service; otherwise, creation of the service will fail. This field can not be changed through updates. Valid values are \"None\", empty string (\"\"), or a valid IP address. \"None\" can be specified for headless services when proxying is not required. Only applies to types ClusterIP, NodePort, and LoadBalancer. Ignored if type is ExternalName. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42;string false clusterIPs ClusterIPs is a list of IP addresses assigned to this service, and are usually assigned randomly. If an address is specified manually, is in-range (as per system configuration), and is not in use, it will be allocated to the service; otherwise creation of the service will fail. This field may not be changed through updates unless the type field is also being changed to ExternalName (which requires this field to be empty) or the type field is being changed from ExternalName (in which case this field may optionally be specified, as describe above). Valid values are \"None\", empty string (\"\"), or a valid IP address. Setting this to \"None\" makes a \"headless service\" (no virtual IP), which is useful when direct endpoint connections are preferred and proxying is not required. Only applies to types ClusterIP, NodePort, and LoadBalancer. If this field is specified when creating a Service of type ExternalName, creation will fail. This field will be wiped when updating a Service to type ExternalName. If this field is not specified, it will be initialized from the clusterIP field. If this field is specified, clients must ensure that clusterIPs[0] and clusterIP have the same value. Unless the \"IPv6DualStack\" feature gate is enabled, this field is limited to one value, which must be the same as the clusterIP field. If the feature gate is enabled, this field may hold a maximum of two entries (dual-stack IPs, in either order). These IPs must correspond to the values of the ipFamilies field. Both clusterIPs and ipFamilies are governed by the ipFamilyPolicy field. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies []string false loadBalancerIP LoadBalancerIP is the IP address of the load balancer &#42;string false labels The extra labels to add to the service. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ map[string]string false annotations Annotations is free form yaml that will be added to the service annotations map[string]string false sessionAffinity Supports \"ClientIP\" and \"None\". Used to maintain session affinity. Enable client IP based session affinity. Must be ClientIP or None. Defaults to None. More info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies &#42; https://pkg.go.dev/k8s.io/api/core/v1#ServiceAffinity false loadBalancerSourceRanges If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature.\" []string false externalName externalName is the external reference that kubedns or equivalent will return as a CNAME record for this service. No proxying will be involved. Must be a valid RFC-1123 hostname ( https://tools.ietf.org/html/rfc1123 ) and requires Kind to be ExternalName. &#42;string false externalTrafficPolicy externalTrafficPolicy denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints. \"Local\" preserves the client source IP and avoids a second hop for LoadBalancer and Nodeport type services, but risks potentially imbalanced traffic spreading. \"Cluster\" obscures the client source IP and may cause a second hop to another node, but should have good overall load-spreading. &#42; https://pkg.go.dev/k8s.io/api/core/v1#ServiceExternalTrafficPolicyType false healthCheckNodePort healthCheckNodePort specifies the healthcheck nodePort for the service. If not specified, HealthCheckNodePort is created by the service api backend with the allocated nodePort. Will use user-specified nodePort value if specified by the client. Only effects when Kind is set to LoadBalancer and ExternalTrafficPolicy is set to Local. &#42;int32 false publishNotReadyAddresses publishNotReadyAddresses, when set to true, indicates that DNS implementations must publish the notReadyAddresses of subsets for the Endpoints associated with the Service. The default value is false. The primary use case for setting this field is to use a StatefulSet&#8217;s Headless Service to propagate SRV records for its Pods without respect to their readiness for purpose of peer discovery. &#42;bool false sessionAffinityConfig sessionAffinityConfig contains the configurations of session affinity. &#42; corev1.SessionAffinityConfig false ipFamilies IPFamilies is a list of IP families (e.g. IPv4, IPv6) assigned to this service, and is gated by the \"IPv6DualStack\" feature gate. This field is usually assigned automatically based on cluster configuration and the ipFamilyPolicy field. If this field is specified manually, the requested family is available in the cluster, and ipFamilyPolicy allows it, it will be used; otherwise creation of the service will fail. This field is conditionally mutable: it allows for adding or removing a secondary IP family, but it does not allow changing the primary IP family of the Service. Valid values are \"IPv4\" and \"IPv6\". This field only applies to Services of types ClusterIP, NodePort, and LoadBalancer, and does apply to \"headless\" services. This field will be wiped when updating a Service to type ExternalName. This field may hold a maximum of two entries (dual-stack families, in either order). These families must correspond to the values of the clusterIPs field, if specified. Both clusterIPs and ipFamilies are governed by the ipFamilyPolicy field. [] https://pkg.go.dev/k8s.io/api/core/v1#IPFamily false ipFamilyPolicy IPFamilyPolicy represents the dual-stack-ness requested or required by this Service, and is gated by the \"IPv6DualStack\" feature gate. If there is no value provided, then this field will be set to SingleStack. Services can be \"SingleStack\" (a single IP family), \"PreferDualStack\" (two IP families on dual-stack configured clusters or a single IP family on single-stack clusters), or \"RequireDualStack\" (two IP families on dual-stack configured clusters, otherwise fail). The ipFamilies and clusterIPs fields depend on the value of this field. This field will be wiped when updating a service to type ExternalName. &#42; https://pkg.go.dev/k8s.io/api/core/v1#IPFamilyPolicyType false allocateLoadBalancerNodePorts allocateLoadBalancerNodePorts defines if NodePorts will be automatically allocated for services with type LoadBalancer. Default is \"true\". It may be set to \"false\" if the cluster load-balancer does not rely on NodePorts. allocateLoadBalancerNodePorts may only be set for services with type LoadBalancer and will be cleared if the type is changed to any other type. This field is alpha-level and is only honored by servers that enable the ServiceLBNodePortControl feature. &#42;bool false Back to TOC StartQuorum StartQuorum defines the order that deployments will be started in a Coherence cluster made up of multiple deployments. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false Back to TOC StartQuorumStatus StartQuorumStatus tracks the state of a deployment&#8217;s start quorums. Field Description Type Required deployment The name of deployment that this deployment depends on. string true namespace The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment string false podCount The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment. int32 false ready Whether this quorum&#8217;s condition has been met bool true Back to TOC Coherence Coherence is the top level schema for the Coherence API and custom resource definition (CRD). Field Description Type Required metadata &#160; metav1.ObjectMeta false spec &#160; CoherenceStatefulSetResourceSpec false status &#160; CoherenceResourceStatus false Back to TOC CoherenceList CoherenceList is a list of Coherence resources. Field Description Type Required metadata &#160; metav1.ListMeta false items &#160; [] Coherence true Back to TOC CoherenceResourceStatus CoherenceResourceStatus defines the observed state of Coherence resource. Field Description Type Required phase The phase of a Coherence resource is a simple, high-level summary of where the Coherence resource is in its lifecycle. The conditions array, the reason and message fields, and the individual container status arrays contain more detail about the pod&#8217;s status. There are eight possible phase values: Initialized: The deployment has been accepted by the Kubernetes system. Created: The deployments secondary resources, (e.g. the StatefulSet, Services etc.) have been created. Ready: The StatefulSet for the deployment has the correct number of replicas and ready replicas. Waiting: The deployment&#8217;s start quorum conditions have not yet been met. Scaling: The number of replicas in the deployment is being scaled up or down. RollingUpgrade: The StatefulSet is performing a rolling upgrade. Stopped: The replica count has been set to zero. Completed: The Coherence resource is running a Job and the Job has completed. Failed: An error occurred reconciling the deployment and its secondary resources. ConditionType false coherenceCluster The name of the Coherence cluster that this deployment is part of. string false type The type of the Coherence resource. CoherenceType false replicas Replicas is the desired number of members in the Coherence deployment represented by the Coherence resource. int32 true currentReplicas CurrentReplicas is the current number of members in the Coherence deployment represented by the Coherence resource. int32 true readyReplicas ReadyReplicas is the number of members in the Coherence deployment represented by the Coherence resource that are in the ready state. int32 true active When the Coherence resource is running a Job, the number of pending and running pods. int32 false succeeded When the Coherence resource is running a Job, the number of pods which reached phase Succeeded. int32 false failed When the Coherence resource is running a Job, the number of pods which reached phase Failed. int32 false role The effective role name for this deployment. This will come from the Spec.Role field if set otherwise the deployment name will be used for the role name string false selector label query over deployments that should match the replicas count. This is same as the label selector but in the string format to avoid introspection by clients. The string will be in the same format as the query-param syntax. More info about label selectors: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ string false conditions The status conditions. Conditions false hash Hash is the hash of the latest applied Coherence spec string false actionsExecuted ActionsExecuted tracks whether actions were executed bool false jobProbes &#160; []CoherenceJobProbeStatus false Back to TOC CoherenceStatefulSetResourceSpec CoherenceStatefulSetResourceSpec defines the specification of a Coherence resource. A Coherence resource is typically one or more Pods that perform the same functionality, for example storage members. Field Description Type Required cluster The optional name of the Coherence cluster that this Coherence resource belongs to. If this value is set the Pods controlled by this Coherence resource will form a cluster with other Pods controlled by Coherence resources with the same cluster name. If not set the Coherence resource&#8217;s name will be used as the cluster name. &#42;string false statefulSetAnnotations StatefulSetAnnotations are free-form yaml that will be added to the Coherence cluster StatefulSet as annotations. Any annotations should be placed BELOW this \"annotations:\" key, for example: The default behaviour is to copy all annotations from the Coherence resource to the StatefulSet , specifying any annotations in the StatefulSetAnnotations will override this behaviour and only include the StatefulSetAnnotations . annotations: foo.io/one: \"value1\" + foo.io/two: \"value2\" + see: Kubernetes Annotations map[string]string false volumeClaimTemplates VolumeClaimTemplates defines extra PVC mappings that will be added to the Coherence Pod. The content of this yaml should match the normal k8s volumeClaimTemplates section of a StatefulSet spec as described in https://kubernetes.io/docs/concepts/storage/persistent-volumes/ Every claim in this list must have at least one matching (by name) volumeMount in one container in the template. A claim in this list takes precedence over any volumes in the template, with the same name. [] PersistentVolumeClaim false scaling The configuration to control safe scaling. &#42; ScalingSpec false suspendProbe The configuration of the probe used to signal that services must be suspended before a deployment is stopped. &#42; Probe false suspendServicesOnShutdown A flag controlling whether storage enabled cache services in this deployment will be suspended before the deployment is shutdown or scaled to zero. The action of suspending storage enabled services when the whole deployment is being stopped ensures that cache services with persistence enabled will shut down cleanly without the possibility of Coherence trying to recover and re-balance partitions as Pods are stopped. The default value if not specified is true. &#42;bool false resumeServicesOnStartup ResumeServicesOnStartup allows the Operator to resume suspended Coherence services when the Coherence container is started. This only applies to storage enabled distributed cache services. This ensures that services that are suspended due to the shutdown of a storage tier, but those services are still running (albeit suspended) in other storage disabled deployments, will be resumed when storage comes back. Note that starting Pods with suspended partitioned cache services may stop the Pod reaching the ready state. The default value if not specified is true. &#42;bool false autoResumeServices AutoResumeServices is a map of Coherence service names to allow more fine-grained control over which services may be auto-resumed by the operator when a Coherence Pod starts. The key to the map is the name of the Coherence service. This should be the fully qualified name if scoped services are being used in Coherence. The value is a bool, set to true to allow the service to be auto-resumed or false to not allow the service to be auto-resumed. Adding service names to this list will override any value set in ResumeServicesOnStartup , so if the ResumeServicesOnStartup field is false but there are service names in the AutoResumeServices , mapped to true , those services will still be resumed. Note that starting Pods with suspended partitioned cache services may stop the Pod reaching the ready state. map[string]bool false suspendServiceTimeout SuspendServiceTimeout sets the number of seconds to wait for the service suspend call to return (the default is 60 seconds) &#42;int false haBeforeUpdate Whether to perform a StatusHA test on the cluster before performing an update or deletion. This field can be set to \"false\" to force through an update even when a Coherence deployment is in an unstable state. The default is true, to always check for StatusHA before updating a Coherence deployment. &#42;bool false allowUnsafeDelete AllowUnsafeDelete controls whether the Operator will add a finalizer to the Coherence resource so that it can intercept deletion of the resource and initiate a controlled shutdown of the Coherence cluster. The default value is false . The primary use for setting this flag to true is in CI/CD environments so that cleanup jobs can delete a whole namespace without requiring the Operator to have removed finalizers from any Coherence resources deployed into that namespace. It is not recommended to set this flag to true in a production environment, especially when using Coherence persistence features. &#42;bool false actions Actions to execute once all the Pods are ready after an initial deployment [] Action false envFrom List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. [] corev1.EnvFromSource false global Global contains attributes that will be applied to all resources managed by the Coherence Operator. &#42; GlobalSpec false initResources InitResources is the optional resource requests and limits for the init-container that the Operator adds to the Pod. ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ + The Coherence operator does not apply any default resources. &#42; corev1.ResourceRequirements false Back to TOC ",
            "title": "Coherence Operator API Docs"
        },
        {
            "location": "/docs/coherence/070_wka",
            "text": " A Coherence cluster is made up of one or more JVMs. In order for these JVMs to form a cluster they need to be able to discover other cluster members. The default mechanism for discovery is multicast broadcast but this does not work in most container environments. Coherence provides an alternative mechanism where the addresses of the hosts where the members of the cluster will run is provided in the form of a \"well known address\" (or WKA) list . This address list is then used by Coherence when it starts in a JVM to discover other cluster members running on the hosts in the WKA list. When running in containers each container is effectively a host and has its own host name and IP address (or addresses) and in Kubernetes it is the Pod that is effectively a host. When starting a container it is usually not possible to know in advance what the host names of the containers or Pods will be so there needs to be another solution to providing the WKA list. Coherence processes a WKA list it by performing a DNS lookup for each host name in the list. If a host name resolves to more than one IP address then all of those IP addresses will be used in cluster discovery. This feature of Coherence when combined with Kubernetes Services allows discovery of cluster members without resorting to a custom discovery mechanism. A Kubernetes Service has a DNS name and that name will resolve to all the IP addresses of the Pods that match that Service selector. This means that a Coherence JVM only needs to be given the DNS name of a Service as the single host name in its WKA list so that it will form a cluster with any other JVM using in a Pod matching the selector. When the Coherence Operator creates reconciles a Coherence CRD configuration to create a running set of Pods it creates a headless service specifically for the purposes of WKA for that Coherence resource with a selector that matches any Pod with the same cluster name. For example, if a Coherence resource is created with the following yaml: <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: storage spec: cluster: test-cluster In this yaml the Coherence resource has a cluster name of test-cluster The Operator will create a Service for the Coherence resource using the same name as the deployment with a -wka suffix. So in the example above the Operator would create a Service with the name storage-wka . The yaml for the WKA Service would look like the following: <markup lang=\"yaml\" title=\"wka-service.yaml\" >apiVersion: v1 kind: Service metadata: name: storage-wka labels: coherenceCluster: test-cluster component: coherenceWkaService spec: clusterIP: None publishNotReadyAddresses: true ports: - name: coherence protocol: TCP port: 7 targetPort: 7 selector: coherenceCluster: test-cluster component: coherencePod The Service name is made up of the cluster name with the suffix -wka so in this case storage-wka The service has a clusterIP of None so it is headless The Service is configured to allow unready Pods so that all Pods matching the selector will be resolved as members of this service regardless of their ready state. This is important so that Coherence JVMs can discover other members before they are fully ready. A single port is exposed, in this case the echo port (7), even though nothing in the Coherence Pods binds to this port. Ideally no port would be included, but a Kubernetes service has to have at least one port defined. The selector will match all Pods with the labels coherenceCluster=test-cluster and component=coherencePod which are labels that the Coherence Operator will assign to all Pods in this cluster Because this Service is created in the same Namespace as the deployment&#8217;s Pods the JVMs can use the raw Service name as the WKA list, in the example above the WKA list would just be test-cluster-wka . ",
            "title": "Well Known Addressing and Cluster Discovery"
        },
        {
            "location": "/docs/coherence/070_wka",
            "text": " In some situations it may be desirable to exclude the Pods belonging to certain deployments in the cluster from being members of the well known address list. For example certain K8s network configurations such as host networking can cause issues with WKA if other deployments in the cluster are using host networking. A role can be excluded from the WKA list by setting the excludeFromWKA field of the coherence section of the deployment&#8217;s spec to true . <markup lang=\"yaml\" title=\"test-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: test-client spec: cluster: `my-cluster` coherence: excludeFromWKA: true The cluster field is set to the name of the Coherence cluster that this deployment wil be part of (there is no point in excluding a deployment from WKA unless it is part of a wider cluster). The excludeFromWKA field is true so that Pods in the test-client deployment will not form part of the WKA list for the Coherence cluster. The operator does not validate the excludeFromWKA field for a deployment so it is possible to try to create a cluster where all of the deployment have excludeFromWKA set to true which will cause the cluster fail to start. When excluding a deployment from WKA it is important that at least one deployment that is part of the WKA list has been started first otherwise the non-WKA role members cannot start.Eventually the K8s readiness probe for these Pods would time-out causing K8s to restart them but this would not be a desirable way to start a cluster. The start-up order can be controlled by configuring the deployment&#8217;s startQuorum list, as described in the documentation section on deployment start-up ordering . ",
            "title": "Exclude a Deployment From WKA"
        },
        {
            "location": "/docs/coherence/070_wka",
            "text": " It is possible to configure a Coherence cluster made up of multiple Coherence deployments that are deployed into different namespaces in the same Kubernetes cluster (with some caveats). The coherence.wka section of the Coherence CRD spec can be used to override the default WKA behaviour. For example, suppose that there is a Coherence deployment named data that is the storage enabled cluster members holding data for an online store. This data deployment will be deployed into the back-end namespace in a Kubernetes cluster. Another Coherence deployment of storage disabled members will provide the front end REST API for the online store. This will be named web-store and deployed in the front-end namespace. Although both the data and web-store deployments are in different namespaces they need to form a single Coherence cluster. <markup lang=\"yaml\" title=\"data-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: data namespace: back-end spec: cluster: `shop` The data deployment is deployed into the back-end namespace The Coherence cluster name is set to shop <markup lang=\"yaml\" title=\"web-store-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web-store namespace: front-end spec: cluster: `shop` coherence: wka: deployment: data namespace: back-end The web-store deployment is deployed into the front-end namespace. The Coherence cluster name is set to shop to match the data deployment The coherence.wka section specifies the name of the Coherence deployment to use for WKA so in this case the data deployment in the back-end namespace. As described already above the data deployment will have a headless Service created for WKA named data-wka , which will be in the back-end namespace. The full name of this Service in Kubernetes will be data-wka.back-end.svc and this will be the name that the members of the web-store deployment will be configured to use for WKA. When using WKA in this way the Coherence deployment that is providing the WKA Service should be running before any deployment that depends on it is deployed. ",
            "title": "Multi-Namespace Clusters"
        },
        {
            "location": "/docs/coherence/070_wka",
            "text": " It is possible to fully override the WKA address that will be configured by the Operator. This is useful where a different service exists that will perform the DNS resolution (for example when using Submariner[ https://submariner.io ] to communicate over k8s clusters). In this case set the spec.coherence.wka.addresses field to be the WKA address (which is a list of string values). <markup lang=\"yaml\" title=\"web-store-deployment.yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web-store namespace: front-end spec: cluster: `shop` coherence: wka: addresses: - data.back-end.svc In the example above, the the Coherence WKA list would be configured as COHERENCE_WKA=data.back-end.svc . It is possible to use multiple addresses for WKA in the addresses field. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: Coherence metadata: name: web-store namespace: front-end spec: cluster: `shop` coherence: wka: addresses: - data-01.back-end.svc - data-02.back-end.svc In the example above, the Coherence WKA list would be configured as COHERENCE_WKA=data-01.back-end.svc,data-02.back-end.svc ",
            "title": "Override the WKA Address(es)"
        }
 ]
}