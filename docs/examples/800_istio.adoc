= Istio Support

== Overview

Coherence Operator 3.1.6 and later works with Istio 1.9.1 and later. You can run the operator and Coherence cluster managed by the operator with Istio. Coherence caches can be accessed from outside the Coherence cluster via Coherence*Extend, REST, and other supported Coherence clients. Using Coherence clusters with Istio does not require the Coherence Operator to also be using Istio (and vice-versa) . The Coherence Operator can manage Coherence clusters independent of whether those clusters are using Istio or not.

== Prometheus

The coherence metrics that record and track the health of Coherence cluster using Prometheus are also available in Istio environment and can be viewed through Granfana.  However, Coherence cluster traffic is not visible by Istio.

== Traffic Visualization

Istio provides traffic management capabilities, including the ability to visualize traffic in Kiali. You do not need to change your applications to use this feature. The Istio proxy (envoy) sidecar that is injected into your pods provides it. The image below shows an example with traffic flow. In this example, you can see how the traffic flows in from the Istio gateway on the left, to the cluster services, and then to the individual cluster members.  This example has storage members (example-cluster-storage), a proxy member running proxy service (example-cluster-proxy), and a REST member running http server (example-cluster-rest).  However, Coherence cluster traffic between members is not visible.

image::../images/istioKiali.png[width=1024,height=512]

To learn more, see https://istio.io/latest/docs/concepts/traffic-management/[Istio traffic management].

== Limitations

The current support for Istio has the following limitation:

 * Ports that are exposed in the ports list of pod spec are intercepted by Envoy proxies, thus break Coherence cluster traffic. As a result, Coherence cluster traffic must passthrough Envoy proxies.

== Prerequisites

The instructions assume that you are using a Kubernetes cluster with Istio installed and configured already.

== Using the Coherence operator with Istio

To use Coherence operator with Istio, you can deploy the operator into a namespace which has Istio automatic sidecar injection enabled.  Before installing the operator, create the namespace in which you want to run the Coherence operator and label it for automatic injection.


[source,bash]
----
kubectl create namespace coherence
kubectl label namespace coherence istio-injection=enabled
----

Istio Sidecar AutoInjection is done automatically when you label the coherence namespace with istio-injection.

After the namespace is labeled, you can install the operator using your preferred method in the Operator https://oracle.github.io/coherence-operator/docs/latest/#/installation/01_installation[Installation Guide].

After installed operator, use the following command to confirm the operator is running:

[source,bash]
----
kubectl get pods -n coherence

NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-controller-manager-7d76f9f475-q2vwv   2/2     Running   1          17h
----

2/2 in READY column means that there are 2 containers running in the operator Pod. One is Coherence operator and the other is Envoy Proxy.

== Creating a Coherence cluster with Istio

You can configure your cluster to run with Istio automatic sidecar injection enabled. Before creating your cluster, create the namespace in which you want to run the cluster and label it for automatic injection.

[source,bash]
----
kubectl create namespace coherence-example
kubectl label namespace coherence-example istio-injection=enabled
----

There is no other requirements to run Coherence in Istio environment.

The following is an example that creates a cluster named example-cluster-storage:

example.yaml
[source,bash]
----
# Example
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: example-cluster-storage
----

[source,bash]
----
$ kubectl -n coherence-example apply -f example.yaml
----

After you installed the Coherence cluster, run the following command to view the pods:

[source,bash]
----
$ kubectl -n coherence-example get pods

NAME                                             READY   STATUS    RESTARTS   AGE
example-cluster-storage-0                        2/2     Running   0          45m
example-cluster-storage-1                        2/2     Running   0          45m
example-cluster-storage-2                        2/2     Running   0          45m
----

You can see that 3 members in the cluster are running with 3 pods. 2/2 in READY column means that there are 2 containers running in each Pod. One is Coherence member and the other is Envoy Proxy.

== TLS

Coherence cluster works with mTLS. Coherence client can also support TLS through Istio Gateway with TLS termination to connect to Coherence cluster running inside kubernetes.  For example, you can apply the following Istio Gateway and Virtual Service in the namespace of the Coherence cluster.  Before applying the gateway, create a secret for the credential from the certiticate and key (e.g. server.crt and server.key) to be used by the Gateway:

[source,bash]
----
kubectl create -n istio-system secret tls extend-credential --key=server.key --cert=server.crt
----

Then, create a keystore (server.jks) to be used by the Coherence Extend client, e.g.:
[source,bash]
----
openssl pkcs12 -export -in server.crt -inkey server.key -chain -CAfile ca.crt -name "server" -out server.p12

keytool -importkeystore -deststorepass password -destkeystore server.jks -srckeystore server.p12 -srcstoretype PKCS12
----


tlsGateway.yaml
[source,bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: tlsgateway
spec:
  selector:
    istio: ingressgateway # use istio default ingress gateway
  servers:
  - port:
      number: 8043
      name: tls
      protocol: TLS
    tls:
      mode: SIMPLE
      credentialName: "extend-credential" # the secret created in the previous step
      maxProtocolVersion: TLSV1_3
    hosts:
    - "*"
----

tlsVS.yaml
[source,bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: extend
spec:
  hosts:
  - "*"
  gateways:
  - tlsgateway
  tcp:
  - match:
    route:
    - destination:
        host: example-cluster-proxy-proxy  # the service name used to expose the Extend proxy port
----

Apply the Gateway and VirtualService:

[source,bash]
----
kubectl apply -f tlsGateway.yaml -n coherence-example
kubectl apply -f tlsVS.yaml -n coherence-example
----

Then configure a Coherence*Extend client to connect to the proxy server via TLS protocol.  Below is an example of a <remoce-cache-scheme> configuration of an Extend client using TLS port 8043 configured in the Gateway and server.jks created earlier in the example.

client-cache-config.xml
----
...
    <remote-cache-scheme>
        <scheme-name>extend-direct</scheme-name>
        <service-name>ExtendTcpProxyService</service-name>
        <initiator-config>
            <tcp-initiator>
                <socket-provider>
                    <ssl>
                        <protocol>TLS</protocol>
                        <trust-manager>
                            <algorithm>PeerX509</algorithm>
                            <key-store>
                                <url>file:server.jks</url>
                                <password>password</password>
                            </key-store>
                        </trust-manager>
                    </ssl>
                </socket-provider>
                <remote-addresses>
                    <socket-address>
                        <address>$INGRESS_HOST</address>
                        <port>8043</port>
                    </socket-address>
                </remote-addresses>
            </tcp-initiator>
        </initiator-config>
    </remote-cache-scheme>
...
----

If you are using Docker for Desktop, $INGRESS_HOST is 127.0.0.1 and you can use the Kubectl port-forward to allow the Extend client to access the Coherence cluster from your localhost: 

[source,bash]
----
kubectl port-forward -n istio-system <istio-ingressgateway-pod> 8043:8043
----
