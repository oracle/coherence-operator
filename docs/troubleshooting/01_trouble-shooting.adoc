///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2021, Oracle and/or its affiliates.
    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.

///////////////////////////////////////////////////////////////////////////////

= Troubleshooting Guide

== Troubleshooting Guide

The purpose of this page is to list troubleshooting guides and work arounds for issues that you may run into when using the Coherence Operator.
This paged will be updated and maintained over time to include common issues we see from customers

== Contents

* <<#stuck-pending,My Coherence cluster is stuck with pending Pods>>

* <<#site-safe,My cache services will not reach Site Safe>>



== Issues

[#stuck-pending]
=== My Coherence cluster is stuck with pending Pods

If you try to create a Coherence deployment that has a replica count that is greater than your k8s cluster can actually
provision then one or more Pods will fail to be created or can be left in a pending state.
The obvious solution to this is to just scale down your Coherence deployment to a smaller size that can be provisioned.
The issue here is that the safe scaling functionality built into the operator will not allow scaling down to take place
because it cannot guarantee no parition/data loss. The Coherence deployment is now stuck in this state.

The simplest solution would be to completely delete the the Coherence deployment and redeploy with a lower replica count.

If this is not possible then the following steps will allow the deployment to be scaled down.

1 Update the stuck Coherence deployment's scaling policy to be `Parallel`
[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    policy: Parallel
----

2 Scale down the cluster to the required size using whatever scaling commands you want, i.e `kubectl scale`
or just update the replica value of the Coherence deployment yaml. Note: If updating the Coherence yaml, this
should not be done as part of step 1, above.

3 Once the Coherence deployment has scaled to the required size then change the scaling policy value back to the
default by updating the Coherence yaml to have no scaling policy value in it.

WARNING: When using this work around to scale down a stuck deployment that contains data it is important that
only the missing or pending Pods are removed. For example if a Coherence deployment is deployed with a replica count
of 100 and 90 Pods are ready, but the other 10 are either missing or stuck pending then the replica value used in
step 2 above must be 90. Because the scaling policy has been set to `Parallel` the operator will not check any
Status HA values before scaling down Pods, so removing "ready" Pods that contain data will almost certainly result
in data loss. To safely scale down lower, then first follow the three steps above then after changing the scaling policy
back to the default further scaling down can be done as normal.


[#site-safe]
=== My cache services will not reach Site-Safe

Coherence distributes data in a cluster to achieve the highest status HA value that it can, the best being site-safe.
This is done using the various values configured for the site, rack, machine, and member names.
The Coherence Operator configures these values for the Pods in a Coherence deployment.
By default, the values for the site and rack names are taken from standard k8s labels applied to the Nodes in the k8s cluster.
If the Nodes in the cluster do not have these labels set then the site and rack names will be unset and Coherence
will not be able to reach rack or site safe.

There are a few possible solutions to this, see the explanation in the
documentation explaining <<coherence/021_member_identity.adoc,Member Identity>>

