///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2021, Oracle and/or its affiliates.
    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.

///////////////////////////////////////////////////////////////////////////////

= Troubleshooting Guide

== Troubleshooting Guide

The purpose of this page is to list troubleshooting guides and work-arounds for issues that you may run into when using the Coherence Operator.
This paged will be updated and maintained over time to include common issues we see from customers

== Contents

* <<#stuck-pending,My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down>>

* <<#stuck-delete,My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment>>

* <<#site-safe,My cache services will not reach Site Safe>>

* <<dashboards,My Grafana Dashboards do not display any metrics>>

== Issues

[#stuck-pending]
=== My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down

If you try to create a Coherence deployment that has a replica count that is greater than your k8s cluster can actually
provision then one or more Pods will fail to be created or can be left in a pending state.
The obvious solution to this is to just scale down your Coherence deployment to a smaller size that can be provisioned.
The issue here is that the safe scaling functionality built into the operator will not allow scaling down to take place
because it cannot guarantee no parition/data loss. The Coherence deployment is now stuck in this state.

The simplest solution would be to completely delete the the Coherence deployment and redeploy with a lower replica count.

If this is not possible then the following steps will allow the deployment to be scaled down.

1 Update the stuck Coherence deployment's scaling policy to be `Parallel`
[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    policy: Parallel
----

2 Scale down the cluster to the required size using whatever scaling commands you want, i.e `kubectl scale`
or just update the replica value of the Coherence deployment yaml. Note: If updating the Coherence yaml, this
should not be done as part of step 1, above.

3 Once the Coherence deployment has scaled to the required size then change the scaling policy value back to the
default by updating the Coherence yaml to have no scaling policy value in it.

WARNING: When using this work around to scale down a stuck deployment that contains data it is important that
only the missing or pending Pods are removed. For example if a Coherence deployment is deployed with a replica count
of 100 and 90 Pods are ready, but the other 10 are either missing or stuck pending then the replica value used in
step 2 above must be 90. Because the scaling policy has been set to `Parallel` the operator will not check any
Status HA values before scaling down Pods, so removing "ready" Pods that contain data will almost certainly result
in data loss. To safely scale down lower, then first follow the three steps above then after changing the scaling policy
back to the default further scaling down can be done as normal.

[#stuck-delete]
=== My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment

A Coherence deployment can become stuck where none of the Pods can start, for example the image used is incorrect
and all Pods are stuck in ImagePullBackoff. It can then become impossible to delete the broken deployment.
This is because the Operator has installed a finalizer but this finalizer cannot execute.

For example, suppose we have deployed a Coherence deployment named `my-cluster` into namespace `coherence-test`.

First try to delete the deployment as normal:
[source,console]
----
kubectl -n coherence-test delete coherence/my-cluster
----

If this command hangs, then press `ctrl-c` to exit and then run the following patch command.

[source,console]
----
kubectl -n coherence-test patch coherence/my-cluster -p '{"metadata”:{“finalizers":[]}}' --type=merge
----
This will remove the Operator's finalizer from the Coherence deployment.

At this point the `my-cluster` Coherence deployment might already have been removed,
if not try the delete command again.


[#site-safe]
=== My cache services will not reach Site-Safe

Coherence distributes data in a cluster to achieve the highest status HA value that it can, the best being site-safe.
This is done using the various values configured for the site, rack, machine, and member names.
The Coherence Operator configures these values for the Pods in a Coherence deployment.
By default, the values for the site and rack names are taken from standard k8s labels applied to the Nodes in the k8s cluster.
If the Nodes in the cluster do not have these labels set then the site and rack names will be unset and Coherence
will not be able to reach rack or site safe.

There are a few possible solutions to this, see the explanation in the
documentation explaining <<coherence/021_member_identity.adoc,Member Identity>>

[#dashboards]
=== My Grafana Dashboards do not display any metrics

If you have imported the Grafana dashboards provided by the Operator into Grafana, but they are not displaying any metric
values, it may be that you have imported the wrong format dashboards. The Operator has multiple sets of dashboards,
one for the default Coherence metric name format, one for Microprofile metric name format, and one for
https://micrometer.io[Micrometer] metric name format.

The simplest way to find out which version corresponds to your Coherence cluster
is to query the metrics endpoint with something like `curl`.
If the metric names are in the format `vendor:coherence_cluster_size`, i.e. prefixed with `vendor:` then this is
the default Coherence format.
If metric names are in the format `vendor_Coherence_Cluster_Size`, i.e. prefixed with `vendor_` then this is
Microprofile format.
If the metric name has no `vendor` prefix then it is using Micrometer metrics.

See: the <<metrics/030_importing.adoc,Importing Grafana Dashboards>> documentation.

