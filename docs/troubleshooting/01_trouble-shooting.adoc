///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2021, 2023, Oracle and/or its affiliates.
    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.

///////////////////////////////////////////////////////////////////////////////

= Troubleshooting Guide

== Troubleshooting Guide

The purpose of this page is to list troubleshooting guides and work-arounds for issues that you may run into when using the Coherence Operator.
This page will be updated and maintained over time to include common issues we see from customers

== Contents

* <<no-operator,I Uninstalled the Operator and Cannot Delete the Coherence Clusters>>

* <<ns-delete,Deleting a Namespace Containing Coherence Resource(s) is Stuck Pending>>

* <<#restart,Why Does the Operator Pod Restart>>

* <<#ready,Why are the Coherence Pods not reaching ready>>

* <<#stuck-pending,My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down>>

* <<#stuck-delete,My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment>>

* <<#site-safe,My cache services will not reach Site Safe>>

* <<dashboards,My Grafana Dashboards do not display any metrics>>

* <<arm-java8, I'm using Arm64 and Java 8 and the JVM will not start due to using G1GC>>

== Issues

[#no-operator]
=== I Uninstalled the Operator and Cannot Delete the Coherence Clusters

The `Coherence` resources managed by the Operator are marked in k8s as being owned by the Operator, and have finalizers to stop them being deleted. In normal operation the Operator will remove the finalizer when it deletes a `Coherence` cluster. The Operator also installs a validating and mutating web-hook, which will also stop k8s allowing mutations and deletions to a `Coherence` resource if the Coherence Operator is not running.

If the Operator has been uninstalled, first remove the two web-hooks.

[source,bash]
----
kubectl delete mutatingwebhookconfiguration coherence-operator-mutating-webhook-configuration
kubectl delete validatingwebhookconfiguration coherence-operator-validating-webhook-configuration
----

Now patch and delete each Coherence resource to delete its finalizers using the command below and replacing `<NAMESPACE>` with the correct namespace the `Coherence` resource is in and `<COHERENCE_RESOURCE_NAME>` with the
`Coherence` resource name.

[source,bash]
----
kubectl -n <NAMESPACE> patch coherence/<COHERENCE_RESOURCE_NAME> -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl -n <NAMESPACE> delete coherence/<COHERENCE_RESOURCE_NAME>
----

[#ns-delete]
=== Deleting a Namespace Containing Coherence Resource(s) is Stuck Pending

If you have tried to delete a namespace using `kubectl delete` and the namespace is now stuck pending deletion, this
could be related to the issue above. This is especially true if the Operator is either not running, or it is in the
same namespace that is being deleted.

If deleting a namespace containing the Operator and a running Coherence cluster, or deleting a namespace containing
a running Coherence cluster when the Operator is stopped will mean the finalizers the Operator added to the Coherence
resources cannot be removed so the namespace will remain in a pending deletion state. The solution to this is the same
as the point above in <<no-operator,I Uninstalled the Operator and Cannot Delete the Coherence Clusters>>

Alternatively, if you are running the Operator in a CI/CD environment and just want to be able to clean up after
tests you can run Coherence clusters with the `allowUnsafeDelete` option enabled.
By setting the `allowUnsafeDelete` field to `true` in the `Coherence` resource the Operator will not add a finalizer
to that Coherence resource, allowing it to be deleted if its namespace is deleted.

For example:

[source,yaml]
.cluster.yaml
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: unsafe-cluster
spec:
  allowUnsafeDelete: true
----

[CAUTION]
====
Setting the `allowUnsafeDelete` field to `true` will mean that the Operator will not be able to intercept the deletion
and shutdown of a Coherence cluster and ensure it has a clean, safe shutdown. This is usually ok in CI/CD environments
where the cluster and namespace are being cleaned up at the end of a test. This options should not be used in
a production cluster, especially where features such as Coherence persistence are being used, otherwise the cluster may
not cleanly shut down and will then not be able to be restarted using the persisted data.
====

[#restart]
=== Why Does the Operator Pod Restart After Installation

You might notice that when the Operator is installed that the Operator Pod starts, dies and is then restarted.
This is expected behaviour. The Operator uses a K8s web-hook for defaulting and validation of the Coherence resources.
A web-hook requires certificates to be present in a `Secret` mounted to the Operator Pod as a volume.
If this Secret is not present the Operator creates it and populates it with self-signed certs.
In order for the Secret to then be mounted correctly the Pod must be restarted.
See the <<docs/webhooks/01_introduction.adoc,the WebHooks>> documentation.

[#ready]
=== Why are the Coherence Pods not reaching ready
The readiness/liveness probe used by the Operator in the Coherence Pods checks a number of things to determine whether the Pods is ready, one of these is whether the JVM is a cluster member.
If your application uses a custom main class and is not properly bootstrapping Coherence then the Pod will not be ready until your application code actually touches a Coherence resource causing Coherence to start and join the cluster.

When running in clusters with the Operator using custom main classes it is advisable to properly bootstrap Coherence
from within your `main` method. This can be done using the new Coherence bootstrap API available from CE release 20.12
or by calling `com.tangosol.net.DefaultCacheServer.startServerDaemon().waitForServiceStart();`

[#stuck-pending]
=== My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down

If you try to create a Coherence deployment that has a replica count that is greater than your k8s cluster can actually
provision then one or more Pods will fail to be created or can be left in a pending state.
The obvious solution to this is to just scale down your Coherence deployment to a smaller size that can be provisioned.
The issue here is that the safe scaling functionality built into the operator will not allow scaling down to take place
because it cannot guarantee no parition/data loss. The Coherence deployment is now stuck in this state.

The simplest solution would be to completely delete the the Coherence deployment and redeploy with a lower replica count.

If this is not possible then the following steps will allow the deployment to be scaled down.

1 Update the stuck Coherence deployment's scaling policy to be `Parallel`
[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    policy: Parallel
----

2 Scale down the cluster to the required size using whatever scaling commands you want, i.e `kubectl scale`
or just update the replica value of the Coherence deployment yaml. Note: If updating the Coherence yaml, this
should not be done as part of step 1, above.

3 Once the Coherence deployment has scaled to the required size then change the scaling policy value back to the
default by updating the Coherence yaml to have no scaling policy value in it.

WARNING: When using this work around to scale down a stuck deployment that contains data it is important that
only the missing or pending Pods are removed. For example if a Coherence deployment is deployed with a replica count
of 100 and 90 Pods are ready, but the other 10 are either missing or stuck pending then the replica value used in
step 2 above must be 90. Because the scaling policy has been set to `Parallel` the operator will not check any
Status HA values before scaling down Pods, so removing "ready" Pods that contain data will almost certainly result
in data loss. To safely scale down lower, then first follow the three steps above then after changing the scaling policy
back to the default further scaling down can be done as normal.

[#stuck-delete]
=== My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment

A Coherence deployment can become stuck where none of the Pods can start, for example the image used is incorrect
and all Pods are stuck in ImagePullBackoff. It can then become impossible to delete the broken deployment.
This is because the Operator has installed a finalizer but this finalizer cannot execute.

For example, suppose we have deployed a Coherence deployment named `my-cluster` into namespace `coherence-test`.

First try to delete the deployment as normal:
[source,console]
----
kubectl -n coherence-test delete coherence/my-cluster
----

If this command hangs, then press `ctrl-c` to exit and then run the following patch command.

[source,console]
----
kubectl -n coherence-test patch coherence/my-cluster -p '{"metadata":{"finalizers":[]}}' --type=merge
----
This will remove the Operator's finalizer from the Coherence deployment.

At this point the `my-cluster` Coherence deployment might already have been removed,
if not try the delete command again.


[#site-safe]
=== My cache services will not reach Site-Safe

Coherence distributes data in a cluster to achieve the highest status HA value that it can, the best being site-safe.
This is done using the various values configured for the site, rack, machine, and member names.
The Coherence Operator configures these values for the Pods in a Coherence deployment.
By default, the values for the site and rack names are taken from standard k8s labels applied to the Nodes in the k8s cluster.
If the Nodes in the cluster do not have these labels set then the site and rack names will be unset and Coherence
will not be able to reach rack or site safe.

There are a few possible solutions to this, see the explanation in the
documentation explaining <<docs/coherence/021_member_identity.adoc,Member Identity>>

[#dashboards]
=== My Grafana Dashboards do not display any metrics

If you have imported the Grafana dashboards provided by the Operator into Grafana, but they are not displaying any metric
values, it may be that you have imported the wrong format dashboards. The Operator has multiple sets of dashboards,
one for the default Coherence metric name format, one for Microprofile metric name format, and one for
https://micrometer.io[Micrometer] metric name format.

The simplest way to find out which version corresponds to your Coherence cluster
is to query the metrics endpoint with something like `curl`.
If the metric names are in the format `vendor:coherence_cluster_size`, i.e. prefixed with `vendor:` then this is
the default Coherence format.
If metric names are in the format `vendor_Coherence_Cluster_Size`, i.e. prefixed with `vendor_` then this is
Microprofile format.
If the metric name has no `vendor` prefix then it is using Micrometer metrics.

See: the <<docs/metrics/030_importing.adoc,Importing Grafana Dashboards>> documentation.

[#arm-java8]
=== I'm using Arm64 and Java 8 and the JVM will not start due to using G1GC
If running Kubernetes on ARM processors and using Coherence images built on Java 8 for ARM,
note that the G1 garbage collector in that version of Java on ARM is marked as experimental.

By default, the Operator configures the Coherence JVM to use G1.
This will cause errors on Arm64 Java 8 JMS unless the JVM option `-XX:+UnlockExperimentalVMOptions` is
added in the Coherence resource spec (see <<docs/jvm/030_jvm_args.adoc,Adding Arbitrary JVM Arguments>>).
Alternatively specify a different garbage collector, ideally on a version of Java this old, use CMS
(see <<docs/jvm/040_gc.adoc,Garbage Collector Settings>>).
