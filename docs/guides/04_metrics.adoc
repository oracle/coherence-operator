///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

///////////////////////////////////////////////////////////////////////////////

= Metrics

Coherence clusters can be deployed with a metrics endpoint enabled that can be scraped by common metrics applications
such as Prometheus.

== Deploying Coherence Clusters with Metrics

NOTE: Note: Use of metrics is available only when using the operator with clusters running
Coherence 12.2.1.4 or later version.

The Coherence Operator can be installed with a demo Prometheus installation using embedded Prometheus Operator and
Grafana Helm charts. This Prometheus deployment is not intended for production use but is useful for development,
testing and demo purposes.

=== 1. Install the Coherence Operator with Prometheus

To enable Prometheus, add the following options to the Operator Helm install command:

[source,bash]
----
--set prometheusoperator.enabled=true
--set prometheusoperator.prometheusOperator.createCustomResource=false
----

A more complete helm install command to enable Prometheus is as follows:

[source,bash]
----
helm install \
    --namespace <namespace> \                                                  <1>
    --name coherence-operator \
    --set prometheusoperator.enabled=true \
    --set prometheusoperator.prometheusOperator.createCustomResource=false \
    coherence/coherence-operator
----
<1> Set `<namespace>` to the Kubernetes namespace that the Coherence Operator should be installed into.

After the installation completes, list the pods in the namespace that the Operator was installed into:
[source,bash]
----
kubectl -n <namespace> get pods
----

The results returned should look something like the following:

[source,bash]
----
NAME                                                   READY   STATUS    RESTARTS   AGE
operator-coherence-operator-5d779ffc7-7xz7j            1/1     Running   0          53s  <1>
operator-grafana-9d7fc9486-46zb7                       2/2     Running   0          53s  <2>
operator-kube-state-metrics-7b4fcc5b74-ljdf8           1/1     Running   0          53s
operator-prometheus-node-exporter-kwdr7                1/1     Running   0          53s
operator-prometheusoperato-operator-77c784b8c5-v4bfz   1/1     Running   0          53s
prometheus-operator-prometheusoperato-prometheus-0     3/3     Running   2          38s  <3>
----
<1> The Coherence Operator `Pod`
<2> The Grafana `Pod`
<3> The Prometheus `Pod`

The demo install of Prometheus in the Operator configures Prometheus to use service monitors to work out which Pods
to scrape metrics from. A `ServiceMonitor` in Prometheus will scrape from a port defined in a Kubernetes `Service` from
all `Pods` that match that service's selector.


=== 2. Install a Coherence Cluster with Metrics Enabled

Now that Prometheus is running Coherence clusters can be created that expose metrics on a port on each `Pod` and also
deploy a `Service` to expose the metrics that Prometheus can use.

Deploy a simple metrics enabled `CoherenceCluster` resource with a single role like this:
[source,yaml]
.metrics-cluster.yaml
----
apiVersion: coherence.oracle.com/v1
kind: CoherenceCluster
metadata:
  name: test-cluster
spec:
  role: storage       <1>
  replicas: 2
  metrics:
    enabled: true     <2>
  ports:
    - name: metrics   <3>
      port: 9612      <4>
----

<1> This cluster will have a single role called `storage`
<2> The cluster will have two replicas (`Pods`)
<3> The Coherence `Pod` spec contains a port spec for metrics named `metric` so this needs to be exposed as a
service by specifying the `metrics` port in the role's `ports` list
<4> The port must be set as `9612` which is the port that Coherence will expose metrics on

The yaml above can be installed into Kubernetes using `kubectl`:

[source,bash]
----
kubectl -n <namespace> create -f metrics-cluster.yaml
----

The Coherence Operator will see the new `CoherenceCluster` resource and create the cluster with two `Pods`.
If `kubectl get pods -n <namespace>` is run again it should now look something like this:

[source,bash]
----
NAME                                                   READY   STATUS    RESTARTS   AGE
operator-coherence-operator-5d779ffc7-7xz7j            1/1     Running   0          53s
operator-grafana-9d7fc9486-46zb7                       2/2     Running   0          53s
operator-kube-state-metrics-7b4fcc5b74-ljdf8           1/1     Running   0          53s
operator-prometheus-node-exporter-kwdr7                1/1     Running   0          53s
operator-prometheusoperato-operator-77c784b8c5-v4bfz   1/1     Running   0          53s
prometheus-operator-prometheusoperato-prometheus-0     3/3     Running   2          38s
test-cluster-storage-0                                 1/1     Running   0          70s  <1>
test-cluster-storage-1                                 1/1     Running   0          70s  <2>
----
<1> `Pod` one of the Coherence cluster.
<2> `Pod` two of the Coherence cluster.

If the services are listed for the namespace:
[source,bash]
----
kubectl -n <namespace> get svc
----

The list of services will look something like this.

[source,bash]
----
NAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
operator-grafana                        ClusterIP   10.104.251.51    <none>        80/TCP      31m
operator-kube-state-metrics             ClusterIP   10.110.18.78     <none>        8080/TCP    31m
operator-prometheus-node-exporter       ClusterIP   10.102.181.6     <none>        9100/TCP    31m
operator-prometheusoperato-operator     ClusterIP   10.107.59.229    <none>        8080/TCP    31m
operator-prometheusoperato-prometheus   ClusterIP   10.99.208.18     <none>        9090/TCP    31m
prometheus-operated                     ClusterIP   None             <none>        9090/TCP    31m
test-cluster-storage-headless           ClusterIP   None             <none>        30000/TCP   16m
test-cluster-storage-metrics            ClusterIP   10.109.201.211   <none>        9612/TCP    16m  <1>
test-cluster-wka                        ClusterIP   None             <none>        30000/TCP   16m
----
<1> One of the services will be the service exposing the Coherence metrics.
The service name is typically in the format `<cluster-name>-<role-name>-<port-name>`

The Prometheus `ServiceMonitor` installed by the Coherence Operator is configured to look for services with the
label `component=coherence-service-metrics`. When ports are exposed in a `CoherenceCluster`, as has been done here
for metrics, the service created will have a label of the format `component=coherence-service-<port-name>`, so in
this case the `test-cluster-storage-metrics` service above will have the label `component=coherence-service-metrics`.

The labels for the service can be displayed:
[source,bash]
----
kubectl -n <namespace>> get svc/test-cluster-storage-metrics --label-columns=component
----

[source,bash]
----
NAME                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   COMPONENT
test-cluster-storage-metrics   ClusterIP   10.109.201.211   <none>        9612/TCP   26m   coherence-service-metrics
----
Which shows that the service does indeed have the required label.


=== 3. Access Prometheus

Now that Prometheus is running and is able to scrape metrics from the Coherence cluster it should be possible to access
those metrics in Prometheus.

First find the Prometheus `Pod` name using `kubectl`

[source,bash]
----
kubectl -n <namespace> get pod -l app=prometheus -o name
----

Using the `Pod` name use `kubectl` to create a port forward session to the Prometheus `Pod` so that the
Prometheus API on port `9090` in the `Pod` can be accessed from the local host.

[source,bash]
----
kubectl -n <namespace> port-forward \
    $(kubectl -n <namespace> get pod -l app=prometheus -o name) \
    9090:9090
----

It is now possible to access the Prometheus API on localhost port 9090. This can be used to directly retrieve
Coherence metrics using `curl`, for example to obtain the cluster size metric:

[source,bash]
----
curl -w '\n' -X GET http://127.0.0.1:9090/api/v1/query?query=vendor:coherence_cluster_size
----

It is also possible to browse directly to the Prometheus web UI at http://127.0.0.1:9090[]


=== 3. Access Grafana

By default when the Coherence Operator configured to install Prometheus the Prometheus Operator also install a
Grafana `Pod` and the Coherence Operator imports into Grafana a number of custom dashboards for displaying Coherence
metrics. Grafana can be accessed by using port forwarding in the same way that was done for Prometheus

First find the Grafana `Pod`:
[source,bash]
----
kubectl -n <namespace> get pod -l app=grafana -o name
----

Using the `Pod` name use `kubectl` to create a port forward session to the Grafana `Pod` so that the
Grafana API on port `3000` in the `Pod` can be accessed from the local host.

[source,bash]
----
kubectl -n <namespace> port-forward \
    $(kubectl -n <namespace> get pod -l app=grafana -o name) \
    3000:3000
----

The custom Coherence dashboards can be accessed by pointing a browser to
http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main

The Grafana credentials are username `admin` password `prom-operator`

=== 4. Cleaning Up
After running the demo above the Coherence cluster can be removed using `kubectl`:

[source,bash]
----
kubectl -n <namespace> delete -f metrics-cluster.yaml
----

The Coherence Operator, along with Prometheus and Grafana can be removed using Helm:

[source,bash]
----
helm delete --purge coherence-operator
----
