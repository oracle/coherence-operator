///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

///////////////////////////////////////////////////////////////////////////////

= Using Your Own Elasticsearch

The Coherence Operator can be configured to instruct Fluentd to push logs to a separate Elasticsearch instance rather thatn the in-built one.

== Pushing logs to your own Elasticsearch instance

This example shows how to instruct Fluentd to push data to your own Elasticsearch instance.

[#install]
=== 1. Install the Coherence Operator with custom Elasticsearch endpoint

==== Elasticsearch Endpoint Host & Port
To enable an different Elasticsearch endpoint, add the following options to the Operator Helm install command:

[source,bash]
----
--set elasticsearchEndpoint.host=your-es-host
--set elasticsearchEndpoint.port=your-es-port
----

==== Setting the Elasticsearch Scheme

If your ElasticSearch server requires https then you must set the scheme value that will be applied to the fluentd configuration.
See: https://docs.fluentd.org/output/elasticsearch#scheme-optional[Fluend ES Config Docs]
[source,bash]
----
--set elasticsearchEndpoint.scheme=https
----

Valid values for `elasticsearchEndpoint.scheme` are http or `https` *Note* the scheme value is
case sensetive, fluentd will fail to start with any value other than exactly `http` or `https`.

==== Multiple Elasticsearch Hosts

Instead of specifying host and port as described above you can use the `elasticsearchEndpoint.hosts`
value to specify one or more host strings as a comma delimited list.
See: https://docs.fluentd.org/output/elasticsearch#hosts-optional[Fluend ES Config Docs]

For example
[source,bash]
----
--set elasticsearchEndpoint.hosts=https://customhost.com:443/path,https://username:password@host-failover.com:443
----

If `elasticsearchEndpoint.hosts` is used then Fluentd will ignore any values set for `scheme` host` and `port`
as described in the https://docs.fluentd.org/output/elasticsearch#hosts-optional[Fluend ES Config Docs]

==== Elasticsearch Credentials

You can also set the user and password if you Elasticsearch instance requires it:

[source,bash]
----
--set elasticsearchEndpoint.user=user
--set elasticsearchEndpoint.password=password
----

NOTE: For this example we have used the Stable ELK Stack at https://github.com/helm/charts/tree/master/stable/elastic-stack[https://github.com/helm/charts/tree/master/stable/elastic-stack]
to install the required components and have Elasticsearch runing on coherence-example-elastic-stack.default.svc.cluster.local:9200

=== Example

A more complete helm install command to enable Prometheus is as follows:

[source,bash]
----
helm install \
    --namespace <namespace> \
    --name coherence-operator \
    --set elasticsearchEndpoint.host=coherence-example-elastic-stack.default.svc.cluster.local \
    --set elasticsearchEndpoint.port=9200 \
    coherence/coherence-operator
----


After the installation completes, list the pods in the namespace that the Operator was installed into:
[source,bash]
----
kubectl -n <namespace> get pods
----

The results returned should look something like the following:

[source,bash]
----
NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-66c6d868b9-rd429                      1/1     Running   0          8m
coherence-operator-grafana-8454698bcf-v5kxw              2/2     Running   0          8m
coherence-operator-kube-state-metrics-6dc8675d87-qnfdw   1/1     Running   0          8m
coherence-operator-prometh-operator-58d94ffbb8-94d4m     1/1     Running   0          8m
coherence-operator-prometheus-node-exporter-vpjjt        1/1     Running   0          8m
prometheus-coherence-operator-prometh-prometheus-0       3/3     Running   0          8m
----

NOTE: You will notice that there are no Kibana and Elasticsearch pods.

[#install-coh]
=== 2. Install a Coherence Cluster with Logging Enabled

NOTE: From this point on there is no difference in installation from when EFK is installed by the Coherence Operator.
This is because when Coherence is installed it will querying the Coherence Operator to receive the new Elasticsearch endpoint.
.

Deploy a simple logging enabled `CoherenceCluster` resource with a single role like this:
[source,yaml]
.logging-cluster.yaml
----
apiVersion: coherence.oracle.com/v1
kind: CoherenceCluster
metadata:
  name: logging-cluster
spec:
  role: storage
  replicas: 3
  logging:
    fluentd:
      enabled: true  <1>
----

<1> Enables log capture via Fluentd

The yaml above can be installed into Kubernetes using `kubectl`:

[source,bash]
----
kubectl -n <namespace> create -f logging-cluster.yaml

coherencecluster.coherence.oracle.com/logging-cluster created

kubectl -n <namespace> get pod -l coherenceCluster=logging-cluster

NAME                        READY   STATUS    RESTARTS   AGE
logging-cluster-storage-0   2/2     Running   0          86s
logging-cluster-storage-1   2/2     Running   0          86s
logging-cluster-storage-2   2/2     Running   0          86s
----

=== 3. Inspect the Fluentd container logs

Issue the following to view the logs for the Fluentd container on the first Pod:

[source,bash]
----
kubectl logs -n <namespace> logging-cluster-storage-0 -c fluentd
----

In the output you will see something similar to the following indicating your Fluentd container
will send data to your own Elasticsearch.

[source,bash]
----
 <match coherence-cluster>
    @type elasticsearch
    host "coherence-example-elastic-stack.default.svc.cluster.local"
    port 9020
    user ""
    password xxxxxx
    logstash_format true
    logstash_prefix "coherence-cluster"
  </match>
----

=== 4. Connect to your Kibana UI

Connect to your Kibana UI and create an index pattern called `coherence-cluster-*` to view the
incoming logs.

=== 5. Clean Up
After running the above the Coherence cluster can be removed using `kubectl`:

[source,bash]
----
kubectl -n <namespace> delete -f logging-cluster.yaml
----