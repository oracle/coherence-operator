///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

///////////////////////////////////////////////////////////////////////////////

= Enabling Log Capture

The Coherence Operator manages data logging through the Elasticsearch, Fluentd and Kibana (EFK) stack.

== Capturing and viewing Coherence cluster Logs

This example shows how to enable log capture and access the Kibana user interface (UI) to view the captured logs.

Logs are scraped via a Fluentd sidecar image, parsed and sent to Elasticsearch. A default
index pattern called `coherence-cluster-*` is created which holds all captured logs.

[#install]
=== 1. Install the Coherence Operator with Fluentd logging enabled

To enable the EFK stack, add the following options to the Operator Helm install command:

[source,bash]
----
--set installEFK=true
----

A more complete helm install command to enable Prometheus is as follows:

[source,bash]
----
helm install \
    --namespace <namespace> \
    --name coherence-operator \
    --set installEFK=true \
    coherence/coherence-operator
----

After the installation completes, list the pods in the namespace that the Operator was installed into:
[source,bash]
----
kubectl -n <namespace> get pods
----

The results returned should look something like the following:

[source,bash]
----
NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-66c6d868b9-rd429                      1/1     Running   0          8m
coherence-operator-grafana-8454698bcf-v5kxw              2/2     Running   0          8m
coherence-operator-kube-state-metrics-6dc8675d87-qnfdw   1/1     Running   0          8m
coherence-operator-prometh-operator-58d94ffbb8-94d4m     1/1     Running   0          8m
coherence-operator-prometheus-node-exporter-vpjjt        1/1     Running   0          8m
elasticsearch-f978d6fdd-dw7qg                            1/1     Running   0          8m   <1>
kibana-9964496fd-5tpv9                                   1/1     Running   0          8m   <2>
prometheus-coherence-operator-prometh-prometheus-0       3/3     Running   0          8m
----
<1> The Elasticsearch `Pod`
<2> The Kibana `Pod`

[#install-coh]
=== 2. Install a Coherence Cluster with Logging Enabled

Deploy a simple logging enabled `CoherenceCluster` resource with a single role like this:
[source,yaml]
.logging-cluster.yaml
----
apiVersion: coherence.oracle.com/v1
kind: CoherenceCluster
metadata:
  name: logging-cluster
spec:
  role: storage       
  replicas: 3
  logging:
    fluentd:
      enabled: true  <1>
----

<1> Enables log capture via Fluentd

The yaml above can be installed into Kubernetes using `kubectl`:

[source,bash]
----
kubectl -n <namespace> create -f logging-cluster.yaml

coherencecluster.coherence.oracle.com/logging-cluster created

kubectl -n <namespace> get pod -l coherenceCluster=logging-cluster

NAME                        READY   STATUS    RESTARTS   AGE
logging-cluster-storage-0   2/2     Running   0          86s
logging-cluster-storage-1   2/2     Running   0          86s
logging-cluster-storage-2   2/2     Running   0          86s
----

NOTE: Notice that under the `Ready` column it shows `2/2`. This means that there are two containers for this
Pod, Coherence and Fluentd, and they are both ready.  The Fluentd container will capture the logs, parse them
and send them to Elasticsearch. Kibana can then be used to view the logs.

=== 3. Port-forward the Kibana pod

First find the Kibana `Pod`:
[source,bash]
----
kubectl -n coherence-example get pod -l component=kibana -o name
----

Using the `Pod` name use `kubectl` to create a port forward session to the Kibana `Pod` so that the
Kibana API on port `5601` in the `Pod` can be accessed from the local host.

[source,bash]
----
kubectl -n <namespace> port-forward \
    $(kubectl -n <namespace> get pod -l component=kibana -o name) \
    5601:5601

Forwarding from [::1]:5601 -> 5601
Forwarding from 127.0.0.1:5601 -> 5601
----

=== 4. Access the Kibana Application UI

Access Kibana using the following URL: http://127.0.0.1:5601/

NOTE: It may take approximately 2-3 minutes for the first logs to reach the Elasticsearch instance.

==== Default Dashboards

* Coherence Cluster - All Messages : Shows all messages

* Coherence Cluster - Errors and Warnings : Shows errors and warning messages only

* Coherence Cluster - Persistence : Shows Persistence related messages

* Coherence Cluster - Configuration Messages: Shows configuration related messages

* Coherence Cluster - Network : Shows network related messages, such as communication delays and TCP ring disconnects

* Coherence Cluster - Partitions : Shows partition transfer and loss messages

* Coherence Cluster - Message Sources : Shows the source (thread) for messages

==== Default Queries

There are many searches related to common Coherence messages, warnings,
and errors that are loaded and can be accessed via the `Discover` `side-bar
and selecting `Open`.

See <<logging/040_dashboards.adoc,here>> for more information on the default dashboards and searches.

=== 4. Clean Up
After running the above the Coherence cluster can be removed using `kubectl`:

[source,bash]
----
kubectl -n <namespace> delete -f logging-cluster.yaml
----