///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2020, Oracle and/or its affiliates.
    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.

///////////////////////////////////////////////////////////////////////////////

= Coherence Operator Introduction

== What is the Coherence Operator?
The Coherence Operator is a https://kubernetes.io/docs/concepts/extend-kubernetes/operator/[Kubernetes Operator] that
is used to manage https://oracle.github.io/coherence[Oracle Coherence] clusters in Kubernetes.
The Coherence Operator takes on the tasks of that human DevOps resource might carry out when managing Coherence clusters,
such as configuration, installation, safe scaling, management and metrics.

The Coherence Operator is a Go based application built using the https://github.com/operator-framework/operator-sdk[Operator SDK].
It is distributed as a Docker image and Helm chart for easy installation and configuration.

== Why use the Coherence Kubernetes Operator

Using the Coherence Operator to manage Coherence clusters running in Kubernetes has many advantages over just deploying
and running clusters with the resources provided by Kubernetes.
Coherence can be treated as just another library that your application depends on and uses and hence, a Coherence
application can run in Kubernetes without requiring the Operator, but if the Operator is not being used then there are
a number of things that the DevOps team for an application would need to build or do manually.

=== Cluster Discovery
JVMs that run as Coherence cluster members need to discover the other members of the cluster.
This is discussed in the <<coherence/070_wka.adoc,Coherence Well Known Addressing>> section of the documentation.
When using the Operator the well known addressing configuration for clusters is managed automatically to allow a Coherence
deployment to create its own cluster or to join with other deployments to form larger clusters.

=== Better Fault Tolerant Data Distribution
The Operator configures the Coherence site and rack properties for cluster members based on Kubernetes Node topology
labels. This allows Coherence to better distribute data across sites when a Kubernetes cluster spans availability domains.

=== Safe Scaling
When scaling down a Coherence cluster, care must be taken to ensure that there will be no data loss.
This typically means scaling down by a single Pod at a time and waiting for the cluster to become "safe" before scaling
down the next Pod.
The Operator has built in functionality to do this, so scaling a Coherence cluster is as simple as scaling any other
Kubernetes Deployment or StatefulSet.

=== Autoscaling
Alongside safe scaling, because the Coherence CRD supports the Kubernetes scale sub-resource it is possible to configure
the Kubernetes Horizontal Pod Autoscaler to scale Coherence
clusters based on metrics.

=== Readiness Probes
The Operator has an understanding of when a Coherence JVM is "ready", so it configures a readiness probe that k8s will
use to signal whether a Pod is ready or not.

=== Persistence
Using the Operator makes it simple to configure and use Coherence Persistence, storing data on Kubernetes Persistent
Volumes to allow state to be maintained between cluster restarts.

=== Graceful Shutdown
When a Coherence cluster is deployed with persistence enabled, the Operator will gracefully shutdown a cluster by suspending
services before stopping all the Pods.
This ensures that all persistence files are properly closed and allows for quicker recovery and restart of the cluster.
Without the Operator, if a cluster is shutdown, typically by removing the controlling StatefulSet from Kubernetes then
the Pods will be shutdown but not all at the same time.
It is obviously impossible for k8s to kill all the Pods at the exact same instant in time. As some Pods die the remaining
storage enabled Pods will be trying to recover data for the lost Pods, this can cause a lot of needles work and moving of
data over the network. It is much cleaner to suspend all the services before shutdown.

=== Simpler Configuration
The Coherence CRD is designed to make the more commonly used configuration parameters for Coherence, and the JVM simpler
to configure. The Coherence CRD is simple to use, in fact none of its fields are mandatory, so an application can be
deployed with nothing more than a name, and a container image.

=== Consistency
By using the Operator to manage Coherence clusters all clusters are configured and managed the same way making it easier
for DevOps to manage multiple clusters and applications.

=== Expertise
The Operator has been built and tested by the Coherence engineering team, who understand Coherence and the various scenarios
and edge cases that can occur when managing Coherence clusters at scale in Kubernetes.


== Coherence Clusters
A Coherence cluster is a number of distributed Java Virtual Machines (JVMs) that communicate to form a single coherent cluster.
In Kubernetes, this concept can be related to a number of Pods that form a single cluster. 
In each `Pod` is a JVM running a Coherence `DefaultCacheServer`, or a custom application using Coherence.

The operator uses a Kubernetes Custom Resource Definition (CRD) to represent a group of members in a Coherence cluster.
Typically, a deployment would be used to configure one or more members of a specific role in a cluster.
Every field in the `Coherence` CRD `Spec` is optional, so a simple cluster can be defined in  yaml as:

[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: my-cluster # <1>
----

<1> In this case the `metadata.name` field in the `Coherence` resource yaml will be used as the Coherence cluster name.

The operator will use default values for fields that have not been entered, so the above yaml will create
a Coherence deployment using a `StatefulSet` with a replica count of three, which means that will be three storage
enabled Coherence `Pods`.

See the <<about/04_coherence_spec.adoc,Coherence CRD spec>> page for details of all the fields in the CRD.

In the above example no `spec.image` field has been set, so the Operator will use a publicly available Coherence CE
image as its default. These images are meant for demos, POCs and experimentation, but for a production application you
should build your own image.


== Using Commercial Coherence Versions

NOTE: Whilst the Coherence CE version can be freely deployed anywhere, if your application image uses a commercial
version of Oracle Coherence then you are responsible for making sure your deployment has been properly licensed.

Oracle's current policy is that a license will be required for each Kubernetes Node that images are to be pulled to.
While an image exists on a node it is effectively the same as having installed the software on that node.

One way to ensure that the Pods of a Coherence deployment only get scheduled onto nodes that meet the
license requirement is to configure Pod scheduling, for example a node selector. Node selectors, and other scheduling,
is simple to configure in the `Coherence` CRD, see the <<other/090_pod_scheduling.adoc,scheduling documentation>>

For example, if a commercial Coherence license exists such that a sub-set of nodes in a Kubernetes cluster
have been covered by the license then those nodes could all be given a label, e.g. `coherenceLicense=true`

When creating a `Coherence` deployment specify a node selector to match the label:

[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: my-app:1.0.0         # <1>
  nodeSelector:
    coherenceLicense: 'true'  # <2>
----

<1> The `my-app:1.0.0` image contains a commercial Coherence version.
<2> The `nodeSelector` will ensure Pods only get scheduled to nodes with the `coherenceLicense=true` label.

There are other ways to configure Pod scheduling supported by the Coherence Operator (such as taints and tolerations)
and there are alternative ways to restrict nodes that Pods can be schedule to, for example a namespace in kubernetes
can be restricted to a sub-set of the cluster's nodes. Using a node selector as described above is probably the
simplest approach.

