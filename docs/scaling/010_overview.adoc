///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2020, Oracle and/or its affiliates.
    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.

///////////////////////////////////////////////////////////////////////////////

= Scale Coherence Deployments

== Scale Coherence Deployments

The Coherence Operator provides the ability to safely scale up and down a `Coherence` deployment.
A `Coherence` deployment is backed by a `StatefulSet`, which can easily be scaled using existing Kubernetes features.
The problem with directly scaling down the `StatefulSet` is that Kubernetes will immediately kill the required number
of `Pods`. This is obviously very bad for Coherence as killing multiple storage enabled members would almost certainly
cause data loss.

The Coherence Operator supports scaling by applying the scaling update directly to `Coherence` deployment rather than
to the underlying `StatefulSet`. There are two methods to scale a `Coherence` deployment:

* Update the `replicas` field in the `Coherence` CRD spec.
* Use the `kubectl scale` command 

When either of these methods is used the Operator will detect that a change to the size of the deployment is required
and ensure that the change will be applied safely. The logical steps the Operator will perform are:

1. Detect desired replicas is different to current replicas
2. Check the cluster is StatusHA - i.e. no cache services are endangered. If any service is not StatusHA requeue the
scale request  (go back to step one).
3. If scaling up, add the required number of members.
4. If scaling down, scale down by one member and requeue the request (go back to step one).

What these steps ensure is that the deployment will not be resized unless the cluster is in a safe state.
When scaling down only a single member will be removed at a time, ensuring that the cluster is in a safe state before
removing the next member.

NOTE: The Operator will only apply safe scaling functionality to deployments that are storage enabled.
If a deployment is storage disabled then it can be scaled up or down by the required number of members
in one step as there is no fear of data loss in a storage disabled member.

== Controlling Safe Scaling

The `Coherence` CRD has a number of fields that control the behaviour of scaling.

=== Scaling Policy

The `Coherence` CRD spec has a field `scaling.policy` that can be used to override the default scaling
behaviour. The scaling policy has three possible values:

[cols=2*,options=header]
|===
|Value
|Description

|`ParallelUpSafeDown`
|This is the default scaling policy.
With this policy when scaling up `Pods` are added in parallel (the same as using the `Parallel` `podManagementPolicy`
in a https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#statefulsetspec-v1-apps[StatefulSet]) and
when scaling down `Pods` are removed one at a time (the same as the `OrderedReady` `podManagementPolicy` for a
StatefulSet). When scaling down a check is done to ensure that the members of the cluster have a safe StatusHA value
before a `Pod` is removed (i.e. none of the Coherence cache services have an endangered status).
This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when
adding members, but offers safe, albeit slower,  scaling down as `Pods` are removed one by one.

|`Parallel`
|With this policy when scaling up `Pods` are added in parallel (the same as using the `Parallel` `podManagementPolicy`
in a https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#statefulsetspec-v1-apps[StatefulSet]).
With this policy no StatusHA check is performed either when scaling up or when scaling down.
This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for deployments that are
storage disabled.

|`Safe`
|With this policy when scaling up and down `Pods` are removed one at a time (the same as the `OrderedReady`
`podManagementPolicy` for a StatefulSet). When scaling down a check is done to ensure that the members of the deployment
have a safe StatusHA value before a `Pod` is removed (i.e. none of the Coherence cache services have an endangered status).
This policy is slower to start, scale up and scale down.
|===

Both the `ParallelUpSafeDown` and `Safe` policies will ensure no data loss when scaling a deployment.

The policy can be set as shown below:
[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    policy: Safe # <1>
----
<1> This deployment will scale both up and down with StatusHA checks.

=== Scaling StatusHA Probe

The StatusHA check performed by the Operator uses a http endpoint that the Operator runs on a well-known port in the
Coherence JVM. This endpoint does performs a simple check to verify that none of the partitioned cache services known
about by Coherence have an endangered status. If an application has a different concept of what "safe" means it can
implement a different method to check the status during scaling.

The operator supports different types of safety check probes, these are exactly the same as those supported by
Kubernetes for readiness and liveness probes. The `scaling.probe` section of the `Coherence` CRD allows different
types of probe to be configured.

==== Using a HTTP Get Probe

A HTTP get probe works the same way as a
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-liveness-http-request[Kubernetes liveness http request]

The probe can be configured as follows
[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    probe:
      httpGet:             # <1>
        port: 8080
        path: /statusha
----
<1> This deployment will check the status of the services by performing a http GET on `http://<pod-ip>:8080/statusha`.
If the response is `200` the check will pass, any other response the check is assumed to be false.

==== Using a TCP Probe

A TCP probe works the same way as a
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-tcp-liveness-probe[Kubernetes TCP liveness probe]

The probe can be configured as follows
[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    probe:
      tcpSocket:    # <1>
        port: 7000
----
<1> This deployment will check the status of the services by connecting to the socket on port `7000`.

==== Using an Exec Command Probe

A TCP probe works the same way as a
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-liveness-command[Kubernetes Exec liveness probe]

The probe can be configured as follows
[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    probe:
      exec:
        command:      # <1>
          - /bin/ah
          - safe.sh
----
<1> This deployment will check the status of the services by running the `sh safe.sh` command in the `Pod`.

